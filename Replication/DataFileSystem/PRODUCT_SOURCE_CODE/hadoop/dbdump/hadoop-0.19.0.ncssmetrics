 Nr. NCSS CCN JVDC Function
   1    1   1    1 org.apache.hadoop.conf.Configurable.setConf(Configuration)
   2    1   1    1 org.apache.hadoop.conf.Configurable.getConf()
   3    2   1    1 org.apache.hadoop.conf.Configuration.Configuration()
   4    6   3    1 org.apache.hadoop.conf.Configuration.Configuration(boolean)
   5    9   4    0 org.apache.hadoop.conf.Configuration.Configuration(Configuration)
   6    2   1    1 org.apache.hadoop.conf.Configuration.addResource(String)
   7    2   1    1 org.apache.hadoop.conf.Configuration.addResource(URL)
   8    2   1    1 org.apache.hadoop.conf.Configuration.addResource(Path)
   9    2   1    1 org.apache.hadoop.conf.Configuration.addResource(InputStream)
  10    3   1    1 org.apache.hadoop.conf.Configuration.reloadConfiguration()
  11    3   1    0 org.apache.hadoop.conf.Configuration.addResourceObject(Object)
  12   21  11    0 org.apache.hadoop.conf.Configuration.substituteVars(String)
  13    2   1    1 org.apache.hadoop.conf.Configuration.get(String)
  14    2   1    1 org.apache.hadoop.conf.Configuration.getRaw(String)
  15    3   1    1 org.apache.hadoop.conf.Configuration.set(String,String)
  16    4   2    0 org.apache.hadoop.conf.Configuration.getOverlay()
  17    2   1    1 org.apache.hadoop.conf.Configuration.get(String,String)
  18   10   7    1 org.apache.hadoop.conf.Configuration.getInt(String,int)
  19    2   1    1 org.apache.hadoop.conf.Configuration.setInt(String,int)
  20   10   7    1 org.apache.hadoop.conf.Configuration.getLong(String,long)
  21   13   6    0 org.apache.hadoop.conf.Configuration.getHexDigits(String)
  22    2   1    1 org.apache.hadoop.conf.Configuration.setLong(String,long)
  23    7   5    1 org.apache.hadoop.conf.Configuration.getFloat(String,float)
  24    9   5    1 org.apache.hadoop.conf.Configuration.getBoolean(String,boolean)
  25    2   1    1 org.apache.hadoop.conf.Configuration.setBoolean(String,boolean)
  26    1   1    0 org.apache.hadoop.conf.Configuration.IntegerRanges.IntegerRanges()
  27   16   8    0 org.apache.hadoop.conf.Configuration.IntegerRanges.IntegerRanges(String)
  28    5   3    1 org.apache.hadoop.conf.Configuration.IntegerRanges.convertToInt(String,int)
  29    5   5    1 org.apache.hadoop.conf.Configuration.IntegerRanges.isIncluded(int)
  30   12   3    0 org.apache.hadoop.conf.Configuration.IntegerRanges.toString()
  31    2   1    1 org.apache.hadoop.conf.Configuration.getRange(String,String)
  32    3   1    1 org.apache.hadoop.conf.Configuration.getStringCollection(String)
  33    3   1    1 org.apache.hadoop.conf.Configuration.getStrings(String)
  34    6   3    1 org.apache.hadoop.conf.Configuration.getStrings(String,String)
  35    2   1    1 org.apache.hadoop.conf.Configuration.setStrings(String,String)
  36    2   1    1 org.apache.hadoop.conf.Configuration.getClassByName(String)
  37   10   7    1 org.apache.hadoop.conf.Configuration.getClasses(String,Class)
  38    7   6    1 org.apache.hadoop.conf.Configuration.getClass(String,Class)
  39   11   9    1 org.apache.hadoop.conf.Configuration.getClass(String,Class,Class)
  40    4   3    1 org.apache.hadoop.conf.Configuration.setClass(String,Class,Class)
  41   15   7    1 org.apache.hadoop.conf.Configuration.getLocalPath(String,String)
  42   10   6    1 org.apache.hadoop.conf.Configuration.getFile(String,String)
  43    2   1    1 org.apache.hadoop.conf.Configuration.getResource(String)
  44   10   5    1 org.apache.hadoop.conf.Configuration.getConfResourceAsInputStream(String)
  45   10   5    1 org.apache.hadoop.conf.Configuration.getConfResourceAsReader(String)
  46    7   3    0 org.apache.hadoop.conf.Configuration.getProps()
  47    2   1    1 org.apache.hadoop.conf.Configuration.size()
  48    3   1    1 org.apache.hadoop.conf.Configuration.clear()
  49    6   4    1 org.apache.hadoop.conf.Configuration.iterator()
  50    3   2    0 org.apache.hadoop.conf.Configuration.loadResources(Properties,ArrayList,boolean)
  51   82  39    0 org.apache.hadoop.conf.Configuration.loadResource(Properties,Object,boolean)
  52   30   5    1 org.apache.hadoop.conf.Configuration.writeXml(OutputStream)
  53    2   1    1 org.apache.hadoop.conf.Configuration.getClassLoader()
  54    2   1    1 org.apache.hadoop.conf.Configuration.setClassLoader(ClassLoader)
  55    5   1    0 org.apache.hadoop.conf.Configuration.toString()
  56    6   3    0 org.apache.hadoop.conf.Configuration.toString(ArrayList,StringBuffer)
  57    2   1    1 org.apache.hadoop.conf.Configuration.setQuietMode(boolean)
  58    2   1    1 org.apache.hadoop.conf.Configuration.main(String[])
  59    5   2    0 org.apache.hadoop.conf.Configuration.readFields(DataInput)
  60    6   2    0 org.apache.hadoop.conf.Configuration.write(DataOutput)
  61    2   1    1 org.apache.hadoop.conf.Configured.Configured()
  62    2   1    1 org.apache.hadoop.conf.Configured.Configured(Configuration)
  63    2   1    0 org.apache.hadoop.conf.Configured.setConf(Configuration)
  64    2   1    0 org.apache.hadoop.conf.Configured.getConf()
  65    2   1    1 org.apache.hadoop.filecache.DistributedCache.getLocalCache(URI,Configuration,Path,FileStatus,boolean,long,Path)
  66   17   3    1 org.apache.hadoop.filecache.DistributedCache.getLocalCache(URI,Configuration,Path,FileStatus,boolean,long,Path,boolean)
  67    2   1    1 org.apache.hadoop.filecache.DistributedCache.getLocalCache(URI,Configuration,Path,boolean,long,Path)
  68    8   3    1 org.apache.hadoop.filecache.DistributedCache.releaseCache(URI,Configuration)
  69    9   3    0 org.apache.hadoop.filecache.DistributedCache.deleteCache(Configuration)
  70   12   4    0 org.apache.hadoop.filecache.DistributedCache.makeRelative(URI,Configuration)
  71    2   1    0 org.apache.hadoop.filecache.DistributedCache.cacheFilePath(Path)
  72   55  27    0 org.apache.hadoop.filecache.DistributedCache.localizeCache(Configuration,URI,long,CacheStatus,FileStatus,boolean,Path,boolean)
  73    2   3    0 org.apache.hadoop.filecache.DistributedCache.isTarFile(String)
  74   15   8    0 org.apache.hadoop.filecache.DistributedCache.ifExistsAndFresh(Configuration,FileSystem,URI,long,CacheStatus,FileStatus)
  75    4   1    1 org.apache.hadoop.filecache.DistributedCache.getTimestamp(Configuration,URI)
  76    8   8    1 org.apache.hadoop.filecache.DistributedCache.createAllSymlink(Configuration,File,File)
  77    8   4    0 org.apache.hadoop.filecache.DistributedCache.getFileSysName(URI)
  78    6   3    0 org.apache.hadoop.filecache.DistributedCache.getFileSystem(URI,Configuration)
  79    3   1    1 org.apache.hadoop.filecache.DistributedCache.setCacheArchives(URI[],Configuration)
  80    3   1    1 org.apache.hadoop.filecache.DistributedCache.setCacheFiles(URI[],Configuration)
  81    2   1    1 org.apache.hadoop.filecache.DistributedCache.getCacheArchives(Configuration)
  82    2   1    1 org.apache.hadoop.filecache.DistributedCache.getCacheFiles(Configuration)
  83    2   1    1 org.apache.hadoop.filecache.DistributedCache.getLocalCacheArchives(Configuration)
  84    2   1    1 org.apache.hadoop.filecache.DistributedCache.getLocalCacheFiles(Configuration)
  85    2   1    1 org.apache.hadoop.filecache.DistributedCache.getArchiveTimestamps(Configuration)
  86    2   1    1 org.apache.hadoop.filecache.DistributedCache.getFileTimestamps(Configuration)
  87    2   1    1 org.apache.hadoop.filecache.DistributedCache.setArchiveTimestamps(Configuration,String)
  88    2   1    1 org.apache.hadoop.filecache.DistributedCache.setFileTimestamps(Configuration,String)
  89    2   1    1 org.apache.hadoop.filecache.DistributedCache.setLocalArchives(Configuration,String)
  90    2   1    1 org.apache.hadoop.filecache.DistributedCache.setLocalFiles(Configuration,String)
  91    3   2    1 org.apache.hadoop.filecache.DistributedCache.addCacheArchive(URI,Configuration)
  92    3   2    1 org.apache.hadoop.filecache.DistributedCache.addCacheFile(URI,Configuration)
  93    6   2    1 org.apache.hadoop.filecache.DistributedCache.addFileToClassPath(Path,Configuration)
  94    9   4    1 org.apache.hadoop.filecache.DistributedCache.getFileClassPaths(Configuration)
  95    6   2    1 org.apache.hadoop.filecache.DistributedCache.addArchiveToClassPath(Path,Configuration)
  96    9   4    1 org.apache.hadoop.filecache.DistributedCache.getArchiveClassPaths(Configuration)
  97    2   1    1 org.apache.hadoop.filecache.DistributedCache.createSymlink(Configuration)
  98    5   3    1 org.apache.hadoop.filecache.DistributedCache.getSymlink(Configuration)
  99   28  24    1 org.apache.hadoop.filecache.DistributedCache.checkURIs(URI[],URI[])
 100    6   1    0 org.apache.hadoop.filecache.DistributedCache.CacheStatus.CacheStatus(Path)
 101    8   3    1 org.apache.hadoop.filecache.DistributedCache.purgeCache(Configuration)
 102    1   1    1 org.apache.hadoop.fs.FSInputStream.seek(long)
 103    1   1    1 org.apache.hadoop.fs.FSInputStream.getPos()
 104    1   1    1 org.apache.hadoop.fs.FSInputStream.seekToNewSource(long)
 105    9   1    0 org.apache.hadoop.fs.FSInputStream.read(long,byte[],int,int)
 106    7   4    0 org.apache.hadoop.fs.FSInputStream.readFully(long,byte[],int,int)
 107    2   1    0 org.apache.hadoop.fs.FSInputStream.readFully(long,byte[])
 108    2   1    0 org.apache.hadoop.fs.BlockLocation.WritableFactory$1.newInstance()
 109    2   1    1 org.apache.hadoop.fs.BlockLocation.BlockLocation()
 110   11   3    1 org.apache.hadoop.fs.BlockLocation.BlockLocation(String[],String[],long,long)
 111    5   4    1 org.apache.hadoop.fs.BlockLocation.getHosts()
 112    5   4    1 org.apache.hadoop.fs.BlockLocation.getNames()
 113    2   1    1 org.apache.hadoop.fs.BlockLocation.getOffset()
 114    2   1    1 org.apache.hadoop.fs.BlockLocation.getLength()
 115    2   1    1 org.apache.hadoop.fs.BlockLocation.setOffset(long)
 116    2   1    1 org.apache.hadoop.fs.BlockLocation.setLength(long)
 117    5   2    1 org.apache.hadoop.fs.BlockLocation.setHosts(String[])
 118    5   2    1 org.apache.hadoop.fs.BlockLocation.setNames(String[])
 119   11   3    1 org.apache.hadoop.fs.BlockLocation.write(DataOutput)
 120   14   3    1 org.apache.hadoop.fs.BlockLocation.readFields(DataInput)
 121    9   2    0 org.apache.hadoop.fs.BlockLocation.toString()
 122    2   1    1 org.apache.hadoop.fs.BufferedFSInputStream.BufferedFSInputStream(FSInputStream,int)
 123    2   1    0 org.apache.hadoop.fs.BufferedFSInputStream.getPos()
 124    5   3    0 org.apache.hadoop.fs.BufferedFSInputStream.skip(long)
 125   11   6    0 org.apache.hadoop.fs.BufferedFSInputStream.seek(long)
 126    4   1    0 org.apache.hadoop.fs.BufferedFSInputStream.seekToNewSource(long)
 127    2   1    0 org.apache.hadoop.fs.BufferedFSInputStream.read(long,byte[],int,int)
 128    2   1    0 org.apache.hadoop.fs.BufferedFSInputStream.readFully(long,byte[],int,int)
 129    2   1    0 org.apache.hadoop.fs.BufferedFSInputStream.readFully(long,byte[])
 130    3   1    0 org.apache.hadoop.fs.ChecksumException.ChecksumException(String,long)
 131    2   1    0 org.apache.hadoop.fs.ChecksumException.getPos()
 132    2   1    0 org.apache.hadoop.fs.ChecksumFileSystem.getApproxChkSumLength(long)
 133    2   1    0 org.apache.hadoop.fs.ChecksumFileSystem.ChecksumFileSystem(FileSystem)
 134    4   2    0 org.apache.hadoop.fs.ChecksumFileSystem.setConf(Configuration)
 135    2   1    1 org.apache.hadoop.fs.ChecksumFileSystem.getRawFileSystem()
 136    2   1    1 org.apache.hadoop.fs.ChecksumFileSystem.getChecksumFile(Path)
 137    3   2    1 org.apache.hadoop.fs.ChecksumFileSystem.isChecksumFile(Path)
 138    2   1    1 org.apache.hadoop.fs.ChecksumFileSystem.getChecksumFileLength(Path,long)
 139    2   1    1 org.apache.hadoop.fs.ChecksumFileSystem.getBytesPerSum()
 140    4   1    0 org.apache.hadoop.fs.ChecksumFileSystem.getSumBufferSize(int,int)
 141    2   1    0 org.apache.hadoop.fs.ChecksumFileSystem.ChecksumFSInputChecker.ChecksumFSInputChecker(ChecksumFileSystem,Path)
 142   18   5    0 org.apache.hadoop.fs.ChecksumFileSystem.ChecksumFSInputChecker.ChecksumFSInputChecker(ChecksumFileSystem,Path,int)
 143    2   1    0 org.apache.hadoop.fs.ChecksumFileSystem.ChecksumFSInputChecker.getChecksumFilePos(long)
 144    2   1    0 org.apache.hadoop.fs.ChecksumFileSystem.ChecksumFSInputChecker.getChunkPosition(long)
 145    2   1    0 org.apache.hadoop.fs.ChecksumFileSystem.ChecksumFSInputChecker.available()
 146   13   7    0 org.apache.hadoop.fs.ChecksumFileSystem.ChecksumFSInputChecker.read(long,byte[],int,int)
 147    5   2    0 org.apache.hadoop.fs.ChecksumFileSystem.ChecksumFSInputChecker.close()
 148    5   2    0 org.apache.hadoop.fs.ChecksumFileSystem.ChecksumFSInputChecker.seekToNewSource(long)
 149   16   8    0 org.apache.hadoop.fs.ChecksumFileSystem.ChecksumFSInputChecker.readChunk(long,byte[],int,int,byte[])
 150    4   2    0 org.apache.hadoop.fs.ChecksumFileSystem.ChecksumFSInputChecker.getFileLength()
 151    6   2    1 org.apache.hadoop.fs.ChecksumFileSystem.ChecksumFSInputChecker.skip(long)
 152    4   3    1 org.apache.hadoop.fs.ChecksumFileSystem.ChecksumFSInputChecker.seek(long)
 153    2   1    0 org.apache.hadoop.fs.ChecksumFileSystem.open(Path,int)
 154    2   2    1 org.apache.hadoop.fs.ChecksumFileSystem.append(Path,int,Progressable)
 155    2   1    1 org.apache.hadoop.fs.ChecksumFileSystem.getChecksumLength(long,int)
 156    2   1    0 org.apache.hadoop.fs.ChecksumFileSystem.ChecksumFSOutputSummer.ChecksumFSOutputSummer(ChecksumFileSystem,Path,boolean,short,long,Configuration)
 157    8   1    0 org.apache.hadoop.fs.ChecksumFileSystem.ChecksumFSOutputSummer.ChecksumFSOutputSummer(ChecksumFileSystem,Path,boolean,int,short,long,Progressable)
 158    4   1    0 org.apache.hadoop.fs.ChecksumFileSystem.ChecksumFSOutputSummer.close()
 159    3   1    0 org.apache.hadoop.fs.ChecksumFileSystem.ChecksumFSOutputSummer.writeChunk(byte[],int,int,byte[])
 160    8   5    0 org.apache.hadoop.fs.ChecksumFileSystem.create(Path,FsPermission,boolean,int,short,long,Progressable)
 161    8   4    1 org.apache.hadoop.fs.ChecksumFileSystem.setReplication(Path,short)
 162   14   7    1 org.apache.hadoop.fs.ChecksumFileSystem.rename(Path,Path)
 163   12   6    1 org.apache.hadoop.fs.ChecksumFileSystem.delete(Path,boolean)
 164    2   1    0 org.apache.hadoop.fs.ChecksumFileSystem.PathFilter$1.accept(Path)
 165    2   1    0 org.apache.hadoop.fs.ChecksumFileSystem.listStatus(Path)
 166    2   1    0 org.apache.hadoop.fs.ChecksumFileSystem.mkdirs(Path)
 167    3   1    0 org.apache.hadoop.fs.ChecksumFileSystem.copyFromLocalFile(boolean,Path,Path)
 168    3   1    0 org.apache.hadoop.fs.ChecksumFileSystem.copyToLocalFile(boolean,Path,Path)
 169   18   8    1 org.apache.hadoop.fs.ChecksumFileSystem.copyToLocalFile(Path,Path,boolean)
 170    2   1    0 org.apache.hadoop.fs.ChecksumFileSystem.startLocalOutput(Path,Path)
 171    2   1    0 org.apache.hadoop.fs.ChecksumFileSystem.completeLocalOutput(Path,Path)
 172    2   1    1 org.apache.hadoop.fs.ChecksumFileSystem.reportChecksumFailure(Path,FSDataInputStream,long,FSDataInputStream,long)
 173    1   1    1 org.apache.hadoop.fs.ContentSummary.ContentSummary()
 174    2   1    1 org.apache.hadoop.fs.ContentSummary.ContentSummary(long,long,long)
 175    7   1    1 org.apache.hadoop.fs.ContentSummary.ContentSummary(long,long,long,long,long,long)
 176    2   1    1 org.apache.hadoop.fs.ContentSummary.getLength()
 177    2   1    1 org.apache.hadoop.fs.ContentSummary.getDirectoryCount()
 178    2   1    1 org.apache.hadoop.fs.ContentSummary.getFileCount()
 179    2   1    1 org.apache.hadoop.fs.ContentSummary.getQuota()
 180    2   1    1 org.apache.hadoop.fs.ContentSummary.getSpaceConsumed()
 181    2   1    1 org.apache.hadoop.fs.ContentSummary.getSpaceQuota()
 182    7   1    1 org.apache.hadoop.fs.ContentSummary.write(DataOutput)
 183    7   1    1 org.apache.hadoop.fs.ContentSummary.readFields(DataInput)
 184    2   2    1 org.apache.hadoop.fs.ContentSummary.getHeader(boolean)
 185    2   1    1 org.apache.hadoop.fs.ContentSummary.toString()
 186   15   4    1 org.apache.hadoop.fs.ContentSummary.toString(boolean)
 187    2   1    0 org.apache.hadoop.fs.DF.DF(File,Configuration)
 188    3   1    0 org.apache.hadoop.fs.DF.DF(File,long)
 189    2   1    0 org.apache.hadoop.fs.DF.getDirPath()
 190    3   1    0 org.apache.hadoop.fs.DF.getFilesystem()
 191    3   1    0 org.apache.hadoop.fs.DF.getCapacity()
 192    3   1    0 org.apache.hadoop.fs.DF.getUsed()
 193    3   1    0 org.apache.hadoop.fs.DF.getAvailable()
 194    3   1    0 org.apache.hadoop.fs.DF.getPercentUsed()
 195    3   1    0 org.apache.hadoop.fs.DF.getMount()
 196    2   1    0 org.apache.hadoop.fs.DF.toString()
 197    2   1    0 org.apache.hadoop.fs.DF.getExecString()
 198   17   6    0 org.apache.hadoop.fs.DF.parseExecResult(BufferedReader)
 199    5   2    0 org.apache.hadoop.fs.DF.main(String[])
 200    5   1    1 org.apache.hadoop.fs.DU.DU(File,long)
 201    2   1    1 org.apache.hadoop.fs.DU.DU(File,Configuration)
 202    9   4    0 org.apache.hadoop.fs.DU.DURefreshThread.run()
 203    2   1    1 org.apache.hadoop.fs.DU.decDfsUsed(long)
 204    2   1    1 org.apache.hadoop.fs.DU.incDfsUsed(long)
 205   10   4    1 org.apache.hadoop.fs.DU.getUsed()
 206    2   1    1 org.apache.hadoop.fs.DU.getDirPath()
 207    5   2    1 org.apache.hadoop.fs.DU.start()
 208    4   2    1 org.apache.hadoop.fs.DU.shutdown()
 209    2   1    0 org.apache.hadoop.fs.DU.toString()
 210    2   1    0 org.apache.hadoop.fs.DU.getExecString()
 211    8   5    0 org.apache.hadoop.fs.DU.parseExecResult(BufferedReader)
 212    5   2    0 org.apache.hadoop.fs.DU.main(String[])
 213    1   1    1 org.apache.hadoop.fs.FileChecksum.getAlgorithmName()
 214    1   1    1 org.apache.hadoop.fs.FileChecksum.getLength()
 215    1   1    1 org.apache.hadoop.fs.FileChecksum.getBytes()
 216    7   7    1 org.apache.hadoop.fs.FileChecksum.equals(Object)
 217    2   1    1 org.apache.hadoop.fs.FileChecksum.hashCode()
 218    2   1    0 org.apache.hadoop.fs.FileStatus.FileStatus()
 219    2   1    0 org.apache.hadoop.fs.FileStatus.FileStatus(long,boolean,int,long,long,Path)
 220   11   4    0 org.apache.hadoop.fs.FileStatus.FileStatus(long,boolean,int,long,long,long,FsPermission,String,String,Path)
 221    2   1    0 org.apache.hadoop.fs.FileStatus.getLen()
 222    2   1    1 org.apache.hadoop.fs.FileStatus.isDir()
 223    2   1    1 org.apache.hadoop.fs.FileStatus.getBlockSize()
 224    2   1    1 org.apache.hadoop.fs.FileStatus.getReplication()
 225    2   1    1 org.apache.hadoop.fs.FileStatus.getModificationTime()
 226    2   1    1 org.apache.hadoop.fs.FileStatus.getAccessTime()
 227    2   1    1 org.apache.hadoop.fs.FileStatus.getPermission()
 228    2   1    1 org.apache.hadoop.fs.FileStatus.getOwner()
 229    2   1    1 org.apache.hadoop.fs.FileStatus.getGroup()
 230    2   1    0 org.apache.hadoop.fs.FileStatus.getPath()
 231    2   2    1 org.apache.hadoop.fs.FileStatus.setPermission(FsPermission)
 232    2   2    1 org.apache.hadoop.fs.FileStatus.setOwner(String)
 233    2   2    1 org.apache.hadoop.fs.FileStatus.setGroup(String)
 234   11   1    0 org.apache.hadoop.fs.FileStatus.write(DataOutput)
 235   12   1    0 org.apache.hadoop.fs.FileStatus.readFields(DataInput)
 236    3   1    1 org.apache.hadoop.fs.FileStatus.compareTo(Object)
 237    9   7    1 org.apache.hadoop.fs.FileStatus.equals(Object)
 238    2   1    1 org.apache.hadoop.fs.FileStatus.hashCode()
 239   19   4    0 org.apache.hadoop.fs.FileSystem.parseArgs(String[],int,Configuration)
 240    2   1    1 org.apache.hadoop.fs.FileSystem.get(Configuration)
 241    2   1    1 org.apache.hadoop.fs.FileSystem.getDefaultUri(Configuration)
 242    2   1    1 org.apache.hadoop.fs.FileSystem.setDefaultUri(Configuration,URI)
 243    2   1    1 org.apache.hadoop.fs.FileSystem.setDefaultUri(Configuration,String)
 244    1   1    1 org.apache.hadoop.fs.FileSystem.initialize(URI,Configuration)
 245    1   1    1 org.apache.hadoop.fs.FileSystem.getUri()
 246    2   1    1 org.apache.hadoop.fs.FileSystem.getName()
 247    2   1    1 org.apache.hadoop.fs.FileSystem.getNamed(String,Configuration)
 248    9   3    1 org.apache.hadoop.fs.FileSystem.fixName(String)
 249    2   1    1 org.apache.hadoop.fs.FileSystem.getLocal(Configuration)
 250   10   7    1 org.apache.hadoop.fs.FileSystem.get(URI,Configuration)
 251    4   2    0 org.apache.hadoop.fs.FileSystem.ClientFinalizer.run()
 252    2   1    1 org.apache.hadoop.fs.FileSystem.closeAll()
 253    3   1    1 org.apache.hadoop.fs.FileSystem.makeQualified(Path)
 254    4   1    1 org.apache.hadoop.fs.FileSystem.create(FileSystem,Path,FsPermission)
 255    4   1    1 org.apache.hadoop.fs.FileSystem.mkdirs(FileSystem,Path,FsPermission)
 256    3   1    0 org.apache.hadoop.fs.FileSystem.FileSystem()
 257   16  14    1 org.apache.hadoop.fs.FileSystem.checkPath(Path)
 258    6   3    1 org.apache.hadoop.fs.FileSystem.getFileBlockLocations(FileStatus,long,long)
 259    1   1    1 org.apache.hadoop.fs.FileSystem.open(Path,int)
 260    2   1    1 org.apache.hadoop.fs.FileSystem.open(Path)
 261    2   1    1 org.apache.hadoop.fs.FileSystem.create(Path)
 262    2   1    1 org.apache.hadoop.fs.FileSystem.create(Path,boolean)
 263    2   1    1 org.apache.hadoop.fs.FileSystem.create(Path,Progressable)
 264    2   1    1 org.apache.hadoop.fs.FileSystem.create(Path,short)
 265    2   1    1 org.apache.hadoop.fs.FileSystem.create(Path,short,Progressable)
 266    2   1    1 org.apache.hadoop.fs.FileSystem.create(Path,boolean,int)
 267    2   1    1 org.apache.hadoop.fs.FileSystem.create(Path,boolean,int,Progressable)
 268    2   1    1 org.apache.hadoop.fs.FileSystem.create(Path,boolean,int,short,long)
 269    2   1    1 org.apache.hadoop.fs.FileSystem.create(Path,boolean,int,short,long,Progressable)
 270    1   1    1 org.apache.hadoop.fs.FileSystem.create(Path,FsPermission,boolean,int,short,long,Progressable)
 271    6   3    1 org.apache.hadoop.fs.FileSystem.createNewFile(Path)
 272    2   1    1 org.apache.hadoop.fs.FileSystem.append(Path)
 273    2   1    1 org.apache.hadoop.fs.FileSystem.append(Path,int)
 274    1   1    1 org.apache.hadoop.fs.FileSystem.append(Path,int,Progressable)
 275    2   1    0 org.apache.hadoop.fs.FileSystem.getReplication(Path)
 276    2   1    1 org.apache.hadoop.fs.FileSystem.setReplication(Path,short)
 277    1   1    1 org.apache.hadoop.fs.FileSystem.rename(Path,Path)
 278    1   1    0 org.apache.hadoop.fs.FileSystem.delete(Path)
 279    1   1    1 org.apache.hadoop.fs.FileSystem.delete(Path,boolean)
 280    6   3    1 org.apache.hadoop.fs.FileSystem.deleteOnExit(Path)
 281    8   3    1 org.apache.hadoop.fs.FileSystem.processDeleteOnExit()
 282    4   3    1 org.apache.hadoop.fs.FileSystem.exists(Path)
 283    4   3    0 org.apache.hadoop.fs.FileSystem.isDirectory(Path)
 284    4   3    1 org.apache.hadoop.fs.FileSystem.isFile(Path)
 285    2   1    0 org.apache.hadoop.fs.FileSystem.getLength(Path)
 286   11   4    1 org.apache.hadoop.fs.FileSystem.getContentSummary(Path)
 287    2   1    0 org.apache.hadoop.fs.FileSystem.PathFilter$1.accept(Path)
 288    1   1    1 org.apache.hadoop.fs.FileSystem.listStatus(Path)
 289    6   4    0 org.apache.hadoop.fs.FileSystem.listStatus(ArrayList,Path,PathFilter)
 290    4   1    1 org.apache.hadoop.fs.FileSystem.listStatus(Path,PathFilter)
 291    2   1    1 org.apache.hadoop.fs.FileSystem.listStatus(Path[])
 292    5   2    1 org.apache.hadoop.fs.FileSystem.listStatus(Path[],PathFilter)
 293    2   1    1 org.apache.hadoop.fs.FileSystem.globStatus(Path)
 294   12   5    1 org.apache.hadoop.fs.FileSystem.globStatus(Path,PathFilter)
 295   36  13    0 org.apache.hadoop.fs.FileSystem.globStatusInternal(Path,PathFilter)
 296   13   8    0 org.apache.hadoop.fs.FileSystem.globPathsLevel(Path[],String[],int,PathFilter,boolean[])
 297    1   1    0 org.apache.hadoop.fs.FileSystem.GlobFilter.GlobFilter()
 298    2   1    0 org.apache.hadoop.fs.FileSystem.GlobFilter.GlobFilter(String)
 299    3   1    0 org.apache.hadoop.fs.FileSystem.GlobFilter.GlobFilter(String,PathFilter)
 300    2   6    0 org.apache.hadoop.fs.FileSystem.GlobFilter.isJavaRegexSpecialChar(char)
 301   72  29    0 org.apache.hadoop.fs.FileSystem.GlobFilter.setRegex(String)
 302    2   1    0 org.apache.hadoop.fs.FileSystem.GlobFilter.hasPattern()
 303    2   2    0 org.apache.hadoop.fs.FileSystem.GlobFilter.accept(Path)
 304    2   2    0 org.apache.hadoop.fs.FileSystem.GlobFilter.error(String,String,int)
 305    2   1    1 org.apache.hadoop.fs.FileSystem.getHomeDirectory()
 306    1   1    1 org.apache.hadoop.fs.FileSystem.setWorkingDirectory(Path)
 307    1   1    1 org.apache.hadoop.fs.FileSystem.getWorkingDirectory()
 308    2   1    1 org.apache.hadoop.fs.FileSystem.mkdirs(Path)
 309    1   1    1 org.apache.hadoop.fs.FileSystem.mkdirs(Path,FsPermission)
 310    2   1    1 org.apache.hadoop.fs.FileSystem.copyFromLocalFile(Path,Path)
 311    2   1    1 org.apache.hadoop.fs.FileSystem.moveFromLocalFile(Path[],Path)
 312    2   1    1 org.apache.hadoop.fs.FileSystem.moveFromLocalFile(Path,Path)
 313    2   1    1 org.apache.hadoop.fs.FileSystem.copyFromLocalFile(boolean,Path,Path)
 314    3   1    1 org.apache.hadoop.fs.FileSystem.copyFromLocalFile(boolean,boolean,Path[],Path)
 315    3   1    1 org.apache.hadoop.fs.FileSystem.copyFromLocalFile(boolean,boolean,Path,Path)
 316    2   1    1 org.apache.hadoop.fs.FileSystem.copyToLocalFile(Path,Path)
 317    2   1    1 org.apache.hadoop.fs.FileSystem.moveToLocalFile(Path,Path)
 318    2   1    1 org.apache.hadoop.fs.FileSystem.copyToLocalFile(boolean,Path,Path)
 319    2   1    1 org.apache.hadoop.fs.FileSystem.startLocalOutput(Path,Path)
 320    2   1    1 org.apache.hadoop.fs.FileSystem.completeLocalOutput(Path,Path)
 321    3   1    1 org.apache.hadoop.fs.FileSystem.close()
 322    6   2    1 org.apache.hadoop.fs.FileSystem.getUsed()
 323    2   1    0 org.apache.hadoop.fs.FileSystem.getBlockSize(Path)
 324    2   1    1 org.apache.hadoop.fs.FileSystem.getDefaultBlockSize()
 325    2   1    1 org.apache.hadoop.fs.FileSystem.getDefaultReplication()
 326    1   1    1 org.apache.hadoop.fs.FileSystem.getFileStatus(Path)
 327    2   1    1 org.apache.hadoop.fs.FileSystem.getFileChecksum(Path)
 328    8   5    1 org.apache.hadoop.fs.FileSystem.getFileStatus(Path[])
 329    1   1    1 org.apache.hadoop.fs.FileSystem.setPermission(Path,FsPermission)
 330    1   1    1 org.apache.hadoop.fs.FileSystem.setOwner(Path,String,String)
 331    1   1    1 org.apache.hadoop.fs.FileSystem.setTimes(Path,long,long)
 332    7   3    0 org.apache.hadoop.fs.FileSystem.createFileSystem(URI,Configuration)
 333   10   4    0 org.apache.hadoop.fs.FileSystem.Cache.get(URI,Configuration)
 334    6   6    0 org.apache.hadoop.fs.FileSystem.Cache.remove(Key,FileSystem)
 335   13   6    0 org.apache.hadoop.fs.FileSystem.Cache.closeAll()
 336    9   6    0 org.apache.hadoop.fs.FileSystem.Cache.Key.Key(URI,Configuration)
 337    2   1    1 org.apache.hadoop.fs.FileSystem.Cache.Key.hashCode()
 338    2   3    0 org.apache.hadoop.fs.FileSystem.Cache.Key.isEqual(Object,Object)
 339    7   8    1 org.apache.hadoop.fs.FileSystem.Cache.Key.equals(Object)
 340    2   1    1 org.apache.hadoop.fs.FileSystem.Cache.Key.toString()
 341    2   1    1 org.apache.hadoop.fs.FileSystem.Statistics.incrementBytesRead(long)
 342    2   1    1 org.apache.hadoop.fs.FileSystem.Statistics.incrementBytesWritten(long)
 343    2   1    1 org.apache.hadoop.fs.FileSystem.Statistics.getBytesRead()
 344    2   1    1 org.apache.hadoop.fs.FileSystem.Statistics.getBytesWritten()
 345    2   1    0 org.apache.hadoop.fs.FileSystem.Statistics.toString()
 346    6   2    1 org.apache.hadoop.fs.FileSystem.getStatistics(Class)
 347    3   2    0 org.apache.hadoop.fs.FileSystem.printStatistics()
 348    7   4    1 org.apache.hadoop.fs.FileUtil.stat2Paths(FileStatus[])
 349    5   3    1 org.apache.hadoop.fs.FileUtil.stat2Paths(FileStatus[],Path)
 350   15   9    1 org.apache.hadoop.fs.FileUtil.fullyDelete(File)
 351    2   1    0 org.apache.hadoop.fs.FileUtil.fullyDelete(FileSystem,Path)
 352    9   6    0 org.apache.hadoop.fs.FileUtil.checkDependencies(FileSystem,Path,FileSystem,Path)
 353    2   1    1 org.apache.hadoop.fs.FileUtil.copy(FileSystem,Path,FileSystem,Path,boolean,Configuration)
 354   22  12    0 org.apache.hadoop.fs.FileUtil.copy(FileSystem,Path[],FileSystem,Path,boolean,boolean,Configuration)
 355   26  11    1 org.apache.hadoop.fs.FileUtil.copy(FileSystem,Path,FileSystem,Path,boolean,boolean,Configuration)
 356   20   8    1 org.apache.hadoop.fs.FileUtil.copyMerge(FileSystem,Path,FileSystem,Path,boolean,Configuration,String)
 357   25  11    1 org.apache.hadoop.fs.FileUtil.copy(File,FileSystem,Path,boolean,Configuration)
 358   17   9    1 org.apache.hadoop.fs.FileUtil.copy(FileSystem,Path,File,boolean,Configuration)
 359   11   8    0 org.apache.hadoop.fs.FileUtil.checkDest(String,FileSystem,Path,boolean)
 360    3   1    0 org.apache.hadoop.fs.FileUtil.CygPathCommand.CygPathCommand(String)
 361    2   1    0 org.apache.hadoop.fs.FileUtil.CygPathCommand.getResult()
 362    2   1    0 org.apache.hadoop.fs.FileUtil.CygPathCommand.getExecString()
 363    5   3    0 org.apache.hadoop.fs.FileUtil.CygPathCommand.parseExecResult(BufferedReader)
 364    5   3    1 org.apache.hadoop.fs.FileUtil.makeShellPath(String)
 365    2   1    1 org.apache.hadoop.fs.FileUtil.makeShellPath(File)
 366    5   3    1 org.apache.hadoop.fs.FileUtil.makeShellPath(File,boolean)
 367   12   6    1 org.apache.hadoop.fs.FileUtil.getDU(File)
 368   23   7    1 org.apache.hadoop.fs.FileUtil.unZip(File,File)
 369   24   8    1 org.apache.hadoop.fs.FileUtil.unTar(File,File)
 370   12  10    0 org.apache.hadoop.fs.FileUtil.HardLink.getOSType()
 371   21   8    1 org.apache.hadoop.fs.FileUtil.HardLink.createHardLink(File,File)
 372   38  15    1 org.apache.hadoop.fs.FileUtil.HardLink.getLinkCount(File)
 373    7   2    1 org.apache.hadoop.fs.FileUtil.symLink(String,String)
 374    4   1    1 org.apache.hadoop.fs.FileUtil.chmod(String,String)
 375    5   2    1 org.apache.hadoop.fs.FileUtil.createLocalTempFile(File,String,boolean)
 376    9   9    0 org.apache.hadoop.fs.FileUtil.replaceFile(File,File)
 377    1   1    0 org.apache.hadoop.fs.FilterFileSystem.FilterFileSystem()
 378    2   1    0 org.apache.hadoop.fs.FilterFileSystem.FilterFileSystem(FileSystem)
 379    2   1    1 org.apache.hadoop.fs.FilterFileSystem.initialize(URI,Configuration)
 380    2   1    1 org.apache.hadoop.fs.FilterFileSystem.getUri()
 381    2   1    1 org.apache.hadoop.fs.FilterFileSystem.getName()
 382    2   1    1 org.apache.hadoop.fs.FilterFileSystem.makeQualified(Path)
 383    2   1    1 org.apache.hadoop.fs.FilterFileSystem.checkPath(Path)
 384    2   1    0 org.apache.hadoop.fs.FilterFileSystem.getFileBlockLocations(FileStatus,long,long)
 385    2   1    1 org.apache.hadoop.fs.FilterFileSystem.open(Path,int)
 386    2   1    1 org.apache.hadoop.fs.FilterFileSystem.append(Path,int,Progressable)
 387    2   1    0 org.apache.hadoop.fs.FilterFileSystem.create(Path,FsPermission,boolean,int,short,long,Progressable)
 388    2   1    1 org.apache.hadoop.fs.FilterFileSystem.setReplication(Path,short)
 389    2   1    1 org.apache.hadoop.fs.FilterFileSystem.rename(Path,Path)
 390    2   1    0 org.apache.hadoop.fs.FilterFileSystem.delete(Path)
 391    2   1    1 org.apache.hadoop.fs.FilterFileSystem.delete(Path,boolean)
 392    2   1    1 org.apache.hadoop.fs.FilterFileSystem.listStatus(Path)
 393    2   1    0 org.apache.hadoop.fs.FilterFileSystem.getHomeDirectory()
 394    2   1    1 org.apache.hadoop.fs.FilterFileSystem.setWorkingDirectory(Path)
 395    2   1    1 org.apache.hadoop.fs.FilterFileSystem.getWorkingDirectory()
 396    2   1    0 org.apache.hadoop.fs.FilterFileSystem.mkdirs(Path,FsPermission)
 397    2   1    1 org.apache.hadoop.fs.FilterFileSystem.copyFromLocalFile(boolean,Path,Path)
 398    2   1    1 org.apache.hadoop.fs.FilterFileSystem.copyToLocalFile(boolean,Path,Path)
 399    2   1    1 org.apache.hadoop.fs.FilterFileSystem.startLocalOutput(Path,Path)
 400    2   1    1 org.apache.hadoop.fs.FilterFileSystem.completeLocalOutput(Path,Path)
 401    2   1    1 org.apache.hadoop.fs.FilterFileSystem.getDefaultBlockSize()
 402    2   1    1 org.apache.hadoop.fs.FilterFileSystem.getDefaultReplication()
 403    2   1    1 org.apache.hadoop.fs.FilterFileSystem.getFileStatus(Path)
 404    2   1    1 org.apache.hadoop.fs.FilterFileSystem.getFileChecksum(Path)
 405    2   1    0 org.apache.hadoop.fs.FilterFileSystem.getConf()
 406    3   1    0 org.apache.hadoop.fs.FilterFileSystem.close()
 407    2   1    0 org.apache.hadoop.fs.FilterFileSystem.setOwner(Path,String,String)
 408    2   1    0 org.apache.hadoop.fs.FilterFileSystem.setPermission(Path,FsPermission)
 409    4   4    0 org.apache.hadoop.fs.FSDataInputStream.FSDataInputStream(InputStream)
 410    2   1    0 org.apache.hadoop.fs.FSDataInputStream.seek(long)
 411    2   1    0 org.apache.hadoop.fs.FSDataInputStream.getPos()
 412    2   1    0 org.apache.hadoop.fs.FSDataInputStream.read(long,byte[],int,int)
 413    2   1    0 org.apache.hadoop.fs.FSDataInputStream.readFully(long,byte[],int,int)
 414    2   1    0 org.apache.hadoop.fs.FSDataInputStream.readFully(long,byte[])
 415    2   1    0 org.apache.hadoop.fs.FSDataInputStream.seekToNewSource(long)
 416    3   1    0 org.apache.hadoop.fs.FSDataOutputStream.PositionCache.PositionCache(OutputStream,FileSystem.Statistics)
 417    5   2    0 org.apache.hadoop.fs.FSDataOutputStream.PositionCache.write(int)
 418    5   2    0 org.apache.hadoop.fs.FSDataOutputStream.PositionCache.write(byte[],int,int)
 419    2   1    0 org.apache.hadoop.fs.FSDataOutputStream.PositionCache.getPos()
 420    2   1    0 org.apache.hadoop.fs.FSDataOutputStream.PositionCache.close()
 421    2   1    0 org.apache.hadoop.fs.FSDataOutputStream.FSDataOutputStream(OutputStream)
 422    3   1    0 org.apache.hadoop.fs.FSDataOutputStream.FSDataOutputStream(OutputStream,FileSystem.Statistics)
 423    2   1    0 org.apache.hadoop.fs.FSDataOutputStream.getPos()
 424    2   1    0 org.apache.hadoop.fs.FSDataOutputStream.close()
 425    2   1    0 org.apache.hadoop.fs.FSDataOutputStream.getWrappedStream()
 426    3   2    1 org.apache.hadoop.fs.FSDataOutputStream.sync()
 427    2   1    0 org.apache.hadoop.fs.FSError.FSError(Throwable)
 428    3   1    1 org.apache.hadoop.fs.FSInputChecker.FSInputChecker(Path,int)
 429    4   1    1 org.apache.hadoop.fs.FSInputChecker.FSInputChecker(Path,int,boolean,Checksum,int,int)
 430    1   1    1 org.apache.hadoop.fs.FSInputChecker.readChunk(long,byte[],int,int,byte[])
 431    1   1    1 org.apache.hadoop.fs.FSInputChecker.getChunkPosition(long)
 432    2   2    1 org.apache.hadoop.fs.FSInputChecker.needChecksum()
 433    6   4    1 org.apache.hadoop.fs.FSInputChecker.read()
 434   14  10    1 org.apache.hadoop.fs.FSInputChecker.read(byte[],int,int)
 435    3   1    1 org.apache.hadoop.fs.FSInputChecker.fill()
 436   16   7    0 org.apache.hadoop.fs.FSInputChecker.read1(byte[],int,int)
 437   23   9    0 org.apache.hadoop.fs.FSInputChecker.readChecksumChunk(byte[],int,int)
 438    6   3    0 org.apache.hadoop.fs.FSInputChecker.verifySum(long)
 439    2   1    0 org.apache.hadoop.fs.FSInputChecker.getChecksum()
 440    5   2    1 org.apache.hadoop.fs.FSInputChecker.checksum2long(byte[])
 441    2   1    0 org.apache.hadoop.fs.FSInputChecker.getPos()
 442    2   1    0 org.apache.hadoop.fs.FSInputChecker.available()
 443    5   3    1 org.apache.hadoop.fs.FSInputChecker.skip(long)
 444   12   7    1 org.apache.hadoop.fs.FSInputChecker.seek(long)
 445    9   6    1 org.apache.hadoop.fs.FSInputChecker.readFully(InputStream,byte[],int,int)
 446    6   1    1 org.apache.hadoop.fs.FSInputChecker.set(Checksum,int,int)
 447    2   1    0 org.apache.hadoop.fs.FSInputChecker.markSupported()
 448    1   1    0 org.apache.hadoop.fs.FSInputChecker.mark(int)
 449    2   2    0 org.apache.hadoop.fs.FSInputChecker.reset()
 450    5   2    0 org.apache.hadoop.fs.FSInputChecker.resetState()
 451    5   1    0 org.apache.hadoop.fs.FSOutputSummer.FSOutputSummer(Checksum,int,int)
 452    1   1    0 org.apache.hadoop.fs.FSOutputSummer.writeChunk(byte[],int,int,byte[])
 453    5   2    1 org.apache.hadoop.fs.FSOutputSummer.write(int)
 454    4   6    1 org.apache.hadoop.fs.FSOutputSummer.write(byte[],int,int)
 455   14   6    1 org.apache.hadoop.fs.FSOutputSummer.write1(byte[],int,int)
 456    2   1    0 org.apache.hadoop.fs.FSOutputSummer.flushBuffer()
 457    7   3    0 org.apache.hadoop.fs.FSOutputSummer.flushBuffer(boolean)
 458    6   2    1 org.apache.hadoop.fs.FSOutputSummer.writeChecksumChunk(byte[],int,int,boolean)
 459    2   1    1 org.apache.hadoop.fs.FSOutputSummer.convertToByteStream(Checksum,int)
 460    6   1    0 org.apache.hadoop.fs.FSOutputSummer.int2byte(int,byte[])
 461    4   1    1 org.apache.hadoop.fs.FSOutputSummer.resetChecksumChunk(int)
 462    2   1    1 org.apache.hadoop.fs.FsShell.FsShell()
 463    4   1    0 org.apache.hadoop.fs.FsShell.FsShell(Configuration)
 464    6   3    0 org.apache.hadoop.fs.FsShell.init()
 465    9   5    1 org.apache.hadoop.fs.FsShell.copyFromStdin(Path,FileSystem)
 466    4   1    1 org.apache.hadoop.fs.FsShell.printToStdout(InputStream)
 467    7   3    1 org.apache.hadoop.fs.FsShell.copyFromLocal(Path[],String)
 468    4   1    1 org.apache.hadoop.fs.FsShell.moveFromLocal(Path[],String)
 469    2   1    1 org.apache.hadoop.fs.FsShell.moveFromLocal(Path,String)
 470   28  10    1 org.apache.hadoop.fs.FsShell.copyToLocal(String[],int)
 471    5   2    1 org.apache.hadoop.fs.FsShell.getSrcFileSystem(Path,boolean)
 472   17  10    1 org.apache.hadoop.fs.FsShell.copyToLocal(FileSystem,Path,File,boolean)
 473    2   1    1 org.apache.hadoop.fs.FsShell.copyMergeToLocal(String,Path)
 474    9   3    1 org.apache.hadoop.fs.FsShell.copyMergeToLocal(String,Path,boolean)
 475    2   1    1 org.apache.hadoop.fs.FsShell.moveToLocal(String,Path)
 476    4   3    0 org.apache.hadoop.fs.FsShell.DelayedExceptionThrowing$1.process(Path,FileSystem)
 477    7   1    1 org.apache.hadoop.fs.FsShell.cat(String,boolean)
 478    6   1    0 org.apache.hadoop.fs.FsShell.TextRecordInputStream.TextRecordInputStream(FileStatus)
 479   15   5    0 org.apache.hadoop.fs.FsShell.TextRecordInputStream.read()
 480   13   6    0 org.apache.hadoop.fs.FsShell.forMagic(Path,FileSystem)
 481    4   3    0 org.apache.hadoop.fs.FsShell.DelayedExceptionThrowing$2.process(Path,FileSystem)
 482    7   1    0 org.apache.hadoop.fs.FsShell.text(String)
 483   20   9    1 org.apache.hadoop.fs.FsShell.setReplication(String[],int)
 484   21   9    1 org.apache.hadoop.fs.FsShell.waitForReplication(List,int)
 485    6   2    1 org.apache.hadoop.fs.FsShell.setReplication(short,String,boolean,List)
 486   14   8    0 org.apache.hadoop.fs.FsShell.setReplication(short,FileSystem,Path,boolean,List)
 487    7   3    1 org.apache.hadoop.fs.FsShell.setFileReplication(Path,FileSystem,short,List)
 488   11   7    1 org.apache.hadoop.fs.FsShell.ls(String,boolean)
 489   41  19    0 org.apache.hadoop.fs.FsShell.ls(Path,FileSystem,boolean,boolean)
 490   17   8    1 org.apache.hadoop.fs.FsShell.du(String)
 491   10   6    1 org.apache.hadoop.fs.FsShell.dus(String)
 492   12   7    1 org.apache.hadoop.fs.FsShell.mkdir(String)
 493   13   6    1 org.apache.hadoop.fs.FsShell.touchz(String)
 494   15  14    1 org.apache.hadoop.fs.FsShell.test(String[],int)
 495   40  15    1 org.apache.hadoop.fs.FsShell.stat(char[],String)
 496   26  17    1 org.apache.hadoop.fs.FsShell.rename(String,String)
 497   23   8    1 org.apache.hadoop.fs.FsShell.rename(String[],Configuration)
 498   10   5    1 org.apache.hadoop.fs.FsShell.copy(String,String,Configuration)
 499   22   8    1 org.apache.hadoop.fs.FsShell.copy(String[],Configuration)
 500    2   1    0 org.apache.hadoop.fs.FsShell.DelayedExceptionThrowing$3.process(Path,FileSystem)
 501    5   1    1 org.apache.hadoop.fs.FsShell.delete(String,boolean)
 502   13  10    0 org.apache.hadoop.fs.FsShell.delete(Path,FileSystem,boolean)
 503    3   1    0 org.apache.hadoop.fs.FsShell.expunge()
 504    2   1    1 org.apache.hadoop.fs.FsShell.getCurrentTrashDir()
 505   29  11    1 org.apache.hadoop.fs.FsShell.tail(String[],int)
 506    2   1    0 org.apache.hadoop.fs.FsShell.CmdHandler.getErrorCode()
 507    2   1    0 org.apache.hadoop.fs.FsShell.CmdHandler.okToContinue()
 508    2   1    0 org.apache.hadoop.fs.FsShell.CmdHandler.getName()
 509    2   1    0 org.apache.hadoop.fs.FsShell.CmdHandler.CmdHandler(String,FileSystem)
 510    1   1    0 org.apache.hadoop.fs.FsShell.CmdHandler.run(FileStatus,FileSystem)
 511    8   4    1 org.apache.hadoop.fs.FsShell.shellListStatus(String,FileSystem,Path)
 512   10   7    1 org.apache.hadoop.fs.FsShell.runCmdHandler(CmdHandler,FileStatus,FileSystem,boolean)
 513   17   9    0 org.apache.hadoop.fs.FsShell.runCmdHandler(CmdHandler,String[],int,boolean)
 514   22   5    1 org.apache.hadoop.fs.FsShell.byteDesc(long)
 515    2   1    0 org.apache.hadoop.fs.FsShell.limitDecimalTo2(double)
 516  149  32    0 org.apache.hadoop.fs.FsShell.printHelp(String)
 517   50  17    1 org.apache.hadoop.fs.FsShell.doall(String,String[],int)
 518   79  27    1 org.apache.hadoop.fs.FsShell.printUsage(String)
 519  158  74    1 org.apache.hadoop.fs.FsShell.run(String[])
 520    4   2    0 org.apache.hadoop.fs.FsShell.close()
 521    7   1    1 org.apache.hadoop.fs.FsShell.main(String[])
 522    1   1    0 org.apache.hadoop.fs.FsShell.DelayedExceptionThrowing.process(Path,FileSystem)
 523   11   7    0 org.apache.hadoop.fs.FsShell.DelayedExceptionThrowing.globAndProcess(Path,FileSystem)
 524   53  24    0 org.apache.hadoop.fs.FsShellPermissions.ChmodHandler.applyNormalPattern(String,Matcher)
 525    6   1    0 org.apache.hadoop.fs.FsShellPermissions.ChmodHandler.applyOctalPattern(String,Matcher)
 526    2   2    0 org.apache.hadoop.fs.FsShellPermissions.ChmodHandler.patternError(String)
 527   10   3    0 org.apache.hadoop.fs.FsShellPermissions.ChmodHandler.ChmodHandler(FileSystem,String)
 528   20  10    0 org.apache.hadoop.fs.FsShellPermissions.ChmodHandler.applyChmod(char,int,int,boolean)
 529    9   4    0 org.apache.hadoop.fs.FsShellPermissions.ChmodHandler.run(FileStatus,FileSystem)
 530    2   1    0 org.apache.hadoop.fs.FsShellPermissions.ChownHandler.ChownHandler(String,FileSystem)
 531   11   8    0 org.apache.hadoop.fs.FsShellPermissions.ChownHandler.ChownHandler(FileSystem,String)
 532    7   8    0 org.apache.hadoop.fs.FsShellPermissions.ChownHandler.run(FileStatus,FileSystem)
 533    6   3    0 org.apache.hadoop.fs.FsShellPermissions.ChgrpHandler.ChgrpHandler(FileSystem,String)
 534   16   8    0 org.apache.hadoop.fs.FsShellPermissions.changePermissions(FileSystem,String,String[],int,FsShell)
 535    3   1    0 org.apache.hadoop.fs.FsUrlConnection.FsUrlConnection(Configuration,URL)
 536    5   3    0 org.apache.hadoop.fs.FsUrlConnection.connect()
 537    4   2    0 org.apache.hadoop.fs.FsUrlConnection.getInputStream()
 538    2   1    0 org.apache.hadoop.fs.FsUrlStreamHandler.FsUrlStreamHandler(Configuration)
 539    2   1    0 org.apache.hadoop.fs.FsUrlStreamHandler.FsUrlStreamHandler()
 540    2   1    0 org.apache.hadoop.fs.FsUrlStreamHandler.openConnection(URL)
 541    4   1    0 org.apache.hadoop.fs.FsUrlStreamHandlerFactory.FsUrlStreamHandlerFactory()
 542    4   1    0 org.apache.hadoop.fs.FsUrlStreamHandlerFactory.FsUrlStreamHandlerFactory(Configuration)
 543    8   4    0 org.apache.hadoop.fs.FsUrlStreamHandlerFactory.createURLStreamHandler(String)
 544    2   1    0 org.apache.hadoop.fs.ftp.FTPException.FTPException(String)
 545    2   1    0 org.apache.hadoop.fs.ftp.FTPException.FTPException(Throwable)
 546    2   1    0 org.apache.hadoop.fs.ftp.FTPException.FTPException(String,Throwable)
 547   22   9    0 org.apache.hadoop.fs.ftp.FTPFileSystem.initialize(URI,Configuration)
 548   20   5    1 org.apache.hadoop.fs.ftp.FTPFileSystem.connect()
 549    8   5    1 org.apache.hadoop.fs.ftp.FTPFileSystem.disconnect(FTPClient)
 550    4   3    1 org.apache.hadoop.fs.ftp.FTPFileSystem.makeAbsolute(Path,Path)
 551   17   5    0 org.apache.hadoop.fs.ftp.FTPFileSystem.open(Path,int)
 552    8   5    0 org.apache.hadoop.fs.ftp.FTPFileSystem.FSDataOutputStream$1.close()
 553   30  10    0 org.apache.hadoop.fs.ftp.FTPFileSystem.create(Path,FsPermission,boolean,int,short,long,Progressable)
 554    2   2    1 org.apache.hadoop.fs.ftp.FTPFileSystem.append(Path,int,Progressable)
 555    6   6    1 org.apache.hadoop.fs.ftp.FTPFileSystem.exists(FTPClient,Path)
 556    2   1    0 org.apache.hadoop.fs.ftp.FTPFileSystem.delete(Path)
 557    6   2    0 org.apache.hadoop.fs.ftp.FTPFileSystem.delete(Path,boolean)
 558    2   1    0 org.apache.hadoop.fs.ftp.FTPFileSystem.delete(FTPClient,Path)
 559   14   9    1 org.apache.hadoop.fs.ftp.FTPFileSystem.delete(FTPClient,Path,boolean)
 560    9   4    0 org.apache.hadoop.fs.ftp.FTPFileSystem.getFsAction(int,FTPFile)
 561    6   1    0 org.apache.hadoop.fs.ftp.FTPFileSystem.getPermissions(FTPFile)
 562    2   1    0 org.apache.hadoop.fs.ftp.FTPFileSystem.getUri()
 563    6   2    0 org.apache.hadoop.fs.ftp.FTPFileSystem.listStatus(Path)
 564   11   4    1 org.apache.hadoop.fs.ftp.FTPFileSystem.listStatus(FTPClient,Path)
 565    6   2    0 org.apache.hadoop.fs.ftp.FTPFileSystem.getFileStatus(Path)
 566   25   9    1 org.apache.hadoop.fs.ftp.FTPFileSystem.getFileStatus(FTPClient,Path)
 567   12   1    1 org.apache.hadoop.fs.ftp.FTPFileSystem.getFileStatus(FTPFile,Path)
 568    6   2    0 org.apache.hadoop.fs.ftp.FTPFileSystem.mkdirs(Path,FsPermission)
 569   16   6    1 org.apache.hadoop.fs.ftp.FTPFileSystem.mkdirs(FTPClient,Path,FsPermission)
 570    6   6    1 org.apache.hadoop.fs.ftp.FTPFileSystem.isFile(FTPClient,Path)
 571    6   2    0 org.apache.hadoop.fs.ftp.FTPFileSystem.rename(Path,Path)
 572   17   7    1 org.apache.hadoop.fs.ftp.FTPFileSystem.rename(FTPClient,Path,Path)
 573    2   1    0 org.apache.hadoop.fs.ftp.FTPFileSystem.getWorkingDirectory()
 574   11   6    0 org.apache.hadoop.fs.ftp.FTPFileSystem.getHomeDirectory()
 575    1   1    0 org.apache.hadoop.fs.ftp.FTPFileSystem.setWorkingDirectory(Path)
 576   10   6    0 org.apache.hadoop.fs.ftp.FTPInputStream.FTPInputStream(InputStream,FTPClient,FileSystem.Statistics)
 577    2   1    0 org.apache.hadoop.fs.ftp.FTPInputStream.getPos()
 578    2   2    0 org.apache.hadoop.fs.ftp.FTPInputStream.seek(long)
 579    2   2    0 org.apache.hadoop.fs.ftp.FTPInputStream.seekToNewSource(long)
 580    9   5    0 org.apache.hadoop.fs.ftp.FTPInputStream.read()
 581    9   5    0 org.apache.hadoop.fs.ftp.FTPInputStream.read(byte[],int,int)
 582   12   7    0 org.apache.hadoop.fs.ftp.FTPInputStream.close()
 583    2   1    0 org.apache.hadoop.fs.ftp.FTPInputStream.markSupported()
 584    1   1    0 org.apache.hadoop.fs.ftp.FTPInputStream.mark(int)
 585    2   2    0 org.apache.hadoop.fs.ftp.FTPInputStream.reset()
 586    4   1    0 org.apache.hadoop.fs.GlobExpander.StringWithOffset.StringWithOffset(String,int)
 587   12   3    1 org.apache.hadoop.fs.GlobExpander.expand(String)
 588   50  16    1 org.apache.hadoop.fs.GlobExpander.expandLeftmost(StringWithOffset)
 589   22  15    1 org.apache.hadoop.fs.GlobExpander.leftmostOuterCurlyContainingSlash(String,int)
 590    1   1    1 org.apache.hadoop.fs.HarFileSystem.HarFileSystem()
 591    2   1    1 org.apache.hadoop.fs.HarFileSystem.HarFileSystem(FileSystem)
 592   19  11    1 org.apache.hadoop.fs.HarFileSystem.initialize(URI,Configuration)
 593   11   2    0 org.apache.hadoop.fs.HarFileSystem.getHarVersion()
 594    9   3    0 org.apache.hadoop.fs.HarFileSystem.archivePath(Path)
 595   18  11    1 org.apache.hadoop.fs.HarFileSystem.decodeHarURI(URI,Configuration)
 596    2   1    1 org.apache.hadoop.fs.HarFileSystem.getWorkingDirectory()
 597    9   3    1 org.apache.hadoop.fs.HarFileSystem.getHarAuth(URI)
 598    2   1    0 org.apache.hadoop.fs.HarFileSystem.getUri()
 599   15   6    1 org.apache.hadoop.fs.HarFileSystem.getPathInHar(Path)
 600   10   4    0 org.apache.hadoop.fs.HarFileSystem.makeRelative(String,Path)
 601   13   5    0 org.apache.hadoop.fs.HarFileSystem.makeQualified(Path)
 602   12   5    0 org.apache.hadoop.fs.HarFileSystem.getFileBlockLocations(FileStatus,long,long)
 603    5   2    1 org.apache.hadoop.fs.HarFileSystem.fakeBlockLocations(BlockLocation[],long)
 604    2   1    1 org.apache.hadoop.fs.HarFileSystem.getHarHash(Path)
 605    2   1    0 org.apache.hadoop.fs.HarFileSystem.Store.Store()
 606    5   1    0 org.apache.hadoop.fs.HarFileSystem.Store.Store(long,long,int,int)
 607   40  10    0 org.apache.hadoop.fs.HarFileSystem.fileStatusInIndex(Path)
 608   11   4    0 org.apache.hadoop.fs.HarFileSystem.HarStatus.HarStatus(String)
 609    2   1    0 org.apache.hadoop.fs.HarFileSystem.HarStatus.isDir()
 610    2   1    0 org.apache.hadoop.fs.HarFileSystem.HarStatus.getName()
 611    2   1    0 org.apache.hadoop.fs.HarFileSystem.HarStatus.getChildren()
 612    2   1    0 org.apache.hadoop.fs.HarFileSystem.HarStatus.getFileName()
 613    2   1    0 org.apache.hadoop.fs.HarFileSystem.HarStatus.getPartName()
 614    2   1    0 org.apache.hadoop.fs.HarFileSystem.HarStatus.getStartIndex()
 615    2   1    0 org.apache.hadoop.fs.HarFileSystem.HarStatus.getLength()
 616   12   6    0 org.apache.hadoop.fs.HarFileSystem.getFileStatus(Path)
 617   12   7    0 org.apache.hadoop.fs.HarFileSystem.open(Path,int)
 618    2   2    0 org.apache.hadoop.fs.HarFileSystem.create(Path,int)
 619    2   2    0 org.apache.hadoop.fs.HarFileSystem.create(Path,FsPermission,boolean,int,short,long,Progressable)
 620    4   3    0 org.apache.hadoop.fs.HarFileSystem.close()
 621    2   2    0 org.apache.hadoop.fs.HarFileSystem.setReplication(Path,short)
 622    2   2    0 org.apache.hadoop.fs.HarFileSystem.delete(Path,boolean)
 623   16   5    0 org.apache.hadoop.fs.HarFileSystem.listStatus(Path)
 624    2   1    1 org.apache.hadoop.fs.HarFileSystem.getHomeDirectory()
 625    1   1    0 org.apache.hadoop.fs.HarFileSystem.setWorkingDirectory(Path)
 626    2   2    1 org.apache.hadoop.fs.HarFileSystem.mkdirs(Path,FsPermission)
 627    2   2    1 org.apache.hadoop.fs.HarFileSystem.copyFromLocalFile(boolean,Path,Path)
 628    2   1    1 org.apache.hadoop.fs.HarFileSystem.copyToLocalFile(boolean,Path,Path)
 629    2   2    1 org.apache.hadoop.fs.HarFileSystem.startLocalOutput(Path,Path)
 630    2   2    1 org.apache.hadoop.fs.HarFileSystem.completeLocalOutput(Path,Path)
 631    2   2    1 org.apache.hadoop.fs.HarFileSystem.setOwner(Path,String,String)
 632    2   2    1 org.apache.hadoop.fs.HarFileSystem.setPermission(Path,FsPermission)
 633    6   1    0 org.apache.hadoop.fs.HarFileSystem.HarFSDataInputStream.HarFsInputStream.HarFsInputStream(FileSystem,Path,long,long,int)
 634    5   3    0 org.apache.hadoop.fs.HarFileSystem.HarFSDataInputStream.HarFsInputStream.available()
 635    3   1    0 org.apache.hadoop.fs.HarFileSystem.HarFSDataInputStream.HarFsInputStream.close()
 636    1   1    0 org.apache.hadoop.fs.HarFileSystem.HarFSDataInputStream.HarFsInputStream.mark(int)
 637    2   2    1 org.apache.hadoop.fs.HarFileSystem.HarFSDataInputStream.HarFsInputStream.reset()
 638    3   2    0 org.apache.hadoop.fs.HarFileSystem.HarFSDataInputStream.HarFsInputStream.read()
 639    5   2    0 org.apache.hadoop.fs.HarFileSystem.HarFSDataInputStream.HarFsInputStream.read(byte[])
 640   10   4    1 org.apache.hadoop.fs.HarFileSystem.HarFSDataInputStream.HarFsInputStream.read(byte[],int,int)
 641    9   5    0 org.apache.hadoop.fs.HarFileSystem.HarFSDataInputStream.HarFsInputStream.skip(long)
 642    2   1    0 org.apache.hadoop.fs.HarFileSystem.HarFSDataInputStream.HarFsInputStream.getPos()
 643    5   4    0 org.apache.hadoop.fs.HarFileSystem.HarFSDataInputStream.HarFsInputStream.seek(long)
 644    2   1    0 org.apache.hadoop.fs.HarFileSystem.HarFSDataInputStream.HarFsInputStream.seekToNewSource(long)
 645    5   2    1 org.apache.hadoop.fs.HarFileSystem.HarFSDataInputStream.HarFsInputStream.read(long,byte[],int,int)
 646    4   3    1 org.apache.hadoop.fs.HarFileSystem.HarFSDataInputStream.HarFsInputStream.readFully(long,byte[],int,int)
 647    2   1    0 org.apache.hadoop.fs.HarFileSystem.HarFSDataInputStream.HarFsInputStream.readFully(long,byte[])
 648    2   1    1 org.apache.hadoop.fs.HarFileSystem.HarFSDataInputStream.HarFSDataInputStream(FileSystem,Path,long,long,int)
 649    2   1    1 org.apache.hadoop.fs.HarFileSystem.HarFSDataInputStream.HarFSDataInputStream(FileSystem,Path,long,long)
 650    2   1    0 org.apache.hadoop.fs.InMemoryFileSystem.RawInMemoryFileSystem.RawInMemoryFileSystem()
 651    2   1    0 org.apache.hadoop.fs.InMemoryFileSystem.RawInMemoryFileSystem.RawInMemoryFileSystem(URI,Configuration)
 652   10   2    0 org.apache.hadoop.fs.InMemoryFileSystem.RawInMemoryFileSystem.initialize(URI,Configuration)
 653    2   1    0 org.apache.hadoop.fs.InMemoryFileSystem.RawInMemoryFileSystem.getUri()
 654    6   3    0 org.apache.hadoop.fs.InMemoryFileSystem.RawInMemoryFileSystem.InMemoryInputStream.InMemoryInputStream(Path)
 655    2   1    0 org.apache.hadoop.fs.InMemoryFileSystem.RawInMemoryFileSystem.InMemoryInputStream.getPos()
 656    4   3    0 org.apache.hadoop.fs.InMemoryFileSystem.RawInMemoryFileSystem.InMemoryInputStream.seek(long)
 657    2   1    0 org.apache.hadoop.fs.InMemoryFileSystem.RawInMemoryFileSystem.InMemoryInputStream.seekToNewSource(long)
 658    2   1    0 org.apache.hadoop.fs.InMemoryFileSystem.RawInMemoryFileSystem.InMemoryInputStream.available()
 659    2   1    0 org.apache.hadoop.fs.InMemoryFileSystem.RawInMemoryFileSystem.InMemoryInputStream.markSupport()
 660    2   1    0 org.apache.hadoop.fs.InMemoryFileSystem.RawInMemoryFileSystem.InMemoryInputStream.read()
 661    2   1    0 org.apache.hadoop.fs.InMemoryFileSystem.RawInMemoryFileSystem.InMemoryInputStream.read(byte[],int,int)
 662    2   1    0 org.apache.hadoop.fs.InMemoryFileSystem.RawInMemoryFileSystem.InMemoryInputStream.skip(long)
 663    2   1    0 org.apache.hadoop.fs.InMemoryFileSystem.RawInMemoryFileSystem.open(Path,int)
 664    3   1    0 org.apache.hadoop.fs.InMemoryFileSystem.RawInMemoryFileSystem.InMemoryOutputStream.InMemoryOutputStream(Path,FileAttributes)
 665    2   1    0 org.apache.hadoop.fs.InMemoryFileSystem.RawInMemoryFileSystem.InMemoryOutputStream.getPos()
 666    3   1    0 org.apache.hadoop.fs.InMemoryFileSystem.RawInMemoryFileSystem.InMemoryOutputStream.close()
 667   11  11    0 org.apache.hadoop.fs.InMemoryFileSystem.RawInMemoryFileSystem.InMemoryOutputStream.write(byte[],int,int)
 668    6   3    0 org.apache.hadoop.fs.InMemoryFileSystem.RawInMemoryFileSystem.InMemoryOutputStream.write(int)
 669    2   2    1 org.apache.hadoop.fs.InMemoryFileSystem.RawInMemoryFileSystem.append(Path,int,Progressable)
 670    8   6    1 org.apache.hadoop.fs.InMemoryFileSystem.RawInMemoryFileSystem.create(Path,FsPermission,boolean,int,short,long,Progressable)
 671    2   1    0 org.apache.hadoop.fs.InMemoryFileSystem.RawInMemoryFileSystem.create(Path,FileAttributes)
 672    9   3    0 org.apache.hadoop.fs.InMemoryFileSystem.RawInMemoryFileSystem.close()
 673    2   1    0 org.apache.hadoop.fs.InMemoryFileSystem.RawInMemoryFileSystem.setReplication(Path,short)
 674    9   5    0 org.apache.hadoop.fs.InMemoryFileSystem.RawInMemoryFileSystem.rename(Path,Path)
 675    2   1    0 org.apache.hadoop.fs.InMemoryFileSystem.RawInMemoryFileSystem.delete(Path)
 676    8   3    0 org.apache.hadoop.fs.InMemoryFileSystem.RawInMemoryFileSystem.delete(Path,boolean)
 677    2   1    1 org.apache.hadoop.fs.InMemoryFileSystem.RawInMemoryFileSystem.listStatus(Path)
 678    2   1    0 org.apache.hadoop.fs.InMemoryFileSystem.RawInMemoryFileSystem.setWorkingDirectory(Path)
 679    2   1    0 org.apache.hadoop.fs.InMemoryFileSystem.RawInMemoryFileSystem.getWorkingDirectory()
 680    2   1    1 org.apache.hadoop.fs.InMemoryFileSystem.RawInMemoryFileSystem.mkdirs(Path,FsPermission)
 681    6   3    0 org.apache.hadoop.fs.InMemoryFileSystem.RawInMemoryFileSystem.getFileStatus(Path)
 682   11   5    1 org.apache.hadoop.fs.InMemoryFileSystem.RawInMemoryFileSystem.reserveSpace(Path,long)
 683    6   2    0 org.apache.hadoop.fs.InMemoryFileSystem.RawInMemoryFileSystem.unreserveSpace(Path)
 684   17   7    1 org.apache.hadoop.fs.InMemoryFileSystem.RawInMemoryFileSystem.getFiles(PathFilter)
 685    2   1    0 org.apache.hadoop.fs.InMemoryFileSystem.RawInMemoryFileSystem.getNumFiles(PathFilter)
 686    2   1    0 org.apache.hadoop.fs.InMemoryFileSystem.RawInMemoryFileSystem.getFSSize()
 687    5   3    0 org.apache.hadoop.fs.InMemoryFileSystem.RawInMemoryFileSystem.getPercentUsed()
 688    4   4    1 org.apache.hadoop.fs.InMemoryFileSystem.RawInMemoryFileSystem.canFitInMemory(long)
 689    2   1    0 org.apache.hadoop.fs.InMemoryFileSystem.RawInMemoryFileSystem.getPath(Path)
 690    3   1    0 org.apache.hadoop.fs.InMemoryFileSystem.RawInMemoryFileSystem.FileAttributes.FileAttributes(int)
 691    2   1    0 org.apache.hadoop.fs.InMemoryFileSystem.RawInMemoryFileSystem.InMemoryFileStatus.InMemoryFileStatus(Path,FileAttributes)
 692    2   1    0 org.apache.hadoop.fs.InMemoryFileSystem.InMemoryFileSystem()
 693    2   1    0 org.apache.hadoop.fs.InMemoryFileSystem.InMemoryFileSystem(URI,Configuration)
 694   10   3    1 org.apache.hadoop.fs.InMemoryFileSystem.reserveSpaceWithCheckSum(Path,long)
 695    2   1    0 org.apache.hadoop.fs.InMemoryFileSystem.getFiles(PathFilter)
 696    2   1    0 org.apache.hadoop.fs.InMemoryFileSystem.getNumFiles(PathFilter)
 697    2   1    0 org.apache.hadoop.fs.InMemoryFileSystem.getFSSize()
 698    2   1    0 org.apache.hadoop.fs.InMemoryFileSystem.getPercentUsed()
 699    1   1    0 org.apache.hadoop.fs.kfs.IFSImpl.exists(String)
 700    1   1    0 org.apache.hadoop.fs.kfs.IFSImpl.isDirectory(String)
 701    1   1    0 org.apache.hadoop.fs.kfs.IFSImpl.isFile(String)
 702    1   1    0 org.apache.hadoop.fs.kfs.IFSImpl.readdir(String)
 703    1   1    0 org.apache.hadoop.fs.kfs.IFSImpl.readdirplus(Path)
 704    1   1    0 org.apache.hadoop.fs.kfs.IFSImpl.mkdirs(String)
 705    1   1    0 org.apache.hadoop.fs.kfs.IFSImpl.rename(String,String)
 706    1   1    0 org.apache.hadoop.fs.kfs.IFSImpl.rmdir(String)
 707    1   1    0 org.apache.hadoop.fs.kfs.IFSImpl.remove(String)
 708    1   1    0 org.apache.hadoop.fs.kfs.IFSImpl.filesize(String)
 709    1   1    0 org.apache.hadoop.fs.kfs.IFSImpl.getReplication(String)
 710    1   1    0 org.apache.hadoop.fs.kfs.IFSImpl.setReplication(String,short)
 711    1   1    0 org.apache.hadoop.fs.kfs.IFSImpl.getDataLocation(String,long,long)
 712    1   1    0 org.apache.hadoop.fs.kfs.IFSImpl.getModificationTime(String)
 713    1   1    0 org.apache.hadoop.fs.kfs.IFSImpl.create(String,short,int)
 714    1   1    0 org.apache.hadoop.fs.kfs.IFSImpl.open(String,int)
 715    2   1    0 org.apache.hadoop.fs.kfs.KFSImpl.KFSImpl(String,int)
 716    3   1    0 org.apache.hadoop.fs.kfs.KFSImpl.KFSImpl(String,int,FileSystem.Statistics)
 717    2   1    0 org.apache.hadoop.fs.kfs.KFSImpl.exists(String)
 718    2   1    0 org.apache.hadoop.fs.kfs.KFSImpl.isDirectory(String)
 719    2   1    0 org.apache.hadoop.fs.kfs.KFSImpl.isFile(String)
 720    2   1    0 org.apache.hadoop.fs.kfs.KFSImpl.readdir(String)
 721   22  10    0 org.apache.hadoop.fs.kfs.KFSImpl.readdirplus(Path)
 722    2   1    0 org.apache.hadoop.fs.kfs.KFSImpl.mkdirs(String)
 723    2   1    0 org.apache.hadoop.fs.kfs.KFSImpl.rename(String,String)
 724    2   1    0 org.apache.hadoop.fs.kfs.KFSImpl.rmdir(String)
 725    2   1    0 org.apache.hadoop.fs.kfs.KFSImpl.remove(String)
 726    2   1    0 org.apache.hadoop.fs.kfs.KFSImpl.filesize(String)
 727    2   1    0 org.apache.hadoop.fs.kfs.KFSImpl.getReplication(String)
 728    2   1    0 org.apache.hadoop.fs.kfs.KFSImpl.setReplication(String,short)
 729    2   1    0 org.apache.hadoop.fs.kfs.KFSImpl.getDataLocation(String,long,long)
 730    2   1    0 org.apache.hadoop.fs.kfs.KFSImpl.getModificationTime(String)
 731    2   1    0 org.apache.hadoop.fs.kfs.KFSImpl.create(String,short,int)
 732    2   1    0 org.apache.hadoop.fs.kfs.KFSImpl.open(String,int)
 733    2   1    0 org.apache.hadoop.fs.kfs.KFSInputStream.KFSInputStream(KfsAccess,String)
 734    7   2    0 org.apache.hadoop.fs.kfs.KFSInputStream.KFSInputStream(KfsAccess,String,FileSystem.Statistics)
 735    4   3    0 org.apache.hadoop.fs.kfs.KFSInputStream.getPos()
 736    4   3    0 org.apache.hadoop.fs.kfs.KFSInputStream.available()
 737    4   3    0 org.apache.hadoop.fs.kfs.KFSInputStream.seek(long)
 738    2   1    0 org.apache.hadoop.fs.kfs.KFSInputStream.seekToNewSource(long)
 739   10   6    0 org.apache.hadoop.fs.kfs.KFSInputStream.read()
 740   10   6    0 org.apache.hadoop.fs.kfs.KFSInputStream.read(byte[],int,int)
 741    5   3    0 org.apache.hadoop.fs.kfs.KFSInputStream.close()
 742    2   1    0 org.apache.hadoop.fs.kfs.KFSInputStream.markSupported()
 743    1   1    0 org.apache.hadoop.fs.kfs.KFSInputStream.mark(int)
 744    2   2    0 org.apache.hadoop.fs.kfs.KFSInputStream.reset()
 745    3   1    0 org.apache.hadoop.fs.kfs.KFSOutputStream.KFSOutputStream(KfsAccess,String,short)
 746    4   3    0 org.apache.hadoop.fs.kfs.KFSOutputStream.getPos()
 747    6   3    0 org.apache.hadoop.fs.kfs.KFSOutputStream.write(int)
 748    4   3    0 org.apache.hadoop.fs.kfs.KFSOutputStream.write(byte[],int,int)
 749    4   3    0 org.apache.hadoop.fs.kfs.KFSOutputStream.flush()
 750    6   3    0 org.apache.hadoop.fs.kfs.KFSOutputStream.close()
 751    1   1    0 org.apache.hadoop.fs.kfs.KosmosFileSystem.KosmosFileSystem()
 752    2   1    0 org.apache.hadoop.fs.kfs.KosmosFileSystem.KosmosFileSystem(IFSImpl)
 753    2   1    0 org.apache.hadoop.fs.kfs.KosmosFileSystem.getUri()
 754    9   3    0 org.apache.hadoop.fs.kfs.KosmosFileSystem.initialize(URI,Configuration)
 755    2   1    0 org.apache.hadoop.fs.kfs.KosmosFileSystem.getName()
 756    2   1    0 org.apache.hadoop.fs.kfs.KosmosFileSystem.getWorkingDirectory()
 757    2   1    0 org.apache.hadoop.fs.kfs.KosmosFileSystem.setWorkingDirectory(Path)
 758    4   3    0 org.apache.hadoop.fs.kfs.KosmosFileSystem.makeAbsolute(Path)
 759    6   1    0 org.apache.hadoop.fs.kfs.KosmosFileSystem.mkdirs(Path,FsPermission)
 760    4   1    0 org.apache.hadoop.fs.kfs.KosmosFileSystem.isDirectory(Path)
 761    4   1    0 org.apache.hadoop.fs.kfs.KosmosFileSystem.isFile(Path)
 762    6   3    0 org.apache.hadoop.fs.kfs.KosmosFileSystem.listStatus(Path)
 763    9   5    0 org.apache.hadoop.fs.kfs.KosmosFileSystem.getFileStatus(Path)
 764    2   2    1 org.apache.hadoop.fs.kfs.KosmosFileSystem.append(Path,int,Progressable)
 765   12   7    0 org.apache.hadoop.fs.kfs.KosmosFileSystem.create(Path,FsPermission,boolean,int,short,long,Progressable)
 766    6   3    0 org.apache.hadoop.fs.kfs.KosmosFileSystem.open(Path,int)
 767    6   1    0 org.apache.hadoop.fs.kfs.KosmosFileSystem.rename(Path,Path)
 768   12   9    0 org.apache.hadoop.fs.kfs.KosmosFileSystem.delete(Path,boolean)
 769    2   1    0 org.apache.hadoop.fs.kfs.KosmosFileSystem.delete(Path)
 770    4   1    0 org.apache.hadoop.fs.kfs.KosmosFileSystem.getLength(Path)
 771    4   1    0 org.apache.hadoop.fs.kfs.KosmosFileSystem.getReplication(Path)
 772    2   1    0 org.apache.hadoop.fs.kfs.KosmosFileSystem.getDefaultReplication()
 773    5   1    0 org.apache.hadoop.fs.kfs.KosmosFileSystem.setReplication(Path,short)
 774    2   1    0 org.apache.hadoop.fs.kfs.KosmosFileSystem.getDefaultBlockSize()
 775    1   1    0 org.apache.hadoop.fs.kfs.KosmosFileSystem.lock(Path,boolean)
 776    1   1    0 org.apache.hadoop.fs.kfs.KosmosFileSystem.release(Path)
 777   12   5    0 org.apache.hadoop.fs.kfs.KosmosFileSystem.getFileBlockLocations(FileStatus,long,long)
 778    2   1    0 org.apache.hadoop.fs.kfs.KosmosFileSystem.copyFromLocalFile(boolean,Path,Path)
 779    2   1    0 org.apache.hadoop.fs.kfs.KosmosFileSystem.copyToLocalFile(boolean,Path,Path)
 780    2   1    0 org.apache.hadoop.fs.kfs.KosmosFileSystem.startLocalOutput(Path,Path)
 781    2   1    0 org.apache.hadoop.fs.kfs.KosmosFileSystem.completeLocalOutput(Path,Path)
 782    2   1    1 org.apache.hadoop.fs.LocalDirAllocator.LocalDirAllocator(String)
 783    6   2    1 org.apache.hadoop.fs.LocalDirAllocator.obtainContext(String)
 784    2   1    1 org.apache.hadoop.fs.LocalDirAllocator.getLocalPathForWrite(String,Configuration)
 785    3   1    1 org.apache.hadoop.fs.LocalDirAllocator.getLocalPathForWrite(String,long,Configuration)
 786    3   1    1 org.apache.hadoop.fs.LocalDirAllocator.getLocalPathToRead(String,Configuration)
 787    3   1    1 org.apache.hadoop.fs.LocalDirAllocator.createTmpFileForWrite(String,long,Configuration)
 788    3   1    1 org.apache.hadoop.fs.LocalDirAllocator.isContextValid(String)
 789    3   1    1 org.apache.hadoop.fs.LocalDirAllocator.ifExists(String,Configuration)
 790    3   1    1 org.apache.hadoop.fs.LocalDirAllocator.getCurrentDirectoryIndex()
 791    2   1    0 org.apache.hadoop.fs.LocalDirAllocator.AllocatorPerContext.AllocatorPerContext(String)
 792   24   7    1 org.apache.hadoop.fs.LocalDirAllocator.AllocatorPerContext.confChanged(Configuration)
 793    7   3    0 org.apache.hadoop.fs.LocalDirAllocator.AllocatorPerContext.createPath(String)
 794    2   1    1 org.apache.hadoop.fs.LocalDirAllocator.AllocatorPerContext.getCurrentDirectoryIndex()
 795    2   1    1 org.apache.hadoop.fs.LocalDirAllocator.AllocatorPerContext.getLocalPathForWrite(String,Configuration)
 796   32  11    1 org.apache.hadoop.fs.LocalDirAllocator.AllocatorPerContext.getLocalPathForWrite(String,long,Configuration)
 797    7   1    1 org.apache.hadoop.fs.LocalDirAllocator.AllocatorPerContext.createTmpFileForWrite(String,long,Configuration)
 798   12   6    1 org.apache.hadoop.fs.LocalDirAllocator.AllocatorPerContext.getLocalPathToRead(String,Configuration)
 799   12   6    1 org.apache.hadoop.fs.LocalDirAllocator.AllocatorPerContext.ifExists(String,Configuration)
 800    2   1    0 org.apache.hadoop.fs.LocalFileSystem.LocalFileSystem()
 801    2   1    0 org.apache.hadoop.fs.LocalFileSystem.getRaw()
 802    3   1    0 org.apache.hadoop.fs.LocalFileSystem.LocalFileSystem(FileSystem)
 803    2   1    1 org.apache.hadoop.fs.LocalFileSystem.pathToFile(Path)
 804    2   1    0 org.apache.hadoop.fs.LocalFileSystem.copyFromLocalFile(boolean,Path,Path)
 805    2   1    0 org.apache.hadoop.fs.LocalFileSystem.copyToLocalFile(boolean,Path,Path)
 806   24  10    1 org.apache.hadoop.fs.LocalFileSystem.reportChecksumFailure(Path,FSDataInputStream,long,FSDataInputStream,long)
 807    2   1    1 org.apache.hadoop.fs.MD5MD5CRC32FileChecksum.MD5MD5CRC32FileChecksum()
 808    4   1    1 org.apache.hadoop.fs.MD5MD5CRC32FileChecksum.MD5MD5CRC32FileChecksum(int,long,MD5Hash)
 809    2   1    1 org.apache.hadoop.fs.MD5MD5CRC32FileChecksum.getAlgorithmName()
 810    2   1    1 org.apache.hadoop.fs.MD5MD5CRC32FileChecksum.getLength()
 811    2   1    1 org.apache.hadoop.fs.MD5MD5CRC32FileChecksum.getBytes()
 812    4   1    1 org.apache.hadoop.fs.MD5MD5CRC32FileChecksum.readFields(DataInput)
 813    4   1    1 org.apache.hadoop.fs.MD5MD5CRC32FileChecksum.write(DataOutput)
 814    7   2    1 org.apache.hadoop.fs.MD5MD5CRC32FileChecksum.write(XMLOutputter,MD5MD5CRC32FileChecksum)
 815    9   8    1 org.apache.hadoop.fs.MD5MD5CRC32FileChecksum.valueOf(Attributes)
 816    2   1    1 org.apache.hadoop.fs.MD5MD5CRC32FileChecksum.toString()
 817    2   1    1 org.apache.hadoop.fs.Path.Path(String,String)
 818    2   1    1 org.apache.hadoop.fs.Path.Path(Path,String)
 819    2   1    1 org.apache.hadoop.fs.Path.Path(String,Path)
 820    9   5    1 org.apache.hadoop.fs.Path.Path(Path,Path)
 821    5   5    0 org.apache.hadoop.fs.Path.checkPathArg(String)
 822   19   8    1 org.apache.hadoop.fs.Path.Path(String)
 823    3   1    1 org.apache.hadoop.fs.Path.Path(String,String,String)
 824    4   3    0 org.apache.hadoop.fs.Path.initialize(String,String,String)
 825    7   4    0 org.apache.hadoop.fs.Path.normalizePath(String)
 826    5  11    0 org.apache.hadoop.fs.Path.hasWindowsDrive(String,boolean)
 827    2   1    1 org.apache.hadoop.fs.Path.toUri()
 828    2   1    1 org.apache.hadoop.fs.Path.getFileSystem(Configuration)
 829    3   2    1 org.apache.hadoop.fs.Path.isAbsolute()
 830    4   1    1 org.apache.hadoop.fs.Path.getName()
 831   13   9    1 org.apache.hadoop.fs.Path.getParent()
 832    2   1    1 org.apache.hadoop.fs.Path.suffix(String)
 833   14   8    0 org.apache.hadoop.fs.Path.toString()
 834    5   3    0 org.apache.hadoop.fs.Path.equals(Object)
 835    2   1    0 org.apache.hadoop.fs.Path.hashCode()
 836    3   1    0 org.apache.hadoop.fs.Path.compareTo(Object)
 837    8   4    1 org.apache.hadoop.fs.Path.depth()
 838   17   9    1 org.apache.hadoop.fs.Path.makeQualified(FileSystem)
 839    1   1    1 org.apache.hadoop.fs.PathFilter.accept(Path)
 840    2   1    1 org.apache.hadoop.fs.permission.AccessControlException.AccessControlException()
 841    2   1    1 org.apache.hadoop.fs.permission.AccessControlException.AccessControlException(String)
 842    3   1    0 org.apache.hadoop.fs.permission.FsAction.FsAction(int,String)
 843    4   3    1 org.apache.hadoop.fs.permission.FsAction.implies(FsAction)
 844    2   1    1 org.apache.hadoop.fs.permission.FsAction.and(FsAction)
 845    2   1    1 org.apache.hadoop.fs.permission.FsAction.or(FsAction)
 846    2   1    1 org.apache.hadoop.fs.permission.FsAction.not()
 847    2   1    0 org.apache.hadoop.fs.permission.FsPermission.WritableFactory$1.newInstance()
 848    2   2    0 org.apache.hadoop.fs.permission.FsPermission.FsPermission$2.applyUMask(FsPermission)
 849    2   2    0 org.apache.hadoop.fs.permission.FsPermission.FsPermission$2.readFields(DataInput)
 850    6   1    1 org.apache.hadoop.fs.permission.FsPermission.createImmutable(short)
 851    1   1    0 org.apache.hadoop.fs.permission.FsPermission.FsPermission()
 852    2   1    1 org.apache.hadoop.fs.permission.FsPermission.FsPermission(FsAction,FsAction,FsAction)
 853    2   1    1 org.apache.hadoop.fs.permission.FsPermission.FsPermission(short)
 854    4   1    1 org.apache.hadoop.fs.permission.FsPermission.FsPermission(FsPermission)
 855    2   1    1 org.apache.hadoop.fs.permission.FsPermission.getUserAction()
 856    2   1    1 org.apache.hadoop.fs.permission.FsPermission.getGroupAction()
 857    2   1    1 org.apache.hadoop.fs.permission.FsPermission.getOtherAction()
 858    4   1    0 org.apache.hadoop.fs.permission.FsPermission.set(FsAction,FsAction,FsAction)
 859    3   1    0 org.apache.hadoop.fs.permission.FsPermission.fromShort(short)
 860    2   1    1 org.apache.hadoop.fs.permission.FsPermission.write(DataOutput)
 861    2   1    1 org.apache.hadoop.fs.permission.FsPermission.readFields(DataInput)
 862    4   1    1 org.apache.hadoop.fs.permission.FsPermission.read(DataInput)
 863    3   1    1 org.apache.hadoop.fs.permission.FsPermission.toShort()
 864    5   5    1 org.apache.hadoop.fs.permission.FsPermission.equals(Object)
 865    2   1    1 org.apache.hadoop.fs.permission.FsPermission.hashCode()
 866    2   1    1 org.apache.hadoop.fs.permission.FsPermission.toString()
 867    2   1    1 org.apache.hadoop.fs.permission.FsPermission.applyUMask(FsPermission)
 868    5   2    1 org.apache.hadoop.fs.permission.FsPermission.getUMask(Configuration)
 869    2   1    1 org.apache.hadoop.fs.permission.FsPermission.setUMask(Configuration,FsPermission)
 870    2   1    1 org.apache.hadoop.fs.permission.FsPermission.getDefault()
 871   12   9    1 org.apache.hadoop.fs.permission.FsPermission.valueOf(String)
 872    2   1    0 org.apache.hadoop.fs.permission.PermissionStatus.WritableFactory$1.newInstance()
 873    2   2    0 org.apache.hadoop.fs.permission.PermissionStatus.PermissionStatus$2.applyUMask(FsPermission)
 874    2   2    0 org.apache.hadoop.fs.permission.PermissionStatus.PermissionStatus$2.readFields(DataInput)
 875    6   1    1 org.apache.hadoop.fs.permission.PermissionStatus.createImmutable(String,String,FsPermission)
 876    1   1    0 org.apache.hadoop.fs.permission.PermissionStatus.PermissionStatus()
 877    4   1    1 org.apache.hadoop.fs.permission.PermissionStatus.PermissionStatus(String,String,FsPermission)
 878    2   1    1 org.apache.hadoop.fs.permission.PermissionStatus.getUserName()
 879    2   1    1 org.apache.hadoop.fs.permission.PermissionStatus.getGroupName()
 880    2   1    1 org.apache.hadoop.fs.permission.PermissionStatus.getPermission()
 881    3   1    1 org.apache.hadoop.fs.permission.PermissionStatus.applyUMask(FsPermission)
 882    4   1    1 org.apache.hadoop.fs.permission.PermissionStatus.readFields(DataInput)
 883    2   1    1 org.apache.hadoop.fs.permission.PermissionStatus.write(DataOutput)
 884    4   1    1 org.apache.hadoop.fs.permission.PermissionStatus.read(DataInput)
 885    4   1    1 org.apache.hadoop.fs.permission.PermissionStatus.write(DataOutput,String,String,FsPermission)
 886    2   1    1 org.apache.hadoop.fs.permission.PermissionStatus.toString()
 887    1   1    1 org.apache.hadoop.fs.PositionedReadable.read(long,byte[],int,int)
 888    1   1    1 org.apache.hadoop.fs.PositionedReadable.readFully(long,byte[],int,int)
 889    1   1    1 org.apache.hadoop.fs.PositionedReadable.readFully(long,byte[])
 890    2   1    0 org.apache.hadoop.fs.RawLocalFileSystem.RawLocalFileSystem()
 891    5   2    1 org.apache.hadoop.fs.RawLocalFileSystem.pathToFile(Path)
 892    2   1    1 org.apache.hadoop.fs.RawLocalFileSystem.getName()
 893    2   1    0 org.apache.hadoop.fs.RawLocalFileSystem.getUri()
 894    2   1    0 org.apache.hadoop.fs.RawLocalFileSystem.initialize(URI,Configuration)
 895    2   1    0 org.apache.hadoop.fs.RawLocalFileSystem.TrackingFileInputStream.TrackingFileInputStream(File)
 896    5   2    0 org.apache.hadoop.fs.RawLocalFileSystem.TrackingFileInputStream.read()
 897    5   2    0 org.apache.hadoop.fs.RawLocalFileSystem.TrackingFileInputStream.read(byte[])
 898    5   2    0 org.apache.hadoop.fs.RawLocalFileSystem.TrackingFileInputStream.read(byte[],int,int)
 899    2   1    0 org.apache.hadoop.fs.RawLocalFileSystem.LocalFSFileInputStream.LocalFSFileInputStream(Path)
 900    3   1    0 org.apache.hadoop.fs.RawLocalFileSystem.LocalFSFileInputStream.seek(long)
 901    2   1    0 org.apache.hadoop.fs.RawLocalFileSystem.LocalFSFileInputStream.getPos()
 902    2   1    0 org.apache.hadoop.fs.RawLocalFileSystem.LocalFSFileInputStream.seekToNewSource(long)
 903    2   1    0 org.apache.hadoop.fs.RawLocalFileSystem.LocalFSFileInputStream.available()
 904    2   1    0 org.apache.hadoop.fs.RawLocalFileSystem.LocalFSFileInputStream.close()
 905    2   1    0 org.apache.hadoop.fs.RawLocalFileSystem.LocalFSFileInputStream.markSupport()
 906    7   5    0 org.apache.hadoop.fs.RawLocalFileSystem.LocalFSFileInputStream.read()
 907    7   5    0 org.apache.hadoop.fs.RawLocalFileSystem.LocalFSFileInputStream.read(byte[],int,int)
 908    5   4    0 org.apache.hadoop.fs.RawLocalFileSystem.LocalFSFileInputStream.read(long,byte[],int,int)
 909    5   2    0 org.apache.hadoop.fs.RawLocalFileSystem.LocalFSFileInputStream.skip(long)
 910    4   3    0 org.apache.hadoop.fs.RawLocalFileSystem.open(Path,int)
 911    2   1    0 org.apache.hadoop.fs.RawLocalFileSystem.LocalFSFileOutputStream.LocalFSFileOutputStream(Path,boolean)
 912    2   1    0 org.apache.hadoop.fs.RawLocalFileSystem.LocalFSFileOutputStream.close()
 913    2   1    0 org.apache.hadoop.fs.RawLocalFileSystem.LocalFSFileOutputStream.flush()
 914    4   3    0 org.apache.hadoop.fs.RawLocalFileSystem.LocalFSFileOutputStream.write(byte[],int,int)
 915    4   3    0 org.apache.hadoop.fs.RawLocalFileSystem.LocalFSFileOutputStream.write(int)
 916    2   1    1 org.apache.hadoop.fs.RawLocalFileSystem.LocalFSFileOutputStream.sync()
 917    6   5    1 org.apache.hadoop.fs.RawLocalFileSystem.append(Path,int,Progressable)
 918    7   7    1 org.apache.hadoop.fs.RawLocalFileSystem.create(Path,boolean,int,short,long,Progressable)
 919    4   1    0 org.apache.hadoop.fs.RawLocalFileSystem.create(Path,FsPermission,boolean,int,short,long,Progressable)
 920    4   3    0 org.apache.hadoop.fs.RawLocalFileSystem.rename(Path,Path)
 921    2   1    0 org.apache.hadoop.fs.RawLocalFileSystem.delete(Path)
 922    8   7    0 org.apache.hadoop.fs.RawLocalFileSystem.delete(Path,boolean)
 923   14   8    0 org.apache.hadoop.fs.RawLocalFileSystem.listStatus(Path)
 924    4   4    1 org.apache.hadoop.fs.RawLocalFileSystem.mkdirs(Path)
 925    4   1    0 org.apache.hadoop.fs.RawLocalFileSystem.mkdirs(Path,FsPermission)
 926    2   1    0 org.apache.hadoop.fs.RawLocalFileSystem.getHomeDirectory()
 927    2   1    0 org.apache.hadoop.fs.RawLocalFileSystem.setWorkingDirectory(Path)
 928    2   1    0 org.apache.hadoop.fs.RawLocalFileSystem.getWorkingDirectory()
 929   15   2    0 org.apache.hadoop.fs.RawLocalFileSystem.lock(Path,boolean)
 930   18   7    0 org.apache.hadoop.fs.RawLocalFileSystem.release(Path)
 931    2   1    0 org.apache.hadoop.fs.RawLocalFileSystem.moveFromLocalFile(Path,Path)
 932    2   1    0 org.apache.hadoop.fs.RawLocalFileSystem.startLocalOutput(Path,Path)
 933    1   1    0 org.apache.hadoop.fs.RawLocalFileSystem.completeLocalOutput(Path,Path)
 934    2   1    0 org.apache.hadoop.fs.RawLocalFileSystem.close()
 935    2   1    0 org.apache.hadoop.fs.RawLocalFileSystem.toString()
 936    6   4    0 org.apache.hadoop.fs.RawLocalFileSystem.getFileStatus(Path)
 937    2   1    0 org.apache.hadoop.fs.RawLocalFileSystem.RawLocalFileStatus.isPermissionLoaded()
 938    2   1    0 org.apache.hadoop.fs.RawLocalFileSystem.RawLocalFileStatus.RawLocalFileStatus(File,long,FileSystem)
 939    4   2    0 org.apache.hadoop.fs.RawLocalFileSystem.RawLocalFileStatus.getPermission()
 940    4   2    0 org.apache.hadoop.fs.RawLocalFileSystem.RawLocalFileStatus.getOwner()
 941    4   2    0 org.apache.hadoop.fs.RawLocalFileSystem.RawLocalFileStatus.getGroup()
 942   22   7    0 org.apache.hadoop.fs.RawLocalFileSystem.RawLocalFileStatus.loadPermissionInfo()
 943    4   2    0 org.apache.hadoop.fs.RawLocalFileSystem.RawLocalFileStatus.write(DataOutput)
 944    8   6    0 org.apache.hadoop.fs.RawLocalFileSystem.setOwner(Path,String,String)
 945    2   1    0 org.apache.hadoop.fs.RawLocalFileSystem.setPermission(Path,FsPermission)
 946    6   1    0 org.apache.hadoop.fs.RawLocalFileSystem.execCommand(File,String)
 947    3   1    0 org.apache.hadoop.fs.s3.Block.Block(long,long)
 948    2   1    0 org.apache.hadoop.fs.s3.Block.getId()
 949    2   1    0 org.apache.hadoop.fs.s3.Block.getLength()
 950    2   1    0 org.apache.hadoop.fs.s3.Block.toString()
 951    1   1    0 org.apache.hadoop.fs.s3.FileSystemStore.initialize(URI,Configuration)
 952    1   1    0 org.apache.hadoop.fs.s3.FileSystemStore.getVersion()
 953    1   1    0 org.apache.hadoop.fs.s3.FileSystemStore.storeINode(Path,INode)
 954    1   1    0 org.apache.hadoop.fs.s3.FileSystemStore.storeBlock(Block,File)
 955    1   1    0 org.apache.hadoop.fs.s3.FileSystemStore.inodeExists(Path)
 956    1   1    0 org.apache.hadoop.fs.s3.FileSystemStore.blockExists(long)
 957    1   1    0 org.apache.hadoop.fs.s3.FileSystemStore.retrieveINode(Path)
 958    1   1    0 org.apache.hadoop.fs.s3.FileSystemStore.retrieveBlock(Block,long)
 959    1   1    0 org.apache.hadoop.fs.s3.FileSystemStore.deleteINode(Path)
 960    1   1    0 org.apache.hadoop.fs.s3.FileSystemStore.deleteBlock(Block)
 961    1   1    0 org.apache.hadoop.fs.s3.FileSystemStore.listSubPaths(Path)
 962    1   1    0 org.apache.hadoop.fs.s3.FileSystemStore.listDeepSubPaths(Path)
 963    1   1    1 org.apache.hadoop.fs.s3.FileSystemStore.purge()
 964    1   1    1 org.apache.hadoop.fs.s3.FileSystemStore.dump()
 965    5   4    0 org.apache.hadoop.fs.s3.INode.INode(FileType,Block[])
 966    2   1    0 org.apache.hadoop.fs.s3.INode.getBlocks()
 967    2   1    0 org.apache.hadoop.fs.s3.INode.getFileType()
 968    2   1    0 org.apache.hadoop.fs.s3.INode.isDirectory()
 969    2   1    0 org.apache.hadoop.fs.s3.INode.isFile()
 970    2   2    0 org.apache.hadoop.fs.s3.INode.getSerializedLength()
 971   11   3    0 org.apache.hadoop.fs.s3.INode.serialize()
 972   20   9    0 org.apache.hadoop.fs.s3.INode.deserialize(InputStream)
 973   13   5    0 org.apache.hadoop.fs.s3.Jets3tFileSystemStore.initialize(URI,Configuration)
 974    6   5    0 org.apache.hadoop.fs.s3.Jets3tFileSystemStore.createBucket(String)
 975    2   1    0 org.apache.hadoop.fs.s3.Jets3tFileSystemStore.getVersion()
 976    6   5    0 org.apache.hadoop.fs.s3.Jets3tFileSystemStore.delete(String)
 977    2   1    0 org.apache.hadoop.fs.s3.Jets3tFileSystemStore.deleteINode(Path)
 978    2   1    0 org.apache.hadoop.fs.s3.Jets3tFileSystemStore.deleteBlock(Block)
 979    6   3    0 org.apache.hadoop.fs.s3.Jets3tFileSystemStore.inodeExists(Path)
 980    6   3    0 org.apache.hadoop.fs.s3.Jets3tFileSystemStore.blockExists(long)
 981   11   9    0 org.apache.hadoop.fs.s3.Jets3tFileSystemStore.get(String,boolean)
 982    9   8    0 org.apache.hadoop.fs.s3.Jets3tFileSystemStore.get(String,long)
 983   10   7    0 org.apache.hadoop.fs.s3.Jets3tFileSystemStore.checkMetadata(S3Object)
 984    2   1    0 org.apache.hadoop.fs.s3.Jets3tFileSystemStore.retrieveINode(Path)
 985   21   6    0 org.apache.hadoop.fs.s3.Jets3tFileSystemStore.retrieveBlock(Block,long)
 986    7   4    0 org.apache.hadoop.fs.s3.Jets3tFileSystemStore.newBackupFile()
 987   14   8    0 org.apache.hadoop.fs.s3.Jets3tFileSystemStore.listSubPaths(Path)
 988   14   8    0 org.apache.hadoop.fs.s3.Jets3tFileSystemStore.listDeepSubPaths(Path)
 989   12   6    0 org.apache.hadoop.fs.s3.Jets3tFileSystemStore.put(String,InputStream,long,boolean)
 990    2   1    0 org.apache.hadoop.fs.s3.Jets3tFileSystemStore.storeINode(Path,INode)
 991    6   1    0 org.apache.hadoop.fs.s3.Jets3tFileSystemStore.storeBlock(Block,File)
 992    4   3    0 org.apache.hadoop.fs.s3.Jets3tFileSystemStore.closeQuietly(Closeable)
 993    4   3    0 org.apache.hadoop.fs.s3.Jets3tFileSystemStore.pathToKey(Path)
 994    2   1    0 org.apache.hadoop.fs.s3.Jets3tFileSystemStore.keyToPath(String)
 995    2   1    0 org.apache.hadoop.fs.s3.Jets3tFileSystemStore.blockToKey(long)
 996    2   1    0 org.apache.hadoop.fs.s3.Jets3tFileSystemStore.blockToKey(Block)
 997    8   6    0 org.apache.hadoop.fs.s3.Jets3tFileSystemStore.purge()
 998   18   8    0 org.apache.hadoop.fs.s3.Jets3tFileSystemStore.dump()
 999    3   1    0 org.apache.hadoop.fs.s3.MigrationTool.main(String[])
1000   33  10    0 org.apache.hadoop.fs.s3.MigrationTool.run(String[])
1001   30  16    0 org.apache.hadoop.fs.s3.MigrationTool.initialize(URI)
1002    5   2    0 org.apache.hadoop.fs.s3.MigrationTool.migrate(Store,FileSystemStore)
1003    6   5    0 org.apache.hadoop.fs.s3.MigrationTool.get(String)
1004    1   1    0 org.apache.hadoop.fs.s3.MigrationTool.Store.listAllPaths()
1005    1   1    0 org.apache.hadoop.fs.s3.MigrationTool.Store.retrieveINode(Path)
1006    1   1    0 org.apache.hadoop.fs.s3.MigrationTool.Store.deleteINode(Path)
1007   11   7    0 org.apache.hadoop.fs.s3.MigrationTool.UnversionedStore.listAllPaths()
1008    2   1    0 org.apache.hadoop.fs.s3.MigrationTool.UnversionedStore.deleteINode(Path)
1009    6   5    0 org.apache.hadoop.fs.s3.MigrationTool.UnversionedStore.delete(String)
1010    2   1    0 org.apache.hadoop.fs.s3.MigrationTool.UnversionedStore.retrieveINode(Path)
1011    9   8    0 org.apache.hadoop.fs.s3.MigrationTool.UnversionedStore.get(String)
1012    4   3    0 org.apache.hadoop.fs.s3.MigrationTool.UnversionedStore.pathToKey(Path)
1013    2   1    0 org.apache.hadoop.fs.s3.MigrationTool.UnversionedStore.keyToPath(String)
1014    4   4    0 org.apache.hadoop.fs.s3.MigrationTool.UnversionedStore.urlEncode(String)
1015    4   4    0 org.apache.hadoop.fs.s3.MigrationTool.UnversionedStore.urlDecode(String)
1016   26  14    1 org.apache.hadoop.fs.s3.S3Credentials.initialize(URI,Configuration)
1017    2   1    0 org.apache.hadoop.fs.s3.S3Credentials.getAccessKey()
1018    2   1    0 org.apache.hadoop.fs.s3.S3Credentials.getSecretAccessKey()
1019    2   1    0 org.apache.hadoop.fs.s3.S3Exception.S3Exception(Throwable)
1020    1   1    0 org.apache.hadoop.fs.s3.S3FileSystem.S3FileSystem()
1021    2   1    0 org.apache.hadoop.fs.s3.S3FileSystem.S3FileSystem(FileSystemStore)
1022    2   1    0 org.apache.hadoop.fs.s3.S3FileSystem.getUri()
1023    7   2    0 org.apache.hadoop.fs.s3.S3FileSystem.initialize(URI,Configuration)
1024   11   1    0 org.apache.hadoop.fs.s3.S3FileSystem.createDefaultStore(Configuration)
1025    2   1    0 org.apache.hadoop.fs.s3.S3FileSystem.getName()
1026    2   1    0 org.apache.hadoop.fs.s3.S3FileSystem.getWorkingDirectory()
1027    2   1    0 org.apache.hadoop.fs.s3.S3FileSystem.setWorkingDirectory(Path)
1028    4   3    0 org.apache.hadoop.fs.s3.S3FileSystem.makeAbsolute(Path)
1029   10   3    0 org.apache.hadoop.fs.s3.S3FileSystem.mkdirs(Path,FsPermission)
1030    9   4    0 org.apache.hadoop.fs.s3.S3FileSystem.mkdir(Path)
1031    5   3    0 org.apache.hadoop.fs.s3.S3FileSystem.isFile(Path)
1032    7   5    0 org.apache.hadoop.fs.s3.S3FileSystem.checkFile(Path)
1033   11   6    0 org.apache.hadoop.fs.s3.S3FileSystem.listStatus(Path)
1034    2   2    1 org.apache.hadoop.fs.s3.S3FileSystem.append(Path,int,Progressable)
1035   13   7    0 org.apache.hadoop.fs.s3.S3FileSystem.create(Path,FsPermission,boolean,int,short,long,Progressable)
1036    3   1    0 org.apache.hadoop.fs.s3.S3FileSystem.open(Path,int)
1037   18  11    0 org.apache.hadoop.fs.s3.S3FileSystem.rename(Path,Path)
1038   16   5    0 org.apache.hadoop.fs.s3.S3FileSystem.renameRecursive(Path,Path)
1039   20  13    0 org.apache.hadoop.fs.s3.S3FileSystem.delete(Path,boolean)
1040    2   1    0 org.apache.hadoop.fs.s3.S3FileSystem.delete(Path)
1041    5   3    0 org.apache.hadoop.fs.s3.S3FileSystem.getFileStatus(Path)
1042    2   1    0 org.apache.hadoop.fs.s3.S3FileSystem.dump()
1043    2   1    0 org.apache.hadoop.fs.s3.S3FileSystem.purge()
1044    2   1    0 org.apache.hadoop.fs.s3.S3FileSystem.S3FileStatus.S3FileStatus(Path,INode)
1045    7   4    0 org.apache.hadoop.fs.s3.S3FileSystem.S3FileStatus.findLength(INode)
1046    3   2    0 org.apache.hadoop.fs.s3.S3FileSystem.S3FileStatus.findBlocksize(INode)
1047    2   1    0 org.apache.hadoop.fs.s3.S3FileSystemException.S3FileSystemException(String)
1048    2   1    0 org.apache.hadoop.fs.s3.S3InputStream.S3InputStream(Configuration,FileSystemStore,INode)
1049    6   2    0 org.apache.hadoop.fs.s3.S3InputStream.S3InputStream(Configuration,FileSystemStore,INode,FileSystem.Statistics)
1050    2   1    0 org.apache.hadoop.fs.s3.S3InputStream.getPos()
1051    2   1    0 org.apache.hadoop.fs.s3.S3InputStream.available()
1052    5   3    0 org.apache.hadoop.fs.s3.S3InputStream.seek(long)
1053    2   1    0 org.apache.hadoop.fs.s3.S3InputStream.seekToNewSource(long)
1054   13   7    0 org.apache.hadoop.fs.s3.S3InputStream.read()
1055   14   9    0 org.apache.hadoop.fs.s3.S3InputStream.read(byte[],int,int)
1056   19   6    0 org.apache.hadoop.fs.s3.S3InputStream.blockSeekTo(long)
1057   10   5    0 org.apache.hadoop.fs.s3.S3InputStream.close()
1058    2   1    0 org.apache.hadoop.fs.s3.S3InputStream.markSupported()
1059    1   1    0 org.apache.hadoop.fs.s3.S3InputStream.mark(int)
1060    2   2    0 org.apache.hadoop.fs.s3.S3InputStream.reset()
1061    9   1    0 org.apache.hadoop.fs.s3.S3OutputStream.S3OutputStream(Configuration,FileSystemStore,Path,long,Progressable,int)
1062    7   4    0 org.apache.hadoop.fs.s3.S3OutputStream.newBackupFile()
1063    2   1    0 org.apache.hadoop.fs.s3.S3OutputStream.getPos()
1064    7   5    0 org.apache.hadoop.fs.s3.S3OutputStream.write(int)
1065   13   6    0 org.apache.hadoop.fs.s3.S3OutputStream.write(byte[],int,int)
1066    8   5    0 org.apache.hadoop.fs.s3.S3OutputStream.flush()
1067    7   2    0 org.apache.hadoop.fs.s3.S3OutputStream.flushData(int)
1068    9   1    0 org.apache.hadoop.fs.s3.S3OutputStream.endBlock()
1069    7   2    0 org.apache.hadoop.fs.s3.S3OutputStream.nextBlockOutputStream()
1070    3   1    0 org.apache.hadoop.fs.s3.S3OutputStream.internalClose()
1071   10   5    0 org.apache.hadoop.fs.s3.S3OutputStream.close()
1072    2   2    0 org.apache.hadoop.fs.s3.VersionMismatchException.VersionMismatchException(String,String)
1073    4   1    0 org.apache.hadoop.fs.s3native.FileMetadata.FileMetadata(String,long,long)
1074    2   1    0 org.apache.hadoop.fs.s3native.FileMetadata.getKey()
1075    2   1    0 org.apache.hadoop.fs.s3native.FileMetadata.getLength()
1076    2   1    0 org.apache.hadoop.fs.s3native.FileMetadata.getLastModified()
1077    2   1    0 org.apache.hadoop.fs.s3native.FileMetadata.toString()
1078   11   5    0 org.apache.hadoop.fs.s3native.Jets3tNativeFileSystemStore.initialize(URI,Configuration)
1079    6   5    0 org.apache.hadoop.fs.s3native.Jets3tNativeFileSystemStore.createBucket(String)
1080   18   8    0 org.apache.hadoop.fs.s3native.Jets3tNativeFileSystemStore.storeFile(String,File,byte[])
1081   10   5    0 org.apache.hadoop.fs.s3native.Jets3tNativeFileSystemStore.storeEmptyFile(String)
1082    9   8    0 org.apache.hadoop.fs.s3native.Jets3tNativeFileSystemStore.retrieveMetadata(String)
1083    9   8    0 org.apache.hadoop.fs.s3native.Jets3tNativeFileSystemStore.retrieve(String)
1084    9   8    0 org.apache.hadoop.fs.s3native.Jets3tNativeFileSystemStore.retrieve(String,long)
1085    2   1    0 org.apache.hadoop.fs.s3native.Jets3tNativeFileSystemStore.list(String,int)
1086    2   1    0 org.apache.hadoop.fs.s3native.Jets3tNativeFileSystemStore.list(String,int,String)
1087    2   1    0 org.apache.hadoop.fs.s3native.Jets3tNativeFileSystemStore.listAll(String,int,String)
1088   13   9    0 org.apache.hadoop.fs.s3native.Jets3tNativeFileSystemStore.list(String,String,int,String)
1089    6   5    0 org.apache.hadoop.fs.s3native.Jets3tNativeFileSystemStore.delete(String)
1090    6   5    0 org.apache.hadoop.fs.s3native.Jets3tNativeFileSystemStore.rename(String,String)
1091    8   6    0 org.apache.hadoop.fs.s3native.Jets3tNativeFileSystemStore.purge(String)
1092   11   6    0 org.apache.hadoop.fs.s3native.Jets3tNativeFileSystemStore.dump()
1093    1   1    0 org.apache.hadoop.fs.s3native.NativeFileSystemStore.initialize(URI,Configuration)
1094    1   1    0 org.apache.hadoop.fs.s3native.NativeFileSystemStore.storeFile(String,File,byte[])
1095    1   1    0 org.apache.hadoop.fs.s3native.NativeFileSystemStore.storeEmptyFile(String)
1096    1   1    0 org.apache.hadoop.fs.s3native.NativeFileSystemStore.retrieveMetadata(String)
1097    1   1    0 org.apache.hadoop.fs.s3native.NativeFileSystemStore.retrieve(String)
1098    1   1    0 org.apache.hadoop.fs.s3native.NativeFileSystemStore.retrieve(String,long)
1099    1   1    0 org.apache.hadoop.fs.s3native.NativeFileSystemStore.list(String,int)
1100    1   1    0 org.apache.hadoop.fs.s3native.NativeFileSystemStore.list(String,int,String)
1101    1   1    0 org.apache.hadoop.fs.s3native.NativeFileSystemStore.listAll(String,int,String)
1102    1   1    0 org.apache.hadoop.fs.s3native.NativeFileSystemStore.delete(String)
1103    1   1    0 org.apache.hadoop.fs.s3native.NativeFileSystemStore.rename(String,String)
1104    1   1    1 org.apache.hadoop.fs.s3native.NativeFileSystemStore.purge(String)
1105    1   1    1 org.apache.hadoop.fs.s3native.NativeFileSystemStore.dump()
1106    3   1    0 org.apache.hadoop.fs.s3native.NativeS3FileSystem.NativeS3FsInputStream.NativeS3FsInputStream(InputStream,String)
1107    5   2    0 org.apache.hadoop.fs.s3native.NativeS3FileSystem.NativeS3FsInputStream.read()
1108    5   2    0 org.apache.hadoop.fs.s3native.NativeS3FileSystem.NativeS3FsInputStream.read(byte[],int,int)
1109    2   1    0 org.apache.hadoop.fs.s3native.NativeS3FileSystem.NativeS3FsInputStream.close()
1110    4   1    0 org.apache.hadoop.fs.s3native.NativeS3FileSystem.NativeS3FsInputStream.seek(long)
1111    2   1    0 org.apache.hadoop.fs.s3native.NativeS3FileSystem.NativeS3FsInputStream.getPos()
1112    2   1    0 org.apache.hadoop.fs.s3native.NativeS3FileSystem.NativeS3FsInputStream.seekToNewSource(long)
1113    9   2    0 org.apache.hadoop.fs.s3native.NativeS3FileSystem.NativeS3FsOutputStream.NativeS3FsOutputStream(Configuration,NativeFileSystemStore,String,Progressable,int)
1114    7   4    0 org.apache.hadoop.fs.s3native.NativeS3FileSystem.NativeS3FsOutputStream.newBackupFile()
1115    2   1    0 org.apache.hadoop.fs.s3native.NativeS3FileSystem.NativeS3FsOutputStream.flush()
1116   11   5    0 org.apache.hadoop.fs.s3native.NativeS3FileSystem.NativeS3FsOutputStream.close()
1117    2   1    0 org.apache.hadoop.fs.s3native.NativeS3FileSystem.NativeS3FsOutputStream.write(int)
1118    2   1    0 org.apache.hadoop.fs.s3native.NativeS3FileSystem.NativeS3FsOutputStream.write(byte[],int,int)
1119    1   1    0 org.apache.hadoop.fs.s3native.NativeS3FileSystem.NativeS3FileSystem()
1120    2   1    0 org.apache.hadoop.fs.s3native.NativeS3FileSystem.NativeS3FileSystem(NativeFileSystemStore)
1121    7   2    0 org.apache.hadoop.fs.s3native.NativeS3FileSystem.initialize(URI,Configuration)
1122   10   1    0 org.apache.hadoop.fs.s3native.NativeS3FileSystem.createDefaultStore(Configuration)
1123    4   3    0 org.apache.hadoop.fs.s3native.NativeS3FileSystem.pathToKey(Path)
1124    2   1    0 org.apache.hadoop.fs.s3native.NativeS3FileSystem.keyToPath(String)
1125    4   3    0 org.apache.hadoop.fs.s3native.NativeS3FileSystem.makeAbsolute(Path)
1126    2   2    1 org.apache.hadoop.fs.s3native.NativeS3FileSystem.append(Path,int,Progressable)
1127    6   4    0 org.apache.hadoop.fs.s3native.NativeS3FileSystem.create(Path,FsPermission,boolean,int,short,long,Progressable)
1128    2   1    0 org.apache.hadoop.fs.s3native.NativeS3FileSystem.delete(Path)
1129   18  10    0 org.apache.hadoop.fs.s3native.NativeS3FileSystem.delete(Path,boolean)
1130   14  11    0 org.apache.hadoop.fs.s3native.NativeS3FileSystem.getFileStatus(Path)
1131    2   1    0 org.apache.hadoop.fs.s3native.NativeS3FileSystem.getUri()
1132   27  11    0 org.apache.hadoop.fs.s3native.NativeS3FileSystem.listStatus(Path)
1133    2   1    0 org.apache.hadoop.fs.s3native.NativeS3FileSystem.newFile(FileMetadata,Path)
1134    2   1    0 org.apache.hadoop.fs.s3native.NativeS3FileSystem.newDirectory(Path)
1135   10   3    0 org.apache.hadoop.fs.s3native.NativeS3FileSystem.mkdirs(Path,FsPermission)
1136    8   4    0 org.apache.hadoop.fs.s3native.NativeS3FileSystem.mkdir(Path)
1137    6   3    0 org.apache.hadoop.fs.s3native.NativeS3FileSystem.open(Path,int)
1138    6   3    0 org.apache.hadoop.fs.s3native.NativeS3FileSystem.createParent(Path)
1139   14  11    0 org.apache.hadoop.fs.s3native.NativeS3FileSystem.existsAndIsFile(Path)
1140   32  15    0 org.apache.hadoop.fs.s3native.NativeS3FileSystem.rename(Path,Path)
1141    2   1    0 org.apache.hadoop.fs.s3native.NativeS3FileSystem.setWorkingDirectory(Path)
1142    2   1    0 org.apache.hadoop.fs.s3native.NativeS3FileSystem.getWorkingDirectory()
1143    4   1    0 org.apache.hadoop.fs.s3native.PartialListing.PartialListing(String,FileMetadata[],String[])
1144    2   1    0 org.apache.hadoop.fs.s3native.PartialListing.getFiles()
1145    2   1    0 org.apache.hadoop.fs.s3native.PartialListing.getCommonPrefixes()
1146    2   1    0 org.apache.hadoop.fs.s3native.PartialListing.getPriorLastKey()
1147    1   1    1 org.apache.hadoop.fs.Seekable.seek(long)
1148    1   1    1 org.apache.hadoop.fs.Seekable.getPos()
1149    1   1    1 org.apache.hadoop.fs.Seekable.seekToNewSource(long)
1150    2   1    1 org.apache.hadoop.fs.shell.Command.Command(Configuration)
1151    1   1    1 org.apache.hadoop.fs.shell.Command.getCommandName()
1152    1   1    1 org.apache.hadoop.fs.shell.Command.run(Path)
1153   23   7    1 org.apache.hadoop.fs.shell.Command.runAll()
1154    6   2    1 org.apache.hadoop.fs.shell.CommandFormat.CommandFormat(String,int,int,String)
1155   15   9    1 org.apache.hadoop.fs.shell.CommandFormat.parse(String[],int)
1156    2   1    1 org.apache.hadoop.fs.shell.CommandFormat.getOpt(String)
1157    5   2    0 org.apache.hadoop.fs.shell.CommandUtils.formatDescription(String,String)
1158    8   3    1 org.apache.hadoop.fs.shell.Count.Count(String[],int,Configuration)
1159    2   1    1 org.apache.hadoop.fs.shell.Count.matches(String)
1160    2   1    0 org.apache.hadoop.fs.shell.Count.getCommandName()
1161    3   1    0 org.apache.hadoop.fs.shell.Count.run(Path)
1162    1   1    1 org.apache.hadoop.fs.Syncable.sync()
1163    2   1    1 org.apache.hadoop.fs.Trash.Trash(Configuration)
1164    6   1    0 org.apache.hadoop.fs.Trash.Trash(Path,Configuration)
1165   29  20    1 org.apache.hadoop.fs.Trash.moveToTrash(Path)
1166   10   5    1 org.apache.hadoop.fs.Trash.checkpoint()
1167   22   8    1 org.apache.hadoop.fs.Trash.expunge()
1168    2   1    0 org.apache.hadoop.fs.Trash.getCurrentTrashDir()
1169    2   1    1 org.apache.hadoop.fs.Trash.getEmptier()
1170    4   1    0 org.apache.hadoop.fs.Trash.Emptier.Emptier(Configuration)
1171   29  13    0 org.apache.hadoop.fs.Trash.Emptier.run()
1172    2   1    0 org.apache.hadoop.fs.Trash.Emptier.ceiling(long,long)
1173    2   1    0 org.apache.hadoop.fs.Trash.Emptier.floor(long,long)
1174    2   1    1 org.apache.hadoop.fs.Trash.main(String[])
1175    1   1    1 org.apache.hadoop.http.FilterInitializer.initFilter(FilterContainer)
1176    2   1    1 org.apache.hadoop.http.HttpServer.HttpServer(String,String,int,boolean)
1177   15   3    1 org.apache.hadoop.http.HttpServer.HttpServer(String,String,int,boolean,Configuration)
1178   10   6    1 org.apache.hadoop.http.HttpServer.getFilterInitializers(Configuration)
1179    5   2    1 org.apache.hadoop.http.HttpServer.addDefaultApps(String)
1180    3   1    0 org.apache.hadoop.http.HttpServer.addDefaultServlets()
1181    3   1    0 org.apache.hadoop.http.HttpServer.addContext(String,String,boolean)
1182    2   1    1 org.apache.hadoop.http.HttpServer.setAttribute(String,Object)
1183    3   1    1 org.apache.hadoop.http.HttpServer.addServlet(String,String,Class)
1184   11   8    0 org.apache.hadoop.http.HttpServer.addInternalServlet(String,String,Class)
1185   10   3    1 org.apache.hadoop.http.HttpServer.addFilter(String,String,Map)
1186    8   4    1 org.apache.hadoop.http.HttpServer.defineFilter(WebApplicationContext,String,String,Map,String[])
1187    4   2    1 org.apache.hadoop.http.HttpServer.addFilterPathMapping(String)
1188    2   1    1 org.apache.hadoop.http.HttpServer.getAttribute(String)
1189    5   3    1 org.apache.hadoop.http.HttpServer.getWebAppsPath()
1190    2   1    1 org.apache.hadoop.http.HttpServer.getPort()
1191    3   1    0 org.apache.hadoop.http.HttpServer.setThreads(int,int)
1192   10   4    1 org.apache.hadoop.http.HttpServer.addSslListener(InetSocketAddress,String,String,String)
1193   21  13    1 org.apache.hadoop.http.HttpServer.start()
1194    2   1    1 org.apache.hadoop.http.HttpServer.stop()
1195    5   1    0 org.apache.hadoop.http.HttpServer.StackServlet.doGet(HttpServletRequest,HttpServletResponse)
1196   13   7    1 org.apache.hadoop.io.IOUtils.copyBytes(InputStream,OutputStream,int,boolean)
1197    2   1    1 org.apache.hadoop.io.IOUtils.copyBytes(InputStream,OutputStream,Configuration)
1198    2   1    1 org.apache.hadoop.io.IOUtils.copyBytes(InputStream,OutputStream,Configuration,boolean)
1199    8   4    1 org.apache.hadoop.io.IOUtils.readFully(InputStream,byte[],int,int)
1200    6   4    1 org.apache.hadoop.io.IOUtils.skipFully(InputStream,long)
1201    7   6    1 org.apache.hadoop.io.IOUtils.cleanup(Log,java.io.Closeable)
1202    2   1    1 org.apache.hadoop.io.IOUtils.closeStream(java.io.Closeable)
1203    4   3    1 org.apache.hadoop.io.IOUtils.closeSocket(Socket)
1204    1   1    0 org.apache.hadoop.io.IOUtils.NullOutputStream.write(byte[],int,int)
1205    1   1    0 org.apache.hadoop.io.IOUtils.NullOutputStream.write(int)
1206    2   1    1 org.apache.hadoop.io.AbstractMapWritable.getNewClasses()
1207   11   7    1 org.apache.hadoop.io.AbstractMapWritable.addToMap(Class,byte)
1208    7   5    1 org.apache.hadoop.io.AbstractMapWritable.addToMap(Class)
1209    2   1    1 org.apache.hadoop.io.AbstractMapWritable.getClass(byte)
1210    2   2    1 org.apache.hadoop.io.AbstractMapWritable.getId(Class)
1211   11   5    1 org.apache.hadoop.io.AbstractMapWritable.copy(Writable)
1212   17   1    1 org.apache.hadoop.io.AbstractMapWritable.AbstractMapWritable()
1213    2   1    1 org.apache.hadoop.io.AbstractMapWritable.getConf()
1214    2   1    1 org.apache.hadoop.io.AbstractMapWritable.setConf(Configuration)
1215    5   2    1 org.apache.hadoop.io.AbstractMapWritable.write(DataOutput)
1216    8   4    1 org.apache.hadoop.io.AbstractMapWritable.readFields(DataInput)
1217    1   1    0 org.apache.hadoop.io.ArrayFile.ArrayFile()
1218    2   1    1 org.apache.hadoop.io.ArrayFile.Writer.Writer(Configuration,FileSystem,String,Class)
1219    2   1    1 org.apache.hadoop.io.ArrayFile.Writer.Writer(Configuration,FileSystem,String,Class,CompressionType,Progressable)
1220    3   1    1 org.apache.hadoop.io.ArrayFile.Writer.append(Writable)
1221    2   1    1 org.apache.hadoop.io.ArrayFile.Reader.Reader(FileSystem,String,Configuration)
1222    3   1    1 org.apache.hadoop.io.ArrayFile.Reader.seek(long)
1223    2   2    1 org.apache.hadoop.io.ArrayFile.Reader.next(Writable)
1224    2   1    1 org.apache.hadoop.io.ArrayFile.Reader.key()
1225    3   1    1 org.apache.hadoop.io.ArrayFile.Reader.get(long,Writable)
1226    4   3    0 org.apache.hadoop.io.ArrayWritable.ArrayWritable(Class)
1227    3   1    0 org.apache.hadoop.io.ArrayWritable.ArrayWritable(Class,Writable[])
1228    4   2    0 org.apache.hadoop.io.ArrayWritable.ArrayWritable(String[])
1229    2   1    0 org.apache.hadoop.io.ArrayWritable.getValueClass()
1230    5   2    0 org.apache.hadoop.io.ArrayWritable.toStrings()
1231    5   2    0 org.apache.hadoop.io.ArrayWritable.toArray()
1232    2   1    0 org.apache.hadoop.io.ArrayWritable.set(Writable[])
1233    2   1    0 org.apache.hadoop.io.ArrayWritable.get()
1234    6   2    0 org.apache.hadoop.io.ArrayWritable.readFields(DataInput)
1235    4   2    0 org.apache.hadoop.io.ArrayWritable.write(DataOutput)
1236    1   1    1 org.apache.hadoop.io.BinaryComparable.getLength()
1237    1   1    1 org.apache.hadoop.io.BinaryComparable.getBytes()
1238    4   3    1 org.apache.hadoop.io.BinaryComparable.compareTo(BinaryComparable)
1239    2   1    1 org.apache.hadoop.io.BinaryComparable.compareTo(byte[],int,int)
1240    7   5    1 org.apache.hadoop.io.BinaryComparable.equals(Object)
1241    2   1    1 org.apache.hadoop.io.BinaryComparable.hashCode()
1242    1   1    1 org.apache.hadoop.io.BooleanWritable.BooleanWritable()
1243    2   1    1 org.apache.hadoop.io.BooleanWritable.BooleanWritable(boolean)
1244    2   1    1 org.apache.hadoop.io.BooleanWritable.set(boolean)
1245    2   1    1 org.apache.hadoop.io.BooleanWritable.get()
1246    2   1    1 org.apache.hadoop.io.BooleanWritable.readFields(DataInput)
1247    2   1    1 org.apache.hadoop.io.BooleanWritable.write(DataOutput)
1248    5   3    1 org.apache.hadoop.io.BooleanWritable.equals(Object)
1249    2   2    0 org.apache.hadoop.io.BooleanWritable.hashCode()
1250    4   3    1 org.apache.hadoop.io.BooleanWritable.compareTo(Object)
1251    2   1    0 org.apache.hadoop.io.BooleanWritable.toString()
1252    2   1    0 org.apache.hadoop.io.BooleanWritable.Comparator.Comparator()
1253    4   5    0 org.apache.hadoop.io.BooleanWritable.Comparator.compare(byte[],int,int,byte[],int,int)
1254    2   1    1 org.apache.hadoop.io.BytesWritable.BytesWritable()
1255    3   1    1 org.apache.hadoop.io.BytesWritable.BytesWritable(byte[])
1256    2   1    1 org.apache.hadoop.io.BytesWritable.getBytes()
1257    2   1    0 org.apache.hadoop.io.BytesWritable.get()
1258    2   1    1 org.apache.hadoop.io.BytesWritable.getLength()
1259    2   1    0 org.apache.hadoop.io.BytesWritable.getSize()
1260    4   2    1 org.apache.hadoop.io.BytesWritable.setSize(int)
1261    2   1    1 org.apache.hadoop.io.BytesWritable.getCapacity()
1262    8   4    1 org.apache.hadoop.io.BytesWritable.setCapacity(int)
1263    2   1    1 org.apache.hadoop.io.BytesWritable.set(BytesWritable)
1264    4   1    1 org.apache.hadoop.io.BytesWritable.set(byte[],int,int)
1265    4   1    0 org.apache.hadoop.io.BytesWritable.readFields(DataInput)
1266    3   1    0 org.apache.hadoop.io.BytesWritable.write(DataOutput)
1267    2   1    0 org.apache.hadoop.io.BytesWritable.hashCode()
1268    4   3    1 org.apache.hadoop.io.BytesWritable.equals(Object)
1269   10   4    1 org.apache.hadoop.io.BytesWritable.toString()
1270    2   1    0 org.apache.hadoop.io.BytesWritable.Comparator.Comparator()
1271    2   1    1 org.apache.hadoop.io.BytesWritable.Comparator.compare(byte[],int,int,byte[],int,int)
1272    1   1    0 org.apache.hadoop.io.ByteWritable.ByteWritable()
1273    2   1    0 org.apache.hadoop.io.ByteWritable.ByteWritable(byte)
1274    2   1    1 org.apache.hadoop.io.ByteWritable.set(byte)
1275    2   1    1 org.apache.hadoop.io.ByteWritable.get()
1276    2   1    0 org.apache.hadoop.io.ByteWritable.readFields(DataInput)
1277    2   1    0 org.apache.hadoop.io.ByteWritable.write(DataOutput)
1278    5   3    1 org.apache.hadoop.io.ByteWritable.equals(Object)
1279    2   1    0 org.apache.hadoop.io.ByteWritable.hashCode()
1280    4   3    1 org.apache.hadoop.io.ByteWritable.compareTo(Object)
1281    2   1    0 org.apache.hadoop.io.ByteWritable.toString()
1282    2   1    0 org.apache.hadoop.io.ByteWritable.Comparator.Comparator()
1283    4   3    0 org.apache.hadoop.io.ByteWritable.Comparator.compare(byte[],int,int,byte[],int,int)
1284    3   1    1 org.apache.hadoop.io.compress.BlockCompressorStream.BlockCompressorStream(OutputStream,Compressor,int,int)
1285    2   1    1 org.apache.hadoop.io.compress.BlockCompressorStream.BlockCompressorStream(OutputStream,Compressor)
1286   32  20    1 org.apache.hadoop.io.compress.BlockCompressorStream.write(byte[],int,int)
1287    6   3    0 org.apache.hadoop.io.compress.BlockCompressorStream.finish()
1288    5   2    0 org.apache.hadoop.io.compress.BlockCompressorStream.compress()
1289    5   1    0 org.apache.hadoop.io.compress.BlockCompressorStream.rawWriteInt(int)
1290    2   1    1 org.apache.hadoop.io.compress.BlockDecompressorStream.BlockDecompressorStream(InputStream,Decompressor,int)
1291    2   1    1 org.apache.hadoop.io.compress.BlockDecompressorStream.BlockDecompressorStream(InputStream,Decompressor)
1292    2   1    0 org.apache.hadoop.io.compress.BlockDecompressorStream.BlockDecompressorStream(InputStream)
1293   16  10    0 org.apache.hadoop.io.compress.BlockDecompressorStream.decompress(byte[],int,int)
1294   12   5    0 org.apache.hadoop.io.compress.BlockDecompressorStream.getCompressedData()
1295    2   1    0 org.apache.hadoop.io.compress.BlockDecompressorStream.resetState()
1296    8   3    0 org.apache.hadoop.io.compress.BlockDecompressorStream.rawReadInt()
1297    2   1    0 org.apache.hadoop.io.compress.bzip2.CBZip2InputStream.reportCRCError()
1298    8   3    0 org.apache.hadoop.io.compress.bzip2.CBZip2InputStream.makeMaps()
1299    4   1    1 org.apache.hadoop.io.compress.bzip2.CBZip2InputStream.CBZip2InputStream(InputStream)
1300    5   4    0 org.apache.hadoop.io.compress.bzip2.CBZip2InputStream.read()
1301   14  12    0 org.apache.hadoop.io.compress.bzip2.CBZip2InputStream.read(byte[],int,int)
1302   26  14    0 org.apache.hadoop.io.compress.bzip2.CBZip2InputStream.read0()
1303   10   6    0 org.apache.hadoop.io.compress.bzip2.CBZip2InputStream.init()
1304   21  15    0 org.apache.hadoop.io.compress.bzip2.CBZip2InputStream.initBlock()
1305    8   2    0 org.apache.hadoop.io.compress.bzip2.CBZip2InputStream.endBlock()
1306    6   2    0 org.apache.hadoop.io.compress.bzip2.CBZip2InputStream.complete()
1307    8   3    0 org.apache.hadoop.io.compress.bzip2.CBZip2InputStream.close()
1308   14   5    0 org.apache.hadoop.io.compress.bzip2.CBZip2InputStream.bsR(int)
1309   12   4    0 org.apache.hadoop.io.compress.bzip2.CBZip2InputStream.bsGetBit()
1310    2   1    0 org.apache.hadoop.io.compress.bzip2.CBZip2InputStream.bsGetUByte()
1311    2   1    0 org.apache.hadoop.io.compress.bzip2.CBZip2InputStream.bsGetInt()
1312   21   9    0 org.apache.hadoop.io.compress.bzip2.CBZip2InputStream.hbCreateDecodeTables(int[],int[],int[],char[],int,int,int)
1313   46  17    0 org.apache.hadoop.io.compress.bzip2.CBZip2InputStream.recvDecodingTables()
1314   19   5    0 org.apache.hadoop.io.compress.bzip2.CBZip2InputStream.createHuffmanDecodingTables(int,int)
1315  128  31    0 org.apache.hadoop.io.compress.bzip2.CBZip2InputStream.getAndMoveToFrontDecode()
1316   24   5    0 org.apache.hadoop.io.compress.bzip2.CBZip2InputStream.getAndMoveToFrontDecode0(int)
1317   25   9    0 org.apache.hadoop.io.compress.bzip2.CBZip2InputStream.setupBlock()
1318   20   5    0 org.apache.hadoop.io.compress.bzip2.CBZip2InputStream.setupRandPartA()
1319   15   2    0 org.apache.hadoop.io.compress.bzip2.CBZip2InputStream.setupNoRandPartA()
1320   23   6    0 org.apache.hadoop.io.compress.bzip2.CBZip2InputStream.setupRandPartB()
1321   10   2    0 org.apache.hadoop.io.compress.bzip2.CBZip2InputStream.setupRandPartC()
1322   12   3    0 org.apache.hadoop.io.compress.bzip2.CBZip2InputStream.setupNoRandPartB()
1323   11   2    0 org.apache.hadoop.io.compress.bzip2.CBZip2InputStream.setupNoRandPartC()
1324    3   1    0 org.apache.hadoop.io.compress.bzip2.CBZip2InputStream.Data.Data(int)
1325    5   3    1 org.apache.hadoop.io.compress.bzip2.CBZip2InputStream.Data.initTT(int)
1326   88  24    1 org.apache.hadoop.io.compress.bzip2.CBZip2OutputStream.hbMakeCodeLengths(char[],int[],int,int)
1327   88  24    0 org.apache.hadoop.io.compress.bzip2.CBZip2OutputStream.hbMakeCodeLengths(byte[],int[],Data,int,int)
1328    2   2    0 org.apache.hadoop.io.compress.bzip2.CBZip2OutputStream.chooseBlockSize(long)
1329    2   1    1 org.apache.hadoop.io.compress.bzip2.CBZip2OutputStream.CBZip2OutputStream(OutputStream)
1330    9   5    1 org.apache.hadoop.io.compress.bzip2.CBZip2OutputStream.CBZip2OutputStream(OutputStream,int)
1331    5   3    0 org.apache.hadoop.io.compress.bzip2.CBZip2OutputStream.write(int)
1332   41   5    0 org.apache.hadoop.io.compress.bzip2.CBZip2OutputStream.writeRun()
1333    3   1    0 org.apache.hadoop.io.compress.bzip2.CBZip2OutputStream.finalize()
1334   12   3    0 org.apache.hadoop.io.compress.bzip2.CBZip2OutputStream.close()
1335    4   2    0 org.apache.hadoop.io.compress.bzip2.CBZip2OutputStream.flush()
1336    6   1    0 org.apache.hadoop.io.compress.bzip2.CBZip2OutputStream.init()
1337    7   2    0 org.apache.hadoop.io.compress.bzip2.CBZip2OutputStream.initBlock()
1338   19   4    0 org.apache.hadoop.io.compress.bzip2.CBZip2OutputStream.endBlock()
1339    9   1    0 org.apache.hadoop.io.compress.bzip2.CBZip2OutputStream.endCompression()
1340    2   1    0 org.apache.hadoop.io.compress.bzip2.CBZip2OutputStream.getBlockSize()
1341   11  10    0 org.apache.hadoop.io.compress.bzip2.CBZip2OutputStream.write(byte[],int,int)
1342   15   4    0 org.apache.hadoop.io.compress.bzip2.CBZip2OutputStream.write0(int)
1343    8   4    0 org.apache.hadoop.io.compress.bzip2.CBZip2OutputStream.hbAssignCodes(int[],byte[],int,int,int)
1344    6   2    0 org.apache.hadoop.io.compress.bzip2.CBZip2OutputStream.bsFinishedWithStream()
1345   10   2    0 org.apache.hadoop.io.compress.bzip2.CBZip2OutputStream.bsW(int,int)
1346    2   1    0 org.apache.hadoop.io.compress.bzip2.CBZip2OutputStream.bsPutUByte(int)
1347    5   1    0 org.apache.hadoop.io.compress.bzip2.CBZip2OutputStream.bsPutInt(int)
1348   16   7    0 org.apache.hadoop.io.compress.bzip2.CBZip2OutputStream.sendMTFValues()
1349   21  11    0 org.apache.hadoop.io.compress.bzip2.CBZip2OutputStream.sendMTFValues0(int,int)
1350   69  14    0 org.apache.hadoop.io.compress.bzip2.CBZip2OutputStream.sendMTFValues1(int,int)
1351   16   4    0 org.apache.hadoop.io.compress.bzip2.CBZip2OutputStream.sendMTFValues2(int,int)
1352   14   5    0 org.apache.hadoop.io.compress.bzip2.CBZip2OutputStream.sendMTFValues3(int,int)
1353   27  11    0 org.apache.hadoop.io.compress.bzip2.CBZip2OutputStream.sendMTFValues4()
1354   22   5    0 org.apache.hadoop.io.compress.bzip2.CBZip2OutputStream.sendMTFValues5(int,int)
1355   39   9    0 org.apache.hadoop.io.compress.bzip2.CBZip2OutputStream.sendMTFValues6(int,int)
1356   30   4    0 org.apache.hadoop.io.compress.bzip2.CBZip2OutputStream.sendMTFValues7(int)
1357    4   1    0 org.apache.hadoop.io.compress.bzip2.CBZip2OutputStream.moveToFrontCodeAndSend()
1358  137  47    0 org.apache.hadoop.io.compress.bzip2.CBZip2OutputStream.mainSimpleSort(Data,int,int,int)
1359    6   2    0 org.apache.hadoop.io.compress.bzip2.CBZip2OutputStream.vswap(int[],int,int,int)
1360    2   6    0 org.apache.hadoop.io.compress.bzip2.CBZip2OutputStream.med3(byte,byte,byte)
1361   17   5    0 org.apache.hadoop.io.compress.bzip2.CBZip2OutputStream.blockSort()
1362   76  17    1 org.apache.hadoop.io.compress.bzip2.CBZip2OutputStream.mainQSort3(Data,int,int,int)
1363   84  28    0 org.apache.hadoop.io.compress.bzip2.CBZip2OutputStream.mainSort()
1364   17   6    0 org.apache.hadoop.io.compress.bzip2.CBZip2OutputStream.randomiseBlock()
1365   73  16    0 org.apache.hadoop.io.compress.bzip2.CBZip2OutputStream.generateMTFValues()
1366    7   1    0 org.apache.hadoop.io.compress.bzip2.CBZip2OutputStream.Data.Data(int)
1367    2   1    0 org.apache.hadoop.io.compress.bzip2.CRC.CRC()
1368    2   1    0 org.apache.hadoop.io.compress.bzip2.CRC.initialiseCRC()
1369    2   1    0 org.apache.hadoop.io.compress.bzip2.CRC.getFinalCRC()
1370    2   1    0 org.apache.hadoop.io.compress.bzip2.CRC.getGlobalCRC()
1371    2   1    0 org.apache.hadoop.io.compress.bzip2.CRC.setGlobalCRC(int)
1372    5   2    0 org.apache.hadoop.io.compress.bzip2.CRC.updateCRC(int)
1373    6   3    0 org.apache.hadoop.io.compress.bzip2.CRC.updateCRC(int,int)
1374    1   1    1 org.apache.hadoop.io.compress.BZip2Codec.BZip2Codec()
1375    2   1    1 org.apache.hadoop.io.compress.BZip2Codec.createOutputStream(OutputStream)
1376    2   2    1 org.apache.hadoop.io.compress.BZip2Codec.createOutputStream(OutputStream,Compressor)
1377    2   2    1 org.apache.hadoop.io.compress.BZip2Codec.getCompressorType()
1378    2   2    1 org.apache.hadoop.io.compress.BZip2Codec.createCompressor()
1379    2   1    1 org.apache.hadoop.io.compress.BZip2Codec.createInputStream(InputStream)
1380    2   2    1 org.apache.hadoop.io.compress.BZip2Codec.createInputStream(InputStream,Decompressor)
1381    2   2    1 org.apache.hadoop.io.compress.BZip2Codec.getDecompressorType()
1382    2   2    1 org.apache.hadoop.io.compress.BZip2Codec.createDecompressor()
1383    2   1    1 org.apache.hadoop.io.compress.BZip2Codec.getDefaultExtension()
1384    4   1    0 org.apache.hadoop.io.compress.BZip2Codec.BZip2CompressionOutputStream.BZip2CompressionOutputStream(OutputStream)
1385    3   2    0 org.apache.hadoop.io.compress.BZip2Codec.BZip2CompressionOutputStream.writeStreamHeader()
1386    2   1    0 org.apache.hadoop.io.compress.BZip2Codec.BZip2CompressionOutputStream.write(byte[],int,int)
1387    2   1    0 org.apache.hadoop.io.compress.BZip2Codec.BZip2CompressionOutputStream.finish()
1388    1   1    0 org.apache.hadoop.io.compress.BZip2Codec.BZip2CompressionOutputStream.resetState()
1389    2   1    0 org.apache.hadoop.io.compress.BZip2Codec.BZip2CompressionOutputStream.write(int)
1390    3   1    0 org.apache.hadoop.io.compress.BZip2Codec.BZip2CompressionOutputStream.close()
1391    3   2    0 org.apache.hadoop.io.compress.BZip2Codec.BZip2CompressionOutputStream.finalize()
1392    4   1    0 org.apache.hadoop.io.compress.BZip2Codec.BZip2CompressionInputStream.BZip2CompressionInputStream(InputStream)
1393   14   6    0 org.apache.hadoop.io.compress.BZip2Codec.BZip2CompressionInputStream.readStreamHeader()
1394    2   1    0 org.apache.hadoop.io.compress.BZip2Codec.BZip2CompressionInputStream.close()
1395    2   1    0 org.apache.hadoop.io.compress.BZip2Codec.BZip2CompressionInputStream.read(byte[],int,int)
1396    1   1    0 org.apache.hadoop.io.compress.BZip2Codec.BZip2CompressionInputStream.resetState()
1397    2   1    0 org.apache.hadoop.io.compress.BZip2Codec.BZip2CompressionInputStream.read()
1398    3   2    0 org.apache.hadoop.io.compress.BZip2Codec.BZip2CompressionInputStream.finalize()
1399   10   4    0 org.apache.hadoop.io.compress.CodecPool.borrow(Map,Class)
1400    9   3    0 org.apache.hadoop.io.compress.CodecPool.payback(Map,T)
1401    8   2    1 org.apache.hadoop.io.compress.CodecPool.getCompressor(CompressionCodec)
1402    8   2    1 org.apache.hadoop.io.compress.CodecPool.getDecompressor(CompressionCodec)
1403    5   3    1 org.apache.hadoop.io.compress.CodecPool.returnCompressor(Compressor)
1404    5   3    1 org.apache.hadoop.io.compress.CodecPool.returnDecompressor(Decompressor)
1405    1   1    1 org.apache.hadoop.io.compress.CompressionCodec.createOutputStream(OutputStream)
1406    1   1    1 org.apache.hadoop.io.compress.CompressionCodec.createOutputStream(OutputStream,Compressor)
1407    1   1    1 org.apache.hadoop.io.compress.CompressionCodec.getCompressorType()
1408    1   1    1 org.apache.hadoop.io.compress.CompressionCodec.createCompressor()
1409    1   1    1 org.apache.hadoop.io.compress.CompressionCodec.createInputStream(InputStream)
1410    1   1    1 org.apache.hadoop.io.compress.CompressionCodec.createInputStream(InputStream,Decompressor)
1411    1   1    1 org.apache.hadoop.io.compress.CompressionCodec.getDecompressorType()
1412    1   1    1 org.apache.hadoop.io.compress.CompressionCodec.createDecompressor()
1413    1   1    1 org.apache.hadoop.io.compress.CompressionCodec.getDefaultExtension()
1414    3   1    0 org.apache.hadoop.io.compress.CompressionCodecFactory.addCodec(CompressionCodec)
1415   17   3    1 org.apache.hadoop.io.compress.CompressionCodecFactory.toString()
1416   17   9    1 org.apache.hadoop.io.compress.CompressionCodecFactory.getCodecClasses(Configuration)
1417   10   3    1 org.apache.hadoop.io.compress.CompressionCodecFactory.setCodecClasses(Configuration,List)
1418   11   3    1 org.apache.hadoop.io.compress.CompressionCodecFactory.CompressionCodecFactory(Configuration)
1419   11   4    1 org.apache.hadoop.io.compress.CompressionCodecFactory.getCodec(Path)
1420    4   3    1 org.apache.hadoop.io.compress.CompressionCodecFactory.removeSuffix(String,String)
1421   34   8    1 org.apache.hadoop.io.compress.CompressionCodecFactory.main(String[])
1422    2   1    1 org.apache.hadoop.io.compress.CompressionInputStream.CompressionInputStream(InputStream)
1423    2   1    0 org.apache.hadoop.io.compress.CompressionInputStream.close()
1424    1   1    1 org.apache.hadoop.io.compress.CompressionInputStream.read(byte[],int,int)
1425    1   1    1 org.apache.hadoop.io.compress.CompressionInputStream.resetState()
1426    2   1    1 org.apache.hadoop.io.compress.CompressionOutputStream.CompressionOutputStream(OutputStream)
1427    3   1    0 org.apache.hadoop.io.compress.CompressionOutputStream.close()
1428    2   1    0 org.apache.hadoop.io.compress.CompressionOutputStream.flush()
1429    1   1    1 org.apache.hadoop.io.compress.CompressionOutputStream.write(byte[],int,int)
1430    1   1    1 org.apache.hadoop.io.compress.CompressionOutputStream.finish()
1431    1   1    1 org.apache.hadoop.io.compress.CompressionOutputStream.resetState()
1432    1   1    1 org.apache.hadoop.io.compress.Compressor.setInput(byte[],int,int)
1433    1   1    1 org.apache.hadoop.io.compress.Compressor.needsInput()
1434    1   1    1 org.apache.hadoop.io.compress.Compressor.setDictionary(byte[],int,int)
1435    1   1    1 org.apache.hadoop.io.compress.Compressor.getBytesRead()
1436    1   1    1 org.apache.hadoop.io.compress.Compressor.getBytesWritten()
1437    1   1    1 org.apache.hadoop.io.compress.Compressor.finish()
1438    1   1    1 org.apache.hadoop.io.compress.Compressor.finished()
1439    1   1    1 org.apache.hadoop.io.compress.Compressor.compress(byte[],int,int)
1440    1   1    1 org.apache.hadoop.io.compress.Compressor.reset()
1441    1   1    1 org.apache.hadoop.io.compress.Compressor.end()
1442    9   6    0 org.apache.hadoop.io.compress.CompressorStream.CompressorStream(OutputStream,Compressor,int)
1443    2   1    0 org.apache.hadoop.io.compress.CompressorStream.CompressorStream(OutputStream,Compressor)
1444    2   1    1 org.apache.hadoop.io.compress.CompressorStream.CompressorStream(OutputStream)
1445   11   8    0 org.apache.hadoop.io.compress.CompressorStream.write(byte[],int,int)
1446    4   2    0 org.apache.hadoop.io.compress.CompressorStream.compress()
1447    5   3    0 org.apache.hadoop.io.compress.CompressorStream.finish()
1448    2   1    0 org.apache.hadoop.io.compress.CompressorStream.resetState()
1449    5   2    0 org.apache.hadoop.io.compress.CompressorStream.close()
1450    3   1    0 org.apache.hadoop.io.compress.CompressorStream.write(int)
1451    1   1    1 org.apache.hadoop.io.compress.Decompressor.setInput(byte[],int,int)
1452    1   1    1 org.apache.hadoop.io.compress.Decompressor.needsInput()
1453    1   1    1 org.apache.hadoop.io.compress.Decompressor.setDictionary(byte[],int,int)
1454    1   1    1 org.apache.hadoop.io.compress.Decompressor.needsDictionary()
1455    1   1    1 org.apache.hadoop.io.compress.Decompressor.finished()
1456    1   1    1 org.apache.hadoop.io.compress.Decompressor.decompress(byte[],int,int)
1457    1   1    1 org.apache.hadoop.io.compress.Decompressor.reset()
1458    1   1    1 org.apache.hadoop.io.compress.Decompressor.end()
1459    9   6    0 org.apache.hadoop.io.compress.DecompressorStream.DecompressorStream(InputStream,Decompressor,int)
1460    2   1    0 org.apache.hadoop.io.compress.DecompressorStream.DecompressorStream(InputStream,Decompressor)
1461    2   1    1 org.apache.hadoop.io.compress.DecompressorStream.DecompressorStream(InputStream)
1462    3   2    0 org.apache.hadoop.io.compress.DecompressorStream.read()
1463    8   5    0 org.apache.hadoop.io.compress.DecompressorStream.read(byte[],int,int)
1464    9   6    0 org.apache.hadoop.io.compress.DecompressorStream.decompress(byte[],int,int)
1465    6   3    0 org.apache.hadoop.io.compress.DecompressorStream.getCompressedData()
1466    3   3    0 org.apache.hadoop.io.compress.DecompressorStream.checkStream()
1467    2   1    0 org.apache.hadoop.io.compress.DecompressorStream.resetState()
1468   13   5    0 org.apache.hadoop.io.compress.DecompressorStream.skip(long)
1469    3   2    0 org.apache.hadoop.io.compress.DecompressorStream.available()
1470    4   2    0 org.apache.hadoop.io.compress.DecompressorStream.close()
1471    2   1    0 org.apache.hadoop.io.compress.DecompressorStream.markSupported()
1472    1   1    0 org.apache.hadoop.io.compress.DecompressorStream.mark(int)
1473    2   2    0 org.apache.hadoop.io.compress.DecompressorStream.reset()
1474    2   1    0 org.apache.hadoop.io.compress.DefaultCodec.setConf(Configuration)
1475    2   1    0 org.apache.hadoop.io.compress.DefaultCodec.getConf()
1476    2   1    0 org.apache.hadoop.io.compress.DefaultCodec.createOutputStream(OutputStream)
1477    2   1    0 org.apache.hadoop.io.compress.DefaultCodec.createOutputStream(OutputStream,Compressor)
1478    2   1    0 org.apache.hadoop.io.compress.DefaultCodec.getCompressorType()
1479    2   1    0 org.apache.hadoop.io.compress.DefaultCodec.createCompressor()
1480    2   1    0 org.apache.hadoop.io.compress.DefaultCodec.createInputStream(InputStream)
1481    2   1    0 org.apache.hadoop.io.compress.DefaultCodec.createInputStream(InputStream,Decompressor)
1482    2   1    0 org.apache.hadoop.io.compress.DefaultCodec.getDecompressorType()
1483    2   1    0 org.apache.hadoop.io.compress.DefaultCodec.createDecompressor()
1484    2   1    0 org.apache.hadoop.io.compress.DefaultCodec.getDefaultExtension()
1485    2   1    0 org.apache.hadoop.io.compress.GzipCodec.GzipOutputStream.ResetableGZIPOutputStream.ResetableGZIPOutputStream(OutputStream)
1486    2   1    0 org.apache.hadoop.io.compress.GzipCodec.GzipOutputStream.ResetableGZIPOutputStream.resetState()
1487    2   1    0 org.apache.hadoop.io.compress.GzipCodec.GzipOutputStream.GzipOutputStream(OutputStream)
1488    2   1    1 org.apache.hadoop.io.compress.GzipCodec.GzipOutputStream.GzipOutputStream(CompressorStream)
1489    2   1    0 org.apache.hadoop.io.compress.GzipCodec.GzipOutputStream.close()
1490    2   1    0 org.apache.hadoop.io.compress.GzipCodec.GzipOutputStream.flush()
1491    2   1    0 org.apache.hadoop.io.compress.GzipCodec.GzipOutputStream.write(int)
1492    2   1    0 org.apache.hadoop.io.compress.GzipCodec.GzipOutputStream.write(byte[],int,int)
1493    2   1    0 org.apache.hadoop.io.compress.GzipCodec.GzipOutputStream.finish()
1494    2   1    0 org.apache.hadoop.io.compress.GzipCodec.GzipOutputStream.resetState()
1495    2   1    0 org.apache.hadoop.io.compress.GzipCodec.GzipInputStream.ResetableGZIPInputStream.ResetableGZIPInputStream(InputStream)
1496    2   1    0 org.apache.hadoop.io.compress.GzipCodec.GzipInputStream.ResetableGZIPInputStream.resetState()
1497    2   1    0 org.apache.hadoop.io.compress.GzipCodec.GzipInputStream.GzipInputStream(InputStream)
1498    2   1    1 org.apache.hadoop.io.compress.GzipCodec.GzipInputStream.GzipInputStream(DecompressorStream)
1499    2   1    0 org.apache.hadoop.io.compress.GzipCodec.GzipInputStream.available()
1500    2   1    0 org.apache.hadoop.io.compress.GzipCodec.GzipInputStream.close()
1501    2   1    0 org.apache.hadoop.io.compress.GzipCodec.GzipInputStream.read()
1502    2   1    0 org.apache.hadoop.io.compress.GzipCodec.GzipInputStream.read(byte[],int,int)
1503    2   1    0 org.apache.hadoop.io.compress.GzipCodec.GzipInputStream.skip(long)
1504    2   1    0 org.apache.hadoop.io.compress.GzipCodec.GzipInputStream.resetState()
1505    2   2    0 org.apache.hadoop.io.compress.GzipCodec.createOutputStream(OutputStream)
1506    2   2    0 org.apache.hadoop.io.compress.GzipCodec.createOutputStream(OutputStream,Compressor)
1507    2   2    0 org.apache.hadoop.io.compress.GzipCodec.createCompressor()
1508    2   1    0 org.apache.hadoop.io.compress.GzipCodec.getCompressorType()
1509    2   2    0 org.apache.hadoop.io.compress.GzipCodec.createInputStream(InputStream)
1510    2   2    0 org.apache.hadoop.io.compress.GzipCodec.createInputStream(InputStream,Decompressor)
1511    2   2    0 org.apache.hadoop.io.compress.GzipCodec.createDecompressor()
1512    2   1    0 org.apache.hadoop.io.compress.GzipCodec.getDecompressorType()
1513    2   1    0 org.apache.hadoop.io.compress.GzipCodec.getDefaultExtension()
1514    2   1    0 org.apache.hadoop.io.compress.lzo.LzoCompressor.CompressionStrategy.CompressionStrategy(int)
1515    2   1    0 org.apache.hadoop.io.compress.lzo.LzoCompressor.CompressionStrategy.getCompressor()
1516    2   1    1 org.apache.hadoop.io.compress.lzo.LzoCompressor.isNativeLzoLoaded()
1517    8   1    1 org.apache.hadoop.io.compress.lzo.LzoCompressor.LzoCompressor(CompressionStrategy,int)
1518    2   1    1 org.apache.hadoop.io.compress.lzo.LzoCompressor.LzoCompressor()
1519   14   8    0 org.apache.hadoop.io.compress.lzo.LzoCompressor.setInput(byte[],int,int)
1520    8   3    1 org.apache.hadoop.io.compress.lzo.LzoCompressor.setInputFromSavedData()
1521    1   1    0 org.apache.hadoop.io.compress.lzo.LzoCompressor.setDictionary(byte[],int,int)
1522    2   3    1 org.apache.hadoop.io.compress.lzo.LzoCompressor.needsInput()
1523    2   1    0 org.apache.hadoop.io.compress.lzo.LzoCompressor.finish()
1524    2   3    0 org.apache.hadoop.io.compress.lzo.LzoCompressor.finished()
1525   27  13    0 org.apache.hadoop.io.compress.lzo.LzoCompressor.compress(byte[],int,int)
1526    9   1    0 org.apache.hadoop.io.compress.lzo.LzoCompressor.reset()
1527    2   1    1 org.apache.hadoop.io.compress.lzo.LzoCompressor.getBytesRead()
1528    2   1    1 org.apache.hadoop.io.compress.lzo.LzoCompressor.getBytesWritten()
1529    1   1    1 org.apache.hadoop.io.compress.lzo.LzoCompressor.end()
1530    1   1    0 org.apache.hadoop.io.compress.lzo.LzoCompressor.initIDs()
1531    1   1    0 org.apache.hadoop.io.compress.lzo.LzoCompressor.getLzoLibraryVersion()
1532    1   1    0 org.apache.hadoop.io.compress.lzo.LzoCompressor.init(int)
1533    1   1    0 org.apache.hadoop.io.compress.lzo.LzoCompressor.compressBytesDirect(int)
1534    2   1    0 org.apache.hadoop.io.compress.lzo.LzoDecompressor.CompressionStrategy.CompressionStrategy(int)
1535    2   1    0 org.apache.hadoop.io.compress.lzo.LzoDecompressor.CompressionStrategy.getDecompressor()
1536    2   1    1 org.apache.hadoop.io.compress.lzo.LzoDecompressor.isNativeLzoLoaded()
1537    7   1    1 org.apache.hadoop.io.compress.lzo.LzoDecompressor.LzoDecompressor(CompressionStrategy,int)
1538    2   1    1 org.apache.hadoop.io.compress.lzo.LzoDecompressor.LzoDecompressor()
1539   11   7    0 org.apache.hadoop.io.compress.lzo.LzoDecompressor.setInput(byte[],int,int)
1540    8   2    0 org.apache.hadoop.io.compress.lzo.LzoDecompressor.setInputFromSavedData()
1541    1   1    0 org.apache.hadoop.io.compress.lzo.LzoDecompressor.setDictionary(byte[],int,int)
1542    9   6    0 org.apache.hadoop.io.compress.lzo.LzoDecompressor.needsInput()
1543    2   1    0 org.apache.hadoop.io.compress.lzo.LzoDecompressor.needsDictionary()
1544    2   2    0 org.apache.hadoop.io.compress.lzo.LzoDecompressor.finished()
1545   22  12    0 org.apache.hadoop.io.compress.lzo.LzoDecompressor.decompress(byte[],int,int)
1546    6   1    0 org.apache.hadoop.io.compress.lzo.LzoDecompressor.reset()
1547    1   1    0 org.apache.hadoop.io.compress.lzo.LzoDecompressor.end()
1548    2   1    0 org.apache.hadoop.io.compress.lzo.LzoDecompressor.finalize()
1549    1   1    0 org.apache.hadoop.io.compress.lzo.LzoDecompressor.initIDs()
1550    1   1    0 org.apache.hadoop.io.compress.lzo.LzoDecompressor.getLzoLibraryVersion()
1551    1   1    0 org.apache.hadoop.io.compress.lzo.LzoDecompressor.init(int)
1552    1   1    0 org.apache.hadoop.io.compress.lzo.LzoDecompressor.decompressBytesDirect(int)
1553    2   1    0 org.apache.hadoop.io.compress.LzoCodec.setConf(Configuration)
1554    2   1    0 org.apache.hadoop.io.compress.LzoCodec.getConf()
1555    2   2    1 org.apache.hadoop.io.compress.LzoCodec.isNativeLzoLoaded(Configuration)
1556    2   1    0 org.apache.hadoop.io.compress.LzoCodec.createOutputStream(OutputStream)
1557    7   4    0 org.apache.hadoop.io.compress.LzoCodec.createOutputStream(OutputStream,Compressor)
1558    4   3    0 org.apache.hadoop.io.compress.LzoCodec.getCompressorType()
1559    6   3    0 org.apache.hadoop.io.compress.LzoCodec.createCompressor()
1560    2   1    0 org.apache.hadoop.io.compress.LzoCodec.createInputStream(InputStream)
1561    4   3    0 org.apache.hadoop.io.compress.LzoCodec.createInputStream(InputStream,Decompressor)
1562    4   3    0 org.apache.hadoop.io.compress.LzoCodec.getDecompressorType()
1563    6   3    0 org.apache.hadoop.io.compress.LzoCodec.createDecompressor()
1564    2   1    1 org.apache.hadoop.io.compress.LzoCodec.getDefaultExtension()
1565    6   3    0 org.apache.hadoop.io.compress.LzopCodec.createOutputStream(OutputStream,Compressor)
1566    4   3    0 org.apache.hadoop.io.compress.LzopCodec.createInputStream(InputStream,Decompressor)
1567    4   3    0 org.apache.hadoop.io.compress.LzopCodec.createDecompressor()
1568    2   1    0 org.apache.hadoop.io.compress.LzopCodec.getDefaultExtension()
1569    3   1    0 org.apache.hadoop.io.compress.LzopCodec.DChecksum.DChecksum(int,Class)
1570    2   1    0 org.apache.hadoop.io.compress.LzopCodec.DChecksum.getHeaderMask()
1571    2   1    0 org.apache.hadoop.io.compress.LzopCodec.DChecksum.getChecksumClass()
1572    3   1    0 org.apache.hadoop.io.compress.LzopCodec.CChecksum.CChecksum(int,Class)
1573    2   1    0 org.apache.hadoop.io.compress.LzopCodec.CChecksum.getHeaderMask()
1574    2   1    0 org.apache.hadoop.io.compress.LzopCodec.CChecksum.getChecksumClass()
1575   33   5    1 org.apache.hadoop.io.compress.LzopCodec.LzopOutputStream.writeLzopHeader(OutputStream,LzoCompressor.CompressionStrategy)
1576    3   2    0 org.apache.hadoop.io.compress.LzopCodec.LzopOutputStream.LzopOutputStream(OutputStream,Compressor,int,LzoCompressor.CompressionStrategy)
1577    6   2    1 org.apache.hadoop.io.compress.LzopCodec.LzopOutputStream.close()
1578    3   1    0 org.apache.hadoop.io.compress.LzopCodec.LzopInputStream.LzopInputStream(InputStream,Decompressor,int)
1579    8   4    1 org.apache.hadoop.io.compress.LzopCodec.LzopInputStream.readInt(InputStream,byte[],int)
1580    6   1    1 org.apache.hadoop.io.compress.LzopCodec.LzopInputStream.readHeaderItem(InputStream,byte[],int,Adler32,CRC32)
1581   64  35    1 org.apache.hadoop.io.compress.LzopCodec.LzopInputStream.readHeader(InputStream)
1582    8   7    1 org.apache.hadoop.io.compress.LzopCodec.LzopInputStream.verifyChecksums()
1583   19   7    1 org.apache.hadoop.io.compress.LzopCodec.LzopInputStream.getCompressedData()
1584    3   1    0 org.apache.hadoop.io.compress.LzopCodec.LzopInputStream.close()
1585    3   1    1 org.apache.hadoop.io.compress.LzopCodec.LzopDecompressor.LzopDecompressor(int)
1586    9   7    1 org.apache.hadoop.io.compress.LzopCodec.LzopDecompressor.initHeaderFlags(EnumSet,EnumSet)
1587    5   3    1 org.apache.hadoop.io.compress.LzopCodec.LzopDecompressor.resetChecksum()
1588    2   1    1 org.apache.hadoop.io.compress.LzopCodec.LzopDecompressor.verifyDChecksum(DChecksum,int)
1589    2   1    1 org.apache.hadoop.io.compress.LzopCodec.LzopDecompressor.verifyCChecksum(CChecksum,int)
1590    4   2    0 org.apache.hadoop.io.compress.LzopCodec.LzopDecompressor.setInput(byte[],int,int)
1591    6   3    0 org.apache.hadoop.io.compress.LzopCodec.LzopDecompressor.decompress(byte[],int,int)
1592    2   1    0 org.apache.hadoop.io.compress.zlib.BuiltInZlibDeflater.BuiltInZlibDeflater(int,boolean)
1593    2   1    0 org.apache.hadoop.io.compress.zlib.BuiltInZlibDeflater.BuiltInZlibDeflater(int)
1594    2   1    0 org.apache.hadoop.io.compress.zlib.BuiltInZlibDeflater.BuiltInZlibDeflater()
1595    2   1    0 org.apache.hadoop.io.compress.zlib.BuiltInZlibDeflater.compress(byte[],int,int)
1596    2   1    0 org.apache.hadoop.io.compress.zlib.BuiltInZlibInflater.BuiltInZlibInflater(boolean)
1597    2   1    0 org.apache.hadoop.io.compress.zlib.BuiltInZlibInflater.BuiltInZlibInflater()
1598    4   4    0 org.apache.hadoop.io.compress.zlib.BuiltInZlibInflater.decompress(byte[],int,int)
1599    2   1    0 org.apache.hadoop.io.compress.zlib.ZlibCompressor.CompressionLevel.CompressionLevel(int)
1600    2   1    0 org.apache.hadoop.io.compress.zlib.ZlibCompressor.CompressionLevel.compressionLevel()
1601    2   1    0 org.apache.hadoop.io.compress.zlib.ZlibCompressor.CompressionStrategy.CompressionStrategy(int)
1602    2   1    0 org.apache.hadoop.io.compress.zlib.ZlibCompressor.CompressionStrategy.compressionStrategy()
1603    2   1    0 org.apache.hadoop.io.compress.zlib.ZlibCompressor.CompressionHeader.CompressionHeader(int)
1604    2   1    0 org.apache.hadoop.io.compress.zlib.ZlibCompressor.CompressionHeader.windowBits()
1605    2   1    0 org.apache.hadoop.io.compress.zlib.ZlibCompressor.isNativeZlibLoaded()
1606    9   1    1 org.apache.hadoop.io.compress.zlib.ZlibCompressor.ZlibCompressor(CompressionLevel,CompressionStrategy,CompressionHeader,int)
1607    2   1    1 org.apache.hadoop.io.compress.zlib.ZlibCompressor.ZlibCompressor()
1608   11   7    0 org.apache.hadoop.io.compress.zlib.ZlibCompressor.setInput(byte[],int,int)
1609    9   2    0 org.apache.hadoop.io.compress.zlib.ZlibCompressor.setInputFromSavedData()
1610    6   8    0 org.apache.hadoop.io.compress.zlib.ZlibCompressor.setDictionary(byte[],int,int)
1611    9   6    0 org.apache.hadoop.io.compress.zlib.ZlibCompressor.needsInput()
1612    2   1    0 org.apache.hadoop.io.compress.zlib.ZlibCompressor.finish()
1613    2   2    0 org.apache.hadoop.io.compress.zlib.ZlibCompressor.finished()
1614   18   9    0 org.apache.hadoop.io.compress.zlib.ZlibCompressor.compress(byte[],int,int)
1615    3   1    1 org.apache.hadoop.io.compress.zlib.ZlibCompressor.getBytesWritten()
1616    3   1    1 org.apache.hadoop.io.compress.zlib.ZlibCompressor.getBytesRead()
1617   10   1    0 org.apache.hadoop.io.compress.zlib.ZlibCompressor.reset()
1618    4   2    0 org.apache.hadoop.io.compress.zlib.ZlibCompressor.end()
1619    3   3    0 org.apache.hadoop.io.compress.zlib.ZlibCompressor.checkStream()
1620    1   1    0 org.apache.hadoop.io.compress.zlib.ZlibCompressor.initIDs()
1621    1   1    0 org.apache.hadoop.io.compress.zlib.ZlibCompressor.init(int,int,int)
1622    1   1    0 org.apache.hadoop.io.compress.zlib.ZlibCompressor.setDictionary(long,byte[],int,int)
1623    1   1    0 org.apache.hadoop.io.compress.zlib.ZlibCompressor.deflateBytesDirect()
1624    1   1    0 org.apache.hadoop.io.compress.zlib.ZlibCompressor.getBytesRead(long)
1625    1   1    0 org.apache.hadoop.io.compress.zlib.ZlibCompressor.getBytesWritten(long)
1626    1   1    0 org.apache.hadoop.io.compress.zlib.ZlibCompressor.reset(long)
1627    1   1    0 org.apache.hadoop.io.compress.zlib.ZlibCompressor.end(long)
1628    2   1    0 org.apache.hadoop.io.compress.zlib.ZlibDecompressor.CompressionHeader.CompressionHeader(int)
1629    2   1    0 org.apache.hadoop.io.compress.zlib.ZlibDecompressor.CompressionHeader.windowBits()
1630    2   1    0 org.apache.hadoop.io.compress.zlib.ZlibDecompressor.isNativeZlibLoaded()
1631    7   1    1 org.apache.hadoop.io.compress.zlib.ZlibDecompressor.ZlibDecompressor(CompressionHeader,int)
1632    2   1    0 org.apache.hadoop.io.compress.zlib.ZlibDecompressor.ZlibDecompressor()
1633   11   7    0 org.apache.hadoop.io.compress.zlib.ZlibDecompressor.setInput(byte[],int,int)
1634    9   2    0 org.apache.hadoop.io.compress.zlib.ZlibDecompressor.setInputFromSavedData()
1635    7   8    0 org.apache.hadoop.io.compress.zlib.ZlibDecompressor.setDictionary(byte[],int,int)
1636    9   6    0 org.apache.hadoop.io.compress.zlib.ZlibDecompressor.needsInput()
1637    2   1    0 org.apache.hadoop.io.compress.zlib.ZlibDecompressor.needsDictionary()
1638    2   2    0 org.apache.hadoop.io.compress.zlib.ZlibDecompressor.finished()
1639   18   9    0 org.apache.hadoop.io.compress.zlib.ZlibDecompressor.decompress(byte[],int,int)
1640    3   1    1 org.apache.hadoop.io.compress.zlib.ZlibDecompressor.getBytesWritten()
1641    3   1    1 org.apache.hadoop.io.compress.zlib.ZlibDecompressor.getBytesRead()
1642    9   1    0 org.apache.hadoop.io.compress.zlib.ZlibDecompressor.reset()
1643    4   2    0 org.apache.hadoop.io.compress.zlib.ZlibDecompressor.end()
1644    2   1    0 org.apache.hadoop.io.compress.zlib.ZlibDecompressor.finalize()
1645    3   3    0 org.apache.hadoop.io.compress.zlib.ZlibDecompressor.checkStream()
1646    1   1    0 org.apache.hadoop.io.compress.zlib.ZlibDecompressor.initIDs()
1647    1   1    0 org.apache.hadoop.io.compress.zlib.ZlibDecompressor.init(int)
1648    1   1    0 org.apache.hadoop.io.compress.zlib.ZlibDecompressor.setDictionary(long,byte[],int,int)
1649    1   1    0 org.apache.hadoop.io.compress.zlib.ZlibDecompressor.inflateBytesDirect()
1650    1   1    0 org.apache.hadoop.io.compress.zlib.ZlibDecompressor.getBytesRead(long)
1651    1   1    0 org.apache.hadoop.io.compress.zlib.ZlibDecompressor.getBytesWritten(long)
1652    1   1    0 org.apache.hadoop.io.compress.zlib.ZlibDecompressor.reset(long)
1653    1   1    0 org.apache.hadoop.io.compress.zlib.ZlibDecompressor.end(long)
1654    2   2    1 org.apache.hadoop.io.compress.zlib.ZlibFactory.isNativeZlibLoaded(Configuration)
1655    2   2    1 org.apache.hadoop.io.compress.zlib.ZlibFactory.getZlibCompressorType(Configuration)
1656    2   2    1 org.apache.hadoop.io.compress.zlib.ZlibFactory.getZlibCompressor(Configuration)
1657    2   2    1 org.apache.hadoop.io.compress.zlib.ZlibFactory.getZlibDecompressorType(Configuration)
1658    2   2    1 org.apache.hadoop.io.compress.zlib.ZlibFactory.getZlibDecompressor(Configuration)
1659    1   1    0 org.apache.hadoop.io.CompressedWritable.CompressedWritable()
1660    3   1    0 org.apache.hadoop.io.CompressedWritable.readFields(DataInput)
1661    8   4    1 org.apache.hadoop.io.CompressedWritable.ensureInflated()
1662    1   1    1 org.apache.hadoop.io.CompressedWritable.readFieldsCompressed(DataInput)
1663   11   2    0 org.apache.hadoop.io.CompressedWritable.write(DataOutput)
1664    1   1    1 org.apache.hadoop.io.CompressedWritable.writeCompressed(DataOutput)
1665    2   1    0 org.apache.hadoop.io.DataInputBuffer.Buffer.Buffer()
1666    5   1    0 org.apache.hadoop.io.DataInputBuffer.Buffer.reset(byte[],int,int)
1667    2   1    0 org.apache.hadoop.io.DataInputBuffer.Buffer.getData()
1668    2   1    0 org.apache.hadoop.io.DataInputBuffer.Buffer.getPosition()
1669    2   1    0 org.apache.hadoop.io.DataInputBuffer.Buffer.getLength()
1670    2   1    1 org.apache.hadoop.io.DataInputBuffer.DataInputBuffer()
1671    3   1    0 org.apache.hadoop.io.DataInputBuffer.DataInputBuffer(Buffer)
1672    2   1    1 org.apache.hadoop.io.DataInputBuffer.reset(byte[],int)
1673    2   1    1 org.apache.hadoop.io.DataInputBuffer.reset(byte[],int,int)
1674    2   1    0 org.apache.hadoop.io.DataInputBuffer.getData()
1675    2   1    1 org.apache.hadoop.io.DataInputBuffer.getPosition()
1676    2   1    1 org.apache.hadoop.io.DataInputBuffer.getLength()
1677    2   1    0 org.apache.hadoop.io.DataOutputBuffer.Buffer.getData()
1678    2   1    0 org.apache.hadoop.io.DataOutputBuffer.Buffer.getLength()
1679    2   1    0 org.apache.hadoop.io.DataOutputBuffer.Buffer.Buffer()
1680    2   1    0 org.apache.hadoop.io.DataOutputBuffer.Buffer.Buffer(int)
1681    8   2    0 org.apache.hadoop.io.DataOutputBuffer.Buffer.write(DataInput,int)
1682    2   1    1 org.apache.hadoop.io.DataOutputBuffer.DataOutputBuffer()
1683    2   1    0 org.apache.hadoop.io.DataOutputBuffer.DataOutputBuffer(int)
1684    3   1    0 org.apache.hadoop.io.DataOutputBuffer.DataOutputBuffer(Buffer)
1685    2   1    1 org.apache.hadoop.io.DataOutputBuffer.getData()
1686    2   1    1 org.apache.hadoop.io.DataOutputBuffer.getLength()
1687    4   1    1 org.apache.hadoop.io.DataOutputBuffer.reset()
1688    2   1    1 org.apache.hadoop.io.DataOutputBuffer.write(DataInput,int)
1689    2   1    1 org.apache.hadoop.io.DataOutputBuffer.writeTo(OutputStream)
1690   10   3    0 org.apache.hadoop.io.DefaultStringifier.DefaultStringifier(Configuration,Class)
1691    7   4    0 org.apache.hadoop.io.DefaultStringifier.fromString(String)
1692    6   1    0 org.apache.hadoop.io.DefaultStringifier.toString(T)
1693    5   1    0 org.apache.hadoop.io.DefaultStringifier.close()
1694    4   1    1 org.apache.hadoop.io.DefaultStringifier.store(Configuration,K,String)
1695    6   2    1 org.apache.hadoop.io.DefaultStringifier.load(Configuration,String,Class)
1696    8   2    1 org.apache.hadoop.io.DefaultStringifier.storeArray(Configuration,K[],String)
1697   11   4    1 org.apache.hadoop.io.DefaultStringifier.loadArray(Configuration,String,Class)
1698    1   1    0 org.apache.hadoop.io.DoubleWritable.DoubleWritable()
1699    2   1    0 org.apache.hadoop.io.DoubleWritable.DoubleWritable(double)
1700    2   1    0 org.apache.hadoop.io.DoubleWritable.readFields(DataInput)
1701    2   1    0 org.apache.hadoop.io.DoubleWritable.write(DataOutput)
1702    2   1    0 org.apache.hadoop.io.DoubleWritable.set(double)
1703    2   1    0 org.apache.hadoop.io.DoubleWritable.get()
1704    5   3    1 org.apache.hadoop.io.DoubleWritable.equals(Object)
1705    2   1    0 org.apache.hadoop.io.DoubleWritable.hashCode()
1706    3   3    0 org.apache.hadoop.io.DoubleWritable.compareTo(Object)
1707    2   1    0 org.apache.hadoop.io.DoubleWritable.toString()
1708    2   1    0 org.apache.hadoop.io.DoubleWritable.Comparator.Comparator()
1709    4   3    0 org.apache.hadoop.io.DoubleWritable.Comparator.compare(byte[],int,int,byte[],int,int)
1710    1   1    0 org.apache.hadoop.io.FloatWritable.FloatWritable()
1711    2   1    0 org.apache.hadoop.io.FloatWritable.FloatWritable(float)
1712    2   1    1 org.apache.hadoop.io.FloatWritable.set(float)
1713    2   1    1 org.apache.hadoop.io.FloatWritable.get()
1714    2   1    0 org.apache.hadoop.io.FloatWritable.readFields(DataInput)
1715    2   1    0 org.apache.hadoop.io.FloatWritable.write(DataOutput)
1716    5   3    1 org.apache.hadoop.io.FloatWritable.equals(Object)
1717    2   1    0 org.apache.hadoop.io.FloatWritable.hashCode()
1718    4   3    1 org.apache.hadoop.io.FloatWritable.compareTo(Object)
1719    2   1    0 org.apache.hadoop.io.FloatWritable.toString()
1720    2   1    0 org.apache.hadoop.io.FloatWritable.Comparator.Comparator()
1721    4   3    0 org.apache.hadoop.io.FloatWritable.Comparator.compare(byte[],int,int,byte[],int,int)
1722   10   5    1 org.apache.hadoop.io.GenericWritable.set(Writable)
1723    2   1    1 org.apache.hadoop.io.GenericWritable.get()
1724    2   2    0 org.apache.hadoop.io.GenericWritable.toString()
1725    8   3    0 org.apache.hadoop.io.GenericWritable.readFields(DataInput)
1726    5   4    0 org.apache.hadoop.io.GenericWritable.write(DataOutput)
1727    1   1    1 org.apache.hadoop.io.GenericWritable.getTypes()
1728    2   1    0 org.apache.hadoop.io.GenericWritable.getConf()
1729    2   1    0 org.apache.hadoop.io.GenericWritable.setConf(Configuration)
1730    2   1    0 org.apache.hadoop.io.InputBuffer.Buffer.Buffer()
1731    5   1    0 org.apache.hadoop.io.InputBuffer.Buffer.reset(byte[],int,int)
1732    2   1    0 org.apache.hadoop.io.InputBuffer.Buffer.getPosition()
1733    2   1    0 org.apache.hadoop.io.InputBuffer.Buffer.getLength()
1734    2   1    1 org.apache.hadoop.io.InputBuffer.InputBuffer()
1735    3   1    0 org.apache.hadoop.io.InputBuffer.InputBuffer(Buffer)
1736    2   1    1 org.apache.hadoop.io.InputBuffer.reset(byte[],int)
1737    2   1    1 org.apache.hadoop.io.InputBuffer.reset(byte[],int,int)
1738    2   1    1 org.apache.hadoop.io.InputBuffer.getPosition()
1739    2   1    1 org.apache.hadoop.io.InputBuffer.getLength()
1740    1   1    0 org.apache.hadoop.io.IntWritable.IntWritable()
1741    2   1    0 org.apache.hadoop.io.IntWritable.IntWritable(int)
1742    2   1    1 org.apache.hadoop.io.IntWritable.set(int)
1743    2   1    1 org.apache.hadoop.io.IntWritable.get()
1744    2   1    0 org.apache.hadoop.io.IntWritable.readFields(DataInput)
1745    2   1    0 org.apache.hadoop.io.IntWritable.write(DataOutput)
1746    5   3    1 org.apache.hadoop.io.IntWritable.equals(Object)
1747    2   1    0 org.apache.hadoop.io.IntWritable.hashCode()
1748    4   3    1 org.apache.hadoop.io.IntWritable.compareTo(Object)
1749    2   1    0 org.apache.hadoop.io.IntWritable.toString()
1750    2   1    0 org.apache.hadoop.io.IntWritable.Comparator.Comparator()
1751    4   3    0 org.apache.hadoop.io.IntWritable.Comparator.compare(byte[],int,int,byte[],int,int)
1752    1   1    0 org.apache.hadoop.io.LongWritable.LongWritable()
1753    2   1    0 org.apache.hadoop.io.LongWritable.LongWritable(long)
1754    2   1    1 org.apache.hadoop.io.LongWritable.set(long)
1755    2   1    1 org.apache.hadoop.io.LongWritable.get()
1756    2   1    0 org.apache.hadoop.io.LongWritable.readFields(DataInput)
1757    2   1    0 org.apache.hadoop.io.LongWritable.write(DataOutput)
1758    5   3    1 org.apache.hadoop.io.LongWritable.equals(Object)
1759    2   1    0 org.apache.hadoop.io.LongWritable.hashCode()
1760    4   3    1 org.apache.hadoop.io.LongWritable.compareTo(Object)
1761    2   1    0 org.apache.hadoop.io.LongWritable.toString()
1762    2   1    0 org.apache.hadoop.io.LongWritable.Comparator.Comparator()
1763    4   3    0 org.apache.hadoop.io.LongWritable.Comparator.compare(byte[],int,int,byte[],int,int)
1764    2   1    0 org.apache.hadoop.io.LongWritable.DecreasingComparator.compare(WritableComparable,WritableComparable)
1765    2   1    0 org.apache.hadoop.io.LongWritable.DecreasingComparator.compare(byte[],int,int,byte[],int,int)
1766    1   1    0 org.apache.hadoop.io.MapFile.MapFile()
1767    2   1    1 org.apache.hadoop.io.MapFile.Writer.Writer(Configuration,FileSystem,String,Class,Class)
1768    2   1    1 org.apache.hadoop.io.MapFile.Writer.Writer(Configuration,FileSystem,String,Class,Class,CompressionType,Progressable)
1769    2   1    1 org.apache.hadoop.io.MapFile.Writer.Writer(Configuration,FileSystem,String,Class,Class,CompressionType,CompressionCodec,Progressable)
1770    2   1    1 org.apache.hadoop.io.MapFile.Writer.Writer(Configuration,FileSystem,String,Class,Class,CompressionType)
1771    2   1    1 org.apache.hadoop.io.MapFile.Writer.Writer(Configuration,FileSystem,String,WritableComparator,Class)
1772    2   1    1 org.apache.hadoop.io.MapFile.Writer.Writer(Configuration,FileSystem,String,WritableComparator,Class,SequenceFile.CompressionType)
1773    2   1    1 org.apache.hadoop.io.MapFile.Writer.Writer(Configuration,FileSystem,String,WritableComparator,Class,SequenceFile.CompressionType,Progressable)
1774   12   3    1 org.apache.hadoop.io.MapFile.Writer.Writer(Configuration,FileSystem,String,WritableComparator,Class,SequenceFile.CompressionType,CompressionCodec,Progressable)
1775    2   1    1 org.apache.hadoop.io.MapFile.Writer.getIndexInterval()
1776    2   1    1 org.apache.hadoop.io.MapFile.Writer.setIndexInterval(int)
1777    2   1    1 org.apache.hadoop.io.MapFile.Writer.setIndexInterval(Configuration,int)
1778    3   1    1 org.apache.hadoop.io.MapFile.Writer.close()
1779    7   2    1 org.apache.hadoop.io.MapFile.Writer.append(WritableComparable,Writable)
1780    7   4    0 org.apache.hadoop.io.MapFile.Writer.checkKey(WritableComparable)
1781    2   1    1 org.apache.hadoop.io.MapFile.Reader.getKeyClass()
1782    2   1    1 org.apache.hadoop.io.MapFile.Reader.getValueClass()
1783    3   1    1 org.apache.hadoop.io.MapFile.Reader.Reader(FileSystem,String,Configuration)
1784    2   1    1 org.apache.hadoop.io.MapFile.Reader.Reader(FileSystem,String,WritableComparator,Configuration)
1785    3   2    1 org.apache.hadoop.io.MapFile.Reader.Reader(FileSystem,String,WritableComparator,Configuration,boolean)
1786   11   2    0 org.apache.hadoop.io.MapFile.Reader.open(FileSystem,String,WritableComparator,Configuration)
1787    2   1    1 org.apache.hadoop.io.MapFile.Reader.createDataFileReader(FileSystem,Path,Configuration)
1788   37  11    0 org.apache.hadoop.io.MapFile.Reader.readIndex()
1789    2   1    1 org.apache.hadoop.io.MapFile.Reader.reset()
1790    6   3    1 org.apache.hadoop.io.MapFile.Reader.midKey()
1791   10   3    1 org.apache.hadoop.io.MapFile.Reader.finalKey(WritableComparable)
1792    2   1    1 org.apache.hadoop.io.MapFile.Reader.seek(WritableComparable)
1793    2   1    1 org.apache.hadoop.io.MapFile.Reader.seekInternal(WritableComparable)
1794   31  16    1 org.apache.hadoop.io.MapFile.Reader.seekInternal(WritableComparable,boolean)
1795   15   5    0 org.apache.hadoop.io.MapFile.Reader.binarySearch(WritableComparable)
1796    2   1    1 org.apache.hadoop.io.MapFile.Reader.next(WritableComparable,Writable)
1797    6   3    1 org.apache.hadoop.io.MapFile.Reader.get(WritableComparable,Writable)
1798    2   1    1 org.apache.hadoop.io.MapFile.Reader.getClosest(WritableComparable,Writable)
1799    6   6    1 org.apache.hadoop.io.MapFile.Reader.getClosest(WritableComparable,Writable,boolean)
1800    4   2    0 org.apache.hadoop.io.MapFile.Reader.close()
1801    5   3    0 org.apache.hadoop.io.MapFile.rename(FileSystem,String,String)
1802    7   1    1 org.apache.hadoop.io.MapFile.delete(FileSystem,String)
1803   34  16    1 org.apache.hadoop.io.MapFile.fix(FileSystem,Path,Class,Class,boolean,Configuration)
1804   16   3    0 org.apache.hadoop.io.MapFile.main(String[])
1805    3   1    1 org.apache.hadoop.io.MapWritable.MapWritable()
1806    3   1    1 org.apache.hadoop.io.MapWritable.MapWritable(MapWritable)
1807    2   1    1 org.apache.hadoop.io.MapWritable.clear()
1808    2   1    1 org.apache.hadoop.io.MapWritable.containsKey(Object)
1809    2   1    1 org.apache.hadoop.io.MapWritable.containsValue(Object)
1810    2   1    1 org.apache.hadoop.io.MapWritable.entrySet()
1811    2   1    1 org.apache.hadoop.io.MapWritable.get(Object)
1812    2   1    1 org.apache.hadoop.io.MapWritable.isEmpty()
1813    2   1    1 org.apache.hadoop.io.MapWritable.keySet()
1814    4   1    0 org.apache.hadoop.io.MapWritable.put(Writable,Writable)
1815    3   2    1 org.apache.hadoop.io.MapWritable.putAll(Map)
1816    2   1    1 org.apache.hadoop.io.MapWritable.remove(Object)
1817    2   1    1 org.apache.hadoop.io.MapWritable.size()
1818    2   1    1 org.apache.hadoop.io.MapWritable.values()
1819    8   2    0 org.apache.hadoop.io.MapWritable.write(DataOutput)
1820   10   2    0 org.apache.hadoop.io.MapWritable.readFields(DataInput)
1821    4   4    0 org.apache.hadoop.io.MD5Hash.ThreadLocal$1.initialValue()
1822    2   1    1 org.apache.hadoop.io.MD5Hash.MD5Hash()
1823    2   1    1 org.apache.hadoop.io.MD5Hash.MD5Hash(String)
1824    4   3    1 org.apache.hadoop.io.MD5Hash.MD5Hash(byte[])
1825    2   1    0 org.apache.hadoop.io.MD5Hash.readFields(DataInput)
1826    4   1    1 org.apache.hadoop.io.MD5Hash.read(DataInput)
1827    2   1    0 org.apache.hadoop.io.MD5Hash.write(DataOutput)
1828    2   1    1 org.apache.hadoop.io.MD5Hash.set(MD5Hash)
1829    2   1    1 org.apache.hadoop.io.MD5Hash.getDigest()
1830    2   1    1 org.apache.hadoop.io.MD5Hash.digest(byte[])
1831    6   2    1 org.apache.hadoop.io.MD5Hash.digest(InputStream)
1832    6   1    1 org.apache.hadoop.io.MD5Hash.digest(byte[],int,int)
1833    2   1    1 org.apache.hadoop.io.MD5Hash.digest(String)
1834    2   1    1 org.apache.hadoop.io.MD5Hash.digest(UTF8)
1835    5   2    1 org.apache.hadoop.io.MD5Hash.halfDigest()
1836    5   2    1 org.apache.hadoop.io.MD5Hash.quarterDigest()
1837    5   3    1 org.apache.hadoop.io.MD5Hash.equals(Object)
1838    2   1    1 org.apache.hadoop.io.MD5Hash.hashCode()
1839    2   1    1 org.apache.hadoop.io.MD5Hash.compareTo(MD5Hash)
1840    2   1    0 org.apache.hadoop.io.MD5Hash.Comparator.Comparator()
1841    2   1    0 org.apache.hadoop.io.MD5Hash.Comparator.compare(byte[],int,int,byte[],int,int)
1842    7   2    1 org.apache.hadoop.io.MD5Hash.toString()
1843    8   4    1 org.apache.hadoop.io.MD5Hash.setDigest(String)
1844   11  11    0 org.apache.hadoop.io.MD5Hash.charToNibble(char)
1845    3   1    1 org.apache.hadoop.io.MultipleIOException.MultipleIOException(List)
1846    2   1    1 org.apache.hadoop.io.MultipleIOException.getExceptions()
1847    6   6    1 org.apache.hadoop.io.MultipleIOException.createIOException(List)
1848    1   1    0 org.apache.hadoop.io.NullWritable.NullWritable()
1849    2   1    1 org.apache.hadoop.io.NullWritable.get()
1850    2   1    0 org.apache.hadoop.io.NullWritable.toString()
1851    2   1    0 org.apache.hadoop.io.NullWritable.hashCode()
1852    4   3    0 org.apache.hadoop.io.NullWritable.compareTo(Object)
1853    2   1    0 org.apache.hadoop.io.NullWritable.equals(Object)
1854    1   1    0 org.apache.hadoop.io.NullWritable.readFields(DataInput)
1855    1   1    0 org.apache.hadoop.io.NullWritable.write(DataOutput)
1856    2   1    0 org.apache.hadoop.io.NullWritable.Comparator.Comparator()
1857    4   1    1 org.apache.hadoop.io.NullWritable.Comparator.compare(byte[],int,int,byte[],int,int)
1858    1   1    0 org.apache.hadoop.io.ObjectWritable.ObjectWritable()
1859    2   1    0 org.apache.hadoop.io.ObjectWritable.ObjectWritable(Object)
1860    3   1    0 org.apache.hadoop.io.ObjectWritable.ObjectWritable(Class,Object)
1861    2   1    1 org.apache.hadoop.io.ObjectWritable.get()
1862    2   1    1 org.apache.hadoop.io.ObjectWritable.getDeclaredClass()
1863    3   1    1 org.apache.hadoop.io.ObjectWritable.set(Object)
1864    2   1    0 org.apache.hadoop.io.ObjectWritable.toString()
1865    2   1    0 org.apache.hadoop.io.ObjectWritable.readFields(DataInput)
1866    2   1    0 org.apache.hadoop.io.ObjectWritable.write(DataOutput)
1867    2   1    0 org.apache.hadoop.io.ObjectWritable.NullInstance.NullInstance()
1868    3   1    0 org.apache.hadoop.io.ObjectWritable.NullInstance.NullInstance(Class,Configuration)
1869    7   4    0 org.apache.hadoop.io.ObjectWritable.NullInstance.readFields(DataInput)
1870    2   1    0 org.apache.hadoop.io.ObjectWritable.NullInstance.write(DataOutput)
1871   51  19    1 org.apache.hadoop.io.ObjectWritable.writeObject(DataOutput,Object,Class,Configuration)
1872    2   1    1 org.apache.hadoop.io.ObjectWritable.readObject(DataInput,Configuration)
1873   66  23    0 org.apache.hadoop.io.ObjectWritable.readObject(DataInput,ObjectWritable,Configuration)
1874    2   1    0 org.apache.hadoop.io.ObjectWritable.setConf(Configuration)
1875    2   1    0 org.apache.hadoop.io.ObjectWritable.getConf()
1876    2   1    0 org.apache.hadoop.io.OutputBuffer.Buffer.getData()
1877    2   1    0 org.apache.hadoop.io.OutputBuffer.Buffer.getLength()
1878    2   1    0 org.apache.hadoop.io.OutputBuffer.Buffer.reset()
1879    8   2    0 org.apache.hadoop.io.OutputBuffer.Buffer.write(InputStream,int)
1880    2   1    1 org.apache.hadoop.io.OutputBuffer.OutputBuffer()
1881    3   1    0 org.apache.hadoop.io.OutputBuffer.OutputBuffer(Buffer)
1882    2   1    1 org.apache.hadoop.io.OutputBuffer.getData()
1883    2   1    1 org.apache.hadoop.io.OutputBuffer.getLength()
1884    3   1    1 org.apache.hadoop.io.OutputBuffer.reset()
1885    2   1    1 org.apache.hadoop.io.OutputBuffer.write(InputStream,int)
1886    1   1    0 org.apache.hadoop.io.RawComparator.compare(byte[],int,int,byte[],int,int)
1887    4   1    0 org.apache.hadoop.io.retry.RetryInvocationHandler.RetryInvocationHandler(Object,RetryPolicy)
1888    4   1    0 org.apache.hadoop.io.retry.RetryInvocationHandler.RetryInvocationHandler(Object,Map)
1889   14   9    0 org.apache.hadoop.io.retry.RetryInvocationHandler.invoke(Object,Method,Object[])
1890    6   5    0 org.apache.hadoop.io.retry.RetryInvocationHandler.invokeMethod(Method,Object[])
1891    2   1    1 org.apache.hadoop.io.retry.RetryPolicies.retryUpToMaximumCountWithFixedSleep(int,long,TimeUnit)
1892    2   1    1 org.apache.hadoop.io.retry.RetryPolicies.retryUpToMaximumTimeWithFixedSleep(long,long,TimeUnit)
1893    2   1    1 org.apache.hadoop.io.retry.RetryPolicies.retryUpToMaximumCountWithProportionalSleep(int,long,TimeUnit)
1894    2   1    1 org.apache.hadoop.io.retry.RetryPolicies.exponentialBackoffRetry(int,long,TimeUnit)
1895    2   1    1 org.apache.hadoop.io.retry.RetryPolicies.retryByException(RetryPolicy,Map)
1896    2   1    1 org.apache.hadoop.io.retry.RetryPolicies.retryByRemoteException(RetryPolicy,Map)
1897    2   2    0 org.apache.hadoop.io.retry.RetryPolicies.TryOnceThenFail.shouldRetry(Exception,int)
1898    2   1    0 org.apache.hadoop.io.retry.RetryPolicies.TryOnceDontFail.shouldRetry(Exception,int)
1899    2   1    0 org.apache.hadoop.io.retry.RetryPolicies.RetryForever.shouldRetry(Exception,int)
1900    4   1    0 org.apache.hadoop.io.retry.RetryPolicies.RetryLimited.RetryLimited(int,long,TimeUnit)
1901    6   4    0 org.apache.hadoop.io.retry.RetryPolicies.RetryLimited.shouldRetry(Exception,int)
1902    1   1    0 org.apache.hadoop.io.retry.RetryPolicies.RetryLimited.calculateSleepTime(int)
1903    2   1    0 org.apache.hadoop.io.retry.RetryPolicies.RetryUpToMaximumCountWithFixedSleep.RetryUpToMaximumCountWithFixedSleep(int,long,TimeUnit)
1904    2   1    0 org.apache.hadoop.io.retry.RetryPolicies.RetryUpToMaximumCountWithFixedSleep.calculateSleepTime(int)
1905    2   1    0 org.apache.hadoop.io.retry.RetryPolicies.RetryUpToMaximumTimeWithFixedSleep.RetryUpToMaximumTimeWithFixedSleep(long,long,TimeUnit)
1906    2   1    0 org.apache.hadoop.io.retry.RetryPolicies.RetryUpToMaximumCountWithProportionalSleep.RetryUpToMaximumCountWithProportionalSleep(int,long,TimeUnit)
1907    2   1    0 org.apache.hadoop.io.retry.RetryPolicies.RetryUpToMaximumCountWithProportionalSleep.calculateSleepTime(int)
1908    3   1    0 org.apache.hadoop.io.retry.RetryPolicies.ExceptionDependentRetry.ExceptionDependentRetry(RetryPolicy,Map)
1909    5   2    0 org.apache.hadoop.io.retry.RetryPolicies.ExceptionDependentRetry.shouldRetry(Exception,int)
1910    5   2    0 org.apache.hadoop.io.retry.RetryPolicies.RemoteExceptionDependentRetry.RemoteExceptionDependentRetry(RetryPolicy,Map)
1911    7   3    0 org.apache.hadoop.io.retry.RetryPolicies.RemoteExceptionDependentRetry.shouldRetry(Exception,int)
1912    2   1    0 org.apache.hadoop.io.retry.RetryPolicies.ExponentialBackoffRetry.ExponentialBackoffRetry(int,long,TimeUnit)
1913    2   1    0 org.apache.hadoop.io.retry.RetryPolicies.ExponentialBackoffRetry.calculateSleepTime(int)
1914    1   1    1 org.apache.hadoop.io.retry.RetryPolicy.shouldRetry(Exception,int)
1915    2   1    1 org.apache.hadoop.io.retry.RetryProxy.create(Class,Object,RetryPolicy)
1916    2   1    1 org.apache.hadoop.io.retry.RetryProxy.create(Class,Object,Map)
1917    1   1    0 org.apache.hadoop.io.SequenceFile.SequenceFile()
1918    3   2    0 org.apache.hadoop.io.SequenceFile.getCompressionType(Configuration)
1919    2   1    0 org.apache.hadoop.io.SequenceFile.setCompressionType(Configuration,CompressionType)
1920    2   1    1 org.apache.hadoop.io.SequenceFile.createWriter(FileSystem,Configuration,Path,Class,Class)
1921    2   1    1 org.apache.hadoop.io.SequenceFile.createWriter(FileSystem,Configuration,Path,Class,Class,CompressionType)
1922    2   1    1 org.apache.hadoop.io.SequenceFile.createWriter(FileSystem,Configuration,Path,Class,Class,CompressionType,Progressable)
1923    2   1    1 org.apache.hadoop.io.SequenceFile.createWriter(FileSystem,Configuration,Path,Class,Class,CompressionType,CompressionCodec)
1924    2   1    1 org.apache.hadoop.io.SequenceFile.createWriter(FileSystem,Configuration,Path,Class,Class,CompressionType,CompressionCodec,Progressable,Metadata)
1925   13   8    1 org.apache.hadoop.io.SequenceFile.createWriter(FileSystem,Configuration,Path,Class,Class,int,short,long,CompressionType,CompressionCodec,Progressable,Metadata)
1926    3   1    1 org.apache.hadoop.io.SequenceFile.createWriter(FileSystem,Configuration,Path,Class,Class,CompressionType,CompressionCodec,Progressable)
1927   12   9    1 org.apache.hadoop.io.SequenceFile.createWriter(Configuration,FSDataOutputStream,Class,Class,boolean,boolean,CompressionCodec,Metadata)
1928   12   9    1 org.apache.hadoop.io.SequenceFile.createWriter(FileSystem,Configuration,Path,Class,Class,boolean,boolean,CompressionCodec,Progressable,Metadata)
1929   13   8    1 org.apache.hadoop.io.SequenceFile.createWriter(Configuration,FSDataOutputStream,Class,Class,CompressionType,CompressionCodec,Metadata)
1930    3   1    1 org.apache.hadoop.io.SequenceFile.createWriter(Configuration,FSDataOutputStream,Class,Class,CompressionType,CompressionCodec)
1931    1   1    1 org.apache.hadoop.io.SequenceFile.ValueBytes.writeUncompressedBytes(DataOutputStream)
1932    1   1    1 org.apache.hadoop.io.SequenceFile.ValueBytes.writeCompressedBytes(DataOutputStream)
1933    1   1    1 org.apache.hadoop.io.SequenceFile.ValueBytes.getSize()
1934    3   1    0 org.apache.hadoop.io.SequenceFile.UncompressedBytes.UncompressedBytes()
1935    5   1    0 org.apache.hadoop.io.SequenceFile.UncompressedBytes.reset(DataInputStream,int)
1936    2   1    0 org.apache.hadoop.io.SequenceFile.UncompressedBytes.getSize()
1937    2   1    0 org.apache.hadoop.io.SequenceFile.UncompressedBytes.writeUncompressedBytes(DataOutputStream)
1938    2   2    0 org.apache.hadoop.io.SequenceFile.UncompressedBytes.writeCompressedBytes(DataOutputStream)
1939    4   1    0 org.apache.hadoop.io.SequenceFile.CompressedBytes.CompressedBytes(CompressionCodec)
1940    5   1    0 org.apache.hadoop.io.SequenceFile.CompressedBytes.reset(DataInputStream,int)
1941    2   1    0 org.apache.hadoop.io.SequenceFile.CompressedBytes.getSize()
1942   11   3    0 org.apache.hadoop.io.SequenceFile.CompressedBytes.writeUncompressedBytes(DataOutputStream)
1943    2   1    0 org.apache.hadoop.io.SequenceFile.CompressedBytes.writeCompressedBytes(DataOutputStream)
1944    2   1    0 org.apache.hadoop.io.SequenceFile.Metadata.Metadata()
1945    5   2    0 org.apache.hadoop.io.SequenceFile.Metadata.Metadata(TreeMap)
1946    2   1    0 org.apache.hadoop.io.SequenceFile.Metadata.get(Text)
1947    2   1    0 org.apache.hadoop.io.SequenceFile.Metadata.set(Text,Text)
1948    2   1    0 org.apache.hadoop.io.SequenceFile.Metadata.getMetadata()
1949    7   2    0 org.apache.hadoop.io.SequenceFile.Metadata.write(DataOutput)
1950   11   4    0 org.apache.hadoop.io.SequenceFile.Metadata.readFields(DataInput)
1951   17  14    0 org.apache.hadoop.io.SequenceFile.Metadata.equals(Metadata)
1952    3   1    0 org.apache.hadoop.io.SequenceFile.Metadata.hashCode()
1953    9   2    0 org.apache.hadoop.io.SequenceFile.Metadata.toString()
1954    1   1    1 org.apache.hadoop.io.SequenceFile.Writer.Writer()
1955    2   1    1 org.apache.hadoop.io.SequenceFile.Writer.Writer(FileSystem,Configuration,Path,Class,Class)
1956    2   1    1 org.apache.hadoop.io.SequenceFile.Writer.Writer(FileSystem,Configuration,Path,Class,Class,Progressable,Metadata)
1957    5   1    1 org.apache.hadoop.io.SequenceFile.Writer.Writer(FileSystem,Configuration,Path,Class,Class,int,short,long,Progressable,Metadata)
1958    6   1    1 org.apache.hadoop.io.SequenceFile.Writer.Writer(Configuration,FSDataOutputStream,Class,Class,Metadata)
1959    2   1    1 org.apache.hadoop.io.SequenceFile.Writer.initializeFileHeader()
1960    3   1    1 org.apache.hadoop.io.SequenceFile.Writer.finalizeFileHeader()
1961    2   1    0 org.apache.hadoop.io.SequenceFile.Writer.isCompressed()
1962    2   1    0 org.apache.hadoop.io.SequenceFile.Writer.isBlockCompressed()
1963    8   2    1 org.apache.hadoop.io.SequenceFile.Writer.writeFileHeader()
1964   20   2    0 org.apache.hadoop.io.SequenceFile.Writer.init(Path,Configuration,FSDataOutputStream,Class,Class,boolean,CompressionCodec,Metadata)
1965    2   1    1 org.apache.hadoop.io.SequenceFile.Writer.getKeyClass()
1966    2   1    1 org.apache.hadoop.io.SequenceFile.Writer.getValueClass()
1967    2   1    1 org.apache.hadoop.io.SequenceFile.Writer.getCompressionCodec()
1968    5   3    1 org.apache.hadoop.io.SequenceFile.Writer.sync()
1969    2   1    1 org.apache.hadoop.io.SequenceFile.Writer.getConf()
1970   13   4    1 org.apache.hadoop.io.SequenceFile.Writer.close()
1971    3   3    0 org.apache.hadoop.io.SequenceFile.Writer.checkAndWriteSync()
1972    2   1    1 org.apache.hadoop.io.SequenceFile.Writer.append(Writable,Writable)
1973   21   8    0 org.apache.hadoop.io.SequenceFile.Writer.append(Object,Object)
1974    9   3    0 org.apache.hadoop.io.SequenceFile.Writer.appendRaw(byte[],int,int,ValueBytes)
1975    2   1    1 org.apache.hadoop.io.SequenceFile.Writer.getLength()
1976    2   1    1 org.apache.hadoop.io.SequenceFile.RecordCompressWriter.RecordCompressWriter(FileSystem,Configuration,Path,Class,Class,CompressionCodec)
1977    2   1    1 org.apache.hadoop.io.SequenceFile.RecordCompressWriter.RecordCompressWriter(FileSystem,Configuration,Path,Class,Class,CompressionCodec,Progressable,Metadata)
1978    5   1    1 org.apache.hadoop.io.SequenceFile.RecordCompressWriter.RecordCompressWriter(FileSystem,Configuration,Path,Class,Class,int,short,long,CompressionCodec,Progressable,Metadata)
1979    2   1    1 org.apache.hadoop.io.SequenceFile.RecordCompressWriter.RecordCompressWriter(FileSystem,Configuration,Path,Class,Class,CompressionCodec,Progressable)
1980    6   1    1 org.apache.hadoop.io.SequenceFile.RecordCompressWriter.RecordCompressWriter(Configuration,FSDataOutputStream,Class,Class,CompressionCodec,Metadata)
1981    2   1    0 org.apache.hadoop.io.SequenceFile.RecordCompressWriter.isCompressed()
1982    2   1    0 org.apache.hadoop.io.SequenceFile.RecordCompressWriter.isBlockCompressed()
1983   18   7    0 org.apache.hadoop.io.SequenceFile.RecordCompressWriter.append(Object,Object)
1984    9   3    1 org.apache.hadoop.io.SequenceFile.RecordCompressWriter.appendRaw(byte[],int,int,ValueBytes)
1985    2   1    1 org.apache.hadoop.io.SequenceFile.BlockCompressWriter.BlockCompressWriter(FileSystem,Configuration,Path,Class,Class,CompressionCodec)
1986    2   1    1 org.apache.hadoop.io.SequenceFile.BlockCompressWriter.BlockCompressWriter(FileSystem,Configuration,Path,Class,Class,CompressionCodec,Progressable,Metadata)
1987    6   1    1 org.apache.hadoop.io.SequenceFile.BlockCompressWriter.BlockCompressWriter(FileSystem,Configuration,Path,Class,Class,int,short,long,CompressionCodec,Progressable,Metadata)
1988    2   1    1 org.apache.hadoop.io.SequenceFile.BlockCompressWriter.BlockCompressWriter(FileSystem,Configuration,Path,Class,Class,CompressionCodec,Progressable)
1989    7   1    1 org.apache.hadoop.io.SequenceFile.BlockCompressWriter.BlockCompressWriter(Configuration,FSDataOutputStream,Class,Class,CompressionCodec,Metadata)
1990    2   1    0 org.apache.hadoop.io.SequenceFile.BlockCompressWriter.isCompressed()
1991    2   1    0 org.apache.hadoop.io.SequenceFile.BlockCompressWriter.isBlockCompressed()
1992    6   1    1 org.apache.hadoop.io.SequenceFile.BlockCompressWriter.init(int)
1993    8   1    1 org.apache.hadoop.io.SequenceFile.BlockCompressWriter.writeBuffer(DataOutputBuffer)
1994   14   2    1 org.apache.hadoop.io.SequenceFile.BlockCompressWriter.sync()
1995    4   2    1 org.apache.hadoop.io.SequenceFile.BlockCompressWriter.close()
1996   19   8    0 org.apache.hadoop.io.SequenceFile.BlockCompressWriter.append(Object,Object)
1997   12   4    1 org.apache.hadoop.io.SequenceFile.BlockCompressWriter.appendRaw(byte[],int,int,ValueBytes)
1998    2   1    1 org.apache.hadoop.io.SequenceFile.Reader.Reader(FileSystem,Path,Configuration)
1999    2   1    0 org.apache.hadoop.io.SequenceFile.Reader.Reader(FileSystem,Path,int,Configuration,boolean)
2000    7   1    0 org.apache.hadoop.io.SequenceFile.Reader.Reader(FileSystem,Path,int,long,long,Configuration,boolean)
2001    2   1    1 org.apache.hadoop.io.SequenceFile.Reader.openFile(FileSystem,Path,int,long)
2002   69  20    1 org.apache.hadoop.io.SequenceFile.Reader.init(boolean)
2003    2   1    0 org.apache.hadoop.io.SequenceFile.Reader.getDeserializer(SerializationFactory,Class)
2004   12   3    1 org.apache.hadoop.io.SequenceFile.Reader.close()
2005    2   1    1 org.apache.hadoop.io.SequenceFile.Reader.getKeyClassName()
2006    6   4    1 org.apache.hadoop.io.SequenceFile.Reader.getKeyClass()
2007    2   1    1 org.apache.hadoop.io.SequenceFile.Reader.getValueClassName()
2008    6   4    1 org.apache.hadoop.io.SequenceFile.Reader.getValueClass()
2009    2   1    1 org.apache.hadoop.io.SequenceFile.Reader.isCompressed()
2010    2   1    1 org.apache.hadoop.io.SequenceFile.Reader.isBlockCompressed()
2011    2   1    1 org.apache.hadoop.io.SequenceFile.Reader.getCompressionCodec()
2012    2   1    1 org.apache.hadoop.io.SequenceFile.Reader.getMetadata()
2013    2   1    1 org.apache.hadoop.io.SequenceFile.Reader.getConf()
2014    8   1    1 org.apache.hadoop.io.SequenceFile.Reader.readBuffer(DataInputBuffer,CompressionInputStream)
2015   23   7    1 org.apache.hadoop.io.SequenceFile.Reader.readBlock()
2016   19   9    1 org.apache.hadoop.io.SequenceFile.Reader.seekToCurrentValue()
2017   15   6    1 org.apache.hadoop.io.SequenceFile.Reader.getCurrentValue(Writable)
2018   16   6    1 org.apache.hadoop.io.SequenceFile.Reader.getCurrentValue(Object)
2019    2   1    0 org.apache.hadoop.io.SequenceFile.Reader.deserializeValue(Object)
2020   25  13    1 org.apache.hadoop.io.SequenceFile.Reader.next(Writable)
2021    7   4    1 org.apache.hadoop.io.SequenceFile.Reader.next(Writable,Writable)
2022   15  10    1 org.apache.hadoop.io.SequenceFile.Reader.readRecordLength()
2023   12   7    1 org.apache.hadoop.io.SequenceFile.Reader.next(DataOutputBuffer)
2024    7   3    0 org.apache.hadoop.io.SequenceFile.Reader.createValueBytes()
2025   34  13    1 org.apache.hadoop.io.SequenceFile.Reader.nextRaw(DataOutputBuffer,ValueBytes)
2026   22  12    1 org.apache.hadoop.io.SequenceFile.Reader.nextRawKey(DataOutputBuffer)
2027   25  14    1 org.apache.hadoop.io.SequenceFile.Reader.next(Object)
2028    2   1    0 org.apache.hadoop.io.SequenceFile.Reader.deserializeKey(Object)
2029   17   4    1 org.apache.hadoop.io.SequenceFile.Reader.nextRawValue(ValueBytes)
2030    6   3    0 org.apache.hadoop.io.SequenceFile.Reader.handleChecksumException(ChecksumException)
2031    5   2    1 org.apache.hadoop.io.SequenceFile.Reader.seek(long)
2032   18   9    1 org.apache.hadoop.io.SequenceFile.Reader.sync(long)
2033    2   1    1 org.apache.hadoop.io.SequenceFile.Reader.syncSeen()
2034    2   1    1 org.apache.hadoop.io.SequenceFile.Reader.getPosition()
2035    2   1    1 org.apache.hadoop.io.SequenceFile.Reader.toString()
2036    2   1    1 org.apache.hadoop.io.SequenceFile.Sorter.Sorter(FileSystem,Class,Class,Configuration)
2037    8   1    1 org.apache.hadoop.io.SequenceFile.Sorter.Sorter(FileSystem,RawComparator,Class,Class,Configuration)
2038    2   1    1 org.apache.hadoop.io.SequenceFile.Sorter.setFactor(int)
2039    2   1    1 org.apache.hadoop.io.SequenceFile.Sorter.getFactor()
2040    2   1    1 org.apache.hadoop.io.SequenceFile.Sorter.setMemory(int)
2041    2   1    1 org.apache.hadoop.io.SequenceFile.Sorter.getMemory()
2042    2   1    1 org.apache.hadoop.io.SequenceFile.Sorter.setProgressable(Progressable)
2043    8   4    1 org.apache.hadoop.io.SequenceFile.Sorter.sort(Path[],Path,boolean)
2044   14   7    1 org.apache.hadoop.io.SequenceFile.Sorter.sortAndIterate(Path[],Path,boolean)
2045    2   1    1 org.apache.hadoop.io.SequenceFile.Sorter.sort(Path,Path)
2046    8   2    0 org.apache.hadoop.io.SequenceFile.Sorter.sortPass(boolean)
2047   52  16    0 org.apache.hadoop.io.SequenceFile.Sorter.SortPass.run(boolean)
2048    7   4    0 org.apache.hadoop.io.SequenceFile.Sorter.SortPass.close()
2049    7   1    0 org.apache.hadoop.io.SequenceFile.Sorter.SortPass.grow()
2050    4   1    0 org.apache.hadoop.io.SequenceFile.Sorter.SortPass.grow(int[],int)
2051    6   2    0 org.apache.hadoop.io.SequenceFile.Sorter.SortPass.grow(ValueBytes[],int)
2052   18   7    0 org.apache.hadoop.io.SequenceFile.Sorter.SortPass.flush(int,int,boolean,boolean,CompressionCodec,boolean)
2053    3   1    0 org.apache.hadoop.io.SequenceFile.Sorter.SortPass.sort(int)
2054    2   1    0 org.apache.hadoop.io.SequenceFile.Sorter.SortPass.SeqFileComparator.compare(IntWritable,IntWritable)
2055    2   1    1 org.apache.hadoop.io.SequenceFile.Sorter.SortPass.setProgressable(Progressable)
2056    1   1    1 org.apache.hadoop.io.SequenceFile.Sorter.RawKeyValueIterator.getKey()
2057    1   1    1 org.apache.hadoop.io.SequenceFile.Sorter.RawKeyValueIterator.getValue()
2058    1   1    1 org.apache.hadoop.io.SequenceFile.Sorter.RawKeyValueIterator.next()
2059    1   1    1 org.apache.hadoop.io.SequenceFile.Sorter.RawKeyValueIterator.close()
2060    1   1    1 org.apache.hadoop.io.SequenceFile.Sorter.RawKeyValueIterator.getProgress()
2061    3   1    1 org.apache.hadoop.io.SequenceFile.Sorter.merge(List,Path)
2062    2   2    1 org.apache.hadoop.io.SequenceFile.Sorter.merge(Path[],boolean,Path)
2063   10   2    1 org.apache.hadoop.io.SequenceFile.Sorter.merge(Path[],boolean,int,Path)
2064   11   3    1 org.apache.hadoop.io.SequenceFile.Sorter.merge(Path[],Path,boolean)
2065    9   1    1 org.apache.hadoop.io.SequenceFile.Sorter.cloneFileAttributes(Path,Path,Progressable)
2066    4   2    1 org.apache.hadoop.io.SequenceFile.Sorter.writeFile(RawKeyValueIterator,Writer)
2067    7   3    1 org.apache.hadoop.io.SequenceFile.Sorter.merge(Path[],Path)
2068    7   1    1 org.apache.hadoop.io.SequenceFile.Sorter.mergePass(Path)
2069    4   1    1 org.apache.hadoop.io.SequenceFile.Sorter.merge(Path,Path,Path)
2070    8   5    0 org.apache.hadoop.io.SequenceFile.Sorter.MergeQueue.put(SegmentDescriptor)
2071    6   2    1 org.apache.hadoop.io.SequenceFile.Sorter.MergeQueue.MergeQueue(List,Path,Progressable)
2072    6   2    0 org.apache.hadoop.io.SequenceFile.Sorter.MergeQueue.lessThan(Object,Object)
2073    5   2    0 org.apache.hadoop.io.SequenceFile.Sorter.MergeQueue.close()
2074    2   1    0 org.apache.hadoop.io.SequenceFile.Sorter.MergeQueue.getKey()
2075    2   1    0 org.apache.hadoop.io.SequenceFile.Sorter.MergeQueue.getValue()
2076   17   7    0 org.apache.hadoop.io.SequenceFile.Sorter.MergeQueue.next()
2077    2   1    0 org.apache.hadoop.io.SequenceFile.Sorter.MergeQueue.getProgress()
2078   10   2    0 org.apache.hadoop.io.SequenceFile.Sorter.MergeQueue.adjustPriorityQueue(SegmentDescriptor)
2079    4   2    0 org.apache.hadoop.io.SequenceFile.Sorter.MergeQueue.updateProgress(long)
2080   52  13    1 org.apache.hadoop.io.SequenceFile.Sorter.MergeQueue.merge()
2081    7   7    0 org.apache.hadoop.io.SequenceFile.Sorter.MergeQueue.getPassFactor(int,int)
2082   10   3    1 org.apache.hadoop.io.SequenceFile.Sorter.MergeQueue.getSegmentDescriptors(int)
2083    4   1    1 org.apache.hadoop.io.SequenceFile.Sorter.SegmentDescriptor.SegmentDescriptor(long,long,Path)
2084    2   1    1 org.apache.hadoop.io.SequenceFile.Sorter.SegmentDescriptor.doSync()
2085    2   1    1 org.apache.hadoop.io.SequenceFile.Sorter.SegmentDescriptor.preserveInput(boolean)
2086    2   1    0 org.apache.hadoop.io.SequenceFile.Sorter.SegmentDescriptor.shouldPreserveInput()
2087    7   7    0 org.apache.hadoop.io.SequenceFile.Sorter.SegmentDescriptor.compareTo(Object)
2088    7   7    0 org.apache.hadoop.io.SequenceFile.Sorter.SegmentDescriptor.equals(Object)
2089    2   1    0 org.apache.hadoop.io.SequenceFile.Sorter.SegmentDescriptor.hashCode()
2090   17   8    1 org.apache.hadoop.io.SequenceFile.Sorter.SegmentDescriptor.nextRawKey()
2091    3   1    1 org.apache.hadoop.io.SequenceFile.Sorter.SegmentDescriptor.nextRawValue(ValueBytes)
2092    2   1    1 org.apache.hadoop.io.SequenceFile.Sorter.SegmentDescriptor.getKey()
2093    3   1    1 org.apache.hadoop.io.SequenceFile.Sorter.SegmentDescriptor.close()
2094    4   2    1 org.apache.hadoop.io.SequenceFile.Sorter.SegmentDescriptor.cleanup()
2095    3   1    1 org.apache.hadoop.io.SequenceFile.Sorter.LinkedSegmentsDescriptor.LinkedSegmentsDescriptor(long,long,Path,SegmentContainer)
2096    5   3    1 org.apache.hadoop.io.SequenceFile.Sorter.LinkedSegmentsDescriptor.cleanup()
2097   12   2    1 org.apache.hadoop.io.SequenceFile.Sorter.SegmentContainer.SegmentContainer(Path,Path)
2098    2   1    0 org.apache.hadoop.io.SequenceFile.Sorter.SegmentContainer.getSegmentList()
2099    4   2    0 org.apache.hadoop.io.SequenceFile.Sorter.SegmentContainer.cleanup()
2100    1   1    1 org.apache.hadoop.io.serializer.Deserializer.open(InputStream)
2101    1   1    1 org.apache.hadoop.io.serializer.Deserializer.deserialize(T)
2102    1   1    1 org.apache.hadoop.io.serializer.Deserializer.close()
2103    3   1    0 org.apache.hadoop.io.serializer.DeserializerComparator.DeserializerComparator(Deserializer)
2104    8   3    0 org.apache.hadoop.io.serializer.DeserializerComparator.compare(byte[],int,int,byte[],int,int)
2105    1   1    0 org.apache.hadoop.io.serializer.JavaSerialization.JavaSerializationDeserializer.ObjectInputStream$1.readStreamHeader()
2106    3   1    0 org.apache.hadoop.io.serializer.JavaSerialization.JavaSerializationDeserializer.open(InputStream)
2107    4   4    0 org.apache.hadoop.io.serializer.JavaSerialization.JavaSerializationDeserializer.deserialize(T)
2108    2   1    0 org.apache.hadoop.io.serializer.JavaSerialization.JavaSerializationDeserializer.close()
2109    1   1    0 org.apache.hadoop.io.serializer.JavaSerialization.JavaSerializationSerializer.ObjectOutputStream$2.writeStreamHeader()
2110    3   1    0 org.apache.hadoop.io.serializer.JavaSerialization.JavaSerializationSerializer.open(OutputStream)
2111    3   1    0 org.apache.hadoop.io.serializer.JavaSerialization.JavaSerializationSerializer.serialize(Serializable)
2112    2   1    0 org.apache.hadoop.io.serializer.JavaSerialization.JavaSerializationSerializer.close()
2113    2   1    0 org.apache.hadoop.io.serializer.JavaSerialization.accept(Class)
2114    2   1    0 org.apache.hadoop.io.serializer.JavaSerialization.getDeserializer(Class)
2115    2   1    0 org.apache.hadoop.io.serializer.JavaSerialization.getSerializer(Class)
2116    2   1    0 org.apache.hadoop.io.serializer.JavaSerializationComparator.JavaSerializationComparator()
2117    2   1    0 org.apache.hadoop.io.serializer.JavaSerializationComparator.compare(T,T)
2118    1   1    1 org.apache.hadoop.io.serializer.Serialization.accept(Class)
2119    1   1    1 org.apache.hadoop.io.serializer.Serialization.getSerializer(Class)
2120    1   1    1 org.apache.hadoop.io.serializer.Serialization.getDeserializer(Class)
2121    4   2    1 org.apache.hadoop.io.serializer.SerializationFactory.SerializationFactory(Configuration)
2122    5   2    0 org.apache.hadoop.io.serializer.SerializationFactory.add(Configuration,String)
2123    2   1    0 org.apache.hadoop.io.serializer.SerializationFactory.getSerializer(Class)
2124    2   1    0 org.apache.hadoop.io.serializer.SerializationFactory.getDeserializer(Class)
2125    5   4    0 org.apache.hadoop.io.serializer.SerializationFactory.getSerialization(Class)
2126    1   1    1 org.apache.hadoop.io.serializer.Serializer.open(OutputStream)
2127    1   1    1 org.apache.hadoop.io.serializer.Serializer.serialize(T)
2128    1   1    1 org.apache.hadoop.io.serializer.Serializer.close()
2129    3   1    0 org.apache.hadoop.io.serializer.WritableSerialization.WritableDeserializer.WritableDeserializer(Configuration,Class)
2130    5   2    0 org.apache.hadoop.io.serializer.WritableSerialization.WritableDeserializer.open(InputStream)
2131    8   2    0 org.apache.hadoop.io.serializer.WritableSerialization.WritableDeserializer.deserialize(Writable)
2132    2   1    0 org.apache.hadoop.io.serializer.WritableSerialization.WritableDeserializer.close()
2133    5   2    0 org.apache.hadoop.io.serializer.WritableSerialization.WritableSerializer.open(OutputStream)
2134    2   1    0 org.apache.hadoop.io.serializer.WritableSerialization.WritableSerializer.serialize(Writable)
2135    2   1    0 org.apache.hadoop.io.serializer.WritableSerialization.WritableSerializer.close()
2136    2   1    0 org.apache.hadoop.io.serializer.WritableSerialization.accept(Class)
2137    2   1    0 org.apache.hadoop.io.serializer.WritableSerialization.getDeserializer(Class)
2138    2   1    0 org.apache.hadoop.io.serializer.WritableSerialization.getSerializer(Class)
2139    1   1    0 org.apache.hadoop.io.SetFile.SetFile()
2140    2   1    1 org.apache.hadoop.io.SetFile.Writer.Writer(FileSystem,String,Class)
2141    2   1    1 org.apache.hadoop.io.SetFile.Writer.Writer(Configuration,FileSystem,String,Class,SequenceFile.CompressionType)
2142    2   1    1 org.apache.hadoop.io.SetFile.Writer.Writer(Configuration,FileSystem,String,WritableComparator,SequenceFile.CompressionType)
2143    2   1    1 org.apache.hadoop.io.SetFile.Writer.append(WritableComparable)
2144    2   1    1 org.apache.hadoop.io.SetFile.Reader.Reader(FileSystem,String,Configuration)
2145    2   1    1 org.apache.hadoop.io.SetFile.Reader.Reader(FileSystem,String,WritableComparator,Configuration)
2146    2   1    0 org.apache.hadoop.io.SetFile.Reader.seek(WritableComparable)
2147    2   1    1 org.apache.hadoop.io.SetFile.Reader.next(WritableComparable)
2148    6   3    1 org.apache.hadoop.io.SetFile.Reader.get(WritableComparable)
2149    3   1    1 org.apache.hadoop.io.SortedMapWritable.SortedMapWritable()
2150    3   1    1 org.apache.hadoop.io.SortedMapWritable.SortedMapWritable(SortedMapWritable)
2151    2   1    1 org.apache.hadoop.io.SortedMapWritable.comparator()
2152    2   1    1 org.apache.hadoop.io.SortedMapWritable.firstKey()
2153    2   1    1 org.apache.hadoop.io.SortedMapWritable.headMap(WritableComparable)
2154    2   1    1 org.apache.hadoop.io.SortedMapWritable.lastKey()
2155    2   1    1 org.apache.hadoop.io.SortedMapWritable.subMap(WritableComparable,WritableComparable)
2156    2   1    1 org.apache.hadoop.io.SortedMapWritable.tailMap(WritableComparable)
2157    2   1    1 org.apache.hadoop.io.SortedMapWritable.clear()
2158    2   1    1 org.apache.hadoop.io.SortedMapWritable.containsKey(Object)
2159    2   1    1 org.apache.hadoop.io.SortedMapWritable.containsValue(Object)
2160    2   1    1 org.apache.hadoop.io.SortedMapWritable.entrySet()
2161    2   1    1 org.apache.hadoop.io.SortedMapWritable.get(Object)
2162    2   1    1 org.apache.hadoop.io.SortedMapWritable.isEmpty()
2163    2   1    1 org.apache.hadoop.io.SortedMapWritable.keySet()
2164    4   1    1 org.apache.hadoop.io.SortedMapWritable.put(WritableComparable,Writable)
2165    3   2    1 org.apache.hadoop.io.SortedMapWritable.putAll(Map)
2166    2   1    1 org.apache.hadoop.io.SortedMapWritable.remove(Object)
2167    2   1    1 org.apache.hadoop.io.SortedMapWritable.size()
2168    2   1    1 org.apache.hadoop.io.SortedMapWritable.values()
2169    9   2    0 org.apache.hadoop.io.SortedMapWritable.readFields(DataInput)
2170    8   2    0 org.apache.hadoop.io.SortedMapWritable.write(DataOutput)
2171    1   1    1 org.apache.hadoop.io.Stringifier.toString(T)
2172    1   1    1 org.apache.hadoop.io.Stringifier.fromString(String)
2173    1   1    1 org.apache.hadoop.io.Stringifier.close()
2174    2   1    0 org.apache.hadoop.io.Text.ThreadLocal$1.initialValue()
2175    2   1    0 org.apache.hadoop.io.Text.ThreadLocal$2.initialValue()
2176    2   1    0 org.apache.hadoop.io.Text.Text()
2177    2   1    1 org.apache.hadoop.io.Text.Text(String)
2178    2   1    1 org.apache.hadoop.io.Text.Text(Text)
2179    2   1    1 org.apache.hadoop.io.Text.Text(byte[])
2180    2   1    1 org.apache.hadoop.io.Text.getBytes()
2181    2   1    1 org.apache.hadoop.io.Text.getLength()
2182    7   5    1 org.apache.hadoop.io.Text.charAt(int)
2183    2   1    0 org.apache.hadoop.io.Text.find(String)
2184   28  10    1 org.apache.hadoop.io.Text.find(String,int)
2185    6   3    1 org.apache.hadoop.io.Text.set(String)
2186    2   1    1 org.apache.hadoop.io.Text.set(byte[])
2187    2   1    1 org.apache.hadoop.io.Text.set(Text)
2188    4   1    1 org.apache.hadoop.io.Text.set(byte[],int,int)
2189    4   1    1 org.apache.hadoop.io.Text.append(byte[],int,int)
2190    2   1    1 org.apache.hadoop.io.Text.clear()
2191    6   5    0 org.apache.hadoop.io.Text.setCapacity(int,boolean)
2192    4   4    1 org.apache.hadoop.io.Text.toString()
2193    5   1    1 org.apache.hadoop.io.Text.readFields(DataInput)
2194    3   1    1 org.apache.hadoop.io.Text.skip(DataInput)
2195    3   1    1 org.apache.hadoop.io.Text.write(DataOutput)
2196    4   3    1 org.apache.hadoop.io.Text.equals(Object)
2197    2   1    0 org.apache.hadoop.io.Text.hashCode()
2198    2   1    0 org.apache.hadoop.io.Text.Comparator.Comparator()
2199    4   1    0 org.apache.hadoop.io.Text.Comparator.compare(byte[],int,int,byte[],int,int)
2200    2   1    1 org.apache.hadoop.io.Text.decode(byte[])
2201    2   1    0 org.apache.hadoop.io.Text.decode(byte[],int,int)
2202    2   1    1 org.apache.hadoop.io.Text.decode(byte[],int,int,boolean)
2203   10   3    0 org.apache.hadoop.io.Text.decode(ByteBuffer,boolean)
2204    2   1    1 org.apache.hadoop.io.Text.encode(String)
2205   10   3    1 org.apache.hadoop.io.Text.encode(String,boolean)
2206    5   1    1 org.apache.hadoop.io.Text.readString(DataInput)
2207    6   1    1 org.apache.hadoop.io.Text.writeString(DataOutput,String)
2208    2   1    1 org.apache.hadoop.io.Text.validateUTF8(byte[])
2209   52  37    1 org.apache.hadoop.io.Text.validateUTF8(byte[],int,int)
2210   28   9    1 org.apache.hadoop.io.Text.bytesToCodePoint(ByteBuffer)
2211   22   8    1 org.apache.hadoop.io.Text.utf8Length(String)
2212    2   1    0 org.apache.hadoop.io.TwoDArrayWritable.TwoDArrayWritable(Class)
2213    3   1    0 org.apache.hadoop.io.TwoDArrayWritable.TwoDArrayWritable(Class,Writable[][])
2214    9   3    0 org.apache.hadoop.io.TwoDArrayWritable.toArray()
2215    2   1    0 org.apache.hadoop.io.TwoDArrayWritable.set(Writable[][])
2216    2   1    0 org.apache.hadoop.io.TwoDArrayWritable.get()
2217   14   8    0 org.apache.hadoop.io.TwoDArrayWritable.readFields(DataInput)
2218    7   4    0 org.apache.hadoop.io.TwoDArrayWritable.write(DataOutput)
2219    1   1    0 org.apache.hadoop.io.UTF8.UTF8()
2220    2   1    1 org.apache.hadoop.io.UTF8.UTF8(String)
2221    2   1    1 org.apache.hadoop.io.UTF8.UTF8(UTF8)
2222    2   1    1 org.apache.hadoop.io.UTF8.getBytes()
2223    2   1    1 org.apache.hadoop.io.UTF8.getLength()
2224   15   8    1 org.apache.hadoop.io.UTF8.set(String)
2225    5   3    1 org.apache.hadoop.io.UTF8.set(UTF8)
2226    5   3    0 org.apache.hadoop.io.UTF8.readFields(DataInput)
2227    3   1    1 org.apache.hadoop.io.UTF8.skip(DataInput)
2228    3   1    0 org.apache.hadoop.io.UTF8.write(DataOutput)
2229    3   1    1 org.apache.hadoop.io.UTF8.compareTo(Object)
2230    8   3    1 org.apache.hadoop.io.UTF8.toString()
2231    8   5    1 org.apache.hadoop.io.UTF8.equals(Object)
2232    2   1    0 org.apache.hadoop.io.UTF8.hashCode()
2233    2   1    0 org.apache.hadoop.io.UTF8.Comparator.Comparator()
2234    4   1    0 org.apache.hadoop.io.UTF8.Comparator.compare(byte[],int,int,byte[],int,int)
2235    9   3    1 org.apache.hadoop.io.UTF8.getBytes(String)
2236    5   1    1 org.apache.hadoop.io.UTF8.readString(DataInput)
2237   15   4    0 org.apache.hadoop.io.UTF8.readChars(DataInput,StringBuffer,int)
2238   10   4    1 org.apache.hadoop.io.UTF8.writeString(DataOutput,String)
2239   13   5    1 org.apache.hadoop.io.UTF8.utf8Length(String)
2240   14   5    0 org.apache.hadoop.io.UTF8.writeChars(DataOutput,String,int,int)
2241    1   1    1 org.apache.hadoop.io.VersionedWritable.getVersion()
2242    2   1    0 org.apache.hadoop.io.VersionedWritable.write(DataOutput)
2243    4   3    0 org.apache.hadoop.io.VersionedWritable.readFields(DataInput)
2244    3   1    0 org.apache.hadoop.io.VersionMismatchException.VersionMismatchException(byte,byte)
2245    2   1    1 org.apache.hadoop.io.VersionMismatchException.toString()
2246    1   1    0 org.apache.hadoop.io.VIntWritable.VIntWritable()
2247    2   1    0 org.apache.hadoop.io.VIntWritable.VIntWritable(int)
2248    2   1    1 org.apache.hadoop.io.VIntWritable.set(int)
2249    2   1    1 org.apache.hadoop.io.VIntWritable.get()
2250    2   1    0 org.apache.hadoop.io.VIntWritable.readFields(DataInput)
2251    2   1    0 org.apache.hadoop.io.VIntWritable.write(DataOutput)
2252    5   3    1 org.apache.hadoop.io.VIntWritable.equals(Object)
2253    2   1    0 org.apache.hadoop.io.VIntWritable.hashCode()
2254    4   3    1 org.apache.hadoop.io.VIntWritable.compareTo(Object)
2255    2   1    0 org.apache.hadoop.io.VIntWritable.toString()
2256    1   1    0 org.apache.hadoop.io.VLongWritable.VLongWritable()
2257    2   1    0 org.apache.hadoop.io.VLongWritable.VLongWritable(long)
2258    2   1    1 org.apache.hadoop.io.VLongWritable.set(long)
2259    2   1    1 org.apache.hadoop.io.VLongWritable.get()
2260    2   1    0 org.apache.hadoop.io.VLongWritable.readFields(DataInput)
2261    2   1    0 org.apache.hadoop.io.VLongWritable.write(DataOutput)
2262    5   3    1 org.apache.hadoop.io.VLongWritable.equals(Object)
2263    2   1    0 org.apache.hadoop.io.VLongWritable.hashCode()
2264    4   3    1 org.apache.hadoop.io.VLongWritable.compareTo(Object)
2265    2   1    0 org.apache.hadoop.io.VLongWritable.toString()
2266    1   1    1 org.apache.hadoop.io.Writable.write(DataOutput)
2267    1   1    1 org.apache.hadoop.io.Writable.readFields(DataInput)
2268    5   2    1 org.apache.hadoop.io.WritableComparator.get(Class)
2269    2   1    1 org.apache.hadoop.io.WritableComparator.define(Class,WritableComparator)
2270    2   1    1 org.apache.hadoop.io.WritableComparator.WritableComparator(Class)
2271    9   2    0 org.apache.hadoop.io.WritableComparator.WritableComparator(Class,boolean)
2272    2   1    1 org.apache.hadoop.io.WritableComparator.getKeyClass()
2273    2   1    1 org.apache.hadoop.io.WritableComparator.newKey()
2274    8   3    1 org.apache.hadoop.io.WritableComparator.compare(byte[],int,int,byte[],int,int)
2275    2   1    0 org.apache.hadoop.io.WritableComparator.compare(WritableComparable,WritableComparable)
2276    2   1    0 org.apache.hadoop.io.WritableComparator.compare(Object,Object)
2277    9   5    1 org.apache.hadoop.io.WritableComparator.compareBytes(byte[],int,int,byte[],int,int)
2278    5   2    1 org.apache.hadoop.io.WritableComparator.hashBytes(byte[],int)
2279    2   1    1 org.apache.hadoop.io.WritableComparator.readUnsignedShort(byte[],int)
2280    2   1    1 org.apache.hadoop.io.WritableComparator.readInt(byte[],int)
2281    2   1    1 org.apache.hadoop.io.WritableComparator.readFloat(byte[],int)
2282    2   1    1 org.apache.hadoop.io.WritableComparator.readLong(byte[],int)
2283    2   1    1 org.apache.hadoop.io.WritableComparator.readDouble(byte[],int)
2284   13   8    1 org.apache.hadoop.io.WritableComparator.readVLong(byte[],int)
2285    2   1    1 org.apache.hadoop.io.WritableComparator.readVInt(byte[],int)
2286    1   1    0 org.apache.hadoop.io.WritableFactories.WritableFactories()
2287    2   1    1 org.apache.hadoop.io.WritableFactories.setFactory(Class,WritableFactory)
2288    2   1    1 org.apache.hadoop.io.WritableFactories.getFactory(Class)
2289    9   4    1 org.apache.hadoop.io.WritableFactories.newInstance(Class,Configuration)
2290    2   1    1 org.apache.hadoop.io.WritableFactories.newInstance(Class)
2291    1   1    1 org.apache.hadoop.io.WritableFactory.newInstance()
2292    1   1    0 org.apache.hadoop.io.WritableName.WritableName()
2293    3   1    1 org.apache.hadoop.io.WritableName.setName(Class,String)
2294    2   1    1 org.apache.hadoop.io.WritableName.addName(Class,String)
2295    5   3    1 org.apache.hadoop.io.WritableName.getName(Class)
2296    9   6    1 org.apache.hadoop.io.WritableName.getClass(String,Configuration)
2297    2   1    1 org.apache.hadoop.ipc.Client.setPingInterval(Configuration,int)
2298    2   1    1 org.apache.hadoop.ipc.Client.getPingInterval(Configuration)
2299    2   1    1 org.apache.hadoop.ipc.Client.incCount()
2300    2   1    1 org.apache.hadoop.ipc.Client.decCount()
2301    2   1    1 org.apache.hadoop.ipc.Client.isZeroReference()
2302    4   1    0 org.apache.hadoop.ipc.Client.Call.Call(Writable)
2303    3   1    1 org.apache.hadoop.ipc.Client.Call.callComplete()
2304    3   1    1 org.apache.hadoop.ipc.Client.Call.setException(IOException)
2305    3   1    1 org.apache.hadoop.ipc.Client.Call.setValue(Writable)
2306    2   1    0 org.apache.hadoop.ipc.Client.Connection.Connection(InetSocketAddress)
2307    7   4    0 org.apache.hadoop.ipc.Client.Connection.Connection(ConnectionId)
2308    2   1    1 org.apache.hadoop.ipc.Client.Connection.touch()
2309    6   3    1 org.apache.hadoop.ipc.Client.Connection.addCall(Call)
2310    2   1    0 org.apache.hadoop.ipc.Client.Connection.PingInputStream.PingInputStream(InputStream)
2311    5   4    0 org.apache.hadoop.ipc.Client.Connection.PingInputStream.handleTimeout(SocketTimeoutException)
2312    5   4    1 org.apache.hadoop.ipc.Client.Connection.PingInputStream.read()
2313    5   4    1 org.apache.hadoop.ipc.Client.Connection.PingInputStream.read(byte[],int,int)
2314   25   9    1 org.apache.hadoop.ipc.Client.Connection.setupIOstreams()
2315   10   5    0 org.apache.hadoop.ipc.Client.Connection.handleConnectionFailure(int,int,IOException)
2316    8   1    0 org.apache.hadoop.ipc.Client.Connection.writeHeader()
2317   18  14    0 org.apache.hadoop.ipc.Client.Connection.waitForWork()
2318    2   1    0 org.apache.hadoop.ipc.Client.Connection.getRemoteAddress()
2319    7   2    0 org.apache.hadoop.ipc.Client.Connection.sendPing()
2320    8   4    0 org.apache.hadoop.ipc.Client.Connection.run()
2321   19   5    1 org.apache.hadoop.ipc.Client.Connection.sendParam(Call)
2322   17   6    0 org.apache.hadoop.ipc.Client.Connection.receiveResponse()
2323    4   2    0 org.apache.hadoop.ipc.Client.Connection.markClosed(IOException)
2324   20   8    1 org.apache.hadoop.ipc.Client.Connection.close()
2325    6   2    0 org.apache.hadoop.ipc.Client.Connection.cleanupCalls()
2326    4   1    0 org.apache.hadoop.ipc.Client.ParallelCall.ParallelCall(Writable,ParallelResults,int)
2327    2   1    1 org.apache.hadoop.ipc.Client.ParallelCall.callComplete()
2328    3   1    0 org.apache.hadoop.ipc.Client.ParallelResults.ParallelResults(int)
2329    5   2    1 org.apache.hadoop.ipc.Client.ParallelResults.callComplete(ParallelCall)
2330   10   2    1 org.apache.hadoop.ipc.Client.Client(Class,Configuration,SocketFactory)
2331    2   1    1 org.apache.hadoop.ipc.Client.Client(Class,Configuration)
2332    2   1    1 org.apache.hadoop.ipc.Client.getSocketFactory()
2333   11   7    1 org.apache.hadoop.ipc.Client.stop()
2334    2   1    1 org.apache.hadoop.ipc.Client.call(Writable,InetSocketAddress)
2335   16   7    0 org.apache.hadoop.ipc.Client.call(Writable,InetSocketAddress,UserGroupInformation)
2336   16   7    1 org.apache.hadoop.ipc.Client.call(Writable[],InetSocketAddress[])
2337   13   5    1 org.apache.hadoop.ipc.Client.getConnection(InetSocketAddress,UserGroupInformation,Call)
2338    3   1    0 org.apache.hadoop.ipc.Client.ConnectionId.ConnectionId(InetSocketAddress,UserGroupInformation)
2339    2   1    0 org.apache.hadoop.ipc.Client.ConnectionId.getAddress()
2340    2   1    0 org.apache.hadoop.ipc.Client.ConnectionId.getTicket()
2341    5   4    0 org.apache.hadoop.ipc.Client.ConnectionId.equals(Object)
2342    2   1    0 org.apache.hadoop.ipc.Client.ConnectionId.hashCode()
2343    7   1    0 org.apache.hadoop.ipc.metrics.RpcMetrics.RpcMetrics(String,String,Server)
2344   11   2    1 org.apache.hadoop.ipc.metrics.RpcMetrics.doUpdates(MetricsContext)
2345    3   2    0 org.apache.hadoop.ipc.metrics.RpcMetrics.shutdown()
2346    4   1    0 org.apache.hadoop.ipc.metrics.RpcMgt.RpcMgt(String,String,RpcMetrics,Server)
2347    3   2    0 org.apache.hadoop.ipc.metrics.RpcMgt.shutdown()
2348    2   1    1 org.apache.hadoop.ipc.metrics.RpcMgt.getRpcOpsAvgProcessingTime()
2349    2   1    1 org.apache.hadoop.ipc.metrics.RpcMgt.getRpcOpsAvgProcessingTimeMax()
2350    2   1    1 org.apache.hadoop.ipc.metrics.RpcMgt.getRpcOpsAvgProcessingTimeMin()
2351    2   1    1 org.apache.hadoop.ipc.metrics.RpcMgt.getRpcOpsAvgQueueTime()
2352    2   1    1 org.apache.hadoop.ipc.metrics.RpcMgt.getRpcOpsAvgQueueTimeMax()
2353    2   1    1 org.apache.hadoop.ipc.metrics.RpcMgt.getRpcOpsAvgQueueTimeMin()
2354    2   1    1 org.apache.hadoop.ipc.metrics.RpcMgt.getRpcOpsNumber()
2355    2   1    1 org.apache.hadoop.ipc.metrics.RpcMgt.getNumOpenConnections()
2356    2   1    1 org.apache.hadoop.ipc.metrics.RpcMgt.getCallQueueLen()
2357    3   1    1 org.apache.hadoop.ipc.metrics.RpcMgt.resetAllMinMax()
2358    3   1    0 org.apache.hadoop.ipc.RemoteException.RemoteException(String,String)
2359    2   1    0 org.apache.hadoop.ipc.RemoteException.getClassName()
2360   10   8    1 org.apache.hadoop.ipc.RemoteException.unwrapRemoteException(Class)
2361    5   3    1 org.apache.hadoop.ipc.RemoteException.unwrapRemoteException()
2362   10   2    0 org.apache.hadoop.ipc.RemoteException.instantiateException(Class)
2363   10   2    1 org.apache.hadoop.ipc.RemoteException.writeXml(String,XMLOutputter)
2364    2   1    1 org.apache.hadoop.ipc.RemoteException.valueOf(Attributes)
2365    1   1    0 org.apache.hadoop.ipc.RPC.RPC()
2366    1   1    0 org.apache.hadoop.ipc.RPC.Invocation.Invocation()
2367    4   1    0 org.apache.hadoop.ipc.RPC.Invocation.Invocation(Method,Object[])
2368    2   1    1 org.apache.hadoop.ipc.RPC.Invocation.getMethodName()
2369    2   1    1 org.apache.hadoop.ipc.RPC.Invocation.getParameterClasses()
2370    2   1    1 org.apache.hadoop.ipc.RPC.Invocation.getParameters()
2371    8   2    0 org.apache.hadoop.ipc.RPC.Invocation.readFields(DataInput)
2372    5   2    0 org.apache.hadoop.ipc.RPC.Invocation.write(DataOutput)
2373   10   3    0 org.apache.hadoop.ipc.RPC.Invocation.toString()
2374    2   1    0 org.apache.hadoop.ipc.RPC.Invocation.setConf(Configuration)
2375    2   1    0 org.apache.hadoop.ipc.RPC.Invocation.getConf()
2376    8   2    1 org.apache.hadoop.ipc.RPC.ClientCache.getClient(Configuration,SocketFactory)
2377    2   1    1 org.apache.hadoop.ipc.RPC.ClientCache.getClient(Configuration)
2378    7   3    1 org.apache.hadoop.ipc.RPC.ClientCache.stopClient(Client)
2379    4   1    0 org.apache.hadoop.ipc.RPC.Invoker.Invoker(InetSocketAddress,UserGroupInformation,Configuration,SocketFactory)
2380   10   3    0 org.apache.hadoop.ipc.RPC.Invoker.invoke(Object,Method,Object[])
2381    4   2    0 org.apache.hadoop.ipc.RPC.Invoker.close()
2382    5   1    1 org.apache.hadoop.ipc.RPC.VersionMismatch.VersionMismatch(String,long,long)
2383    2   1    1 org.apache.hadoop.ipc.RPC.VersionMismatch.getInterfaceName()
2384    2   1    1 org.apache.hadoop.ipc.RPC.VersionMismatch.getClientVersion()
2385    2   1    1 org.apache.hadoop.ipc.RPC.VersionMismatch.getServerVersion()
2386    9   6    0 org.apache.hadoop.ipc.RPC.waitForProxy(Class,long,InetSocketAddress,Configuration)
2387    2   1    1 org.apache.hadoop.ipc.RPC.getProxy(Class,long,InetSocketAddress,Configuration,SocketFactory)
2388    7   4    1 org.apache.hadoop.ipc.RPC.getProxy(Class,long,InetSocketAddress,UserGroupInformation,Configuration,SocketFactory)
2389    2   1    1 org.apache.hadoop.ipc.RPC.getProxy(Class,long,InetSocketAddress,Configuration)
2390    3   2    1 org.apache.hadoop.ipc.RPC.stopProxy(VersionedProtocol)
2391   15   7    1 org.apache.hadoop.ipc.RPC.call(Method,Object[][],InetSocketAddress[],Configuration)
2392    2   1    1 org.apache.hadoop.ipc.RPC.getServer(Object,String,int,Configuration)
2393    2   1    1 org.apache.hadoop.ipc.RPC.getServer(Object,String,int,int,boolean,Configuration)
2394    2   1    1 org.apache.hadoop.ipc.RPC.Server.Server(Object,Configuration,String,int)
2395    5   4    0 org.apache.hadoop.ipc.RPC.Server.classNameBase(String)
2396    5   1    1 org.apache.hadoop.ipc.RPC.Server.Server(Object,Configuration,String,int,int,boolean)
2397   34  11    0 org.apache.hadoop.ipc.RPC.Server.call(Writable,long)
2398    4   3    0 org.apache.hadoop.ipc.RPC.log(String)
2399    2   1    1 org.apache.hadoop.ipc.Server.get()
2400    5   3    1 org.apache.hadoop.ipc.Server.getRemoteIp()
2401    3   2    1 org.apache.hadoop.ipc.Server.getRemoteAddress()
2402   11   7    1 org.apache.hadoop.ipc.Server.bind(ServerSocket,InetSocketAddress,int)
2403    6   1    0 org.apache.hadoop.ipc.Server.Call.Call(int,Writable,Connection)
2404    2   1    0 org.apache.hadoop.ipc.Server.Call.toString()
2405    2   1    0 org.apache.hadoop.ipc.Server.Call.setResponse(ByteBuffer)
2406   10   1    0 org.apache.hadoop.ipc.Server.Listener.Listener()
2407   35  15    1 org.apache.hadoop.ipc.Server.Listener.cleanupConnections(boolean)
2408   39  14    0 org.apache.hadoop.ipc.Server.Listener.run()
2409    8   4    0 org.apache.hadoop.ipc.Server.Listener.closeCurrentConnection(SelectionKey,Throwable)
2410    2   1    0 org.apache.hadoop.ipc.Server.Listener.getAddress()
2411   17   5    0 org.apache.hadoop.ipc.Server.Listener.doAccept(SelectionKey)
2412   19   8    0 org.apache.hadoop.ipc.Server.Listener.doRead(SelectionKey)
2413    8   4    0 org.apache.hadoop.ipc.Server.Listener.doStop()
2414    5   1    0 org.apache.hadoop.ipc.Server.Responder.Responder()
2415   40  15    0 org.apache.hadoop.ipc.Server.Responder.run()
2416   11   7    0 org.apache.hadoop.ipc.Server.Responder.doAsyncWrite(SelectionKey)
2417    9   3    0 org.apache.hadoop.ipc.Server.Responder.doPurge(Call,long)
2418   45  14    0 org.apache.hadoop.ipc.Server.Responder.processResponse(LinkedList,boolean)
2419    5   2    0 org.apache.hadoop.ipc.Server.Responder.doRespond(Call)
2420    2   1    0 org.apache.hadoop.ipc.Server.Responder.incPending()
2421    3   1    0 org.apache.hadoop.ipc.Server.Responder.decPending()
2422    3   2    0 org.apache.hadoop.ipc.Server.Responder.waitPending()
2423   17   4    0 org.apache.hadoop.ipc.Server.Connection.Connection(SelectionKey,SocketChannel,long)
2424    2   1    0 org.apache.hadoop.ipc.Server.Connection.toString()
2425    2   1    0 org.apache.hadoop.ipc.Server.Connection.getHostAddress()
2426    2   1    0 org.apache.hadoop.ipc.Server.Connection.setLastContact(long)
2427    2   1    0 org.apache.hadoop.ipc.Server.Connection.getLastContact()
2428    2   1    0 org.apache.hadoop.ipc.Server.Connection.isIdle()
2429    2   1    0 org.apache.hadoop.ipc.Server.Connection.decRpcCount()
2430    2   1    0 org.apache.hadoop.ipc.Server.Connection.incRpcCount()
2431    4   4    0 org.apache.hadoop.ipc.Server.Connection.timedOut(long)
2432   42  18    0 org.apache.hadoop.ipc.Server.Connection.readAndProcess()
2433    3   1    0 org.apache.hadoop.ipc.Server.Connection.processHeader()
2434    9   2    0 org.apache.hadoop.ipc.Server.Connection.processData()
2435   12   7    0 org.apache.hadoop.ipc.Server.Connection.close()
2436    3   1    0 org.apache.hadoop.ipc.Server.Handler.Handler(int)
2437   38   8    0 org.apache.hadoop.ipc.Server.Handler.run()
2438    2   1    0 org.apache.hadoop.ipc.Server.Server(String,int,Class,int,Configuration)
2439   17   1    1 org.apache.hadoop.ipc.Server.Server(String,int,Class,int,Configuration,String)
2440    6   3    0 org.apache.hadoop.ipc.Server.closeConnection(Connection)
2441    2   1    1 org.apache.hadoop.ipc.Server.setSocketSendBufSize(int)
2442    7   2    1 org.apache.hadoop.ipc.Server.start()
2443   13   5    1 org.apache.hadoop.ipc.Server.stop()
2444    3   2    1 org.apache.hadoop.ipc.Server.join()
2445    2   1    1 org.apache.hadoop.ipc.Server.getListenerAddress()
2446    1   1    1 org.apache.hadoop.ipc.Server.call(Writable,long)
2447    2   1    1 org.apache.hadoop.ipc.Server.getNumOpenConnections()
2448    2   1    1 org.apache.hadoop.ipc.Server.getCallQueueLen()
2449    1   1    1 org.apache.hadoop.ipc.VersionedProtocol.getProtocolVersion(String,long)
2450   10   7    1 org.apache.hadoop.log.LogLevel.main(String[])
2451   12   4    0 org.apache.hadoop.log.LogLevel.process(String)
2452   20   5    0 org.apache.hadoop.log.LogLevel.Servlet.doGet(HttpServletRequest,HttpServletResponse)
2453    5   2    0 org.apache.hadoop.log.LogLevel.Servlet.process(org.apache.log4j.Logger,String,PrintWriter)
2454    7   3    0 org.apache.hadoop.log.LogLevel.Servlet.process(java.util.logging.Logger,String,PrintWriter)
2455    1   1    1 org.apache.hadoop.metrics.ContextFactory.ContextFactory()
2456    2   1    1 org.apache.hadoop.metrics.ContextFactory.getAttribute(String)
2457    7   2    1 org.apache.hadoop.metrics.ContextFactory.getAttributeNames()
2458    2   1    1 org.apache.hadoop.metrics.ContextFactory.setAttribute(String,Object)
2459    2   1    1 org.apache.hadoop.metrics.ContextFactory.removeAttribute(String)
2460   12   3    1 org.apache.hadoop.metrics.ContextFactory.getContext(String)
2461    6   2    1 org.apache.hadoop.metrics.ContextFactory.getNullContext(String)
2462    5   2    1 org.apache.hadoop.metrics.ContextFactory.getFactory()
2463   10   3    0 org.apache.hadoop.metrics.ContextFactory.setAttributes()
2464    1   1    1 org.apache.hadoop.metrics.file.FileContext.FileContext()
2465   13   6    0 org.apache.hadoop.metrics.file.FileContext.init(String,ContextFactory)
2466    5   3    1 org.apache.hadoop.metrics.file.FileContext.getFileName()
2467    6   2    1 org.apache.hadoop.metrics.file.FileContext.startMonitoring()
2468    5   2    1 org.apache.hadoop.metrics.file.FileContext.stopMonitoring()
2469   18   3    1 org.apache.hadoop.metrics.file.FileContext.emitRecord(String,String,OutputRecord)
2470    2   1    1 org.apache.hadoop.metrics.file.FileContext.flush()
2471    1   1    1 org.apache.hadoop.metrics.ganglia.GangliaContext.GangliaContext()
2472   18   6    0 org.apache.hadoop.metrics.ganglia.GangliaContext.init(String,ContextFactory)
2473    5   2    0 org.apache.hadoop.metrics.ganglia.GangliaContext.emitRecord(String,String,OutputRecord)
2474   17   2    0 org.apache.hadoop.metrics.ganglia.GangliaContext.emitMetric(String,String,String)
2475    5   2    0 org.apache.hadoop.metrics.ganglia.GangliaContext.getUnits(String)
2476    5   3    0 org.apache.hadoop.metrics.ganglia.GangliaContext.getSlope(String)
2477    6   3    0 org.apache.hadoop.metrics.ganglia.GangliaContext.getTmax(String)
2478    6   3    0 org.apache.hadoop.metrics.ganglia.GangliaContext.getDmax(String)
2479    7   1    1 org.apache.hadoop.metrics.ganglia.GangliaContext.xdr_string(String)
2480    4   2    1 org.apache.hadoop.metrics.ganglia.GangliaContext.pad()
2481    5   1    1 org.apache.hadoop.metrics.ganglia.GangliaContext.xdr_int(int)
2482    2   1    0 org.apache.hadoop.metrics.jvm.EventCounter.EventCounts.incr(int)
2483    2   1    0 org.apache.hadoop.metrics.jvm.EventCounter.EventCounts.get(int)
2484    2   1    0 org.apache.hadoop.metrics.jvm.EventCounter.getFatal()
2485    2   1    0 org.apache.hadoop.metrics.jvm.EventCounter.getError()
2486    2   1    0 org.apache.hadoop.metrics.jvm.EventCounter.getWarn()
2487    2   1    0 org.apache.hadoop.metrics.jvm.EventCounter.getInfo()
2488   13   5    0 org.apache.hadoop.metrics.jvm.EventCounter.append(LoggingEvent)
2489    1   1    0 org.apache.hadoop.metrics.jvm.EventCounter.close()
2490    2   1    0 org.apache.hadoop.metrics.jvm.EventCounter.requiresLayout()
2491    7   2    0 org.apache.hadoop.metrics.jvm.JvmMetrics.init(String,String)
2492    6   1    1 org.apache.hadoop.metrics.jvm.JvmMetrics.JvmMetrics(String,String)
2493    6   1    1 org.apache.hadoop.metrics.jvm.JvmMetrics.doUpdates(MetricsContext)
2494    8   1    0 org.apache.hadoop.metrics.jvm.JvmMetrics.doMemoryUpdates()
2495   11   2    0 org.apache.hadoop.metrics.jvm.JvmMetrics.doGarbageCollectionUpdates()
2496   37   9    0 org.apache.hadoop.metrics.jvm.JvmMetrics.doThreadUpdates()
2497   13   1    0 org.apache.hadoop.metrics.jvm.JvmMetrics.doEventCountUpdates()
2498    1   1    1 org.apache.hadoop.metrics.MetricsContext.getContextName()
2499    1   1    1 org.apache.hadoop.metrics.MetricsContext.startMonitoring()
2500    1   1    1 org.apache.hadoop.metrics.MetricsContext.stopMonitoring()
2501    1   1    1 org.apache.hadoop.metrics.MetricsContext.isMonitoring()
2502    1   1    1 org.apache.hadoop.metrics.MetricsContext.close()
2503    1   1    1 org.apache.hadoop.metrics.MetricsContext.createRecord(String)
2504    1   1    1 org.apache.hadoop.metrics.MetricsContext.registerUpdater(Updater)
2505    1   1    1 org.apache.hadoop.metrics.MetricsContext.unregisterUpdater(Updater)
2506    1   1    1 org.apache.hadoop.metrics.MetricsException.MetricsException()
2507    2   1    1 org.apache.hadoop.metrics.MetricsException.MetricsException(String)
2508    1   1    1 org.apache.hadoop.metrics.MetricsRecord.getRecordName()
2509    1   1    1 org.apache.hadoop.metrics.MetricsRecord.setTag(String,String)
2510    1   1    1 org.apache.hadoop.metrics.MetricsRecord.setTag(String,int)
2511    1   1    1 org.apache.hadoop.metrics.MetricsRecord.setTag(String,long)
2512    1   1    1 org.apache.hadoop.metrics.MetricsRecord.setTag(String,short)
2513    1   1    1 org.apache.hadoop.metrics.MetricsRecord.setTag(String,byte)
2514    1   1    1 org.apache.hadoop.metrics.MetricsRecord.removeTag(String)
2515    1   1    1 org.apache.hadoop.metrics.MetricsRecord.setMetric(String,int)
2516    1   1    1 org.apache.hadoop.metrics.MetricsRecord.setMetric(String,long)
2517    1   1    1 org.apache.hadoop.metrics.MetricsRecord.setMetric(String,short)
2518    1   1    1 org.apache.hadoop.metrics.MetricsRecord.setMetric(String,byte)
2519    1   1    1 org.apache.hadoop.metrics.MetricsRecord.setMetric(String,float)
2520    1   1    1 org.apache.hadoop.metrics.MetricsRecord.incrMetric(String,int)
2521    1   1    1 org.apache.hadoop.metrics.MetricsRecord.incrMetric(String,long)
2522    1   1    1 org.apache.hadoop.metrics.MetricsRecord.incrMetric(String,short)
2523    1   1    1 org.apache.hadoop.metrics.MetricsRecord.incrMetric(String,byte)
2524    1   1    1 org.apache.hadoop.metrics.MetricsRecord.incrMetric(String,float)
2525    1   1    1 org.apache.hadoop.metrics.MetricsRecord.update()
2526    1   1    1 org.apache.hadoop.metrics.MetricsRecord.remove()
2527    1   1    1 org.apache.hadoop.metrics.MetricsUtil.MetricsUtil()
2528    9   3    1 org.apache.hadoop.metrics.MetricsUtil.getContext(String)
2529    4   1    1 org.apache.hadoop.metrics.MetricsUtil.createRecord(MetricsContext,String)
2530    7   2    1 org.apache.hadoop.metrics.MetricsUtil.getHostName()
2531    2   1    0 org.apache.hadoop.metrics.spi.AbstractMetricsContext.TagMap.TagMap()
2532    2   1    0 org.apache.hadoop.metrics.spi.AbstractMetricsContext.TagMap.TagMap(TagMap)
2533    6   5    1 org.apache.hadoop.metrics.spi.AbstractMetricsContext.TagMap.containsAll(TagMap)
2534    1   1    1 org.apache.hadoop.metrics.spi.AbstractMetricsContext.AbstractMetricsContext()
2535    3   1    1 org.apache.hadoop.metrics.spi.AbstractMetricsContext.init(String,ContextFactory)
2536    3   1    1 org.apache.hadoop.metrics.spi.AbstractMetricsContext.getAttribute(String)
2537    9   3    1 org.apache.hadoop.metrics.spi.AbstractMetricsContext.getAttributeTable(String)
2538    2   1    1 org.apache.hadoop.metrics.spi.AbstractMetricsContext.getContextName()
2539    2   1    1 org.apache.hadoop.metrics.spi.AbstractMetricsContext.getContextFactory()
2540    4   2    1 org.apache.hadoop.metrics.spi.AbstractMetricsContext.startMonitoring()
2541    4   2    1 org.apache.hadoop.metrics.spi.AbstractMetricsContext.stopMonitoring()
2542    2   1    1 org.apache.hadoop.metrics.spi.AbstractMetricsContext.isMonitoring()
2543    3   1    1 org.apache.hadoop.metrics.spi.AbstractMetricsContext.close()
2544    4   2    1 org.apache.hadoop.metrics.spi.AbstractMetricsContext.createRecord(String)
2545    2   1    1 org.apache.hadoop.metrics.spi.AbstractMetricsContext.newRecord(String)
2546    3   2    1 org.apache.hadoop.metrics.spi.AbstractMetricsContext.registerUpdater(Updater)
2547    2   1    0 org.apache.hadoop.metrics.spi.AbstractMetricsContext.unregisterUpdater(Updater)
2548    2   1    0 org.apache.hadoop.metrics.spi.AbstractMetricsContext.clearUpdaters()
2549    4   2    0 org.apache.hadoop.metrics.spi.AbstractMetricsContext.TimerTask$1.run()
2550   10   2    1 org.apache.hadoop.metrics.spi.AbstractMetricsContext.startTimer()
2551    4   2    1 org.apache.hadoop.metrics.spi.AbstractMetricsContext.stopTimer()
2552   10   4    1 org.apache.hadoop.metrics.spi.AbstractMetricsContext.timerEvent()
2553    9   3    1 org.apache.hadoop.metrics.spi.AbstractMetricsContext.emitRecords()
2554    1   1    1 org.apache.hadoop.metrics.spi.AbstractMetricsContext.emitRecord(String,String,OutputRecord)
2555    1   1    1 org.apache.hadoop.metrics.spi.AbstractMetricsContext.flush()
2556   20   5    1 org.apache.hadoop.metrics.spi.AbstractMetricsContext.update(MetricsRecordImpl)
2557    2   1    0 org.apache.hadoop.metrics.spi.AbstractMetricsContext.getRecordMap(String)
2558   17  12    1 org.apache.hadoop.metrics.spi.AbstractMetricsContext.sum(Number,Number)
2559   10   3    1 org.apache.hadoop.metrics.spi.AbstractMetricsContext.remove(MetricsRecordImpl)
2560    2   1    1 org.apache.hadoop.metrics.spi.AbstractMetricsContext.getPeriod()
2561    2   1    1 org.apache.hadoop.metrics.spi.AbstractMetricsContext.setPeriod(int)
2562    3   1    1 org.apache.hadoop.metrics.spi.MetricsRecordImpl.MetricsRecordImpl(String,AbstractMetricsContext)
2563    2   1    1 org.apache.hadoop.metrics.spi.MetricsRecordImpl.getRecordName()
2564    4   2    1 org.apache.hadoop.metrics.spi.MetricsRecordImpl.setTag(String,String)
2565    2   1    1 org.apache.hadoop.metrics.spi.MetricsRecordImpl.setTag(String,int)
2566    2   1    1 org.apache.hadoop.metrics.spi.MetricsRecordImpl.setTag(String,long)
2567    2   1    1 org.apache.hadoop.metrics.spi.MetricsRecordImpl.setTag(String,short)
2568    2   1    1 org.apache.hadoop.metrics.spi.MetricsRecordImpl.setTag(String,byte)
2569    2   1    1 org.apache.hadoop.metrics.spi.MetricsRecordImpl.removeTag(String)
2570    2   1    1 org.apache.hadoop.metrics.spi.MetricsRecordImpl.setMetric(String,int)
2571    2   1    1 org.apache.hadoop.metrics.spi.MetricsRecordImpl.setMetric(String,long)
2572    2   1    1 org.apache.hadoop.metrics.spi.MetricsRecordImpl.setMetric(String,short)
2573    2   1    1 org.apache.hadoop.metrics.spi.MetricsRecordImpl.setMetric(String,byte)
2574    2   1    1 org.apache.hadoop.metrics.spi.MetricsRecordImpl.setMetric(String,float)
2575    2   1    1 org.apache.hadoop.metrics.spi.MetricsRecordImpl.incrMetric(String,int)
2576    2   1    1 org.apache.hadoop.metrics.spi.MetricsRecordImpl.incrMetric(String,long)
2577    2   1    1 org.apache.hadoop.metrics.spi.MetricsRecordImpl.incrMetric(String,short)
2578    2   1    1 org.apache.hadoop.metrics.spi.MetricsRecordImpl.incrMetric(String,byte)
2579    2   1    1 org.apache.hadoop.metrics.spi.MetricsRecordImpl.incrMetric(String,float)
2580    2   1    0 org.apache.hadoop.metrics.spi.MetricsRecordImpl.setAbsolute(String,Number)
2581    2   1    0 org.apache.hadoop.metrics.spi.MetricsRecordImpl.setIncrement(String,Number)
2582    2   1    1 org.apache.hadoop.metrics.spi.MetricsRecordImpl.update()
2583    2   1    1 org.apache.hadoop.metrics.spi.MetricsRecordImpl.remove()
2584    2   1    0 org.apache.hadoop.metrics.spi.MetricsRecordImpl.getTagTable()
2585    2   1    0 org.apache.hadoop.metrics.spi.MetricsRecordImpl.getMetricTable()
2586    3   1    1 org.apache.hadoop.metrics.spi.MetricValue.MetricValue(Number,boolean)
2587    2   1    0 org.apache.hadoop.metrics.spi.MetricValue.isIncrement()
2588    2   1    0 org.apache.hadoop.metrics.spi.MetricValue.isAbsolute()
2589    2   1    0 org.apache.hadoop.metrics.spi.MetricValue.getNumber()
2590    1   1    1 org.apache.hadoop.metrics.spi.NullContext.NullContext()
2591    1   1    1 org.apache.hadoop.metrics.spi.NullContext.startMonitoring()
2592    1   1    1 org.apache.hadoop.metrics.spi.NullContext.emitRecord(String,String,OutputRecord)
2593    1   1    1 org.apache.hadoop.metrics.spi.NullContext.update(MetricsRecordImpl)
2594    1   1    1 org.apache.hadoop.metrics.spi.NullContext.remove(MetricsRecordImpl)
2595    1   1    1 org.apache.hadoop.metrics.spi.NullContextWithUpdateThread.NullContextWithUpdateThread()
2596   10   5    0 org.apache.hadoop.metrics.spi.NullContextWithUpdateThread.init(String,ContextFactory)
2597    1   1    1 org.apache.hadoop.metrics.spi.NullContextWithUpdateThread.emitRecord(String,String,OutputRecord)
2598    1   1    1 org.apache.hadoop.metrics.spi.NullContextWithUpdateThread.update(MetricsRecordImpl)
2599    1   1    1 org.apache.hadoop.metrics.spi.NullContextWithUpdateThread.remove(MetricsRecordImpl)
2600    3   1    1 org.apache.hadoop.metrics.spi.OutputRecord.OutputRecord(TagMap,MetricMap)
2601    2   1    1 org.apache.hadoop.metrics.spi.OutputRecord.getTagNames()
2602    2   1    1 org.apache.hadoop.metrics.spi.OutputRecord.getTag(String)
2603    2   1    1 org.apache.hadoop.metrics.spi.OutputRecord.getMetricNames()
2604    2   1    1 org.apache.hadoop.metrics.spi.OutputRecord.getMetric(String)
2605    1   1    1 org.apache.hadoop.metrics.spi.Util.Util()
2606   15   5    1 org.apache.hadoop.metrics.spi.Util.parse(String,int)
2607    1   1    1 org.apache.hadoop.metrics.Updater.doUpdates(MetricsContext)
2608    9   4    1 org.apache.hadoop.metrics.util.MBeanUtil.registerMBean(String,String,Object)
2609    8   5    0 org.apache.hadoop.metrics.util.MBeanUtil.unregisterMBean(ObjectName)
2610    6   2    0 org.apache.hadoop.metrics.util.MBeanUtil.getMBeanName(String,String)
2611    4   1    1 org.apache.hadoop.metrics.util.MetricsIntValue.MetricsIntValue(String)
2612    3   1    0 org.apache.hadoop.metrics.util.MetricsIntValue.set(int)
2613    2   1    0 org.apache.hadoop.metrics.util.MetricsIntValue.get()
2614    3   1    0 org.apache.hadoop.metrics.util.MetricsIntValue.inc(int)
2615    3   1    0 org.apache.hadoop.metrics.util.MetricsIntValue.inc()
2616    5   2    0 org.apache.hadoop.metrics.util.MetricsIntValue.dec(int)
2617    5   2    0 org.apache.hadoop.metrics.util.MetricsIntValue.dec()
2618    6   3    0 org.apache.hadoop.metrics.util.MetricsIntValue.pushMetric(MetricsRecord)
2619    4   1    1 org.apache.hadoop.metrics.util.MetricsLongValue.MetricsLongValue(String)
2620    3   1    0 org.apache.hadoop.metrics.util.MetricsLongValue.set(long)
2621    2   1    0 org.apache.hadoop.metrics.util.MetricsLongValue.get()
2622    3   1    0 org.apache.hadoop.metrics.util.MetricsLongValue.inc(long)
2623    3   1    0 org.apache.hadoop.metrics.util.MetricsLongValue.inc()
2624    5   2    0 org.apache.hadoop.metrics.util.MetricsLongValue.dec(long)
2625    5   2    0 org.apache.hadoop.metrics.util.MetricsLongValue.dec()
2626    4   2    0 org.apache.hadoop.metrics.util.MetricsLongValue.pushMetric(MetricsRecord)
2627    4   1    1 org.apache.hadoop.metrics.util.MetricsTimeVaryingInt.MetricsTimeVaryingInt(String)
2628    2   1    0 org.apache.hadoop.metrics.util.MetricsTimeVaryingInt.inc(int)
2629    2   1    0 org.apache.hadoop.metrics.util.MetricsTimeVaryingInt.inc()
2630    3   1    0 org.apache.hadoop.metrics.util.MetricsTimeVaryingInt.intervalHeartBeat()
2631    5   2    0 org.apache.hadoop.metrics.util.MetricsTimeVaryingInt.pushMetric(MetricsRecord)
2632    2   1    1 org.apache.hadoop.metrics.util.MetricsTimeVaryingInt.getPreviousIntervalValue()
2633    3   1    0 org.apache.hadoop.metrics.util.MetricsTimeVaryingRate.Metrics.set(Metrics)
2634    3   1    0 org.apache.hadoop.metrics.util.MetricsTimeVaryingRate.Metrics.reset()
2635    3   1    0 org.apache.hadoop.metrics.util.MetricsTimeVaryingRate.MinMax.set(MinMax)
2636    3   1    0 org.apache.hadoop.metrics.util.MetricsTimeVaryingRate.MinMax.reset()
2637    4   2    0 org.apache.hadoop.metrics.util.MetricsTimeVaryingRate.MinMax.update(long)
2638    5   1    1 org.apache.hadoop.metrics.util.MetricsTimeVaryingRate.MetricsTimeVaryingRate(String)
2639    5   1    0 org.apache.hadoop.metrics.util.MetricsTimeVaryingRate.inc(int,long)
2640    4   1    0 org.apache.hadoop.metrics.util.MetricsTimeVaryingRate.inc(long)
2641    4   2    0 org.apache.hadoop.metrics.util.MetricsTimeVaryingRate.intervalHeartBeat()
2642    6   2    0 org.apache.hadoop.metrics.util.MetricsTimeVaryingRate.pushMetric(MetricsRecord)
2643    2   1    1 org.apache.hadoop.metrics.util.MetricsTimeVaryingRate.getPreviousIntervalNumOps()
2644    2   1    1 org.apache.hadoop.metrics.util.MetricsTimeVaryingRate.getPreviousIntervalAverageTime()
2645    2   1    1 org.apache.hadoop.metrics.util.MetricsTimeVaryingRate.getMinTime()
2646    2   1    1 org.apache.hadoop.metrics.util.MetricsTimeVaryingRate.getMaxTime()
2647    2   1    1 org.apache.hadoop.metrics.util.MetricsTimeVaryingRate.resetMinMax()
2648    2   1    0 org.apache.hadoop.net.CachedDNSToSwitchMapping.CachedDNSToSwitchMapping(DNSToSwitchMapping)
2649   20  10    0 org.apache.hadoop.net.CachedDNSToSwitchMapping.resolve(List)
2650    7   2    1 org.apache.hadoop.net.DNS.reverseDns(InetAddress,String)
2651   12   6    1 org.apache.hadoop.net.DNS.getIPs(String)
2652    3   1    1 org.apache.hadoop.net.DNS.getDefaultIP(String)
2653   10   5    1 org.apache.hadoop.net.DNS.getHosts(String,String)
2654    2   1    1 org.apache.hadoop.net.DNS.getHosts(String)
2655    7   6    1 org.apache.hadoop.net.DNS.getDefaultHost(String,String)
2656    2   1    1 org.apache.hadoop.net.DNS.getDefaultHost(String)
2657    1   1    1 org.apache.hadoop.net.DNSToSwitchMapping.resolve(List)
2658    8   4    1 org.apache.hadoop.net.NetUtils.getSocketFactory(Configuration,Class)
2659    5   4    1 org.apache.hadoop.net.NetUtils.getDefaultSocketFactory(Configuration)
2660    5   4    1 org.apache.hadoop.net.NetUtils.getSocketFactoryFromProperty(Configuration,String)
2661    2   1    1 org.apache.hadoop.net.NetUtils.createSocketAddr(String)
2662   21   8    1 org.apache.hadoop.net.NetUtils.createSocketAddr(String,int)
2663   18   8    0 org.apache.hadoop.net.NetUtils.getServerAddress(Configuration,String,String,String)
2664    3   1    1 org.apache.hadoop.net.NetUtils.addStaticResolution(String,String)
2665    3   1    1 org.apache.hadoop.net.NetUtils.getStaticResolution(String)
2666    9   4    1 org.apache.hadoop.net.NetUtils.getAllStaticResolutions()
2667    5   2    1 org.apache.hadoop.net.NetUtils.getConnectAddress(Server)
2668    2   1    1 org.apache.hadoop.net.NetUtils.getInputStream(Socket)
2669    2   2    1 org.apache.hadoop.net.NetUtils.getInputStream(Socket,long)
2670    2   1    1 org.apache.hadoop.net.NetUtils.getOutputStream(Socket)
2671    2   2    1 org.apache.hadoop.net.NetUtils.getOutputStream(Socket,long)
2672    8   5    1 org.apache.hadoop.net.NetUtils.normalizeHostName(String)
2673    5   2    1 org.apache.hadoop.net.NetUtils.normalizeHostNames(Collection)
2674    2   1    1 org.apache.hadoop.net.NetworkTopology.InnerNode.InnerNode(String)
2675    2   1    1 org.apache.hadoop.net.NetworkTopology.InnerNode.InnerNode(String,String)
2676    2   1    1 org.apache.hadoop.net.NetworkTopology.InnerNode.InnerNode(String,String,InnerNode,int)
2677    2   1    1 org.apache.hadoop.net.NetworkTopology.InnerNode.getChildren()
2678    2   1    1 org.apache.hadoop.net.NetworkTopology.InnerNode.getNumOfChildren()
2679    7   5    1 org.apache.hadoop.net.NetworkTopology.InnerNode.isRack()
2680    2   2    1 org.apache.hadoop.net.NetworkTopology.InnerNode.isAncestor(Node)
2681    2   1    1 org.apache.hadoop.net.NetworkTopology.InnerNode.isParent(Node)
2682   10   5    0 org.apache.hadoop.net.NetworkTopology.InnerNode.getNextAncestorName(Node)
2683   28  13    1 org.apache.hadoop.net.NetworkTopology.InnerNode.add(Node)
2684   29  14    1 org.apache.hadoop.net.NetworkTopology.InnerNode.remove(Node)
2685   16  12    1 org.apache.hadoop.net.NetworkTopology.InnerNode.getLoc(String)
2686   26  19    1 org.apache.hadoop.net.NetworkTopology.InnerNode.getLeaf(int,Node)
2687    2   1    0 org.apache.hadoop.net.NetworkTopology.InnerNode.getNumOfLeaves()
2688    2   1    0 org.apache.hadoop.net.NetworkTopology.NetworkTopology()
2689   16  10    1 org.apache.hadoop.net.NetworkTopology.add(Node)
2690   14   7    1 org.apache.hadoop.net.NetworkTopology.remove(Node)
2691   11   7    1 org.apache.hadoop.net.NetworkTopology.contains(Node)
2692    8   3    1 org.apache.hadoop.net.NetworkTopology.getNode(String)
2693    5   2    1 org.apache.hadoop.net.NetworkTopology.getNumOfRacks()
2694    5   2    1 org.apache.hadoop.net.NetworkTopology.getNumOfLeaves()
2695   28  14    1 org.apache.hadoop.net.NetworkTopology.getDistance(Node,Node)
2696    7   5    1 org.apache.hadoop.net.NetworkTopology.isOnSameRack(Node,Node)
2697    8   4    1 org.apache.hadoop.net.NetworkTopology.chooseRandom(String)
2698   21   9    0 org.apache.hadoop.net.NetworkTopology.chooseRandom(String,String)
2699   21   8    1 org.apache.hadoop.net.NetworkTopology.countNumOfAvailableNodes(String,List)
2700   13   2    1 org.apache.hadoop.net.NetworkTopology.toString()
2701    5   1    0 org.apache.hadoop.net.NetworkTopology.swap(Node[],int,int)
2702   23  15    1 org.apache.hadoop.net.NetworkTopology.pseudoSortByDistance(Node,Node[])
2703    1   1    1 org.apache.hadoop.net.Node.getNetworkLocation()
2704    1   1    1 org.apache.hadoop.net.Node.setNetworkLocation(String)
2705    1   1    1 org.apache.hadoop.net.Node.getName()
2706    1   1    1 org.apache.hadoop.net.Node.getParent()
2707    1   1    1 org.apache.hadoop.net.Node.setParent(Node)
2708    1   1    1 org.apache.hadoop.net.Node.getLevel()
2709    1   1    1 org.apache.hadoop.net.Node.setLevel(int)
2710    1   1    1 org.apache.hadoop.net.NodeBase.NodeBase()
2711    7   2    1 org.apache.hadoop.net.NodeBase.NodeBase(String)
2712    2   1    1 org.apache.hadoop.net.NodeBase.NodeBase(String,String)
2713    4   1    1 org.apache.hadoop.net.NodeBase.NodeBase(String,String,Node,int)
2714    5   5    0 org.apache.hadoop.net.NodeBase.set(String,String)
2715    2   1    1 org.apache.hadoop.net.NodeBase.getName()
2716    2   1    1 org.apache.hadoop.net.NodeBase.getNetworkLocation()
2717    2   1    1 org.apache.hadoop.net.NodeBase.setNetworkLocation(String)
2718    2   1    1 org.apache.hadoop.net.NodeBase.getPath(Node)
2719    2   1    1 org.apache.hadoop.net.NodeBase.toString()
2720    9   8    1 org.apache.hadoop.net.NodeBase.normalize(String)
2721    2   1    1 org.apache.hadoop.net.NodeBase.getParent()
2722    2   1    1 org.apache.hadoop.net.NodeBase.setParent(Node)
2723    2   1    1 org.apache.hadoop.net.NodeBase.getLevel()
2724    2   1    1 org.apache.hadoop.net.NodeBase.setLevel(int)
2725    2   1    0 org.apache.hadoop.net.ScriptBasedMapping.ScriptBasedMapping()
2726    3   1    0 org.apache.hadoop.net.ScriptBasedMapping.ScriptBasedMapping(Configuration)
2727    2   1    0 org.apache.hadoop.net.ScriptBasedMapping.getConf()
2728    2   1    0 org.apache.hadoop.net.ScriptBasedMapping.setConf(Configuration)
2729    4   1    0 org.apache.hadoop.net.ScriptBasedMapping.RawScriptBasedMapping.setConf(Configuration)
2730    2   1    0 org.apache.hadoop.net.ScriptBasedMapping.RawScriptBasedMapping.getConf()
2731    1   1    0 org.apache.hadoop.net.ScriptBasedMapping.RawScriptBasedMapping.RawScriptBasedMapping()
2732   15   8    0 org.apache.hadoop.net.ScriptBasedMapping.RawScriptBasedMapping.resolve(List)
2733   24   9    0 org.apache.hadoop.net.ScriptBasedMapping.RawScriptBasedMapping.runResolveCommand(List)
2734    3   1    0 org.apache.hadoop.net.SocketInputStream.Reader.Reader(ReadableByteChannel,long)
2735    2   1    0 org.apache.hadoop.net.SocketInputStream.Reader.performIO(ByteBuffer)
2736    3   1    1 org.apache.hadoop.net.SocketInputStream.SocketInputStream(ReadableByteChannel,long)
2737    2   1    1 org.apache.hadoop.net.SocketInputStream.SocketInputStream(Socket,long)
2738    2   1    1 org.apache.hadoop.net.SocketInputStream.SocketInputStream(Socket)
2739    5   4    0 org.apache.hadoop.net.SocketInputStream.read()
2740    2   1    0 org.apache.hadoop.net.SocketInputStream.read(byte[],int,int)
2741    3   1    0 org.apache.hadoop.net.SocketInputStream.close()
2742    2   1    1 org.apache.hadoop.net.SocketInputStream.getChannel()
2743    2   1    0 org.apache.hadoop.net.SocketInputStream.isOpen()
2744    2   1    0 org.apache.hadoop.net.SocketInputStream.read(ByteBuffer)
2745    2   1    1 org.apache.hadoop.net.SocketInputStream.waitForReadable()
2746    5   1    0 org.apache.hadoop.net.SocketIOWithTimeout.SocketIOWithTimeout(SelectableChannel,long)
2747    2   1    0 org.apache.hadoop.net.SocketIOWithTimeout.close()
2748    2   2    0 org.apache.hadoop.net.SocketIOWithTimeout.isOpen()
2749    2   1    0 org.apache.hadoop.net.SocketIOWithTimeout.getChannel()
2750    5   5    1 org.apache.hadoop.net.SocketIOWithTimeout.checkChannelValidity(Object)
2751    1   1    1 org.apache.hadoop.net.SocketIOWithTimeout.performIO(ByteBuffer)
2752   21  15    1 org.apache.hadoop.net.SocketIOWithTimeout.doIO(ByteBuffer,int)
2753    3   3    1 org.apache.hadoop.net.SocketIOWithTimeout.waitForIO(int)
2754    8   3    0 org.apache.hadoop.net.SocketIOWithTimeout.timeoutExceptionString(int)
2755    5   3    0 org.apache.hadoop.net.SocketIOWithTimeout.SelectorPool.SelectorInfo.close()
2756   25  13    1 org.apache.hadoop.net.SocketIOWithTimeout.SelectorPool.select(SelectableChannel,int,long)
2757   22   5    1 org.apache.hadoop.net.SocketIOWithTimeout.SelectorPool.get(SelectableChannel)
2758    5   1    1 org.apache.hadoop.net.SocketIOWithTimeout.SelectorPool.release(SelectorInfo)
2759   11   5    1 org.apache.hadoop.net.SocketIOWithTimeout.SelectorPool.trimIdleSelectors(long)
2760    3   1    0 org.apache.hadoop.net.SocketOutputStream.Writer.Writer(WritableByteChannel,long)
2761    2   1    0 org.apache.hadoop.net.SocketOutputStream.Writer.performIO(ByteBuffer)
2762    3   1    1 org.apache.hadoop.net.SocketOutputStream.SocketOutputStream(WritableByteChannel,long)
2763    2   1    1 org.apache.hadoop.net.SocketOutputStream.SocketOutputStream(Socket,long)
2764    4   1    0 org.apache.hadoop.net.SocketOutputStream.write(int)
2765    9   7    0 org.apache.hadoop.net.SocketOutputStream.write(byte[],int,int)
2766    3   1    0 org.apache.hadoop.net.SocketOutputStream.close()
2767    2   1    1 org.apache.hadoop.net.SocketOutputStream.getChannel()
2768    2   1    0 org.apache.hadoop.net.SocketOutputStream.isOpen()
2769    2   1    0 org.apache.hadoop.net.SocketOutputStream.write(ByteBuffer)
2770    2   1    1 org.apache.hadoop.net.SocketOutputStream.waitForWritable()
2771   13   7    1 org.apache.hadoop.net.SocketOutputStream.transferToFully(FileChannel,long,int)
2772    2   1    1 org.apache.hadoop.net.SocksSocketFactory.SocksSocketFactory()
2773    2   1    1 org.apache.hadoop.net.SocksSocketFactory.SocksSocketFactory(Proxy)
2774    2   1    0 org.apache.hadoop.net.SocksSocketFactory.createSocket()
2775    4   1    0 org.apache.hadoop.net.SocksSocketFactory.createSocket(InetAddress,int)
2776    5   1    0 org.apache.hadoop.net.SocksSocketFactory.createSocket(InetAddress,int,InetAddress,int)
2777    4   1    0 org.apache.hadoop.net.SocksSocketFactory.createSocket(String,int)
2778    5   1    0 org.apache.hadoop.net.SocksSocketFactory.createSocket(String,int,InetAddress,int)
2779    2   1    0 org.apache.hadoop.net.SocksSocketFactory.hashCode()
2780   15  12    0 org.apache.hadoop.net.SocksSocketFactory.equals(Object)
2781    2   1    0 org.apache.hadoop.net.SocksSocketFactory.getConf()
2782    5   3    0 org.apache.hadoop.net.SocksSocketFactory.setConf(Configuration)
2783    7   3    1 org.apache.hadoop.net.SocksSocketFactory.setProxy(String)
2784    1   1    1 org.apache.hadoop.net.StandardSocketFactory.StandardSocketFactory()
2785    2   1    0 org.apache.hadoop.net.StandardSocketFactory.createSocket()
2786    4   1    0 org.apache.hadoop.net.StandardSocketFactory.createSocket(InetAddress,int)
2787    5   1    0 org.apache.hadoop.net.StandardSocketFactory.createSocket(InetAddress,int,InetAddress,int)
2788    4   1    0 org.apache.hadoop.net.StandardSocketFactory.createSocket(String,int)
2789    5   1    0 org.apache.hadoop.net.StandardSocketFactory.createSocket(String,int,InetAddress,int)
2790    8   7    0 org.apache.hadoop.net.StandardSocketFactory.equals(Object)
2791    2   1    0 org.apache.hadoop.net.StandardSocketFactory.hashCode()
2792    2   1    0 org.apache.hadoop.record.BinaryRecordInput.BinaryIndex.BinaryIndex(int)
2793    2   1    0 org.apache.hadoop.record.BinaryRecordInput.BinaryIndex.done()
2794    2   1    0 org.apache.hadoop.record.BinaryRecordInput.BinaryIndex.incr()
2795    1   1    0 org.apache.hadoop.record.BinaryRecordInput.BinaryRecordInput()
2796    2   1    0 org.apache.hadoop.record.BinaryRecordInput.setDataInput(DataInput)
2797    2   1    0 org.apache.hadoop.record.BinaryRecordInput.ThreadLocal$1.initialValue()
2798    4   1    1 org.apache.hadoop.record.BinaryRecordInput.get(DataInput)
2799    2   1    1 org.apache.hadoop.record.BinaryRecordInput.BinaryRecordInput(InputStream)
2800    2   1    1 org.apache.hadoop.record.BinaryRecordInput.BinaryRecordInput(DataInput)
2801    2   1    0 org.apache.hadoop.record.BinaryRecordInput.readByte(String)
2802    2   1    0 org.apache.hadoop.record.BinaryRecordInput.readBool(String)
2803    2   1    0 org.apache.hadoop.record.BinaryRecordInput.readInt(String)
2804    2   1    0 org.apache.hadoop.record.BinaryRecordInput.readLong(String)
2805    2   1    0 org.apache.hadoop.record.BinaryRecordInput.readFloat(String)
2806    2   1    0 org.apache.hadoop.record.BinaryRecordInput.readDouble(String)
2807    2   1    0 org.apache.hadoop.record.BinaryRecordInput.readString(String)
2808    5   1    0 org.apache.hadoop.record.BinaryRecordInput.readBuffer(String)
2809    1   1    0 org.apache.hadoop.record.BinaryRecordInput.startRecord(String)
2810    1   1    0 org.apache.hadoop.record.BinaryRecordInput.endRecord(String)
2811    2   1    0 org.apache.hadoop.record.BinaryRecordInput.startVector(String)
2812    1   1    0 org.apache.hadoop.record.BinaryRecordInput.endVector(String)
2813    2   1    0 org.apache.hadoop.record.BinaryRecordInput.startMap(String)
2814    1   1    0 org.apache.hadoop.record.BinaryRecordInput.endMap(String)
2815    1   1    0 org.apache.hadoop.record.BinaryRecordOutput.BinaryRecordOutput()
2816    2   1    0 org.apache.hadoop.record.BinaryRecordOutput.setDataOutput(DataOutput)
2817    2   1    0 org.apache.hadoop.record.BinaryRecordOutput.ThreadLocal$1.initialValue()
2818    4   1    1 org.apache.hadoop.record.BinaryRecordOutput.get(DataOutput)
2819    2   1    1 org.apache.hadoop.record.BinaryRecordOutput.BinaryRecordOutput(OutputStream)
2820    2   1    1 org.apache.hadoop.record.BinaryRecordOutput.BinaryRecordOutput(DataOutput)
2821    2   1    0 org.apache.hadoop.record.BinaryRecordOutput.writeByte(byte,String)
2822    2   1    0 org.apache.hadoop.record.BinaryRecordOutput.writeBool(boolean,String)
2823    2   1    0 org.apache.hadoop.record.BinaryRecordOutput.writeInt(int,String)
2824    2   1    0 org.apache.hadoop.record.BinaryRecordOutput.writeLong(long,String)
2825    2   1    0 org.apache.hadoop.record.BinaryRecordOutput.writeFloat(float,String)
2826    2   1    0 org.apache.hadoop.record.BinaryRecordOutput.writeDouble(double,String)
2827    2   1    0 org.apache.hadoop.record.BinaryRecordOutput.writeString(String,String)
2828    5   1    0 org.apache.hadoop.record.BinaryRecordOutput.writeBuffer(Buffer,String)
2829    1   1    0 org.apache.hadoop.record.BinaryRecordOutput.startRecord(Record,String)
2830    1   1    0 org.apache.hadoop.record.BinaryRecordOutput.endRecord(Record,String)
2831    2   1    0 org.apache.hadoop.record.BinaryRecordOutput.startVector(ArrayList,String)
2832    1   1    0 org.apache.hadoop.record.BinaryRecordOutput.endVector(ArrayList,String)
2833    2   1    0 org.apache.hadoop.record.BinaryRecordOutput.startMap(TreeMap,String)
2834    1   1    0 org.apache.hadoop.record.BinaryRecordOutput.endMap(TreeMap,String)
2835    2   1    1 org.apache.hadoop.record.Buffer.Buffer()
2836    3   2    1 org.apache.hadoop.record.Buffer.Buffer(byte[])
2837    2   1    1 org.apache.hadoop.record.Buffer.Buffer(byte[],int,int)
2838    3   2    1 org.apache.hadoop.record.Buffer.set(byte[])
2839    5   3    1 org.apache.hadoop.record.Buffer.copy(byte[],int,int)
2840    4   2    1 org.apache.hadoop.record.Buffer.get()
2841    2   1    1 org.apache.hadoop.record.Buffer.getCount()
2842    2   1    1 org.apache.hadoop.record.Buffer.getCapacity()
2843   14   8    1 org.apache.hadoop.record.Buffer.setCapacity(int)
2844    2   1    1 org.apache.hadoop.record.Buffer.reset()
2845    2   1    1 org.apache.hadoop.record.Buffer.truncate()
2846    4   1    1 org.apache.hadoop.record.Buffer.append(byte[],int,int)
2847    2   1    1 org.apache.hadoop.record.Buffer.append(byte[])
2848    6   2    0 org.apache.hadoop.record.Buffer.hashCode()
2849   10   5    1 org.apache.hadoop.record.Buffer.compareTo(Object)
2850    4   4    0 org.apache.hadoop.record.Buffer.equals(Object)
2851    6   2    0 org.apache.hadoop.record.Buffer.toString()
2852    2   1    1 org.apache.hadoop.record.Buffer.toString(String)
2853    4   1    0 org.apache.hadoop.record.Buffer.clone()
2854    1   1    1 org.apache.hadoop.record.compiler.ant.RccTask.RccTask()
2855    2   1    1 org.apache.hadoop.record.compiler.ant.RccTask.setLanguage(String)
2856    2   1    1 org.apache.hadoop.record.compiler.ant.RccTask.setFile(File)
2857    2   1    1 org.apache.hadoop.record.compiler.ant.RccTask.setFailonerror(boolean)
2858    2   1    1 org.apache.hadoop.record.compiler.ant.RccTask.setDestdir(File)
2859    2   1    1 org.apache.hadoop.record.compiler.ant.RccTask.addFileset(FileSet)
2860   13   7    1 org.apache.hadoop.record.compiler.ant.RccTask.execute()
2861   10   4    0 org.apache.hadoop.record.compiler.ant.RccTask.doCompile(File)
2862    1   1    0 org.apache.hadoop.record.compiler.CGenerator.CGenerator()
2863   13   2    1 org.apache.hadoop.record.compiler.CGenerator.genCode(String,ArrayList,ArrayList,String,ArrayList)
2864    3   1    0 org.apache.hadoop.record.compiler.CodeBuffer.addMarkers(char,char)
2865    2   1    1 org.apache.hadoop.record.compiler.CodeBuffer.CodeBuffer()
2866    2   1    0 org.apache.hadoop.record.compiler.CodeBuffer.CodeBuffer(String)
2867    4   1    0 org.apache.hadoop.record.compiler.CodeBuffer.CodeBuffer(int,String)
2868    5   2    0 org.apache.hadoop.record.compiler.CodeBuffer.append(String)
2869   13   7    0 org.apache.hadoop.record.compiler.CodeBuffer.append(char)
2870    2   1    0 org.apache.hadoop.record.compiler.CodeBuffer.rawAppend(char)
2871    2   1    0 org.apache.hadoop.record.compiler.CodeBuffer.toString()
2872    2   1    0 org.apache.hadoop.record.compiler.CodeGenerator.register(String,CodeGenerator)
2873    2   1    0 org.apache.hadoop.record.compiler.CodeGenerator.get(String)
2874    1   1    0 org.apache.hadoop.record.compiler.CodeGenerator.genCode(String,ArrayList,ArrayList,String,ArrayList)
2875    1   1    1 org.apache.hadoop.record.compiler.Consts.Consts()
2876    1   1    0 org.apache.hadoop.record.compiler.CppGenerator.CppGenerator()
2877   18   3    1 org.apache.hadoop.record.compiler.CppGenerator.genCode(String,ArrayList,ArrayList,String,ArrayList)
2878    6   1    1 org.apache.hadoop.record.compiler.generated.ParseException.ParseException(Token,int[][],String[])
2879    3   1    1 org.apache.hadoop.record.compiler.generated.ParseException.ParseException()
2880    3   1    0 org.apache.hadoop.record.compiler.generated.ParseException.ParseException(String)
2881   31  11    1 org.apache.hadoop.record.compiler.generated.ParseException.getMessage()
2882   39  13    1 org.apache.hadoop.record.compiler.generated.ParseException.add_escapes(String)
2883    2   1    0 org.apache.hadoop.record.compiler.generated.Rcc.main(String[])
2884    2   1    0 org.apache.hadoop.record.compiler.generated.Rcc.usage()
2885   42  20    0 org.apache.hadoop.record.compiler.generated.Rcc.driver(String[])
2886   31  12    0 org.apache.hadoop.record.compiler.generated.Rcc.Input()
2887   29   7    0 org.apache.hadoop.record.compiler.generated.Rcc.Include()
2888   12   4    0 org.apache.hadoop.record.compiler.generated.Rcc.Module()
2889   19   7    0 org.apache.hadoop.record.compiler.generated.Rcc.ModuleName()
2890   16   7    0 org.apache.hadoop.record.compiler.generated.Rcc.RecordList()
2891   37  17    0 org.apache.hadoop.record.compiler.generated.Rcc.Record()
2892    8   4    0 org.apache.hadoop.record.compiler.generated.Rcc.Field()
2893   71  39    0 org.apache.hadoop.record.compiler.generated.Rcc.Type()
2894   12   4    0 org.apache.hadoop.record.compiler.generated.Rcc.Map()
2895    9   4    0 org.apache.hadoop.record.compiler.generated.Rcc.Vector()
2896    2   1    0 org.apache.hadoop.record.compiler.generated.Rcc.jj_la1_0()
2897    2   1    0 org.apache.hadoop.record.compiler.generated.Rcc.jj_la1_1()
2898    2   1    0 org.apache.hadoop.record.compiler.generated.Rcc.Rcc(java.io.InputStream)
2899   10   4    0 org.apache.hadoop.record.compiler.generated.Rcc.Rcc(java.io.InputStream,String)
2900    2   1    0 org.apache.hadoop.record.compiler.generated.Rcc.ReInit(java.io.InputStream)
2901   10   4    0 org.apache.hadoop.record.compiler.generated.Rcc.ReInit(java.io.InputStream,String)
2902    8   2    0 org.apache.hadoop.record.compiler.generated.Rcc.Rcc(java.io.Reader)
2903    8   2    0 org.apache.hadoop.record.compiler.generated.Rcc.ReInit(java.io.Reader)
2904    7   2    0 org.apache.hadoop.record.compiler.generated.Rcc.Rcc(RccTokenManager)
2905    7   2    0 org.apache.hadoop.record.compiler.generated.Rcc.ReInit(RccTokenManager)
2906   13   5    0 org.apache.hadoop.record.compiler.generated.Rcc.jj_consume_token(int)
2907    8   2    0 org.apache.hadoop.record.compiler.generated.Rcc.getNextToken()
2908    8   3    0 org.apache.hadoop.record.compiler.generated.Rcc.getToken(int)
2909    5   3    0 org.apache.hadoop.record.compiler.generated.Rcc.jj_ntk()
2910   24  11    0 org.apache.hadoop.record.compiler.generated.Rcc.generateParseException()
2911    1   1    0 org.apache.hadoop.record.compiler.generated.Rcc.enable_tracing()
2912    1   1    0 org.apache.hadoop.record.compiler.generated.Rcc.disable_tracing()
2913    2   1    0 org.apache.hadoop.record.compiler.generated.RccTokenManager.setDebugStream(java.io.PrintStream)
2914    2   1    0 org.apache.hadoop.record.compiler.generated.RccTokenManager.jjMoveStringLiteralDfa0_1()
2915    4   2    0 org.apache.hadoop.record.compiler.generated.RccTokenManager.jjCheckNAdd(int)
2916    3   2    0 org.apache.hadoop.record.compiler.generated.RccTokenManager.jjAddStates(int,int)
2917    3   1    0 org.apache.hadoop.record.compiler.generated.RccTokenManager.jjCheckNAddTwoStates(int,int)
2918    3   2    0 org.apache.hadoop.record.compiler.generated.RccTokenManager.jjCheckNAddStates(int,int)
2919    3   1    0 org.apache.hadoop.record.compiler.generated.RccTokenManager.jjCheckNAddStates(int)
2920   58  21    0 org.apache.hadoop.record.compiler.generated.RccTokenManager.jjMoveNfa_1(int,int)
2921   47  33    0 org.apache.hadoop.record.compiler.generated.RccTokenManager.jjStopStringLiteralDfa_0(int,long)
2922    2   1    0 org.apache.hadoop.record.compiler.generated.RccTokenManager.jjStartNfa_0(int,long)
2923    4   1    0 org.apache.hadoop.record.compiler.generated.RccTokenManager.jjStopAtPos(int,int)
2924    7   3    0 org.apache.hadoop.record.compiler.generated.RccTokenManager.jjStartNfaWithStates_0(int,int,int)
2925   38  35    0 org.apache.hadoop.record.compiler.generated.RccTokenManager.jjMoveStringLiteralDfa0_0()
2926   33  25    0 org.apache.hadoop.record.compiler.generated.RccTokenManager.jjMoveStringLiteralDfa1_0(long)
2927   33  26    0 org.apache.hadoop.record.compiler.generated.RccTokenManager.jjMoveStringLiteralDfa2_0(long,long)
2928   35  27    0 org.apache.hadoop.record.compiler.generated.RccTokenManager.jjMoveStringLiteralDfa3_0(long,long)
2929   29  21    0 org.apache.hadoop.record.compiler.generated.RccTokenManager.jjMoveStringLiteralDfa4_0(long,long)
2930   31  21    0 org.apache.hadoop.record.compiler.generated.RccTokenManager.jjMoveStringLiteralDfa5_0(long,long)
2931   23  14    0 org.apache.hadoop.record.compiler.generated.RccTokenManager.jjMoveStringLiteralDfa6_0(long,long)
2932   83  31    0 org.apache.hadoop.record.compiler.generated.RccTokenManager.jjMoveNfa_0(int,int)
2933    6   3    0 org.apache.hadoop.record.compiler.generated.RccTokenManager.jjMoveStringLiteralDfa0_2()
2934   12   7    0 org.apache.hadoop.record.compiler.generated.RccTokenManager.jjMoveStringLiteralDfa1_2(long)
2935    4   3    0 org.apache.hadoop.record.compiler.generated.RccTokenManager.RccTokenManager(SimpleCharStream)
2936    3   1    0 org.apache.hadoop.record.compiler.generated.RccTokenManager.RccTokenManager(SimpleCharStream,int)
2937    5   1    0 org.apache.hadoop.record.compiler.generated.RccTokenManager.ReInit(SimpleCharStream)
2938    5   2    0 org.apache.hadoop.record.compiler.generated.RccTokenManager.ReInitRounds()
2939    3   1    0 org.apache.hadoop.record.compiler.generated.RccTokenManager.ReInit(SimpleCharStream,int)
2940    5   4    0 org.apache.hadoop.record.compiler.generated.RccTokenManager.SwitchTo(int)
2941   10   2    0 org.apache.hadoop.record.compiler.generated.RccTokenManager.jjFillToken()
2942   91  33    0 org.apache.hadoop.record.compiler.generated.RccTokenManager.getNextToken()
2943    4   1    0 org.apache.hadoop.record.compiler.generated.RccTokenManager.SkipLexicalActions(Token)
2944    2   1    0 org.apache.hadoop.record.compiler.generated.SimpleCharStream.setTabSize(int)
2945    2   1    0 org.apache.hadoop.record.compiler.generated.SimpleCharStream.getTabSize(int)
2946   28   4    0 org.apache.hadoop.record.compiler.generated.SimpleCharStream.ExpandBuff(boolean)
2947   32  13    0 org.apache.hadoop.record.compiler.generated.SimpleCharStream.FillBuff()
2948    5   1    0 org.apache.hadoop.record.compiler.generated.SimpleCharStream.BeginToken()
2949   27   7    0 org.apache.hadoop.record.compiler.generated.SimpleCharStream.UpdateLineColumn(char)
2950   11   5    0 org.apache.hadoop.record.compiler.generated.SimpleCharStream.readChar()
2951    2   1    0 org.apache.hadoop.record.compiler.generated.SimpleCharStream.getEndColumn()
2952    2   1    0 org.apache.hadoop.record.compiler.generated.SimpleCharStream.getEndLine()
2953    2   1    0 org.apache.hadoop.record.compiler.generated.SimpleCharStream.getBeginColumn()
2954    2   1    0 org.apache.hadoop.record.compiler.generated.SimpleCharStream.getBeginLine()
2955    4   2    0 org.apache.hadoop.record.compiler.generated.SimpleCharStream.backup(int)
2956    8   1    0 org.apache.hadoop.record.compiler.generated.SimpleCharStream.SimpleCharStream(java.io.Reader,int,int,int)
2957    2   1    0 org.apache.hadoop.record.compiler.generated.SimpleCharStream.SimpleCharStream(java.io.Reader,int,int)
2958    2   1    0 org.apache.hadoop.record.compiler.generated.SimpleCharStream.SimpleCharStream(java.io.Reader)
2959   12   3    0 org.apache.hadoop.record.compiler.generated.SimpleCharStream.ReInit(java.io.Reader,int,int,int)
2960    2   1    0 org.apache.hadoop.record.compiler.generated.SimpleCharStream.ReInit(java.io.Reader,int,int)
2961    2   1    0 org.apache.hadoop.record.compiler.generated.SimpleCharStream.ReInit(java.io.Reader)
2962    2   2    0 org.apache.hadoop.record.compiler.generated.SimpleCharStream.SimpleCharStream(java.io.InputStream,String,int,int,int)
2963    2   1    0 org.apache.hadoop.record.compiler.generated.SimpleCharStream.SimpleCharStream(java.io.InputStream,int,int,int)
2964    2   1    0 org.apache.hadoop.record.compiler.generated.SimpleCharStream.SimpleCharStream(java.io.InputStream,String,int,int)
2965    2   1    0 org.apache.hadoop.record.compiler.generated.SimpleCharStream.SimpleCharStream(java.io.InputStream,int,int)
2966    2   1    0 org.apache.hadoop.record.compiler.generated.SimpleCharStream.SimpleCharStream(java.io.InputStream,String)
2967    2   1    0 org.apache.hadoop.record.compiler.generated.SimpleCharStream.SimpleCharStream(java.io.InputStream)
2968    2   2    0 org.apache.hadoop.record.compiler.generated.SimpleCharStream.ReInit(java.io.InputStream,String,int,int,int)
2969    2   1    0 org.apache.hadoop.record.compiler.generated.SimpleCharStream.ReInit(java.io.InputStream,int,int,int)
2970    2   1    0 org.apache.hadoop.record.compiler.generated.SimpleCharStream.ReInit(java.io.InputStream,String)
2971    2   1    0 org.apache.hadoop.record.compiler.generated.SimpleCharStream.ReInit(java.io.InputStream)
2972    2   1    0 org.apache.hadoop.record.compiler.generated.SimpleCharStream.ReInit(java.io.InputStream,String,int,int)
2973    2   1    0 org.apache.hadoop.record.compiler.generated.SimpleCharStream.ReInit(java.io.InputStream,int,int)
2974    5   3    0 org.apache.hadoop.record.compiler.generated.SimpleCharStream.GetImage()
2975    8   2    0 org.apache.hadoop.record.compiler.generated.SimpleCharStream.GetSuffix(int)
2976    4   1    0 org.apache.hadoop.record.compiler.generated.SimpleCharStream.Done()
2977   25   7    1 org.apache.hadoop.record.compiler.generated.SimpleCharStream.adjustBeginLineColumn(int,int)
2978    2   1    1 org.apache.hadoop.record.compiler.generated.Token.toString()
2979    4   1    1 org.apache.hadoop.record.compiler.generated.Token.newToken(int)
2980   39  13    1 org.apache.hadoop.record.compiler.generated.TokenMgrError.addEscapes(String)
2981    2   2    1 org.apache.hadoop.record.compiler.generated.TokenMgrError.LexicalError(boolean,int,int,int,String,char)
2982    2   1    1 org.apache.hadoop.record.compiler.generated.TokenMgrError.getMessage()
2983    1   1    0 org.apache.hadoop.record.compiler.generated.TokenMgrError.TokenMgrError()
2984    3   1    0 org.apache.hadoop.record.compiler.generated.TokenMgrError.TokenMgrError(String,int)
2985    2   1    0 org.apache.hadoop.record.compiler.generated.TokenMgrError.TokenMgrError(boolean,int,int,int,String,char,int)
2986    1   1    0 org.apache.hadoop.record.compiler.JavaGenerator.JavaGenerator()
2987    4   2    1 org.apache.hadoop.record.compiler.JavaGenerator.genCode(String,ArrayList,ArrayList,String,ArrayList)
2988    2   1    0 org.apache.hadoop.record.compiler.JBoolean.JavaBoolean.JavaBoolean()
2989    2   1    0 org.apache.hadoop.record.compiler.JBoolean.JavaBoolean.genCompareTo(CodeBuffer,String,String)
2990    2   1    0 org.apache.hadoop.record.compiler.JBoolean.JavaBoolean.getTypeIDObjectString()
2991    2   1    0 org.apache.hadoop.record.compiler.JBoolean.JavaBoolean.genHashCode(CodeBuffer,String)
2992    7   1    0 org.apache.hadoop.record.compiler.JBoolean.JavaBoolean.genSlurpBytes(CodeBuffer,String,String,String)
2993   10   1    0 org.apache.hadoop.record.compiler.JBoolean.JavaBoolean.genCompareBytes(CodeBuffer)
2994    2   1    0 org.apache.hadoop.record.compiler.JBoolean.CppBoolean.CppBoolean()
2995    2   1    0 org.apache.hadoop.record.compiler.JBoolean.CppBoolean.getTypeIDObjectString()
2996    4   1    1 org.apache.hadoop.record.compiler.JBoolean.JBoolean()
2997    2   1    0 org.apache.hadoop.record.compiler.JBoolean.getSignature()
2998    2   1    0 org.apache.hadoop.record.compiler.JBuffer.JavaBuffer.JavaBuffer()
2999    2   1    0 org.apache.hadoop.record.compiler.JBuffer.JavaBuffer.getTypeIDObjectString()
3000    2   1    0 org.apache.hadoop.record.compiler.JBuffer.JavaBuffer.genCompareTo(CodeBuffer,String,String)
3001    2   1    0 org.apache.hadoop.record.compiler.JBuffer.JavaBuffer.genEquals(CodeBuffer,String,String)
3002    2   1    0 org.apache.hadoop.record.compiler.JBuffer.JavaBuffer.genHashCode(CodeBuffer,String)
3003    6   1    0 org.apache.hadoop.record.compiler.JBuffer.JavaBuffer.genSlurpBytes(CodeBuffer,String,String,String)
3004   11   1    0 org.apache.hadoop.record.compiler.JBuffer.JavaBuffer.genCompareBytes(CodeBuffer)
3005    2   1    0 org.apache.hadoop.record.compiler.JBuffer.CppBuffer.CppBuffer()
3006    7   1    0 org.apache.hadoop.record.compiler.JBuffer.CppBuffer.genGetSet(CodeBuffer,String)
3007    2   1    0 org.apache.hadoop.record.compiler.JBuffer.CppBuffer.getTypeIDObjectString()
3008    4   1    1 org.apache.hadoop.record.compiler.JBuffer.JBuffer()
3009    2   1    0 org.apache.hadoop.record.compiler.JBuffer.getSignature()
3010    2   1    0 org.apache.hadoop.record.compiler.JByte.JavaByte.JavaByte()
3011    2   1    0 org.apache.hadoop.record.compiler.JByte.JavaByte.getTypeIDObjectString()
3012    7   1    0 org.apache.hadoop.record.compiler.JByte.JavaByte.genSlurpBytes(CodeBuffer,String,String,String)
3013   10   1    0 org.apache.hadoop.record.compiler.JByte.JavaByte.genCompareBytes(CodeBuffer)
3014    2   1    0 org.apache.hadoop.record.compiler.JByte.CppByte.CppByte()
3015    2   1    0 org.apache.hadoop.record.compiler.JByte.CppByte.getTypeIDObjectString()
3016    4   1    0 org.apache.hadoop.record.compiler.JByte.JByte()
3017    2   1    0 org.apache.hadoop.record.compiler.JByte.getSignature()
3018    2   1    0 org.apache.hadoop.record.compiler.JCompType.JavaCompType.JavaCompType(String,String,String,String)
3019    2   1    0 org.apache.hadoop.record.compiler.JCompType.JavaCompType.genCompareTo(CodeBuffer,String,String)
3020    2   1    0 org.apache.hadoop.record.compiler.JCompType.JavaCompType.genEquals(CodeBuffer,String,String)
3021    2   1    0 org.apache.hadoop.record.compiler.JCompType.JavaCompType.genHashCode(CodeBuffer,String)
3022    2   1    0 org.apache.hadoop.record.compiler.JCompType.JavaCompType.genClone(CodeBuffer,String)
3023    2   1    0 org.apache.hadoop.record.compiler.JCompType.CppCompType.CppCompType(String)
3024    7   1    0 org.apache.hadoop.record.compiler.JCompType.CppCompType.genGetSet(CodeBuffer,String)
3025    2   1    0 org.apache.hadoop.record.compiler.JDouble.JavaDouble.JavaDouble()
3026    2   1    0 org.apache.hadoop.record.compiler.JDouble.JavaDouble.getTypeIDObjectString()
3027    3   1    0 org.apache.hadoop.record.compiler.JDouble.JavaDouble.genHashCode(CodeBuffer,String)
3028    7   1    0 org.apache.hadoop.record.compiler.JDouble.JavaDouble.genSlurpBytes(CodeBuffer,String,String,String)
3029   12   1    0 org.apache.hadoop.record.compiler.JDouble.JavaDouble.genCompareBytes(CodeBuffer)
3030    2   1    0 org.apache.hadoop.record.compiler.JDouble.CppDouble.CppDouble()
3031    2   1    0 org.apache.hadoop.record.compiler.JDouble.CppDouble.getTypeIDObjectString()
3032    4   1    1 org.apache.hadoop.record.compiler.JDouble.JDouble()
3033    2   1    0 org.apache.hadoop.record.compiler.JDouble.getSignature()
3034    3   1    1 org.apache.hadoop.record.compiler.JField.JField(String,T)
3035    2   1    0 org.apache.hadoop.record.compiler.JField.getName()
3036    2   1    0 org.apache.hadoop.record.compiler.JField.getType()
3037    4   1    1 org.apache.hadoop.record.compiler.JFile.JFile(String,ArrayList,ArrayList)
3038    3   2    1 org.apache.hadoop.record.compiler.JFile.getName()
3039    8   3    1 org.apache.hadoop.record.compiler.JFile.genCode(String,String,ArrayList)
3040    2   1    0 org.apache.hadoop.record.compiler.JFloat.JavaFloat.JavaFloat()
3041    2   1    0 org.apache.hadoop.record.compiler.JFloat.JavaFloat.getTypeIDObjectString()
3042    2   1    0 org.apache.hadoop.record.compiler.JFloat.JavaFloat.genHashCode(CodeBuffer,String)
3043    7   1    0 org.apache.hadoop.record.compiler.JFloat.JavaFloat.genSlurpBytes(CodeBuffer,String,String,String)
3044   12   1    0 org.apache.hadoop.record.compiler.JFloat.JavaFloat.genCompareBytes(CodeBuffer)
3045    2   1    0 org.apache.hadoop.record.compiler.JFloat.CppFloat.CppFloat()
3046    2   1    0 org.apache.hadoop.record.compiler.JFloat.CppFloat.getTypeIDObjectString()
3047    4   1    1 org.apache.hadoop.record.compiler.JFloat.JFloat()
3048    2   1    0 org.apache.hadoop.record.compiler.JFloat.getSignature()
3049    2   1    0 org.apache.hadoop.record.compiler.JInt.JavaInt.JavaInt()
3050    2   1    0 org.apache.hadoop.record.compiler.JInt.JavaInt.getTypeIDObjectString()
3051    6   1    0 org.apache.hadoop.record.compiler.JInt.JavaInt.genSlurpBytes(CodeBuffer,String,String,String)
3052   11   1    0 org.apache.hadoop.record.compiler.JInt.JavaInt.genCompareBytes(CodeBuffer)
3053    2   1    0 org.apache.hadoop.record.compiler.JInt.CppInt.CppInt()
3054    2   1    0 org.apache.hadoop.record.compiler.JInt.CppInt.getTypeIDObjectString()
3055    4   1    1 org.apache.hadoop.record.compiler.JInt.JInt()
3056    2   1    0 org.apache.hadoop.record.compiler.JInt.getSignature()
3057    2   1    0 org.apache.hadoop.record.compiler.JLong.JavaLong.JavaLong()
3058    2   1    0 org.apache.hadoop.record.compiler.JLong.JavaLong.getTypeIDObjectString()
3059    2   1    0 org.apache.hadoop.record.compiler.JLong.JavaLong.genHashCode(CodeBuffer,String)
3060    6   1    0 org.apache.hadoop.record.compiler.JLong.JavaLong.genSlurpBytes(CodeBuffer,String,String,String)
3061   11   1    0 org.apache.hadoop.record.compiler.JLong.JavaLong.genCompareBytes(CodeBuffer)
3062    2   1    0 org.apache.hadoop.record.compiler.JLong.CppLong.CppLong()
3063    2   1    0 org.apache.hadoop.record.compiler.JLong.CppLong.getTypeIDObjectString()
3064    4   1    1 org.apache.hadoop.record.compiler.JLong.JLong()
3065    2   1    0 org.apache.hadoop.record.compiler.JLong.getSignature()
3066    2   1    0 org.apache.hadoop.record.compiler.JMap.getLevel()
3067    2   1    0 org.apache.hadoop.record.compiler.JMap.incrLevel()
3068    2   1    0 org.apache.hadoop.record.compiler.JMap.decrLevel()
3069    2   1    0 org.apache.hadoop.record.compiler.JMap.getId(String)
3070    4   1    0 org.apache.hadoop.record.compiler.JMap.JavaMap.JavaMap(JType.JavaType,JType.JavaType)
3071    2   1    0 org.apache.hadoop.record.compiler.JMap.JavaMap.getTypeIDObjectString()
3072    3   1    0 org.apache.hadoop.record.compiler.JMap.JavaMap.genSetRTIFilter(CodeBuffer,Map)
3073   16   1    0 org.apache.hadoop.record.compiler.JMap.JavaMap.genCompareTo(CodeBuffer,String,String)
3074   15   2    0 org.apache.hadoop.record.compiler.JMap.JavaMap.genReadMethod(CodeBuffer,String,String,boolean)
3075   18   1    0 org.apache.hadoop.record.compiler.JMap.JavaMap.genWriteMethod(CodeBuffer,String,String)
3076   12   1    0 org.apache.hadoop.record.compiler.JMap.JavaMap.genSlurpBytes(CodeBuffer,String,String,String)
3077   16   1    0 org.apache.hadoop.record.compiler.JMap.JavaMap.genCompareBytes(CodeBuffer)
3078    4   1    0 org.apache.hadoop.record.compiler.JMap.CppMap.CppMap(JType.CppType,JType.CppType)
3079    2   1    0 org.apache.hadoop.record.compiler.JMap.CppMap.getTypeIDObjectString()
3080    3   1    0 org.apache.hadoop.record.compiler.JMap.CppMap.genSetRTIFilter(CodeBuffer)
3081    6   1    1 org.apache.hadoop.record.compiler.JMap.JMap(JType,JType)
3082    2   1    0 org.apache.hadoop.record.compiler.JMap.getSignature()
3083    9   2    0 org.apache.hadoop.record.compiler.JRecord.JavaRecord.JavaRecord(String,ArrayList)
3084    2   1    0 org.apache.hadoop.record.compiler.JRecord.JavaRecord.getTypeIDObjectString()
3085    4   2    0 org.apache.hadoop.record.compiler.JRecord.JavaRecord.genSetRTIFilter(CodeBuffer,Map)
3086   27   1    0 org.apache.hadoop.record.compiler.JRecord.JavaRecord.genSetupRtiFields(CodeBuffer)
3087    5   2    0 org.apache.hadoop.record.compiler.JRecord.JavaRecord.genReadMethod(CodeBuffer,String,String,boolean)
3088    2   1    0 org.apache.hadoop.record.compiler.JRecord.JavaRecord.genWriteMethod(CodeBuffer,String,String)
3089    5   1    0 org.apache.hadoop.record.compiler.JRecord.JavaRecord.genSlurpBytes(CodeBuffer,String,String,String)
3090    6   1    0 org.apache.hadoop.record.compiler.JRecord.JavaRecord.genCompareBytes(CodeBuffer)
3091  211  24    0 org.apache.hadoop.record.compiler.JRecord.JavaRecord.genCode(String,ArrayList)
3092    9   2    0 org.apache.hadoop.record.compiler.JRecord.CppRecord.CppRecord(String,ArrayList)
3093    2   1    0 org.apache.hadoop.record.compiler.JRecord.CppRecord.getTypeIDObjectString()
3094    2   1    0 org.apache.hadoop.record.compiler.JRecord.CppRecord.genDecl(String)
3095    2   1    0 org.apache.hadoop.record.compiler.JRecord.CppRecord.genSetRTIFilter(CodeBuffer)
3096   17   1    0 org.apache.hadoop.record.compiler.JRecord.CppRecord.genSetupRTIFields(CodeBuffer)
3097  153  17    0 org.apache.hadoop.record.compiler.JRecord.CppRecord.genCode(FileWriter,FileWriter,ArrayList)
3098   13   2    1 org.apache.hadoop.record.compiler.JRecord.JRecord(String,ArrayList)
3099    2   1    0 org.apache.hadoop.record.compiler.JRecord.getSignature()
3100    2   1    0 org.apache.hadoop.record.compiler.JRecord.genCppCode(FileWriter,FileWriter,ArrayList)
3101    2   1    0 org.apache.hadoop.record.compiler.JRecord.genJavaCode(String,ArrayList)
3102    2   1    0 org.apache.hadoop.record.compiler.JString.JavaString.JavaString()
3103    2   1    0 org.apache.hadoop.record.compiler.JString.JavaString.getTypeIDObjectString()
3104    6   1    0 org.apache.hadoop.record.compiler.JString.JavaString.genSlurpBytes(CodeBuffer,String,String,String)
3105   11   1    0 org.apache.hadoop.record.compiler.JString.JavaString.genCompareBytes(CodeBuffer)
3106    2   1    0 org.apache.hadoop.record.compiler.JString.JavaString.genClone(CodeBuffer,String)
3107    2   1    0 org.apache.hadoop.record.compiler.JString.CppString.CppString()
3108    2   1    0 org.apache.hadoop.record.compiler.JString.CppString.getTypeIDObjectString()
3109    4   1    1 org.apache.hadoop.record.compiler.JString.JString()
3110    2   1    0 org.apache.hadoop.record.compiler.JString.getSignature()
3111    5   3    0 org.apache.hadoop.record.compiler.JType.toCamelCase(String)
3112    5   1    0 org.apache.hadoop.record.compiler.JType.JavaType.JavaType(String,String,String,String)
3113    2   1    0 org.apache.hadoop.record.compiler.JType.JavaType.genDecl(CodeBuffer,String)
3114    2   1    0 org.apache.hadoop.record.compiler.JType.JavaType.genStaticTypeInfo(CodeBuffer,String)
3115    1   1    0 org.apache.hadoop.record.compiler.JType.JavaType.getTypeIDObjectString()
3116    2   1    0 org.apache.hadoop.record.compiler.JType.JavaType.genSetRTIFilter(CodeBuffer,Map)
3117    2   1    0 org.apache.hadoop.record.compiler.JType.JavaType.genConstructorParam(CodeBuffer,String)
3118    7   1    0 org.apache.hadoop.record.compiler.JType.JavaType.genGetSet(CodeBuffer,String)
3119    2   1    0 org.apache.hadoop.record.compiler.JType.JavaType.getType()
3120    2   1    0 org.apache.hadoop.record.compiler.JType.JavaType.getWrapperType()
3121    2   1    0 org.apache.hadoop.record.compiler.JType.JavaType.getMethodSuffix()
3122    2   1    0 org.apache.hadoop.record.compiler.JType.JavaType.getTypeIDByteString()
3123    2   1    0 org.apache.hadoop.record.compiler.JType.JavaType.genWriteMethod(CodeBuffer,String,String)
3124    4   2    0 org.apache.hadoop.record.compiler.JType.JavaType.genReadMethod(CodeBuffer,String,String,boolean)
3125    2   1    0 org.apache.hadoop.record.compiler.JType.JavaType.genCompareTo(CodeBuffer,String,String)
3126    1   1    0 org.apache.hadoop.record.compiler.JType.JavaType.genCompareBytes(CodeBuffer)
3127    1   1    0 org.apache.hadoop.record.compiler.JType.JavaType.genSlurpBytes(CodeBuffer,String,String,String)
3128    2   1    0 org.apache.hadoop.record.compiler.JType.JavaType.genEquals(CodeBuffer,String,String)
3129    2   1    0 org.apache.hadoop.record.compiler.JType.JavaType.genHashCode(CodeBuffer,String)
3130    2   1    0 org.apache.hadoop.record.compiler.JType.JavaType.genConstructorSet(CodeBuffer,String)
3131    2   1    0 org.apache.hadoop.record.compiler.JType.JavaType.genClone(CodeBuffer,String)
3132    2   1    0 org.apache.hadoop.record.compiler.JType.CppType.CppType(String)
3133    2   1    0 org.apache.hadoop.record.compiler.JType.CppType.genDecl(CodeBuffer,String)
3134    2   1    0 org.apache.hadoop.record.compiler.JType.CppType.genStaticTypeInfo(CodeBuffer,String)
3135    7   1    0 org.apache.hadoop.record.compiler.JType.CppType.genGetSet(CodeBuffer,String)
3136    1   1    0 org.apache.hadoop.record.compiler.JType.CppType.getTypeIDObjectString()
3137    2   1    0 org.apache.hadoop.record.compiler.JType.CppType.genSetRTIFilter(CodeBuffer)
3138    2   1    0 org.apache.hadoop.record.compiler.JType.CppType.getType()
3139    1   1    0 org.apache.hadoop.record.compiler.JType.getSignature()
3140    2   1    0 org.apache.hadoop.record.compiler.JType.setJavaType(JavaType)
3141    2   1    0 org.apache.hadoop.record.compiler.JType.getJavaType()
3142    2   1    0 org.apache.hadoop.record.compiler.JType.setCppType(CppType)
3143    2   1    0 org.apache.hadoop.record.compiler.JType.getCppType()
3144    2   1    0 org.apache.hadoop.record.compiler.JType.setCType(CType)
3145    2   1    0 org.apache.hadoop.record.compiler.JType.getCType()
3146    2   1    0 org.apache.hadoop.record.compiler.JVector.getId(String)
3147    2   1    0 org.apache.hadoop.record.compiler.JVector.getLevel()
3148    2   1    0 org.apache.hadoop.record.compiler.JVector.incrLevel()
3149    2   1    0 org.apache.hadoop.record.compiler.JVector.decrLevel()
3150    3   1    0 org.apache.hadoop.record.compiler.JVector.JavaVector.JavaVector(JType.JavaType)
3151    2   1    0 org.apache.hadoop.record.compiler.JVector.JavaVector.getTypeIDObjectString()
3152    2   1    0 org.apache.hadoop.record.compiler.JVector.JavaVector.genSetRTIFilter(CodeBuffer,Map)
3153   14   1    0 org.apache.hadoop.record.compiler.JVector.JavaVector.genCompareTo(CodeBuffer,String,String)
3154   14   2    0 org.apache.hadoop.record.compiler.JVector.JavaVector.genReadMethod(CodeBuffer,String,String,boolean)
3155   12   1    0 org.apache.hadoop.record.compiler.JVector.JavaVector.genWriteMethod(CodeBuffer,String,String)
3156   10   1    0 org.apache.hadoop.record.compiler.JVector.JavaVector.genSlurpBytes(CodeBuffer,String,String,String)
3157   13   1    0 org.apache.hadoop.record.compiler.JVector.JavaVector.genCompareBytes(CodeBuffer)
3158    3   1    0 org.apache.hadoop.record.compiler.JVector.CppVector.CppVector(JType.CppType)
3159    2   1    0 org.apache.hadoop.record.compiler.JVector.CppVector.getTypeIDObjectString()
3160    2   1    0 org.apache.hadoop.record.compiler.JVector.CppVector.genSetRTIFilter(CodeBuffer)
3161    5   1    1 org.apache.hadoop.record.compiler.JVector.JVector(JType)
3162    2   1    0 org.apache.hadoop.record.compiler.JVector.getSignature()
3163    6   3    0 org.apache.hadoop.record.CsvRecordInput.CsvIndex.done()
3164    1   1    0 org.apache.hadoop.record.CsvRecordInput.CsvIndex.incr()
3165    2   2    0 org.apache.hadoop.record.CsvRecordInput.throwExceptionOnError(String)
3166   16  10    0 org.apache.hadoop.record.CsvRecordInput.readField(String)
3167    4   3    1 org.apache.hadoop.record.CsvRecordInput.CsvRecordInput(InputStream)
3168    2   1    0 org.apache.hadoop.record.CsvRecordInput.readByte(String)
3169    3   2    0 org.apache.hadoop.record.CsvRecordInput.readBool(String)
3170    2   1    0 org.apache.hadoop.record.CsvRecordInput.readInt(String)
3171    6   4    0 org.apache.hadoop.record.CsvRecordInput.readLong(String)
3172    2   1    0 org.apache.hadoop.record.CsvRecordInput.readFloat(String)
3173    6   4    0 org.apache.hadoop.record.CsvRecordInput.readDouble(String)
3174    3   1    0 org.apache.hadoop.record.CsvRecordInput.readString(String)
3175    3   1    0 org.apache.hadoop.record.CsvRecordInput.readBuffer(String)
3176    6   6    0 org.apache.hadoop.record.CsvRecordInput.startRecord(String)
3177   13  10    0 org.apache.hadoop.record.CsvRecordInput.endRecord(String)
3178    6   4    0 org.apache.hadoop.record.CsvRecordInput.startVector(String)
3179    8   4    0 org.apache.hadoop.record.CsvRecordInput.endVector(String)
3180    6   4    0 org.apache.hadoop.record.CsvRecordInput.startMap(String)
3181    8   4    0 org.apache.hadoop.record.CsvRecordInput.endMap(String)
3182    3   3    0 org.apache.hadoop.record.CsvRecordOutput.throwExceptionOnError(String)
3183    4   2    0 org.apache.hadoop.record.CsvRecordOutput.printCommaUnlessFirst()
3184    4   3    1 org.apache.hadoop.record.CsvRecordOutput.CsvRecordOutput(OutputStream)
3185    2   1    0 org.apache.hadoop.record.CsvRecordOutput.writeByte(byte,String)
3186    5   2    0 org.apache.hadoop.record.CsvRecordOutput.writeBool(boolean,String)
3187    2   1    0 org.apache.hadoop.record.CsvRecordOutput.writeInt(int,String)
3188    4   1    0 org.apache.hadoop.record.CsvRecordOutput.writeLong(long,String)
3189    2   1    0 org.apache.hadoop.record.CsvRecordOutput.writeFloat(float,String)
3190    4   1    0 org.apache.hadoop.record.CsvRecordOutput.writeDouble(double,String)
3191    4   1    0 org.apache.hadoop.record.CsvRecordOutput.writeString(String,String)
3192    4   1    0 org.apache.hadoop.record.CsvRecordOutput.writeBuffer(Buffer,String)
3193    5   3    0 org.apache.hadoop.record.CsvRecordOutput.startRecord(Record,String)
3194    7   3    0 org.apache.hadoop.record.CsvRecordOutput.endRecord(Record,String)
3195    4   1    0 org.apache.hadoop.record.CsvRecordOutput.startVector(ArrayList,String)
3196    3   1    0 org.apache.hadoop.record.CsvRecordOutput.endVector(ArrayList,String)
3197    4   1    0 org.apache.hadoop.record.CsvRecordOutput.startMap(TreeMap,String)
3198    3   1    0 org.apache.hadoop.record.CsvRecordOutput.endMap(TreeMap,String)
3199    1   1    0 org.apache.hadoop.record.Index.done()
3200    1   1    0 org.apache.hadoop.record.Index.incr()
3201    3   1    1 org.apache.hadoop.record.meta.FieldTypeInfo.FieldTypeInfo(String,TypeID)
3202    2   1    1 org.apache.hadoop.record.meta.FieldTypeInfo.getTypeID()
3203    2   1    1 org.apache.hadoop.record.meta.FieldTypeInfo.getFieldID()
3204    3   1    0 org.apache.hadoop.record.meta.FieldTypeInfo.write(RecordOutput,String)
3205    9   7    1 org.apache.hadoop.record.meta.FieldTypeInfo.equals(Object)
3206    2   1    1 org.apache.hadoop.record.meta.FieldTypeInfo.hashCode()
3207    4   3    0 org.apache.hadoop.record.meta.FieldTypeInfo.equals(FieldTypeInfo)
3208    4   1    0 org.apache.hadoop.record.meta.MapTypeID.MapTypeID(TypeID,TypeID)
3209    2   1    1 org.apache.hadoop.record.meta.MapTypeID.getKeyTypeID()
3210    2   1    1 org.apache.hadoop.record.meta.MapTypeID.getValueTypeID()
3211    4   1    0 org.apache.hadoop.record.meta.MapTypeID.write(RecordOutput,String)
3212    9   7    1 org.apache.hadoop.record.meta.MapTypeID.equals(Object)
3213    2   1    1 org.apache.hadoop.record.meta.MapTypeID.hashCode()
3214    2   1    1 org.apache.hadoop.record.meta.RecordTypeInfo.RecordTypeInfo()
3215    3   1    1 org.apache.hadoop.record.meta.RecordTypeInfo.RecordTypeInfo(String)
3216    3   1    0 org.apache.hadoop.record.meta.RecordTypeInfo.RecordTypeInfo(String,StructTypeID)
3217    2   1    1 org.apache.hadoop.record.meta.RecordTypeInfo.getName()
3218    2   1    1 org.apache.hadoop.record.meta.RecordTypeInfo.setName(String)
3219    2   1    1 org.apache.hadoop.record.meta.RecordTypeInfo.addField(String,TypeID)
3220    2   1    0 org.apache.hadoop.record.meta.RecordTypeInfo.addAll(Collection)
3221    2   1    1 org.apache.hadoop.record.meta.RecordTypeInfo.getFieldTypeInfos()
3222    5   3    1 org.apache.hadoop.record.meta.RecordTypeInfo.getNestedStructTypeInfo(String)
3223    5   1    1 org.apache.hadoop.record.meta.RecordTypeInfo.serialize(RecordOutput,String)
3224    5   1    1 org.apache.hadoop.record.meta.RecordTypeInfo.deserialize(RecordInput,String)
3225    4   4    1 org.apache.hadoop.record.meta.RecordTypeInfo.compareTo(Object)
3226    2   1    0 org.apache.hadoop.record.meta.StructTypeID.StructTypeID()
3227    3   1    1 org.apache.hadoop.record.meta.StructTypeID.StructTypeID(RecordTypeInfo)
3228    2   1    0 org.apache.hadoop.record.meta.StructTypeID.add(FieldTypeInfo)
3229    2   1    0 org.apache.hadoop.record.meta.StructTypeID.getFieldTypeInfos()
3230    5   5    0 org.apache.hadoop.record.meta.StructTypeID.findStruct(String)
3231    3   1    0 org.apache.hadoop.record.meta.StructTypeID.write(RecordOutput,String)
3232    4   2    0 org.apache.hadoop.record.meta.StructTypeID.writeRest(RecordOutput,String)
3233    4   2    0 org.apache.hadoop.record.meta.StructTypeID.read(RecordInput,String)
3234    4   1    0 org.apache.hadoop.record.meta.StructTypeID.genericReadTypeInfo(RecordInput,String)
3235   34  25    0 org.apache.hadoop.record.meta.StructTypeID.genericReadTypeID(RecordInput,String)
3236    2   1    1 org.apache.hadoop.record.meta.TypeID.TypeID(byte)
3237    2   1    1 org.apache.hadoop.record.meta.TypeID.getTypeVal()
3238    2   1    1 org.apache.hadoop.record.meta.TypeID.write(RecordOutput,String)
3239    7   5    1 org.apache.hadoop.record.meta.TypeID.equals(Object)
3240    2   1    1 org.apache.hadoop.record.meta.TypeID.hashCode()
3241    1   1    1 org.apache.hadoop.record.meta.Utils.Utils()
3242   52  16    1 org.apache.hadoop.record.meta.Utils.skip(RecordInput,String,TypeID)
3243    3   1    0 org.apache.hadoop.record.meta.VectorTypeID.VectorTypeID(TypeID)
3244    2   1    0 org.apache.hadoop.record.meta.VectorTypeID.getElementTypeID()
3245    3   1    0 org.apache.hadoop.record.meta.VectorTypeID.write(RecordOutput,String)
3246    7   5    1 org.apache.hadoop.record.meta.VectorTypeID.equals(Object)
3247    2   1    1 org.apache.hadoop.record.meta.VectorTypeID.hashCode()
3248    1   1    1 org.apache.hadoop.record.Record.serialize(RecordOutput,String)
3249    1   1    1 org.apache.hadoop.record.Record.deserialize(RecordInput,String)
3250    1   1    0 org.apache.hadoop.record.Record.compareTo(Object)
3251    2   1    0 org.apache.hadoop.record.Record.serialize(RecordOutput)
3252    2   1    1 org.apache.hadoop.record.Record.deserialize(RecordInput)
3253    3   1    0 org.apache.hadoop.record.Record.write(DataOutput)
3254    3   1    0 org.apache.hadoop.record.Record.readFields(DataInput)
3255    7   4    0 org.apache.hadoop.record.Record.toString()
3256    2   1    1 org.apache.hadoop.record.RecordComparator.RecordComparator(Class)
3257    1   1    0 org.apache.hadoop.record.RecordComparator.compare(byte[],int,int,byte[],int,int)
3258    2   1    1 org.apache.hadoop.record.RecordComparator.define(Class,RecordComparator)
3259    1   1    1 org.apache.hadoop.record.RecordInput.readByte(String)
3260    1   1    1 org.apache.hadoop.record.RecordInput.readBool(String)
3261    1   1    1 org.apache.hadoop.record.RecordInput.readInt(String)
3262    1   1    1 org.apache.hadoop.record.RecordInput.readLong(String)
3263    1   1    1 org.apache.hadoop.record.RecordInput.readFloat(String)
3264    1   1    1 org.apache.hadoop.record.RecordInput.readDouble(String)
3265    1   1    1 org.apache.hadoop.record.RecordInput.readString(String)
3266    1   1    1 org.apache.hadoop.record.RecordInput.readBuffer(String)
3267    1   1    1 org.apache.hadoop.record.RecordInput.startRecord(String)
3268    1   1    1 org.apache.hadoop.record.RecordInput.endRecord(String)
3269    1   1    1 org.apache.hadoop.record.RecordInput.startVector(String)
3270    1   1    1 org.apache.hadoop.record.RecordInput.endVector(String)
3271    1   1    1 org.apache.hadoop.record.RecordInput.startMap(String)
3272    1   1    1 org.apache.hadoop.record.RecordInput.endMap(String)
3273    1   1    1 org.apache.hadoop.record.RecordOutput.writeByte(byte,String)
3274    1   1    1 org.apache.hadoop.record.RecordOutput.writeBool(boolean,String)
3275    1   1    1 org.apache.hadoop.record.RecordOutput.writeInt(int,String)
3276    1   1    1 org.apache.hadoop.record.RecordOutput.writeLong(long,String)
3277    1   1    1 org.apache.hadoop.record.RecordOutput.writeFloat(float,String)
3278    1   1    1 org.apache.hadoop.record.RecordOutput.writeDouble(double,String)
3279    1   1    1 org.apache.hadoop.record.RecordOutput.writeString(String,String)
3280    1   1    1 org.apache.hadoop.record.RecordOutput.writeBuffer(Buffer,String)
3281    1   1    1 org.apache.hadoop.record.RecordOutput.startRecord(Record,String)
3282    1   1    1 org.apache.hadoop.record.RecordOutput.endRecord(Record,String)
3283    1   1    1 org.apache.hadoop.record.RecordOutput.startVector(ArrayList,String)
3284    1   1    1 org.apache.hadoop.record.RecordOutput.endVector(ArrayList,String)
3285    1   1    1 org.apache.hadoop.record.RecordOutput.startMap(TreeMap,String)
3286    1   1    1 org.apache.hadoop.record.RecordOutput.endMap(TreeMap,String)
3287    1   1    1 org.apache.hadoop.record.Utils.Utils()
3288   22   9    1 org.apache.hadoop.record.Utils.toXMLString(String)
3289   10  10    0 org.apache.hadoop.record.Utils.h2c(char)
3290   14   3    1 org.apache.hadoop.record.Utils.fromXMLString(String)
3291   28   8    1 org.apache.hadoop.record.Utils.toCSVString(String)
3292   33  18    1 org.apache.hadoop.record.Utils.fromCSVString(String)
3293    2   1    1 org.apache.hadoop.record.Utils.toXMLBuffer(Buffer)
3294   10   4    1 org.apache.hadoop.record.Utils.fromXMLBuffer(String)
3295    4   1    1 org.apache.hadoop.record.Utils.toCSVBuffer(Buffer)
3296   12   6    1 org.apache.hadoop.record.Utils.fromCSVBuffer(String)
3297   10  16    0 org.apache.hadoop.record.Utils.utf8LenForCodePoint(int)
3298   26  16    0 org.apache.hadoop.record.Utils.writeUtf8(int,byte[],int)
3299   11   3    0 org.apache.hadoop.record.Utils.toBinaryString(DataOutput,String)
3300    2   5    0 org.apache.hadoop.record.Utils.isValidCodePoint(int)
3301    4   1    0 org.apache.hadoop.record.Utils.utf8ToCodePoint(int,int,int,int)
3302    4   1    0 org.apache.hadoop.record.Utils.utf8ToCodePoint(int,int,int)
3303    4   1    0 org.apache.hadoop.record.Utils.utf8ToCodePoint(int,int)
3304    3   3    0 org.apache.hadoop.record.Utils.checkB10(int)
3305   38   9    0 org.apache.hadoop.record.Utils.fromBinaryString(DataInput)
3306    2   1    0 org.apache.hadoop.record.Utils.readFloat(byte[],int)
3307    2   1    1 org.apache.hadoop.record.Utils.readDouble(byte[],int)
3308    2   1    1 org.apache.hadoop.record.Utils.readVLong(byte[],int)
3309    2   1    1 org.apache.hadoop.record.Utils.readVInt(byte[],int)
3310    2   1    1 org.apache.hadoop.record.Utils.readVLong(DataInput)
3311    2   1    1 org.apache.hadoop.record.Utils.readVInt(DataInput)
3312    2   1    1 org.apache.hadoop.record.Utils.getVIntSize(long)
3313    2   1    1 org.apache.hadoop.record.Utils.writeVLong(DataOutput,long)
3314    2   1    1 org.apache.hadoop.record.Utils.writeVInt(DataOutput,int)
3315    2   1    1 org.apache.hadoop.record.Utils.compareBytes(byte[],int,int,byte[],int,int)
3316    3   1    0 org.apache.hadoop.record.XmlRecordInput.Value.Value(String)
3317    2   1    0 org.apache.hadoop.record.XmlRecordInput.Value.addChars(char[],int,int)
3318    2   1    0 org.apache.hadoop.record.XmlRecordInput.Value.getValue()
3319    2   1    0 org.apache.hadoop.record.XmlRecordInput.Value.getType()
3320    2   1    0 org.apache.hadoop.record.XmlRecordInput.XMLParser.XMLParser(ArrayList)
3321    1   1    0 org.apache.hadoop.record.XmlRecordInput.XMLParser.startDocument()
3322    1   1    0 org.apache.hadoop.record.XmlRecordInput.XMLParser.endDocument()
3323    8  11    0 org.apache.hadoop.record.XmlRecordInput.XMLParser.startElement(String,String,String,Attributes)
3324    4   3    0 org.apache.hadoop.record.XmlRecordInput.XMLParser.endElement(String,String,String)
3325    4   2    0 org.apache.hadoop.record.XmlRecordInput.XMLParser.characters(char[],int,int)
3326    8   3    0 org.apache.hadoop.record.XmlRecordInput.XmlIndex.done()
3327    1   1    0 org.apache.hadoop.record.XmlRecordInput.XmlIndex.incr()
3328    8   4    0 org.apache.hadoop.record.XmlRecordInput.next()
3329   10   3    1 org.apache.hadoop.record.XmlRecordInput.XmlRecordInput(InputStream)
3330    5   3    0 org.apache.hadoop.record.XmlRecordInput.readByte(String)
3331    5   3    0 org.apache.hadoop.record.XmlRecordInput.readBool(String)
3332    5   4    0 org.apache.hadoop.record.XmlRecordInput.readInt(String)
3333    5   3    0 org.apache.hadoop.record.XmlRecordInput.readLong(String)
3334    5   3    0 org.apache.hadoop.record.XmlRecordInput.readFloat(String)
3335    5   3    0 org.apache.hadoop.record.XmlRecordInput.readDouble(String)
3336    5   3    0 org.apache.hadoop.record.XmlRecordInput.readString(String)
3337    5   3    0 org.apache.hadoop.record.XmlRecordInput.readBuffer(String)
3338    4   3    0 org.apache.hadoop.record.XmlRecordInput.startRecord(String)
3339    4   3    0 org.apache.hadoop.record.XmlRecordInput.endRecord(String)
3340    5   3    0 org.apache.hadoop.record.XmlRecordInput.startVector(String)
3341    1   1    0 org.apache.hadoop.record.XmlRecordInput.endVector(String)
3342    2   1    0 org.apache.hadoop.record.XmlRecordInput.startMap(String)
3343    2   1    0 org.apache.hadoop.record.XmlRecordInput.endMap(String)
3344    5   2    0 org.apache.hadoop.record.XmlRecordOutput.putIndent()
3345    2   1    0 org.apache.hadoop.record.XmlRecordOutput.addIndent()
3346    2   1    0 org.apache.hadoop.record.XmlRecordOutput.closeIndent()
3347   19   5    0 org.apache.hadoop.record.XmlRecordOutput.printBeginEnvelope(String)
3348   16   5    0 org.apache.hadoop.record.XmlRecordOutput.printEndEnvelope(String)
3349    3   1    0 org.apache.hadoop.record.XmlRecordOutput.insideVector(String)
3350    5   3    0 org.apache.hadoop.record.XmlRecordOutput.outsideVector(String)
3351    3   1    0 org.apache.hadoop.record.XmlRecordOutput.insideMap(String)
3352    5   3    0 org.apache.hadoop.record.XmlRecordOutput.outsideMap(String)
3353    3   1    0 org.apache.hadoop.record.XmlRecordOutput.insideRecord(String)
3354    5   3    0 org.apache.hadoop.record.XmlRecordOutput.outsideRecord(String)
3355    5   3    1 org.apache.hadoop.record.XmlRecordOutput.XmlRecordOutput(OutputStream)
3356    6   1    0 org.apache.hadoop.record.XmlRecordOutput.writeByte(byte,String)
3357    6   2    0 org.apache.hadoop.record.XmlRecordOutput.writeBool(boolean,String)
3358    6   1    0 org.apache.hadoop.record.XmlRecordOutput.writeInt(int,String)
3359    6   1    0 org.apache.hadoop.record.XmlRecordOutput.writeLong(long,String)
3360    6   1    0 org.apache.hadoop.record.XmlRecordOutput.writeFloat(float,String)
3361    6   1    0 org.apache.hadoop.record.XmlRecordOutput.writeDouble(double,String)
3362    6   1    0 org.apache.hadoop.record.XmlRecordOutput.writeString(String,String)
3363    6   1    0 org.apache.hadoop.record.XmlRecordOutput.writeBuffer(Buffer,String)
3364    4   1    0 org.apache.hadoop.record.XmlRecordOutput.startRecord(Record,String)
3365    5   1    0 org.apache.hadoop.record.XmlRecordOutput.endRecord(Record,String)
3366    4   1    0 org.apache.hadoop.record.XmlRecordOutput.startVector(ArrayList,String)
3367    5   1    0 org.apache.hadoop.record.XmlRecordOutput.endVector(ArrayList,String)
3368    4   1    0 org.apache.hadoop.record.XmlRecordOutput.startMap(TreeMap,String)
3369    5   1    0 org.apache.hadoop.record.XmlRecordOutput.endMap(TreeMap,String)
3370    2   1    1 org.apache.hadoop.security.AccessControlException.AccessControlException()
3371    2   1    1 org.apache.hadoop.security.AccessControlException.AccessControlException(String)
3372    2   2    0 org.apache.hadoop.security.UnixUserGroupInformation.UnixUserGroupInformation$1.readFields(DataInput)
3373    4   1    1 org.apache.hadoop.security.UnixUserGroupInformation.createImmutable(String[])
3374    1   1    1 org.apache.hadoop.security.UnixUserGroupInformation.UnixUserGroupInformation()
3375    2   1    1 org.apache.hadoop.security.UnixUserGroupInformation.UnixUserGroupInformation(String,String[])
3376    6   4    1 org.apache.hadoop.security.UnixUserGroupInformation.UnixUserGroupInformation(String[])
3377    8  10    0 org.apache.hadoop.security.UnixUserGroupInformation.setUserGroupNames(String,String[])
3378    2   1    1 org.apache.hadoop.security.UnixUserGroupInformation.getGroupNames()
3379    2   1    1 org.apache.hadoop.security.UnixUserGroupInformation.getUserName()
3380    9   4    1 org.apache.hadoop.security.UnixUserGroupInformation.readFields(DataInput)
3381    6   2    1 org.apache.hadoop.security.UnixUserGroupInformation.write(DataOutput)
3382    2   1    1 org.apache.hadoop.security.UnixUserGroupInformation.saveToConf(Configuration,String,UnixUserGroupInformation)
3383   13   7    1 org.apache.hadoop.security.UnixUserGroupInformation.readFromConf(Configuration,String)
3384   11   6    1 org.apache.hadoop.security.UnixUserGroupInformation.login()
3385    2   1    1 org.apache.hadoop.security.UnixUserGroupInformation.login(Configuration)
3386    8   3    1 org.apache.hadoop.security.UnixUserGroupInformation.login(Configuration,boolean)
3387    8   5    0 org.apache.hadoop.security.UnixUserGroupInformation.toString(String[])
3388    5   3    1 org.apache.hadoop.security.UnixUserGroupInformation.getUnixUserName()
3389    2   1    1 org.apache.hadoop.security.UnixUserGroupInformation.getUnixGroups()
3390    8   2    0 org.apache.hadoop.security.UnixUserGroupInformation.executeShellCommand(String[])
3391   19  17    1 org.apache.hadoop.security.UnixUserGroupInformation.equals(Object)
3392    2   1    1 org.apache.hadoop.security.UnixUserGroupInformation.hashCode()
3393    7   2    1 org.apache.hadoop.security.UnixUserGroupInformation.toString()
3394    2   1    1 org.apache.hadoop.security.UserGroupInformation.getCurrentUGI()
3395    4   2    1 org.apache.hadoop.security.UserGroupInformation.setCurrentUGI(UserGroupInformation)
3396    1   1    1 org.apache.hadoop.security.UserGroupInformation.getUserName()
3397    1   1    1 org.apache.hadoop.security.UserGroupInformation.getGroupNames()
3398    4   2    1 org.apache.hadoop.security.UserGroupInformation.login(Configuration)
3399    4   4    1 org.apache.hadoop.security.UserGroupInformation.readFrom(Configuration)
3400    2   1    1 org.apache.hadoop.util.Daemon.Daemon()
3401    4   1    1 org.apache.hadoop.util.Daemon.Daemon(Runnable)
3402    4   1    1 org.apache.hadoop.util.Daemon.Daemon(ThreadGroup,Runnable)
3403    2   1    0 org.apache.hadoop.util.Daemon.getRunnable()
3404   10   7    0 org.apache.hadoop.util.DataChecksum.newDataChecksum(int,int)
3405    5   4    1 org.apache.hadoop.util.DataChecksum.newDataChecksum(byte[],int)
3406    7   3    1 org.apache.hadoop.util.DataChecksum.newDataChecksum(DataInputStream)
3407    3   1    1 org.apache.hadoop.util.DataChecksum.writeHeader(DataOutputStream)
3408    8   1    0 org.apache.hadoop.util.DataChecksum.getHeader()
3409   10   6    1 org.apache.hadoop.util.DataChecksum.writeValue(DataOutputStream,boolean)
3410   14   6    1 org.apache.hadoop.util.DataChecksum.writeValue(byte[],int,boolean)
3411    5   4    1 org.apache.hadoop.util.DataChecksum.compare(byte[],int)
3412    5   1    0 org.apache.hadoop.util.DataChecksum.DataChecksum(int,Checksum,int,int)
3413    2   1    0 org.apache.hadoop.util.DataChecksum.getChecksumType()
3414    2   1    0 org.apache.hadoop.util.DataChecksum.getChecksumSize()
3415    2   1    0 org.apache.hadoop.util.DataChecksum.getBytesPerChecksum()
3416    2   1    0 org.apache.hadoop.util.DataChecksum.getNumBytesInSum()
3417    2   1    0 org.apache.hadoop.util.DataChecksum.getChecksumHeaderSize()
3418    2   1    0 org.apache.hadoop.util.DataChecksum.getValue()
3419    3   1    0 org.apache.hadoop.util.DataChecksum.reset()
3420    4   2    0 org.apache.hadoop.util.DataChecksum.update(byte[],int,int)
3421    3   1    0 org.apache.hadoop.util.DataChecksum.update(int)
3422    1   1    0 org.apache.hadoop.util.DataChecksum.ChecksumNull.ChecksumNull()
3423    2   1    0 org.apache.hadoop.util.DataChecksum.ChecksumNull.getValue()
3424    1   1    0 org.apache.hadoop.util.DataChecksum.ChecksumNull.reset()
3425    1   1    0 org.apache.hadoop.util.DataChecksum.ChecksumNull.update(byte[],int,int)
3426    1   1    0 org.apache.hadoop.util.DataChecksum.ChecksumNull.update(int)
3427    2   1    0 org.apache.hadoop.util.DiskChecker.DiskErrorException.DiskErrorException(String)
3428    2   1    0 org.apache.hadoop.util.DiskChecker.DiskOutOfSpaceException.DiskOutOfSpaceException(String)
3429    9   9    1 org.apache.hadoop.util.DiskChecker.mkdirsWithExistsCheck(File)
3430    9   9    0 org.apache.hadoop.util.DiskChecker.checkDir(File)
3431    2   1    1 org.apache.hadoop.util.GenericOptionsParser.GenericOptionsParser(Configuration,String[])
3432    2   1    1 org.apache.hadoop.util.GenericOptionsParser.GenericOptionsParser(Configuration,Options,String[])
3433    2   2    1 org.apache.hadoop.util.GenericOptionsParser.getRemainingArgs()
3434    2   1    1 org.apache.hadoop.util.GenericOptionsParser.getCommandLine()
3435   16   1    0 org.apache.hadoop.util.GenericOptionsParser.buildGeneralOptions(Options)
3436   26  12    1 org.apache.hadoop.util.GenericOptionsParser.processGeneralOptions(Configuration,CommandLine)
3437   10   4    1 org.apache.hadoop.util.GenericOptionsParser.getLibJars(Configuration)
3438   24  10    1 org.apache.hadoop.util.GenericOptionsParser.validateFiles(String,Configuration)
3439   11   3    1 org.apache.hadoop.util.GenericOptionsParser.parseGeneralOptions(Options,Configuration,String[])
3440   11   1    1 org.apache.hadoop.util.GenericOptionsParser.printGenericCommandUsage(PrintStream)
3441    1   1    0 org.apache.hadoop.util.HeapSort.HeapSort()
3442   14   7    0 org.apache.hadoop.util.HeapSort.downHeap(IndexedSortable,int,int,int)
3443    2   1    0 org.apache.hadoop.util.HeapSort.sort(IndexedSortable,int,int)
3444   11   5    1 org.apache.hadoop.util.HeapSort.sort(IndexedSortable,int,int,Progressable)
3445    6   1    0 org.apache.hadoop.util.HostsFileReader.HostsFileReader(String,String)
3446   15   6    0 org.apache.hadoop.util.HostsFileReader.readFileToSet(String,Set)
3447    7   3    0 org.apache.hadoop.util.HostsFileReader.refresh()
3448    2   1    0 org.apache.hadoop.util.HostsFileReader.getHosts()
3449    2   1    0 org.apache.hadoop.util.HostsFileReader.getExcludedHosts()
3450    2   1    0 org.apache.hadoop.util.HostsFileReader.setIncludesFile(String)
3451    2   1    0 org.apache.hadoop.util.HostsFileReader.setExcludesFile(String)
3452    3   1    0 org.apache.hadoop.util.HostsFileReader.updateFileNames(String,String)
3453    1   1    1 org.apache.hadoop.util.IndexedSortable.compare(int,int)
3454    1   1    1 org.apache.hadoop.util.IndexedSortable.swap(int,int)
3455    1   1    1 org.apache.hadoop.util.IndexedSorter.sort(IndexedSortable,int,int)
3456    1   1    1 org.apache.hadoop.util.IndexedSorter.sort(IndexedSortable,int,int,Progressable)
3457    2   1    1 org.apache.hadoop.util.LineReader.LineReader(InputStream)
3458    4   1    1 org.apache.hadoop.util.LineReader.LineReader(InputStream,int)
3459    2   1    1 org.apache.hadoop.util.LineReader.LineReader(InputStream,Configuration)
3460    4   1    1 org.apache.hadoop.util.LineReader.backfill()
3461    2   1    1 org.apache.hadoop.util.LineReader.close()
3462   43  17    1 org.apache.hadoop.util.LineReader.readLine(Text,int,int)
3463    2   1    1 org.apache.hadoop.util.LineReader.readLine(Text,int)
3464    2   1    1 org.apache.hadoop.util.LineReader.readLine(Text)
3465    2   1    0 org.apache.hadoop.util.MergeSort.MergeSort(Comparator)
3466   26  14    0 org.apache.hadoop.util.MergeSort.mergeSort(int[],int[],int,int)
3467    4   1    0 org.apache.hadoop.util.MergeSort.swap(int[],int,int)
3468    2   1    1 org.apache.hadoop.util.NativeCodeLoader.isNativeCodeLoaded()
3469    2   1    1 org.apache.hadoop.util.NativeCodeLoader.getLoadNativeLibraries(Configuration)
3470    2   1    1 org.apache.hadoop.util.NativeCodeLoader.setLoadNativeLibraries(Configuration,boolean)
3471    2   1    1 org.apache.hadoop.util.PlatformName.getPlatformName()
3472    2   1    0 org.apache.hadoop.util.PlatformName.main(String[])
3473   12   6    1 org.apache.hadoop.util.PrintJarMainClass.main(String[])
3474    1   1    1 org.apache.hadoop.util.PriorityQueue.lessThan(Object,Object)
3475    5   1    0 org.apache.hadoop.util.PriorityQueue.initialize(int)
3476    4   1    1 org.apache.hadoop.util.PriorityQueue.put(T)
3477   11   6    1 org.apache.hadoop.util.PriorityQueue.insert(T)
3478    5   3    1 org.apache.hadoop.util.PriorityQueue.top()
3479   10   3    1 org.apache.hadoop.util.PriorityQueue.pop()
3480    2   1    1 org.apache.hadoop.util.PriorityQueue.adjustTop()
3481    2   1    1 org.apache.hadoop.util.PriorityQueue.size()
3482    4   2    1 org.apache.hadoop.util.PriorityQueue.clear()
3483    9   3    0 org.apache.hadoop.util.PriorityQueue.upHeap()
3484   15   7    0 org.apache.hadoop.util.PriorityQueue.downHeap()
3485    2   1    0 org.apache.hadoop.util.ProcfsBasedProcessTree.ProcfsBasedProcessTree(String)
3486    2   1    0 org.apache.hadoop.util.ProcfsBasedProcessTree.setSigKillInterval(long)
3487    9   5    1 org.apache.hadoop.util.ProcfsBasedProcessTree.isAvailable()
3488   30  10    1 org.apache.hadoop.util.ProcfsBasedProcessTree.getProcessTree()
3489    5   3    1 org.apache.hadoop.util.ProcfsBasedProcessTree.isAlive()
3490   16   5    1 org.apache.hadoop.util.ProcfsBasedProcessTree.destroy()
3491    6   3    1 org.apache.hadoop.util.ProcfsBasedProcessTree.getCumulativeVmem()
3492   22   8    1 org.apache.hadoop.util.ProcfsBasedProcessTree.getPidFromPidFile(String)
3493    8   3    0 org.apache.hadoop.util.ProcfsBasedProcessTree.getValidPID(String)
3494   10   5    1 org.apache.hadoop.util.ProcfsBasedProcessTree.getProcessList()
3495   27   9    1 org.apache.hadoop.util.ProcfsBasedProcessTree.constructProcessInfo(ProcessInfo)
3496   11   6    1 org.apache.hadoop.util.ProcfsBasedProcessTree.isAlive(Integer)
3497   14   4    0 org.apache.hadoop.util.ProcfsBasedProcessTree.SigKillThread.run()
3498    6   2    1 org.apache.hadoop.util.ProcfsBasedProcessTree.toString()
3499    2   1    0 org.apache.hadoop.util.ProcfsBasedProcessTree.ProcessInfo.ProcessInfo(int)
3500    2   1    0 org.apache.hadoop.util.ProcfsBasedProcessTree.ProcessInfo.getPid()
3501    2   1    0 org.apache.hadoop.util.ProcfsBasedProcessTree.ProcessInfo.getName()
3502    2   1    0 org.apache.hadoop.util.ProcfsBasedProcessTree.ProcessInfo.getPgrpId()
3503    2   1    0 org.apache.hadoop.util.ProcfsBasedProcessTree.ProcessInfo.getPpid()
3504    2   1    0 org.apache.hadoop.util.ProcfsBasedProcessTree.ProcessInfo.getSessionId()
3505    2   1    0 org.apache.hadoop.util.ProcfsBasedProcessTree.ProcessInfo.getVmem()
3506    4   3    0 org.apache.hadoop.util.ProcfsBasedProcessTree.ProcessInfo.isParent(ProcessInfo)
3507    6   1    0 org.apache.hadoop.util.ProcfsBasedProcessTree.ProcessInfo.update(String,Integer,Integer,Integer,Long)
3508    2   1    0 org.apache.hadoop.util.ProcfsBasedProcessTree.ProcessInfo.addChild(ProcessInfo)
3509    2   1    0 org.apache.hadoop.util.ProcfsBasedProcessTree.ProcessInfo.getChildren()
3510    2   1    0 org.apache.hadoop.util.ProgramDriver.ProgramDriver()
3511    3   1    1 org.apache.hadoop.util.ProgramDriver.ProgramDescription.ProgramDescription(Class,String)
3512    4   3    1 org.apache.hadoop.util.ProgramDriver.ProgramDescription.invoke(String[])
3513    2   1    0 org.apache.hadoop.util.ProgramDriver.ProgramDescription.getDescription()
3514    4   2    0 org.apache.hadoop.util.ProgramDriver.printUsage(Map)
3515    2   1    1 org.apache.hadoop.util.ProgramDriver.addClass(String,Class,String)
3516   14   6    1 org.apache.hadoop.util.ProgramDriver.driver(String[])
3517    1   1    1 org.apache.hadoop.util.Progress.Progress()
3518    4   1    1 org.apache.hadoop.util.Progress.addPhase(String)
3519    6   1    1 org.apache.hadoop.util.Progress.addPhase()
3520    2   1    1 org.apache.hadoop.util.Progress.startNextPhase()
3521    2   1    1 org.apache.hadoop.util.Progress.phase()
3522    7   2    1 org.apache.hadoop.util.Progress.complete()
3523    2   1    1 org.apache.hadoop.util.Progress.set(float)
3524    5   2    1 org.apache.hadoop.util.Progress.get()
3525    7   4    1 org.apache.hadoop.util.Progress.getInternal()
3526    2   1    0 org.apache.hadoop.util.Progress.setStatus(String)
3527    4   1    0 org.apache.hadoop.util.Progress.toString()
3528    5   3    0 org.apache.hadoop.util.Progress.toString(StringBuffer)
3529    1   1    1 org.apache.hadoop.util.Progressable.progress()
3530    1   1    0 org.apache.hadoop.util.QuickSort.QuickSort()
3531    3   2    0 org.apache.hadoop.util.QuickSort.fix(IndexedSortable,int,int)
3532    4   3    1 org.apache.hadoop.util.QuickSort.getMaxDepth(int)
3533    2   1    1 org.apache.hadoop.util.QuickSort.sort(IndexedSortable,int,int)
3534    2   1    1 org.apache.hadoop.util.QuickSort.sort(IndexedSortable,int,int,Progressable)
3535   47  23    0 org.apache.hadoop.util.QuickSort.sortInternal(IndexedSortable,int,int,Progressable,int)
3536    6   5    1 org.apache.hadoop.util.ReflectionUtils.setConf(Object,Configuration)
3537   12   4    0 org.apache.hadoop.util.ReflectionUtils.newInstance(Class,Configuration)
3538    2   1    0 org.apache.hadoop.util.ReflectionUtils.setContentionTracing(boolean)
3539    4   3    0 org.apache.hadoop.util.ReflectionUtils.getTaskName(long,String)
3540   29   7    1 org.apache.hadoop.util.ReflectionUtils.printThreadInfo(PrintWriter,String)
3541   12   4    1 org.apache.hadoop.util.ReflectionUtils.logThreadInfo(Log,String,long)
3542    2   1    0 org.apache.hadoop.util.ReflectionUtils.getClass(T)
3543    2   1    0 org.apache.hadoop.util.ReflectionUtils.clearCache()
3544    2   1    0 org.apache.hadoop.util.ReflectionUtils.getCacheSize()
3545   22   7    1 org.apache.hadoop.util.RunJar.unJar(File,File)
3546    3   2    0 org.apache.hadoop.util.RunJar.Thread$1.run()
3547   60  15    1 org.apache.hadoop.util.RunJar.main(String[])
3548    5   1    1 org.apache.hadoop.util.ServletUtil.initHTML(ServletResponse,String)
3549    6   4    1 org.apache.hadoop.util.ServletUtil.getParameter(ServletRequest,String)
3550    2   1    1 org.apache.hadoop.util.ServletUtil.htmlFooter()
3551   17   3    1 org.apache.hadoop.util.ServletUtil.percentageGraph(int,int)
3552    2   1    1 org.apache.hadoop.util.ServletUtil.percentageGraph(float,int)
3553    2   1    1 org.apache.hadoop.util.Shell.getGROUPS_COMMAND()
3554    2   2    1 org.apache.hadoop.util.Shell.getGET_PERMISSION_COMMAND()
3555    8   5    1 org.apache.hadoop.util.Shell.getUlimitMemoryCommand(Configuration)
3556    2   1    0 org.apache.hadoop.util.Shell.Shell()
3557    3   2    1 org.apache.hadoop.util.Shell.Shell(long)
3558    2   1    1 org.apache.hadoop.util.Shell.setEnvironment(Map)
3559    2   1    1 org.apache.hadoop.util.Shell.setWorkingDirectory(File)
3560    5   3    1 org.apache.hadoop.util.Shell.run()
3561    8   4    0 org.apache.hadoop.util.Shell.Thread$1.run()
3562   46  13    1 org.apache.hadoop.util.Shell.runCommand()
3563    1   1    1 org.apache.hadoop.util.Shell.getExecString()
3564    1   1    1 org.apache.hadoop.util.Shell.parseExecResult(BufferedReader)
3565    2   1    1 org.apache.hadoop.util.Shell.getProcess()
3566    2   1    1 org.apache.hadoop.util.Shell.getExitCode()
3567    3   1    0 org.apache.hadoop.util.Shell.ExitCodeException.ExitCodeException(int,String)
3568    2   1    0 org.apache.hadoop.util.Shell.ExitCodeException.getExitCode()
3569    2   1    0 org.apache.hadoop.util.Shell.ShellCommandExecutor.ShellCommandExecutor(String[])
3570    3   1    0 org.apache.hadoop.util.Shell.ShellCommandExecutor.ShellCommandExecutor(String[],File)
3571    3   1    0 org.apache.hadoop.util.Shell.ShellCommandExecutor.ShellCommandExecutor(String[],File,Map)
3572    2   1    1 org.apache.hadoop.util.Shell.ShellCommandExecutor.execute()
3573    2   1    0 org.apache.hadoop.util.Shell.ShellCommandExecutor.getExecString()
3574    6   2    0 org.apache.hadoop.util.Shell.ShellCommandExecutor.parseExecResult(BufferedReader)
3575    2   2    1 org.apache.hadoop.util.Shell.ShellCommandExecutor.getOutput()
3576   10   3    1 org.apache.hadoop.util.Shell.ShellCommandExecutor.toString()
3577    4   1    1 org.apache.hadoop.util.Shell.execCommand(String)
3578    6   1    1 org.apache.hadoop.util.StringUtils.stringifyException(Throwable)
3579    5   3    1 org.apache.hadoop.util.StringUtils.simpleHostname(String)
3580   17   4    1 org.apache.hadoop.util.StringUtils.humanReadableInt(long)
3581    8   1    1 org.apache.hadoop.util.StringUtils.formatPercent(double,int)
3582    9   4    1 org.apache.hadoop.util.StringUtils.arrayToString(String[])
3583    7   4    1 org.apache.hadoop.util.StringUtils.byteToHexString(byte[],int,int)
3584    2   1    1 org.apache.hadoop.util.StringUtils.byteToHexString(byte[])
3585    5   2    1 org.apache.hadoop.util.StringUtils.hexStringToByte(String)
3586    8   4    1 org.apache.hadoop.util.StringUtils.uriToString(URI[])
3587   10   5    1 org.apache.hadoop.util.StringUtils.stringToURI(String[])
3588    7   4    1 org.apache.hadoop.util.StringUtils.stringToPath(String[])
3589    3   1    1 org.apache.hadoop.util.StringUtils.formatTimeDiff(long,long)
3590   16   3    1 org.apache.hadoop.util.StringUtils.formatTime(long)
3591    7   3    1 org.apache.hadoop.util.StringUtils.getFormattedTimeWithDiff(DateFormat,long,long)
3592    5   3    1 org.apache.hadoop.util.StringUtils.getStrings(String)
3593    9   4    1 org.apache.hadoop.util.StringUtils.getStringCollection(String)
3594    2   1    1 org.apache.hadoop.util.StringUtils.split(String)
3595   15   6    1 org.apache.hadoop.util.StringUtils.split(String,char,char)
3596   10   6    1 org.apache.hadoop.util.StringUtils.findNext(String,char,char,int,StringBuilder)
3597    2   1    1 org.apache.hadoop.util.StringUtils.escapeString(String)
3598    2   1    1 org.apache.hadoop.util.StringUtils.escapeString(String,char,char)
3599    5   4    0 org.apache.hadoop.util.StringUtils.hasChar(char[],char)
3600   10   6    1 org.apache.hadoop.util.StringUtils.escapeString(String,char,char[])
3601    2   1    1 org.apache.hadoop.util.StringUtils.unEscapeString(String)
3602    2   1    1 org.apache.hadoop.util.StringUtils.unEscapeString(String,char,char)
3603   23  13    1 org.apache.hadoop.util.StringUtils.unEscapeString(String,char,char[])
3604    4   3    1 org.apache.hadoop.util.StringUtils.getHostname()
3605    7   2    1 org.apache.hadoop.util.StringUtils.toStartupShutdownString(String,String[])
3606    2   1    0 org.apache.hadoop.util.StringUtils.Thread$1.run()
3607    7   1    1 org.apache.hadoop.util.StringUtils.startupShutdownMessage(Class,String[],org.apache.commons.logging.Log)
3608    3   1    0 org.apache.hadoop.util.StringUtils.TraditionalBinaryPrefix.TraditionalBinaryPrefix(long)
3609    6   5    1 org.apache.hadoop.util.StringUtils.TraditionalBinaryPrefix.valueOf(char)
3610   12   6    1 org.apache.hadoop.util.StringUtils.TraditionalBinaryPrefix.string2long(String)
3611   33  10    1 org.apache.hadoop.util.StringUtils.escapeHTML(String)
3612    1   1    1 org.apache.hadoop.util.Tool.run(String[])
3613    7   2    1 org.apache.hadoop.util.ToolRunner.run(Configuration,Tool,String[])
3614    2   1    1 org.apache.hadoop.util.ToolRunner.run(Tool,String[])
3615    2   1    1 org.apache.hadoop.util.ToolRunner.printGenericCommandUsage(PrintStream)
3616    5   4    1 org.apache.hadoop.util.UTF8ByteArrayUtils.findByte(byte[],int,int,byte)
3617   11   6    1 org.apache.hadoop.util.UTF8ByteArrayUtils.findBytes(byte[],int,int,byte[])
3618    9   4    1 org.apache.hadoop.util.UTF8ByteArrayUtils.findNthByte(byte[],int,int,byte,int)
3619    2   1    1 org.apache.hadoop.util.UTF8ByteArrayUtils.findNthByte(byte[],byte,int)
3620    2   1    1 org.apache.hadoop.util.VersionInfo.getPackage()
3621    2   2    1 org.apache.hadoop.util.VersionInfo.getVersion()
3622    2   2    1 org.apache.hadoop.util.VersionInfo.getRevision()
3623    2   2    1 org.apache.hadoop.util.VersionInfo.getDate()
3624    2   2    1 org.apache.hadoop.util.VersionInfo.getUser()
3625    2   2    1 org.apache.hadoop.util.VersionInfo.getUrl()
3626    2   1    1 org.apache.hadoop.util.VersionInfo.getBuildVersion()
3627    4   1    0 org.apache.hadoop.util.VersionInfo.main(String[])
3628    4   1    1 org.apache.hadoop.util.XMLUtils.transform(InputStream,InputStream,Writer)
3629    2   1    0 org.apache.hadoop.hdfs.ChecksumDistributedFileSystem.ChecksumDistributedFileSystem()
3630    2   1    1 org.apache.hadoop.hdfs.ChecksumDistributedFileSystem.ChecksumDistributedFileSystem(InetSocketAddress,Configuration)
3631    2   1    1 org.apache.hadoop.hdfs.ChecksumDistributedFileSystem.getDFS()
3632    2   1    1 org.apache.hadoop.hdfs.ChecksumDistributedFileSystem.getRawCapacity()
3633    2   1    1 org.apache.hadoop.hdfs.ChecksumDistributedFileSystem.getRawUsed()
3634    2   1    1 org.apache.hadoop.hdfs.ChecksumDistributedFileSystem.getDataNodeStats()
3635    2   1    1 org.apache.hadoop.hdfs.ChecksumDistributedFileSystem.setSafeMode(FSConstants.SafeModeAction)
3636    2   1    0 org.apache.hadoop.hdfs.ChecksumDistributedFileSystem.refreshNodes()
3637    2   1    1 org.apache.hadoop.hdfs.ChecksumDistributedFileSystem.finalizeUpgrade()
3638    2   1    0 org.apache.hadoop.hdfs.ChecksumDistributedFileSystem.distributedUpgradeProgress(UpgradeAction)
3639    2   1    0 org.apache.hadoop.hdfs.ChecksumDistributedFileSystem.metaSave(String)
3640    2   1    1 org.apache.hadoop.hdfs.ChecksumDistributedFileSystem.reportChecksumFailure(Path,FSDataInputStream,long,FSDataInputStream,long)
3641    2   1    0 org.apache.hadoop.hdfs.ChecksumDistributedFileSystem.getFileStatus(Path)
3642    2   1    0 org.apache.hadoop.hdfs.DFSClient.createNamenode(Configuration)
3643    4   4    0 org.apache.hadoop.hdfs.DFSClient.createNamenode(InetSocketAddress,Configuration)
3644    2   1    0 org.apache.hadoop.hdfs.DFSClient.createRPCNamenode(InetSocketAddress,Configuration,UnixUserGroupInformation)
3645   10   1    0 org.apache.hadoop.hdfs.DFSClient.createNamenode(ClientProtocol)
3646    5   2    0 org.apache.hadoop.hdfs.DFSClient.createClientDatanodeProtocolProxy(DatanodeID,Configuration)
3647    2   1    1 org.apache.hadoop.hdfs.DFSClient.DFSClient(Configuration)
3648   20   4    1 org.apache.hadoop.hdfs.DFSClient.DFSClient(InetSocketAddress,Configuration,FileSystem.Statistics)
3649    2   1    0 org.apache.hadoop.hdfs.DFSClient.DFSClient(InetSocketAddress,Configuration)
3650    4   3    0 org.apache.hadoop.hdfs.DFSClient.checkOpen()
3651    5   1    1 org.apache.hadoop.hdfs.DFSClient.close()
3652    2   1    1 org.apache.hadoop.hdfs.DFSClient.getDefaultBlockSize()
3653    5   4    0 org.apache.hadoop.hdfs.DFSClient.getBlockSize(String)
3654    2   1    1 org.apache.hadoop.hdfs.DFSClient.reportBadBlocks(LocatedBlock[])
3655    2   1    0 org.apache.hadoop.hdfs.DFSClient.getDefaultReplication()
3656   11   5    0 org.apache.hadoop.hdfs.DFSClient.getHints(String,long,long)
3657    4   4    0 org.apache.hadoop.hdfs.DFSClient.callGetBlockLocations(ClientProtocol,String,long,long)
3658   18   5    1 org.apache.hadoop.hdfs.DFSClient.getBlockLocations(String,long,long)
3659    2   1    0 org.apache.hadoop.hdfs.DFSClient.open(String)
3660    3   1    1 org.apache.hadoop.hdfs.DFSClient.open(String,int,boolean,FileSystem.Statistics)
3661    2   1    1 org.apache.hadoop.hdfs.DFSClient.create(String,boolean)
3662    2   1    1 org.apache.hadoop.hdfs.DFSClient.create(String,boolean,Progressable)
3663    2   1    1 org.apache.hadoop.hdfs.DFSClient.create(String,boolean,short,long)
3664    2   1    1 org.apache.hadoop.hdfs.DFSClient.create(String,boolean,short,long,Progressable)
3665    2   1    1 org.apache.hadoop.hdfs.DFSClient.create(String,boolean,short,long,Progressable,int)
3666    9   2    1 org.apache.hadoop.hdfs.DFSClient.create(String,FsPermission,boolean,short,long,Progressable,int)
3667   11   3    1 org.apache.hadoop.hdfs.DFSClient.append(String,int,Progressable)
3668    4   4    1 org.apache.hadoop.hdfs.DFSClient.setReplication(String,short)
3669    5   4    1 org.apache.hadoop.hdfs.DFSClient.rename(String,String)
3670    3   1    0 org.apache.hadoop.hdfs.DFSClient.delete(String)
3671    5   4    1 org.apache.hadoop.hdfs.DFSClient.delete(String,boolean)
3672    3   1    1 org.apache.hadoop.hdfs.DFSClient.exists(String)
3673    6   4    0 org.apache.hadoop.hdfs.DFSClient.isDirectory(String)
3674    5   4    1 org.apache.hadoop.hdfs.DFSClient.listPaths(String)
3675    5   4    0 org.apache.hadoop.hdfs.DFSClient.getFileInfo(String)
3676    3   1    1 org.apache.hadoop.hdfs.DFSClient.getFileChecksum(String)
3677   53  17    1 org.apache.hadoop.hdfs.DFSClient.getFileChecksum(String,ClientProtocol,SocketFactory,int)
3678    5   3    1 org.apache.hadoop.hdfs.DFSClient.setPermission(String,FsPermission)
3679    5   3    1 org.apache.hadoop.hdfs.DFSClient.setOwner(String,String,String)
3680    3   1    0 org.apache.hadoop.hdfs.DFSClient.getDiskStatus()
3681    3   1    1 org.apache.hadoop.hdfs.DFSClient.totalRawCapacity()
3682    3   1    1 org.apache.hadoop.hdfs.DFSClient.totalRawUsed()
3683    2   1    0 org.apache.hadoop.hdfs.DFSClient.datanodeReport(DatanodeReportType)
3684    2   1    1 org.apache.hadoop.hdfs.DFSClient.setSafeMode(SafeModeAction)
3685    2   1    1 org.apache.hadoop.hdfs.DFSClient.refreshNodes()
3686    2   1    1 org.apache.hadoop.hdfs.DFSClient.metaSave(String)
3687    2   1    1 org.apache.hadoop.hdfs.DFSClient.finalizeUpgrade()
3688    2   1    1 org.apache.hadoop.hdfs.DFSClient.distributedUpgradeProgress(UpgradeAction)
3689    2   1    1 org.apache.hadoop.hdfs.DFSClient.mkdirs(String)
3690    9   5    1 org.apache.hadoop.hdfs.DFSClient.mkdirs(String,FsPermission)
3691    4   4    0 org.apache.hadoop.hdfs.DFSClient.getContentSummary(String)
3692    6  10    1 org.apache.hadoop.hdfs.DFSClient.setQuota(String,long,long)
3693    5   3    1 org.apache.hadoop.hdfs.DFSClient.setTimes(String,long,long)
3694    6   6    1 org.apache.hadoop.hdfs.DFSClient.bestNode(DatanodeInfo[],AbstractMap)
3695    2   1    0 org.apache.hadoop.hdfs.DFSClient.isLeaseCheckerStarted()
3696    6   3    0 org.apache.hadoop.hdfs.DFSClient.LeaseChecker.put(String,OutputStream)
3697    2   1    0 org.apache.hadoop.hdfs.DFSClient.LeaseChecker.remove(String)
3698    3   2    0 org.apache.hadoop.hdfs.DFSClient.LeaseChecker.interrupt()
3699   10   4    0 org.apache.hadoop.hdfs.DFSClient.LeaseChecker.close()
3700    5   3    0 org.apache.hadoop.hdfs.DFSClient.LeaseChecker.renew()
3701   13   6    1 org.apache.hadoop.hdfs.DFSClient.LeaseChecker.run()
3702    5   3    1 org.apache.hadoop.hdfs.DFSClient.LeaseChecker.toString()
3703    3   1    0 org.apache.hadoop.hdfs.DFSClient.DNAddrPair.DNAddrPair(DatanodeInfo,InetSocketAddress)
3704   16  12    0 org.apache.hadoop.hdfs.DFSClient.BlockReader.read(byte[],int,int)
3705   11   5    0 org.apache.hadoop.hdfs.DFSClient.BlockReader.skip(long)
3706    2   2    0 org.apache.hadoop.hdfs.DFSClient.BlockReader.read()
3707    2   1    0 org.apache.hadoop.hdfs.DFSClient.BlockReader.seekToNewSource(long)
3708    2   2    0 org.apache.hadoop.hdfs.DFSClient.BlockReader.seek(long)
3709    2   2    0 org.apache.hadoop.hdfs.DFSClient.BlockReader.getChunkPosition(long)
3710    7   3    1 org.apache.hadoop.hdfs.DFSClient.BlockReader.adjustChecksumBytes(int)
3711   39  22    0 org.apache.hadoop.hdfs.DFSClient.BlockReader.readChunk(long,byte[],int,int,byte[])
3712   11   2    0 org.apache.hadoop.hdfs.DFSClient.BlockReader.BlockReader(String,long,DataInputStream,DataChecksum,boolean,long,long,Socket)
3713    2   1    0 org.apache.hadoop.hdfs.DFSClient.BlockReader.newBlockReader(Socket,String,long,long,long,long,int)
3714    2   1    1 org.apache.hadoop.hdfs.DFSClient.BlockReader.newBlockReader(Socket,String,long,long,long,long,int,boolean)
3715   18   7    0 org.apache.hadoop.hdfs.DFSClient.BlockReader.newBlockReader(Socket,String,long,long,long,long,int,boolean,String)
3716    3   1    0 org.apache.hadoop.hdfs.DFSClient.BlockReader.close()
3717    2   1    1 org.apache.hadoop.hdfs.DFSClient.BlockReader.readAll(byte[],int,int)
3718    7   2    0 org.apache.hadoop.hdfs.DFSClient.BlockReader.checksumOk(Socket)
3719    2   1    0 org.apache.hadoop.hdfs.DFSClient.DFSInputStream.addToDeadNodes(DatanodeInfo)
3720    6   1    0 org.apache.hadoop.hdfs.DFSClient.DFSInputStream.DFSInputStream(String,int,boolean)
3721   12   8    1 org.apache.hadoop.hdfs.DFSClient.DFSInputStream.openInfo()
3722    2   2    0 org.apache.hadoop.hdfs.DFSClient.DFSInputStream.getFileLength()
3723    2   1    1 org.apache.hadoop.hdfs.DFSClient.DFSInputStream.getCurrentDatanode()
3724    2   1    1 org.apache.hadoop.hdfs.DFSClient.DFSInputStream.getCurrentBlock()
3725    2   1    1 org.apache.hadoop.hdfs.DFSClient.DFSInputStream.getAllBlocks()
3726   14   2    1 org.apache.hadoop.hdfs.DFSClient.DFSInputStream.getBlockAt(long)
3727   24   6    1 org.apache.hadoop.hdfs.DFSClient.DFSInputStream.getBlockRange(long,long)
3728   31  10    1 org.apache.hadoop.hdfs.DFSClient.DFSInputStream.blockSeekTo(long)
3729   12   5    0 org.apache.hadoop.hdfs.DFSClient.DFSInputStream.close()
3730    3   2    0 org.apache.hadoop.hdfs.DFSClient.DFSInputStream.read()
3731   23   9    0 org.apache.hadoop.hdfs.DFSClient.DFSInputStream.readBuffer(byte[],int,int)
3732   29  18    0 org.apache.hadoop.hdfs.DFSClient.DFSInputStream.read(byte[],int,int)
3733   19   9    0 org.apache.hadoop.hdfs.DFSClient.DFSInputStream.chooseDataNode(LocatedBlock)
3734   31  10    0 org.apache.hadoop.hdfs.DFSClient.DFSInputStream.fetchBlockByteRange(LocatedBlock,long,long,byte[],int)
3735   23   9    0 org.apache.hadoop.hdfs.DFSClient.DFSInputStream.read(long,byte[],int,int)
3736    9   5    0 org.apache.hadoop.hdfs.DFSClient.DFSInputStream.skip(long)
3737   15   9    0 org.apache.hadoop.hdfs.DFSClient.DFSInputStream.seek(long)
3738    3   1    1 org.apache.hadoop.hdfs.DFSClient.DFSInputStream.seekToBlockSource(long)
3739   12   4    0 org.apache.hadoop.hdfs.DFSClient.DFSInputStream.seekToNewSource(long)
3740    2   1    0 org.apache.hadoop.hdfs.DFSClient.DFSInputStream.getPos()
3741    4   3    0 org.apache.hadoop.hdfs.DFSClient.DFSInputStream.available()
3742    2   1    0 org.apache.hadoop.hdfs.DFSClient.DFSInputStream.markSupported()
3743    1   1    0 org.apache.hadoop.hdfs.DFSClient.DFSInputStream.mark(int)
3744    2   2    0 org.apache.hadoop.hdfs.DFSClient.DFSInputStream.reset()
3745    2   1    0 org.apache.hadoop.hdfs.DFSClient.DFSDataInputStream.DFSDataInputStream(DFSInputStream)
3746    2   1    1 org.apache.hadoop.hdfs.DFSClient.DFSDataInputStream.getCurrentDatanode()
3747    2   1    1 org.apache.hadoop.hdfs.DFSClient.DFSDataInputStream.getCurrentBlock()
3748    2   1    1 org.apache.hadoop.hdfs.DFSClient.DFSDataInputStream.getAllBlocks()
3749    3   2    0 org.apache.hadoop.hdfs.DFSClient.DFSOutputStream.setLastException(IOException)
3750   13   1    0 org.apache.hadoop.hdfs.DFSClient.DFSOutputStream.Packet.Packet(int,int,long)
3751    5   3    0 org.apache.hadoop.hdfs.DFSClient.DFSOutputStream.Packet.writeData(byte[],int,int)
3752    5   3    0 org.apache.hadoop.hdfs.DFSClient.DFSOutputStream.Packet.writeChecksum(byte[],int,int)
3753   18   5    1 org.apache.hadoop.hdfs.DFSClient.DFSOutputStream.Packet.getBuffer()
3754   71  40    0 org.apache.hadoop.hdfs.DFSClient.DFSOutputStream.DataStreamer.run()
3755    7   1    0 org.apache.hadoop.hdfs.DFSClient.DFSOutputStream.DataStreamer.close()
3756    2   1    0 org.apache.hadoop.hdfs.DFSClient.DFSOutputStream.ResponseProcessor.ResponseProcessor(DatanodeInfo[])
3757   35  15    0 org.apache.hadoop.hdfs.DFSClient.DFSOutputStream.ResponseProcessor.run()
3758    3   1    0 org.apache.hadoop.hdfs.DFSClient.DFSOutputStream.ResponseProcessor.close()
3759   84  30    0 org.apache.hadoop.hdfs.DFSClient.DFSOutputStream.processDatanodeError(boolean,boolean)
3760    6   5    0 org.apache.hadoop.hdfs.DFSClient.DFSOutputStream.isClosed()
3761    8   4    0 org.apache.hadoop.hdfs.DFSClient.DFSOutputStream.getPipeline()
3762   10   5    0 org.apache.hadoop.hdfs.DFSClient.DFSOutputStream.DFSOutputStream(String,long,Progressable,int)
3763    7   3    1 org.apache.hadoop.hdfs.DFSClient.DFSOutputStream.DFSOutputStream(String,FsPermission,boolean,short,long,Progressable,int,int)
3764   26   8    1 org.apache.hadoop.hdfs.DFSClient.DFSOutputStream.DFSOutputStream(String,int,Progressable,LocatedBlock,FileStatus,int)
3765    7   2    0 org.apache.hadoop.hdfs.DFSClient.DFSOutputStream.computePacketChunkSize(int,int)
3766   29   8    1 org.apache.hadoop.hdfs.DFSClient.DFSOutputStream.nextBlockOutputStream(String)
3767   46  11    0 org.apache.hadoop.hdfs.DFSClient.DFSOutputStream.createBlockOutputStream(DatanodeInfo[],String,boolean)
3768   21  12    0 org.apache.hadoop.hdfs.DFSClient.DFSOutputStream.locateFollowingBlock(long)
3769   37  15    0 org.apache.hadoop.hdfs.DFSClient.DFSOutputStream.writeChunk(byte[],int,int,byte[])
3770   18   5    1 org.apache.hadoop.hdfs.DFSClient.DFSOutputStream.sync()
3771   23  10    1 org.apache.hadoop.hdfs.DFSClient.DFSOutputStream.flushInternal()
3772    6   2    0 org.apache.hadoop.hdfs.DFSClient.DFSOutputStream.close()
3773    9   4    0 org.apache.hadoop.hdfs.DFSClient.DFSOutputStream.closeThreads()
3774   35  10    1 org.apache.hadoop.hdfs.DFSClient.DFSOutputStream.closeInternal()
3775    2   1    0 org.apache.hadoop.hdfs.DFSClient.DFSOutputStream.setArtificialSlowdown(long)
3776    3   1    0 org.apache.hadoop.hdfs.DFSClient.DFSOutputStream.setChunksPerPacket(int)
3777    2   1    0 org.apache.hadoop.hdfs.DFSClient.DFSOutputStream.setTestFilename(String)
3778    4   1    0 org.apache.hadoop.hdfs.DFSClient.reportChecksumFailure(String,Block,DatanodeInfo)
3779    4   2    0 org.apache.hadoop.hdfs.DFSClient.reportChecksumFailure(String,LocatedBlock[])
3780    2   1    1 org.apache.hadoop.hdfs.DFSClient.toString()
3781    9   9    1 org.apache.hadoop.hdfs.DFSUtil.isValidName(String)
3782    1   1    0 org.apache.hadoop.hdfs.DistributedFileSystem.DistributedFileSystem()
3783    2   1    1 org.apache.hadoop.hdfs.DistributedFileSystem.DistributedFileSystem(InetSocketAddress,Configuration)
3784    2   1    1 org.apache.hadoop.hdfs.DistributedFileSystem.getName()
3785    2   1    0 org.apache.hadoop.hdfs.DistributedFileSystem.getUri()
3786    9   3    0 org.apache.hadoop.hdfs.DistributedFileSystem.initialize(URI,Configuration)
3787    7   7    1 org.apache.hadoop.hdfs.DistributedFileSystem.checkPath(Path)
3788    2   1    0 org.apache.hadoop.hdfs.DistributedFileSystem.getWorkingDirectory()
3789    2   1    0 org.apache.hadoop.hdfs.DistributedFileSystem.getDefaultBlockSize()
3790    2   1    0 org.apache.hadoop.hdfs.DistributedFileSystem.getDefaultReplication()
3791    5   3    0 org.apache.hadoop.hdfs.DistributedFileSystem.makeAbsolute(Path)
3792    5   3    0 org.apache.hadoop.hdfs.DistributedFileSystem.setWorkingDirectory(Path)
3793    2   1    1 org.apache.hadoop.hdfs.DistributedFileSystem.getHomeDirectory()
3794    6   3    0 org.apache.hadoop.hdfs.DistributedFileSystem.getPathName(Path)
3795    4   3    0 org.apache.hadoop.hdfs.DistributedFileSystem.getFileBlockLocations(FileStatus,long,long)
3796    2   1    0 org.apache.hadoop.hdfs.DistributedFileSystem.setVerifyChecksum(boolean)
3797    2   1    0 org.apache.hadoop.hdfs.DistributedFileSystem.open(Path,int)
3798    2   1    1 org.apache.hadoop.hdfs.DistributedFileSystem.append(Path,int,Progressable)
3799    2   1    0 org.apache.hadoop.hdfs.DistributedFileSystem.create(Path,FsPermission,boolean,int,short,long,Progressable)
3800    2   1    0 org.apache.hadoop.hdfs.DistributedFileSystem.setReplication(Path,short)
3801    2   1    1 org.apache.hadoop.hdfs.DistributedFileSystem.rename(Path,Path)
3802    2   1    0 org.apache.hadoop.hdfs.DistributedFileSystem.delete(Path)
3803    2   1    1 org.apache.hadoop.hdfs.DistributedFileSystem.delete(Path,boolean)
3804    2   1    1 org.apache.hadoop.hdfs.DistributedFileSystem.getContentSummary(Path)
3805    2   1    1 org.apache.hadoop.hdfs.DistributedFileSystem.setQuota(Path,long,long)
3806    2   1    0 org.apache.hadoop.hdfs.DistributedFileSystem.makeQualified(FileStatus)
3807    8   4    0 org.apache.hadoop.hdfs.DistributedFileSystem.listStatus(Path)
3808    2   1    0 org.apache.hadoop.hdfs.DistributedFileSystem.mkdirs(Path,FsPermission)
3809    5   1    1 org.apache.hadoop.hdfs.DistributedFileSystem.close()
3810    2   1    0 org.apache.hadoop.hdfs.DistributedFileSystem.toString()
3811    2   1    0 org.apache.hadoop.hdfs.DistributedFileSystem.getClient()
3812    4   1    0 org.apache.hadoop.hdfs.DistributedFileSystem.DiskStatus.DiskStatus(long,long,long)
3813    2   1    0 org.apache.hadoop.hdfs.DistributedFileSystem.DiskStatus.getCapacity()
3814    2   1    0 org.apache.hadoop.hdfs.DistributedFileSystem.DiskStatus.getDfsUsed()
3815    2   1    0 org.apache.hadoop.hdfs.DistributedFileSystem.DiskStatus.getRemaining()
3816    2   1    1 org.apache.hadoop.hdfs.DistributedFileSystem.getDiskStatus()
3817    2   1    1 org.apache.hadoop.hdfs.DistributedFileSystem.getRawCapacity()
3818    2   1    1 org.apache.hadoop.hdfs.DistributedFileSystem.getRawUsed()
3819    2   1    1 org.apache.hadoop.hdfs.DistributedFileSystem.getDataNodeStats()
3820    2   1    1 org.apache.hadoop.hdfs.DistributedFileSystem.setSafeMode(FSConstants.SafeModeAction)
3821    2   1    0 org.apache.hadoop.hdfs.DistributedFileSystem.refreshNodes()
3822    2   1    1 org.apache.hadoop.hdfs.DistributedFileSystem.finalizeUpgrade()
3823    2   1    0 org.apache.hadoop.hdfs.DistributedFileSystem.distributedUpgradeProgress(UpgradeAction)
3824    2   1    0 org.apache.hadoop.hdfs.DistributedFileSystem.metaSave(String)
3825   20   5    1 org.apache.hadoop.hdfs.DistributedFileSystem.reportChecksumFailure(Path,FSDataInputStream,long,FSDataInputStream,long)
3826    6   4    1 org.apache.hadoop.hdfs.DistributedFileSystem.getFileStatus(Path)
3827    2   1    1 org.apache.hadoop.hdfs.DistributedFileSystem.getFileChecksum(Path)
3828    2   1    1 org.apache.hadoop.hdfs.DistributedFileSystem.setPermission(Path,FsPermission)
3829    4   4    1 org.apache.hadoop.hdfs.DistributedFileSystem.setOwner(Path,String,String)
3830    2   1    1 org.apache.hadoop.hdfs.DistributedFileSystem.setTimes(Path,long,long)
3831    6   3    0 org.apache.hadoop.hdfs.HftpFileSystem.initialize(URI,Configuration)
3832    4   3    0 org.apache.hadoop.hdfs.HftpFileSystem.getUri()
3833    7   5    1 org.apache.hadoop.hdfs.HftpFileSystem.openConnection(String,String)
3834    2   1    0 org.apache.hadoop.hdfs.HftpFileSystem.FSInputStream$1.read()
3835    2   1    0 org.apache.hadoop.hdfs.HftpFileSystem.FSInputStream$1.read(byte[],int,int)
3836    2   1    0 org.apache.hadoop.hdfs.HftpFileSystem.FSInputStream$1.close()
3837    2   2    0 org.apache.hadoop.hdfs.HftpFileSystem.FSInputStream$1.seek(long)
3838    2   2    0 org.apache.hadoop.hdfs.HftpFileSystem.FSInputStream$1.getPos()
3839    2   1    0 org.apache.hadoop.hdfs.HftpFileSystem.FSInputStream$1.seekToNewSource(long)
3840   19   1    0 org.apache.hadoop.hdfs.HftpFileSystem.open(Path,int)
3841   17  12    0 org.apache.hadoop.hdfs.HftpFileSystem.LsParser.startElement(String,String,String,Attributes)
3842   13   7    0 org.apache.hadoop.hdfs.HftpFileSystem.LsParser.fetchList(String,boolean)
3843    5   3    0 org.apache.hadoop.hdfs.HftpFileSystem.LsParser.getFileStatus(Path)
3844    5   4    0 org.apache.hadoop.hdfs.HftpFileSystem.LsParser.listStatus(Path,boolean)
3845    2   1    0 org.apache.hadoop.hdfs.HftpFileSystem.LsParser.listStatus(Path)
3846    3   1    0 org.apache.hadoop.hdfs.HftpFileSystem.listStatus(Path)
3847    3   1    0 org.apache.hadoop.hdfs.HftpFileSystem.getFileStatus(Path)
3848    6   5    1 org.apache.hadoop.hdfs.HftpFileSystem.ChecksumParser.startElement(String,String,String,Attributes)
3849   15   6    0 org.apache.hadoop.hdfs.HftpFileSystem.ChecksumParser.getFileChecksum(Path)
3850    2   1    1 org.apache.hadoop.hdfs.HftpFileSystem.getFileChecksum(Path)
3851    2   1    0 org.apache.hadoop.hdfs.HftpFileSystem.getWorkingDirectory()
3852    1   1    0 org.apache.hadoop.hdfs.HftpFileSystem.setWorkingDirectory(Path)
3853    2   2    1 org.apache.hadoop.hdfs.HftpFileSystem.append(Path,int,Progressable)
3854    2   2    0 org.apache.hadoop.hdfs.HftpFileSystem.create(Path,FsPermission,boolean,int,short,long,Progressable)
3855    2   2    0 org.apache.hadoop.hdfs.HftpFileSystem.rename(Path,Path)
3856    2   2    0 org.apache.hadoop.hdfs.HftpFileSystem.delete(Path)
3857    2   2    0 org.apache.hadoop.hdfs.HftpFileSystem.delete(Path,boolean)
3858    2   2    0 org.apache.hadoop.hdfs.HftpFileSystem.mkdirs(Path,FsPermission)
3859    5   4    0 org.apache.hadoop.hdfs.HsftpFileSystem.openConnection(String,String)
3860    4   3    0 org.apache.hadoop.hdfs.HsftpFileSystem.getUri()
3861    2   1    0 org.apache.hadoop.hdfs.protocol.AlreadyBeingCreatedException.AlreadyBeingCreatedException(String)
3862    2   1    0 org.apache.hadoop.hdfs.protocol.Block.WritableFactory$1.newInstance()
3863    6   4    1 org.apache.hadoop.hdfs.protocol.Block.isBlockFilename(File)
3864    2   1    0 org.apache.hadoop.hdfs.protocol.Block.filename2id(String)
3865    2   1    0 org.apache.hadoop.hdfs.protocol.Block.Block()
3866    2   1    0 org.apache.hadoop.hdfs.protocol.Block.Block(long,long,long)
3867    2   1    0 org.apache.hadoop.hdfs.protocol.Block.Block(long)
3868    2   1    0 org.apache.hadoop.hdfs.protocol.Block.Block(Block)
3869    2   1    1 org.apache.hadoop.hdfs.protocol.Block.Block(File,long,long)
3870    4   1    0 org.apache.hadoop.hdfs.protocol.Block.set(long,long,long)
3871    2   1    1 org.apache.hadoop.hdfs.protocol.Block.getBlockId()
3872    2   1    0 org.apache.hadoop.hdfs.protocol.Block.setBlockId(long)
3873    2   1    1 org.apache.hadoop.hdfs.protocol.Block.getBlockName()
3874    2   1    1 org.apache.hadoop.hdfs.protocol.Block.getNumBytes()
3875    2   1    0 org.apache.hadoop.hdfs.protocol.Block.setNumBytes(long)
3876    2   1    0 org.apache.hadoop.hdfs.protocol.Block.getGenerationStamp()
3877    2   1    0 org.apache.hadoop.hdfs.protocol.Block.setGenerationStamp(long)
3878    2   1    1 org.apache.hadoop.hdfs.protocol.Block.toString()
3879    4   1    0 org.apache.hadoop.hdfs.protocol.Block.write(DataOutput)
3880    6   3    0 org.apache.hadoop.hdfs.protocol.Block.readFields(DataInput)
3881    3   3    0 org.apache.hadoop.hdfs.protocol.Block.validateGenerationStamp(long)
3882   10   5    1 org.apache.hadoop.hdfs.protocol.Block.compareTo(Block)
3883    5   4    1 org.apache.hadoop.hdfs.protocol.Block.equals(Object)
3884    2   1    1 org.apache.hadoop.hdfs.protocol.Block.hashCode()
3885    2   1    0 org.apache.hadoop.hdfs.protocol.BlockListAsLongs.index2BlockId(int)
3886    2   1    0 org.apache.hadoop.hdfs.protocol.BlockListAsLongs.index2BlockLen(int)
3887    2   1    0 org.apache.hadoop.hdfs.protocol.BlockListAsLongs.index2BlockGenStamp(int)
3888    7   2    1 org.apache.hadoop.hdfs.protocol.BlockListAsLongs.convertToArrayLongs(Block[])
3889    7   4    1 org.apache.hadoop.hdfs.protocol.BlockListAsLongs.BlockListAsLongs(long[])
3890    2   1    0 org.apache.hadoop.hdfs.protocol.BlockListAsLongs.getNumberOfBlocks()
3891    2   1    0 org.apache.hadoop.hdfs.protocol.BlockListAsLongs.getBlockId(int)
3892    2   1    0 org.apache.hadoop.hdfs.protocol.BlockListAsLongs.getBlockLen(int)
3893    2   1    0 org.apache.hadoop.hdfs.protocol.BlockListAsLongs.getBlockGenStamp(int)
3894    4   1    0 org.apache.hadoop.hdfs.protocol.BlockListAsLongs.setBlock(int,Block)
3895    1   1    1 org.apache.hadoop.hdfs.protocol.ClientDatanodeProtocol.recoverBlock(Block,boolean,DatanodeInfo[])
3896    1   1    1 org.apache.hadoop.hdfs.protocol.ClientProtocol.getBlockLocations(String,long,long)
3897    1   1    1 org.apache.hadoop.hdfs.protocol.ClientProtocol.create(String,FsPermission,String,boolean,short,long)
3898    1   1    1 org.apache.hadoop.hdfs.protocol.ClientProtocol.append(String,String)
3899    1   1    1 org.apache.hadoop.hdfs.protocol.ClientProtocol.setReplication(String,short)
3900    1   1    1 org.apache.hadoop.hdfs.protocol.ClientProtocol.setPermission(String,FsPermission)
3901    1   1    1 org.apache.hadoop.hdfs.protocol.ClientProtocol.setOwner(String,String,String)
3902    1   1    1 org.apache.hadoop.hdfs.protocol.ClientProtocol.abandonBlock(Block,String,String)
3903    1   1    1 org.apache.hadoop.hdfs.protocol.ClientProtocol.addBlock(String,String)
3904    1   1    1 org.apache.hadoop.hdfs.protocol.ClientProtocol.complete(String,String)
3905    1   1    1 org.apache.hadoop.hdfs.protocol.ClientProtocol.reportBadBlocks(LocatedBlock[])
3906    1   1    1 org.apache.hadoop.hdfs.protocol.ClientProtocol.rename(String,String)
3907    1   1    1 org.apache.hadoop.hdfs.protocol.ClientProtocol.delete(String)
3908    1   1    1 org.apache.hadoop.hdfs.protocol.ClientProtocol.delete(String,boolean)
3909    1   1    1 org.apache.hadoop.hdfs.protocol.ClientProtocol.mkdirs(String,FsPermission)
3910    1   1    1 org.apache.hadoop.hdfs.protocol.ClientProtocol.getListing(String)
3911    1   1    1 org.apache.hadoop.hdfs.protocol.ClientProtocol.renewLease(String)
3912    1   1    1 org.apache.hadoop.hdfs.protocol.ClientProtocol.getStats()
3913    1   1    1 org.apache.hadoop.hdfs.protocol.ClientProtocol.getDatanodeReport(FSConstants.DatanodeReportType)
3914    1   1    1 org.apache.hadoop.hdfs.protocol.ClientProtocol.getPreferredBlockSize(String)
3915    1   1    1 org.apache.hadoop.hdfs.protocol.ClientProtocol.setSafeMode(FSConstants.SafeModeAction)
3916    1   1    1 org.apache.hadoop.hdfs.protocol.ClientProtocol.refreshNodes()
3917    1   1    1 org.apache.hadoop.hdfs.protocol.ClientProtocol.finalizeUpgrade()
3918    1   1    1 org.apache.hadoop.hdfs.protocol.ClientProtocol.distributedUpgradeProgress(UpgradeAction)
3919    1   1    1 org.apache.hadoop.hdfs.protocol.ClientProtocol.metaSave(String)
3920    1   1    1 org.apache.hadoop.hdfs.protocol.ClientProtocol.getFileInfo(String)
3921    1   1    1 org.apache.hadoop.hdfs.protocol.ClientProtocol.getContentSummary(String)
3922    1   1    1 org.apache.hadoop.hdfs.protocol.ClientProtocol.setQuota(String,long,long)
3923    1   1    1 org.apache.hadoop.hdfs.protocol.ClientProtocol.fsync(String,String)
3924    1   1    1 org.apache.hadoop.hdfs.protocol.ClientProtocol.setTimes(String,long,long)
3925    2   1    1 org.apache.hadoop.hdfs.protocol.DatanodeID.DatanodeID()
3926    2   1    1 org.apache.hadoop.hdfs.protocol.DatanodeID.DatanodeID(String)
3927    2   1    1 org.apache.hadoop.hdfs.protocol.DatanodeID.DatanodeID(DatanodeID)
3928    5   1    1 org.apache.hadoop.hdfs.protocol.DatanodeID.DatanodeID(String,String,int,int)
3929    2   1    1 org.apache.hadoop.hdfs.protocol.DatanodeID.getName()
3930    2   1    1 org.apache.hadoop.hdfs.protocol.DatanodeID.getStorageID()
3931    2   1    1 org.apache.hadoop.hdfs.protocol.DatanodeID.getInfoPort()
3932    2   1    1 org.apache.hadoop.hdfs.protocol.DatanodeID.getIpcPort()
3933    2   1    1 org.apache.hadoop.hdfs.protocol.DatanodeID.setStorageID(String)
3934    6   3    1 org.apache.hadoop.hdfs.protocol.DatanodeID.getHost()
3935    5   3    0 org.apache.hadoop.hdfs.protocol.DatanodeID.getPort()
3936    6   6    0 org.apache.hadoop.hdfs.protocol.DatanodeID.equals(Object)
3937    2   1    0 org.apache.hadoop.hdfs.protocol.DatanodeID.hashCode()
3938    2   1    0 org.apache.hadoop.hdfs.protocol.DatanodeID.toString()
3939    3   1    1 org.apache.hadoop.hdfs.protocol.DatanodeID.updateRegInfo(DatanodeID)
3940    2   1    1 org.apache.hadoop.hdfs.protocol.DatanodeID.compareTo(DatanodeID)
3941    4   1    1 org.apache.hadoop.hdfs.protocol.DatanodeID.write(DataOutput)
3942    4   1    1 org.apache.hadoop.hdfs.protocol.DatanodeID.readFields(DataInput)
3943    3   1    0 org.apache.hadoop.hdfs.protocol.DatanodeInfo.DatanodeInfo()
3944   10   1    0 org.apache.hadoop.hdfs.protocol.DatanodeInfo.DatanodeInfo(DatanodeInfo)
3945    8   1    0 org.apache.hadoop.hdfs.protocol.DatanodeInfo.DatanodeInfo(DatanodeID)
3946    4   1    0 org.apache.hadoop.hdfs.protocol.DatanodeInfo.DatanodeInfo(DatanodeID,String,String)
3947    2   1    1 org.apache.hadoop.hdfs.protocol.DatanodeInfo.getCapacity()
3948    2   1    1 org.apache.hadoop.hdfs.protocol.DatanodeInfo.getDfsUsed()
3949    3   2    1 org.apache.hadoop.hdfs.protocol.DatanodeInfo.getNonDfsUsed()
3950    4   3    1 org.apache.hadoop.hdfs.protocol.DatanodeInfo.getDfsUsedPercent()
3951    2   1    1 org.apache.hadoop.hdfs.protocol.DatanodeInfo.getRemaining()
3952    4   3    1 org.apache.hadoop.hdfs.protocol.DatanodeInfo.getRemainingPercent()
3953    2   1    1 org.apache.hadoop.hdfs.protocol.DatanodeInfo.getLastUpdate()
3954    2   1    1 org.apache.hadoop.hdfs.protocol.DatanodeInfo.getXceiverCount()
3955    2   1    1 org.apache.hadoop.hdfs.protocol.DatanodeInfo.setCapacity(long)
3956    2   1    1 org.apache.hadoop.hdfs.protocol.DatanodeInfo.setRemaining(long)
3957    2   1    1 org.apache.hadoop.hdfs.protocol.DatanodeInfo.setLastUpdate(long)
3958    2   1    1 org.apache.hadoop.hdfs.protocol.DatanodeInfo.setXceiverCount(int)
3959    2   1    1 org.apache.hadoop.hdfs.protocol.DatanodeInfo.getNetworkLocation()
3960    2   1    1 org.apache.hadoop.hdfs.protocol.DatanodeInfo.setNetworkLocation(String)
3961    2   3    0 org.apache.hadoop.hdfs.protocol.DatanodeInfo.getHostName()
3962    2   1    0 org.apache.hadoop.hdfs.protocol.DatanodeInfo.setHostName(String)
3963   27   4    1 org.apache.hadoop.hdfs.protocol.DatanodeInfo.getDatanodeReport()
3964   21   4    1 org.apache.hadoop.hdfs.protocol.DatanodeInfo.dumpDatanode()
3965    2   1    1 org.apache.hadoop.hdfs.protocol.DatanodeInfo.startDecommission()
3966    2   1    1 org.apache.hadoop.hdfs.protocol.DatanodeInfo.stopDecommission()
3967    4   3    1 org.apache.hadoop.hdfs.protocol.DatanodeInfo.isDecommissionInProgress()
3968    4   3    1 org.apache.hadoop.hdfs.protocol.DatanodeInfo.isDecommissioned()
3969    2   1    1 org.apache.hadoop.hdfs.protocol.DatanodeInfo.setDecommissioned()
3970    4   3    1 org.apache.hadoop.hdfs.protocol.DatanodeInfo.getAdminState()
3971    5   2    1 org.apache.hadoop.hdfs.protocol.DatanodeInfo.setAdminState(AdminStates)
3972    2   1    1 org.apache.hadoop.hdfs.protocol.DatanodeInfo.getParent()
3973    2   1    0 org.apache.hadoop.hdfs.protocol.DatanodeInfo.setParent(Node)
3974    2   1    1 org.apache.hadoop.hdfs.protocol.DatanodeInfo.getLevel()
3975    2   1    0 org.apache.hadoop.hdfs.protocol.DatanodeInfo.setLevel(int)
3976    2   1    0 org.apache.hadoop.hdfs.protocol.DatanodeInfo.WritableFactory$1.newInstance()
3977   11   2    1 org.apache.hadoop.hdfs.protocol.DatanodeInfo.write(DataOutput)
3978   11   1    1 org.apache.hadoop.hdfs.protocol.DatanodeInfo.readFields(DataInput)
3979    2   1    0 org.apache.hadoop.hdfs.protocol.LocatedBlock.WritableFactory$1.newInstance()
3980    2   1    1 org.apache.hadoop.hdfs.protocol.LocatedBlock.LocatedBlock()
3981    2   1    1 org.apache.hadoop.hdfs.protocol.LocatedBlock.LocatedBlock(Block,DatanodeInfo[])
3982    2   1    1 org.apache.hadoop.hdfs.protocol.LocatedBlock.LocatedBlock(Block,DatanodeInfo[],long)
3983    8   2    1 org.apache.hadoop.hdfs.protocol.LocatedBlock.LocatedBlock(Block,DatanodeInfo[],long,boolean)
3984    2   1    1 org.apache.hadoop.hdfs.protocol.LocatedBlock.getBlock()
3985    2   1    1 org.apache.hadoop.hdfs.protocol.LocatedBlock.getLocations()
3986    2   1    0 org.apache.hadoop.hdfs.protocol.LocatedBlock.getStartOffset()
3987    2   1    0 org.apache.hadoop.hdfs.protocol.LocatedBlock.getBlockSize()
3988    2   1    0 org.apache.hadoop.hdfs.protocol.LocatedBlock.setStartOffset(long)
3989    2   1    0 org.apache.hadoop.hdfs.protocol.LocatedBlock.setCorrupt(boolean)
3990    2   1    0 org.apache.hadoop.hdfs.protocol.LocatedBlock.isCorrupt()
3991    7   2    0 org.apache.hadoop.hdfs.protocol.LocatedBlock.write(DataOutput)
3992   10   2    0 org.apache.hadoop.hdfs.protocol.LocatedBlock.readFields(DataInput)
3993    4   1    0 org.apache.hadoop.hdfs.protocol.LocatedBlocks.LocatedBlocks()
3994    4   1    0 org.apache.hadoop.hdfs.protocol.LocatedBlocks.LocatedBlocks(long,List,boolean)
3995    2   1    1 org.apache.hadoop.hdfs.protocol.LocatedBlocks.getLocatedBlocks()
3996    2   1    1 org.apache.hadoop.hdfs.protocol.LocatedBlocks.get(int)
3997    2   2    1 org.apache.hadoop.hdfs.protocol.LocatedBlocks.locatedBlockCount()
3998    2   1    1 org.apache.hadoop.hdfs.protocol.LocatedBlocks.getFileLength()
3999    2   1    1 org.apache.hadoop.hdfs.protocol.LocatedBlocks.isUnderConstruction()
4000   10   8    0 org.apache.hadoop.hdfs.protocol.LocatedBlocks.Comparator$1.compare(LocatedBlock,LocatedBlock)
4001   16   1    1 org.apache.hadoop.hdfs.protocol.LocatedBlocks.findBlock(long)
4002   21   7    0 org.apache.hadoop.hdfs.protocol.LocatedBlocks.insertRange(int,List)
4003    2   2    0 org.apache.hadoop.hdfs.protocol.LocatedBlocks.getInsertIndex(int)
4004    2   1    0 org.apache.hadoop.hdfs.protocol.LocatedBlocks.WritableFactory$2.newInstance()
4005    9   4    0 org.apache.hadoop.hdfs.protocol.LocatedBlocks.write(DataOutput)
4006    9   2    0 org.apache.hadoop.hdfs.protocol.LocatedBlocks.readFields(DataInput)
4007    2   1    0 org.apache.hadoop.hdfs.protocol.QuotaExceededException.QuotaExceededException(String)
4008    5   1    0 org.apache.hadoop.hdfs.protocol.QuotaExceededException.QuotaExceededException(long,long,long,long)
4009    2   1    0 org.apache.hadoop.hdfs.protocol.QuotaExceededException.setPathName(String)
4010    6   4    0 org.apache.hadoop.hdfs.protocol.QuotaExceededException.getMessage()
4011    2   1    0 org.apache.hadoop.hdfs.protocol.UnregisteredDatanodeException.UnregisteredDatanodeException(DatanodeID)
4012    2   1    0 org.apache.hadoop.hdfs.protocol.UnregisteredDatanodeException.UnregisteredDatanodeException(DatanodeID,DatanodeInfo)
4013    1   1    1 org.apache.hadoop.hdfs.server.balancer.Balancer.PendingBlockMove.PendingBlockMove()
4014    6   4    0 org.apache.hadoop.hdfs.server.balancer.Balancer.PendingBlockMove.chooseBlockAndProxy()
4015   11   5    0 org.apache.hadoop.hdfs.server.balancer.Balancer.PendingBlockMove.markMovedIfGoodBlock(BalancerBlock)
4016   11   8    0 org.apache.hadoop.hdfs.server.balancer.Balancer.PendingBlockMove.chooseProxySource()
4017   25   2    0 org.apache.hadoop.hdfs.server.balancer.Balancer.PendingBlockMove.dispatch()
4018    8   1    0 org.apache.hadoop.hdfs.server.balancer.Balancer.PendingBlockMove.sendRequest(DataOutputStream)
4019    4   3    0 org.apache.hadoop.hdfs.server.balancer.Balancer.PendingBlockMove.receiveResponse(DataInputStream)
4020    5   1    0 org.apache.hadoop.hdfs.server.balancer.Balancer.PendingBlockMove.reset()
4021    4   2    0 org.apache.hadoop.hdfs.server.balancer.Balancer.PendingBlockMove.Runnable$1.run()
4022    6   1    0 org.apache.hadoop.hdfs.server.balancer.Balancer.PendingBlockMove.scheduleBlockMove()
4023    2   1    0 org.apache.hadoop.hdfs.server.balancer.Balancer.BalancerBlock.BalancerBlock(Block)
4024    2   1    0 org.apache.hadoop.hdfs.server.balancer.Balancer.BalancerBlock.clearLocations()
4025    3   2    0 org.apache.hadoop.hdfs.server.balancer.Balancer.BalancerBlock.addLocation(BalancerDatanode)
4026    2   1    0 org.apache.hadoop.hdfs.server.balancer.Balancer.BalancerBlock.isLocatedOnDatanode(BalancerDatanode)
4027    2   1    0 org.apache.hadoop.hdfs.server.balancer.Balancer.BalancerBlock.getLocations()
4028    2   1    0 org.apache.hadoop.hdfs.server.balancer.Balancer.BalancerBlock.getBlock()
4029    2   1    0 org.apache.hadoop.hdfs.server.balancer.Balancer.BalancerBlock.getBlockId()
4030    2   1    0 org.apache.hadoop.hdfs.server.balancer.Balancer.BalancerBlock.getNumBytes()
4031    3   1    0 org.apache.hadoop.hdfs.server.balancer.Balancer.NodeTask.NodeTask(BalancerDatanode,long)
4032    2   1    0 org.apache.hadoop.hdfs.server.balancer.Balancer.NodeTask.getDatanode()
4033    2   1    0 org.apache.hadoop.hdfs.server.balancer.Balancer.NodeTask.getSize()
4034    2   1    0 org.apache.hadoop.hdfs.server.balancer.Balancer.getUtilization(DatanodeInfo)
4035   10   4    0 org.apache.hadoop.hdfs.server.balancer.Balancer.BalancerDatanode.BalancerDatanode(DatanodeInfo,double,double)
4036    2   1    1 org.apache.hadoop.hdfs.server.balancer.Balancer.BalancerDatanode.getDatanode()
4037    2   1    1 org.apache.hadoop.hdfs.server.balancer.Balancer.BalancerDatanode.getName()
4038    2   1    0 org.apache.hadoop.hdfs.server.balancer.Balancer.BalancerDatanode.getStorageID()
4039    2   1    1 org.apache.hadoop.hdfs.server.balancer.Balancer.BalancerDatanode.isMoveQuotaFull()
4040    2   1    1 org.apache.hadoop.hdfs.server.balancer.Balancer.BalancerDatanode.availableSizeToMove()
4041    2   1    0 org.apache.hadoop.hdfs.server.balancer.Balancer.BalancerDatanode.incScheduledSize(long)
4042    4   3    0 org.apache.hadoop.hdfs.server.balancer.Balancer.BalancerDatanode.isPendingQNotFull()
4043    2   1    0 org.apache.hadoop.hdfs.server.balancer.Balancer.BalancerDatanode.isPendingQEmpty()
4044    4   3    0 org.apache.hadoop.hdfs.server.balancer.Balancer.BalancerDatanode.addPendingBlock(PendingBlockMove)
4045    2   1    0 org.apache.hadoop.hdfs.server.balancer.Balancer.BalancerDatanode.removePendingBlock(PendingBlockMove)
4046    2   1    1 org.apache.hadoop.hdfs.server.balancer.Balancer.BalancerDatanode.readFields(DataInput)
4047    2   1    1 org.apache.hadoop.hdfs.server.balancer.Balancer.BalancerDatanode.write(DataOutput)
4048    2   1    0 org.apache.hadoop.hdfs.server.balancer.Balancer.Source.BlockMoveDispatcher.run()
4049    2   1    0 org.apache.hadoop.hdfs.server.balancer.Balancer.Source.Source(DatanodeInfo,double,double)
4050    4   1    1 org.apache.hadoop.hdfs.server.balancer.Balancer.Source.addNodeTask(NodeTask)
4051    2   1    0 org.apache.hadoop.hdfs.server.balancer.Balancer.Source.getBlockIterator()
4052   21   7    0 org.apache.hadoop.hdfs.server.balancer.Balancer.Source.getBlockList()
4053    5   4    0 org.apache.hadoop.hdfs.server.balancer.Balancer.Source.isGoodBlockCandidate(BalancerBlock)
4054   18   6    0 org.apache.hadoop.hdfs.server.balancer.Balancer.Source.chooseNextBlockToMove()
4055    4   3    0 org.apache.hadoop.hdfs.server.balancer.Balancer.Source.filterMovedBlocks()
4056    2   2    0 org.apache.hadoop.hdfs.server.balancer.Balancer.Source.shouldFetchMoreBlocks()
4057   22  11    0 org.apache.hadoop.hdfs.server.balancer.Balancer.Source.dispatchBlocks()
4058    1   1    1 org.apache.hadoop.hdfs.server.balancer.Balancer.Balancer()
4059    2   1    1 org.apache.hadoop.hdfs.server.balancer.Balancer.Balancer(Configuration)
4060    3   1    1 org.apache.hadoop.hdfs.server.balancer.Balancer.Balancer(Configuration,double)
4061    5   2    1 org.apache.hadoop.hdfs.server.balancer.Balancer.main(String[])
4062    3   1    0 org.apache.hadoop.hdfs.server.balancer.Balancer.printUsage()
4063   19  11    0 org.apache.hadoop.hdfs.server.balancer.Balancer.parseArgs(String[])
4064    5   1    0 org.apache.hadoop.hdfs.server.balancer.Balancer.init(double)
4065   12   3    0 org.apache.hadoop.hdfs.server.balancer.Balancer.createNamenode(Configuration)
4066    6   2    0 org.apache.hadoop.hdfs.server.balancer.Balancer.shuffleArray(DatanodeInfo[])
4067    2   1    0 org.apache.hadoop.hdfs.server.balancer.Balancer.initNodes()
4068   35  10    0 org.apache.hadoop.hdfs.server.balancer.Balancer.initNodes(DatanodeInfo[])
4069   15   3    0 org.apache.hadoop.hdfs.server.balancer.Balancer.logImbalancedNodes()
4070    8   2    0 org.apache.hadoop.hdfs.server.balancer.Balancer.chooseNodes()
4071    4   1    0 org.apache.hadoop.hdfs.server.balancer.Balancer.chooseNodes(boolean)
4072    7   4    0 org.apache.hadoop.hdfs.server.balancer.Balancer.chooseTargets(Iterator,boolean)
4073    7   4    0 org.apache.hadoop.hdfs.server.balancer.Balancer.chooseSources(Iterator,boolean)
4074   29  12    0 org.apache.hadoop.hdfs.server.balancer.Balancer.chooseTarget(Source,Iterator,boolean)
4075   29  12    0 org.apache.hadoop.hdfs.server.balancer.Balancer.chooseSource(BalancerDatanode,Iterator,boolean)
4076    2   1    0 org.apache.hadoop.hdfs.server.balancer.Balancer.BytesMoved.inc(long)
4077    2   1    0 org.apache.hadoop.hdfs.server.balancer.Balancer.BytesMoved.get()
4078   12   4    0 org.apache.hadoop.hdfs.server.balancer.Balancer.dispatchBlockMoves()
4079    2   1    1 org.apache.hadoop.hdfs.server.balancer.Balancer.setBlockMoveWaitTime(long)
4080   10   6    0 org.apache.hadoop.hdfs.server.balancer.Balancer.waitForMoveCompletion()
4081    3   1    0 org.apache.hadoop.hdfs.server.balancer.Balancer.addToMoved(BalancerBlock)
4082    3   1    0 org.apache.hadoop.hdfs.server.balancer.Balancer.isMoved(BalancerBlock)
4083   23  12    0 org.apache.hadoop.hdfs.server.balancer.Balancer.isGoodBlockCandidate(Source,BalancerDatanode,BalancerBlock)
4084   11   1    0 org.apache.hadoop.hdfs.server.balancer.Balancer.resetData()
4085    5   3    0 org.apache.hadoop.hdfs.server.balancer.Balancer.cleanGlobalBlockList()
4086    2   1    0 org.apache.hadoop.hdfs.server.balancer.Balancer.isOverUtilized(BalancerDatanode)
4087    2   2    0 org.apache.hadoop.hdfs.server.balancer.Balancer.isAboveAvgUtilized(BalancerDatanode)
4088    2   1    0 org.apache.hadoop.hdfs.server.balancer.Balancer.isUnderUtilized(BalancerDatanode)
4089    2   2    0 org.apache.hadoop.hdfs.server.balancer.Balancer.isBelowAvgUtilized(BalancerDatanode)
4090   48  17    1 org.apache.hadoop.hdfs.server.balancer.Balancer.run(String[])
4091   10   6    0 org.apache.hadoop.hdfs.server.balancer.Balancer.checkAndMarkRunningBalancer()
4092   17   4    0 org.apache.hadoop.hdfs.server.balancer.Balancer.time2Str(long)
4093    2   1    1 org.apache.hadoop.hdfs.server.balancer.Balancer.getConf()
4094    2   1    1 org.apache.hadoop.hdfs.server.balancer.Balancer.setConf(Configuration)
4095    2   1    0 org.apache.hadoop.hdfs.server.common.GenerationStamp.WritableFactory$1.newInstance()
4096    2   1    1 org.apache.hadoop.hdfs.server.common.GenerationStamp.GenerationStamp()
4097    2   1    1 org.apache.hadoop.hdfs.server.common.GenerationStamp.GenerationStamp(long)
4098    2   1    1 org.apache.hadoop.hdfs.server.common.GenerationStamp.getStamp()
4099    2   1    1 org.apache.hadoop.hdfs.server.common.GenerationStamp.setStamp(long)
4100    3   1    1 org.apache.hadoop.hdfs.server.common.GenerationStamp.nextStamp()
4101    2   1    0 org.apache.hadoop.hdfs.server.common.GenerationStamp.write(DataOutput)
4102    4   3    0 org.apache.hadoop.hdfs.server.common.GenerationStamp.readFields(DataInput)
4103    2   3    0 org.apache.hadoop.hdfs.server.common.GenerationStamp.compare(long,long)
4104    2   1    1 org.apache.hadoop.hdfs.server.common.GenerationStamp.compareTo(GenerationStamp)
4105    4   3    1 org.apache.hadoop.hdfs.server.common.GenerationStamp.equals(Object)
4106    2   3    0 org.apache.hadoop.hdfs.server.common.GenerationStamp.equalsWithWildcard(long,long)
4107    2   1    1 org.apache.hadoop.hdfs.server.common.GenerationStamp.hashCode()
4108    2   1    0 org.apache.hadoop.hdfs.server.common.HdfsConstants.StartupOption.StartupOption(String)
4109    2   1    0 org.apache.hadoop.hdfs.server.common.HdfsConstants.StartupOption.getName()
4110    2   1    0 org.apache.hadoop.hdfs.server.common.InconsistentFSStateException.InconsistentFSStateException(File,String)
4111    2   1    0 org.apache.hadoop.hdfs.server.common.InconsistentFSStateException.InconsistentFSStateException(File,String,Throwable)
4112    4   3    0 org.apache.hadoop.hdfs.server.common.InconsistentFSStateException.getFilePath(File)
4113    2   1    0 org.apache.hadoop.hdfs.server.common.IncorrectVersionException.IncorrectVersionException(int,String)
4114    2   2    0 org.apache.hadoop.hdfs.server.common.IncorrectVersionException.IncorrectVersionException(int,String,int)
4115    1   1    0 org.apache.hadoop.hdfs.server.common.Storage.StorageDirType.getStorageDirType()
4116    1   1    0 org.apache.hadoop.hdfs.server.common.Storage.StorageDirType.isOfType(StorageDirType)
4117    4   1    0 org.apache.hadoop.hdfs.server.common.Storage.DirIterator.DirIterator(StorageDirType)
4118   11   9    0 org.apache.hadoop.hdfs.server.common.Storage.DirIterator.hasNext()
4119   10   4    0 org.apache.hadoop.hdfs.server.common.Storage.DirIterator.next()
4120    4   1    0 org.apache.hadoop.hdfs.server.common.Storage.DirIterator.remove()
4121    2   1    1 org.apache.hadoop.hdfs.server.common.Storage.dirIterator()
4122    2   1    1 org.apache.hadoop.hdfs.server.common.Storage.dirIterator(StorageDirType)
4123    2   1    0 org.apache.hadoop.hdfs.server.common.Storage.StorageDirectory.StorageDirectory(File)
4124    4   1    0 org.apache.hadoop.hdfs.server.common.Storage.StorageDirectory.StorageDirectory(File,StorageDirType)
4125    2   1    1 org.apache.hadoop.hdfs.server.common.Storage.StorageDirectory.getRoot()
4126    2   1    1 org.apache.hadoop.hdfs.server.common.Storage.StorageDirectory.getStorageDirType()
4127    2   1    1 org.apache.hadoop.hdfs.server.common.Storage.StorageDirectory.read()
4128   12   2    0 org.apache.hadoop.hdfs.server.common.Storage.StorageDirectory.read(File)
4129    3   1    1 org.apache.hadoop.hdfs.server.common.Storage.StorageDirectory.write()
4130   13   2    0 org.apache.hadoop.hdfs.server.common.Storage.StorageDirectory.write(File)
4131    7   6    1 org.apache.hadoop.hdfs.server.common.Storage.StorageDirectory.clearDirectory()
4132    2   1    0 org.apache.hadoop.hdfs.server.common.Storage.StorageDirectory.getCurrentDir()
4133    2   1    0 org.apache.hadoop.hdfs.server.common.Storage.StorageDirectory.getVersionFile()
4134    2   1    0 org.apache.hadoop.hdfs.server.common.Storage.StorageDirectory.getPreviousVersionFile()
4135    2   1    0 org.apache.hadoop.hdfs.server.common.Storage.StorageDirectory.getPreviousDir()
4136    2   1    0 org.apache.hadoop.hdfs.server.common.Storage.StorageDirectory.getPreviousTmp()
4137    2   1    0 org.apache.hadoop.hdfs.server.common.Storage.StorageDirectory.getRemovedTmp()
4138    2   1    0 org.apache.hadoop.hdfs.server.common.Storage.StorageDirectory.getFinalizedTmp()
4139    2   1    0 org.apache.hadoop.hdfs.server.common.Storage.StorageDirectory.getLastCheckpointTmp()
4140    2   1    0 org.apache.hadoop.hdfs.server.common.Storage.StorageDirectory.getPreviousCheckpoint()
4141   57  47    1 org.apache.hadoop.hdfs.server.common.Storage.StorageDirectory.analyzeStorage(StartupOption)
4142   41  19    1 org.apache.hadoop.hdfs.server.common.Storage.StorageDirectory.doRecover(StorageState)
4143    6   3    1 org.apache.hadoop.hdfs.server.common.Storage.StorageDirectory.lock()
4144   14   5    1 org.apache.hadoop.hdfs.server.common.Storage.StorageDirectory.tryLock()
4145    6   3    1 org.apache.hadoop.hdfs.server.common.Storage.StorageDirectory.unlock()
4146    3   1    1 org.apache.hadoop.hdfs.server.common.Storage.Storage(NodeType)
4147    3   1    0 org.apache.hadoop.hdfs.server.common.Storage.Storage(NodeType,int,long)
4148    3   1    0 org.apache.hadoop.hdfs.server.common.Storage.Storage(NodeType,StorageInfo)
4149    2   1    0 org.apache.hadoop.hdfs.server.common.Storage.getNumStorageDirs()
4150    2   1    0 org.apache.hadoop.hdfs.server.common.Storage.getStorageDir(int)
4151    2   1    0 org.apache.hadoop.hdfs.server.common.Storage.addStorageDir(StorageDirectory)
4152    1   1    0 org.apache.hadoop.hdfs.server.common.Storage.isConversionNeeded(StorageDirectory)
4153    3   2    0 org.apache.hadoop.hdfs.server.common.Storage.checkConversionNeeded(StorageDirectory)
4154    5   4    1 org.apache.hadoop.hdfs.server.common.Storage.checkVersionUpgradable(int)
4155   20  13    1 org.apache.hadoop.hdfs.server.common.Storage.getFields(Properties,StorageDirectory)
4156    5   1    1 org.apache.hadoop.hdfs.server.common.Storage.setFields(Properties,StorageDirectory)
4157    3   3    0 org.apache.hadoop.hdfs.server.common.Storage.rename(File,File)
4158    3   3    0 org.apache.hadoop.hdfs.server.common.Storage.deleteDir(File)
4159    4   2    1 org.apache.hadoop.hdfs.server.common.Storage.writeAll()
4160    3   2    1 org.apache.hadoop.hdfs.server.common.Storage.unlockAll()
4161   20   9    1 org.apache.hadoop.hdfs.server.common.Storage.isLockSupported(int)
4162    2   1    0 org.apache.hadoop.hdfs.server.common.Storage.getBuildVersion()
4163    2   1    0 org.apache.hadoop.hdfs.server.common.Storage.getRegistrationID(StorageInfo)
4164    1   1    0 org.apache.hadoop.hdfs.server.common.Storage.corruptPreUpgradeStorage(File)
4165    7   1    0 org.apache.hadoop.hdfs.server.common.Storage.writeCorruptedData(RandomAccessFile)
4166    2   1    0 org.apache.hadoop.hdfs.server.common.StorageInfo.StorageInfo()
4167    4   1    0 org.apache.hadoop.hdfs.server.common.StorageInfo.StorageInfo(int,int,long)
4168    2   1    0 org.apache.hadoop.hdfs.server.common.StorageInfo.StorageInfo(StorageInfo)
4169    2   1    0 org.apache.hadoop.hdfs.server.common.StorageInfo.getLayoutVersion()
4170    2   1    0 org.apache.hadoop.hdfs.server.common.StorageInfo.getNamespaceID()
4171    2   1    0 org.apache.hadoop.hdfs.server.common.StorageInfo.getCTime()
4172    4   1    0 org.apache.hadoop.hdfs.server.common.StorageInfo.setStorageInfo(StorageInfo)
4173    1   1    1 org.apache.hadoop.hdfs.server.common.Upgradeable.getVersion()
4174    1   1    1 org.apache.hadoop.hdfs.server.common.Upgradeable.getType()
4175    1   1    1 org.apache.hadoop.hdfs.server.common.Upgradeable.getDescription()
4176    1   1    1 org.apache.hadoop.hdfs.server.common.Upgradeable.getUpgradeStatus()
4177    1   1    1 org.apache.hadoop.hdfs.server.common.Upgradeable.startUpgrade()
4178    1   1    1 org.apache.hadoop.hdfs.server.common.Upgradeable.completeUpgrade()
4179    1   1    1 org.apache.hadoop.hdfs.server.common.Upgradeable.getUpgradeStatusReport(boolean)
4180    2   1    0 org.apache.hadoop.hdfs.server.common.UpgradeManager.getBroadcastCommand()
4181    2   1    0 org.apache.hadoop.hdfs.server.common.UpgradeManager.getUpgradeState()
4182    2   1    0 org.apache.hadoop.hdfs.server.common.UpgradeManager.getUpgradeVersion()
4183    3   1    0 org.apache.hadoop.hdfs.server.common.UpgradeManager.setUpgradeState(boolean,int)
4184    2   1    0 org.apache.hadoop.hdfs.server.common.UpgradeManager.getDistributedUpgrades()
4185    4   3    0 org.apache.hadoop.hdfs.server.common.UpgradeManager.getUpgradeStatus()
4186    8   3    0 org.apache.hadoop.hdfs.server.common.UpgradeManager.initializeUpgrade()
4187    4   3    0 org.apache.hadoop.hdfs.server.common.UpgradeManager.isUpgradeCompleted()
4188    1   1    0 org.apache.hadoop.hdfs.server.common.UpgradeManager.getType()
4189    1   1    0 org.apache.hadoop.hdfs.server.common.UpgradeManager.startUpgrade()
4190    1   1    0 org.apache.hadoop.hdfs.server.common.UpgradeManager.completeUpgrade()
4191    2   1    0 org.apache.hadoop.hdfs.server.common.UpgradeObject.getUpgradeStatus()
4192    2   1    0 org.apache.hadoop.hdfs.server.common.UpgradeObject.getDescription()
4193    2   1    0 org.apache.hadoop.hdfs.server.common.UpgradeObject.getUpgradeStatusReport(boolean)
4194    7   6    0 org.apache.hadoop.hdfs.server.common.UpgradeObject.compareTo(Upgradeable)
4195    4   3    0 org.apache.hadoop.hdfs.server.common.UpgradeObject.equals(Object)
4196    2   1    0 org.apache.hadoop.hdfs.server.common.UpgradeObject.hashCode()
4197    4   1    0 org.apache.hadoop.hdfs.server.common.UpgradeObjectCollection.UOSignature.UOSignature(Upgradeable)
4198    2   1    0 org.apache.hadoop.hdfs.server.common.UpgradeObjectCollection.UOSignature.getVersion()
4199    2   1    0 org.apache.hadoop.hdfs.server.common.UpgradeObjectCollection.UOSignature.getType()
4200    2   1    0 org.apache.hadoop.hdfs.server.common.UpgradeObjectCollection.UOSignature.getClassName()
4201    8   8    0 org.apache.hadoop.hdfs.server.common.UpgradeObjectCollection.UOSignature.instantiate()
4202    7   6    0 org.apache.hadoop.hdfs.server.common.UpgradeObjectCollection.UOSignature.compareTo(UOSignature)
4203    4   3    0 org.apache.hadoop.hdfs.server.common.UpgradeObjectCollection.UOSignature.equals(Object)
4204    2   3    0 org.apache.hadoop.hdfs.server.common.UpgradeObjectCollection.UOSignature.hashCode()
4205    2   1    0 org.apache.hadoop.hdfs.server.common.UpgradeObjectCollection.initialize()
4206    2   1    0 org.apache.hadoop.hdfs.server.common.UpgradeObjectCollection.registerUpgrade(Upgradeable)
4207   14   7    0 org.apache.hadoop.hdfs.server.common.UpgradeObjectCollection.getDistributedUpgrades(int,HdfsConstants.NodeType)
4208    4   1    0 org.apache.hadoop.hdfs.server.common.UpgradeStatusReport.UpgradeStatusReport()
4209    4   1    0 org.apache.hadoop.hdfs.server.common.UpgradeStatusReport.UpgradeStatusReport(int,short,boolean)
4210    2   1    1 org.apache.hadoop.hdfs.server.common.UpgradeStatusReport.getVersion()
4211    2   1    1 org.apache.hadoop.hdfs.server.common.UpgradeStatusReport.getUpgradeStatus()
4212    2   1    1 org.apache.hadoop.hdfs.server.common.UpgradeStatusReport.isFinalized()
4213    2   3    1 org.apache.hadoop.hdfs.server.common.UpgradeStatusReport.getStatusText(boolean)
4214    2   1    1 org.apache.hadoop.hdfs.server.common.UpgradeStatusReport.toString()
4215    2   1    0 org.apache.hadoop.hdfs.server.common.UpgradeStatusReport.WritableFactory$1.newInstance()
4216    3   1    1 org.apache.hadoop.hdfs.server.common.UpgradeStatusReport.write(DataOutput)
4217    3   1    1 org.apache.hadoop.hdfs.server.common.UpgradeStatusReport.readFields(DataInput)
4218    2   1    1 org.apache.hadoop.hdfs.server.common.Util.now()
4219    3   1    0 org.apache.hadoop.hdfs.server.datanode.BlockMetadataHeader.BlockMetadataHeader(short,DataChecksum)
4220    2   1    0 org.apache.hadoop.hdfs.server.datanode.BlockMetadataHeader.getVersion()
4221    2   1    0 org.apache.hadoop.hdfs.server.datanode.BlockMetadataHeader.getChecksum()
4222    2   1    1 org.apache.hadoop.hdfs.server.datanode.BlockMetadataHeader.readHeader(DataInputStream)
4223    6   2    1 org.apache.hadoop.hdfs.server.datanode.BlockMetadataHeader.readHeader(File)
4224    3   1    0 org.apache.hadoop.hdfs.server.datanode.BlockMetadataHeader.readHeader(short,DataInputStream)
4225    3   1    1 org.apache.hadoop.hdfs.server.datanode.BlockMetadataHeader.writeHeader(DataOutputStream,BlockMetadataHeader)
4226    2   1    1 org.apache.hadoop.hdfs.server.datanode.BlockMetadataHeader.writeHeader(DataOutputStream,DataChecksum)
4227    2   1    1 org.apache.hadoop.hdfs.server.datanode.BlockMetadataHeader.getHeaderSize()
4228   23   6    0 org.apache.hadoop.hdfs.server.datanode.BlockReceiver.BlockReceiver(Block,DataInputStream,String,String,boolean,String,DatanodeInfo,DataNode)
4229   17   7    1 org.apache.hadoop.hdfs.server.datanode.BlockReceiver.close()
4230    5   3    1 org.apache.hadoop.hdfs.server.datanode.BlockReceiver.flush()
4231    5   3    1 org.apache.hadoop.hdfs.server.datanode.BlockReceiver.handleMirrorOutError(IOException)
4232   16   6    1 org.apache.hadoop.hdfs.server.datanode.BlockReceiver.verifyChunks(byte[],int,int,byte[],int)
4233   11   5    1 org.apache.hadoop.hdfs.server.datanode.BlockReceiver.shiftBufData()
4234    9   5    1 org.apache.hadoop.hdfs.server.datanode.BlockReceiver.readToBuf(int)
4235   40  17    1 org.apache.hadoop.hdfs.server.datanode.BlockReceiver.readNextPacket()
4236   58  21    1 org.apache.hadoop.hdfs.server.datanode.BlockReceiver.receivePacket()
4237    2   1    0 org.apache.hadoop.hdfs.server.datanode.BlockReceiver.writeChecksumHeader(DataOutputStream)
4238   34  14    0 org.apache.hadoop.hdfs.server.datanode.BlockReceiver.receiveBlock(DataOutputStream,DataInputStream,DataOutputStream,String,BlockTransferThrottler,int)
4239   19  12    1 org.apache.hadoop.hdfs.server.datanode.BlockReceiver.setBlockPosition(long)
4240   19   3    1 org.apache.hadoop.hdfs.server.datanode.BlockReceiver.computePartialChunkCrc(long,long,int)
4241    2   1    0 org.apache.hadoop.hdfs.server.datanode.BlockReceiver.PacketResponder.toString()
4242    6   1    0 org.apache.hadoop.hdfs.server.datanode.BlockReceiver.PacketResponder.PacketResponder(BlockReceiver,Block,DataInputStream,DataOutputStream,int)
4243    5   2    1 org.apache.hadoop.hdfs.server.datanode.BlockReceiver.PacketResponder.enqueue(long,boolean)
4244    8   5    1 org.apache.hadoop.hdfs.server.datanode.BlockReceiver.PacketResponder.close()
4245   47  19    0 org.apache.hadoop.hdfs.server.datanode.BlockReceiver.PacketResponder.lastDataNodeRun()
4246   80  34    1 org.apache.hadoop.hdfs.server.datanode.BlockReceiver.PacketResponder.run()
4247    3   1    0 org.apache.hadoop.hdfs.server.datanode.BlockReceiver.Packet.Packet(long,boolean)
4248    2   1    0 org.apache.hadoop.hdfs.server.datanode.BlockSender.BlockSender(Block,long,long,boolean,boolean,boolean,DataNode)
4249   47  18    0 org.apache.hadoop.hdfs.server.datanode.BlockSender.BlockSender(Block,long,long,boolean,boolean,boolean,DataNode,String)
4250   14   7    1 org.apache.hadoop.hdfs.server.datanode.BlockSender.close()
4251   59  21    1 org.apache.hadoop.hdfs.server.datanode.BlockSender.sendChunks(ByteBuffer,int,OutputStream)
4252   36  11    1 org.apache.hadoop.hdfs.server.datanode.BlockSender.sendBlock(DataOutputStream,OutputStream,BlockTransferThrottler)
4253    2   1    0 org.apache.hadoop.hdfs.server.datanode.BlockSender.isBlockReadFully()
4254    2   1    1 org.apache.hadoop.hdfs.server.datanode.BlockTransferThrottler.BlockTransferThrottler(long)
4255    5   1    1 org.apache.hadoop.hdfs.server.datanode.BlockTransferThrottler.BlockTransferThrottler(long,long)
4256    2   1    1 org.apache.hadoop.hdfs.server.datanode.BlockTransferThrottler.getBandwidth()
4257    4   3    1 org.apache.hadoop.hdfs.server.datanode.BlockTransferThrottler.setBandwidth(long)
4258   19   7    1 org.apache.hadoop.hdfs.server.datanode.BlockTransferThrottler.throttle(long)
4259    2   1    0 org.apache.hadoop.hdfs.server.datanode.DataBlockScanner.BlockScanInfo.BlockScanInfo(Block)
4260    2   1    0 org.apache.hadoop.hdfs.server.datanode.DataBlockScanner.BlockScanInfo.hashCode()
4261    2   2    0 org.apache.hadoop.hdfs.server.datanode.DataBlockScanner.BlockScanInfo.equals(Object)
4262    2   2    0 org.apache.hadoop.hdfs.server.datanode.DataBlockScanner.BlockScanInfo.getLastScanTime()
4263    4   3    0 org.apache.hadoop.hdfs.server.datanode.DataBlockScanner.BlockScanInfo.compareTo(BlockScanInfo)
4264    7   2    0 org.apache.hadoop.hdfs.server.datanode.DataBlockScanner.DataBlockScanner(DataNode,FSDataset,Configuration)
4265    2   1    0 org.apache.hadoop.hdfs.server.datanode.DataBlockScanner.isInitiliazed()
4266    4   2    0 org.apache.hadoop.hdfs.server.datanode.DataBlockScanner.updateBytesToScan(long,long)
4267    8   3    0 org.apache.hadoop.hdfs.server.datanode.DataBlockScanner.addBlockInfo(BlockScanInfo)
4268    8   3    0 org.apache.hadoop.hdfs.server.datanode.DataBlockScanner.delBlockInfo(BlockScanInfo)
4269    7   4    1 org.apache.hadoop.hdfs.server.datanode.DataBlockScanner.updateBlockInfo(LogEntry)
4270   23   6    0 org.apache.hadoop.hdfs.server.datanode.DataBlockScanner.init()
4271    3   1    0 org.apache.hadoop.hdfs.server.datanode.DataBlockScanner.getNewBlockScanTime()
4272   11   4    1 org.apache.hadoop.hdfs.server.datanode.DataBlockScanner.addBlock(Block)
4273    6   4    1 org.apache.hadoop.hdfs.server.datanode.DataBlockScanner.deleteBlock(Block)
4274    5   4    1 org.apache.hadoop.hdfs.server.datanode.DataBlockScanner.getLastScanTime(Block)
4275    3   2    1 org.apache.hadoop.hdfs.server.datanode.DataBlockScanner.deleteBlocks(Block[])
4276    2   1    0 org.apache.hadoop.hdfs.server.datanode.DataBlockScanner.verifiedByClient(Block)
4277   20   9    0 org.apache.hadoop.hdfs.server.datanode.DataBlockScanner.updateScanStatus(Block,ScanType,boolean)
4278    7   2    0 org.apache.hadoop.hdfs.server.datanode.DataBlockScanner.handleScanFailure(Block)
4279    2   1    0 org.apache.hadoop.hdfs.server.datanode.DataBlockScanner.LogEntry.newEnry(Block,long)
4280   18   7    0 org.apache.hadoop.hdfs.server.datanode.DataBlockScanner.LogEntry.parseEntry(String)
4281    4   1    0 org.apache.hadoop.hdfs.server.datanode.DataBlockScanner.adjustThrottler()
4282   30  11    0 org.apache.hadoop.hdfs.server.datanode.DataBlockScanner.verifyBlock(Block)
4283    4   3    0 org.apache.hadoop.hdfs.server.datanode.DataBlockScanner.getEarliestScanTime()
4284    7   3    0 org.apache.hadoop.hdfs.server.datanode.DataBlockScanner.verifyFirstBlock()
4285   28  12    1 org.apache.hadoop.hdfs.server.datanode.DataBlockScanner.assignInitialVerificationTimes()
4286    4   1    0 org.apache.hadoop.hdfs.server.datanode.DataBlockScanner.startNewPeriod()
4287   21  10    0 org.apache.hadoop.hdfs.server.datanode.DataBlockScanner.run()
4288    5   2    0 org.apache.hadoop.hdfs.server.datanode.DataBlockScanner.shutdown()
4289   37  14    0 org.apache.hadoop.hdfs.server.datanode.DataBlockScanner.printBlockReport(StringBuilder,boolean)
4290    2   2    0 org.apache.hadoop.hdfs.server.datanode.DataBlockScanner.LogFileHandler.isFilePresent(File,String)
4291    6   1    1 org.apache.hadoop.hdfs.server.datanode.DataBlockScanner.LogFileHandler.LogFileHandler(File,String,int)
4292    2   1    0 org.apache.hadoop.hdfs.server.datanode.DataBlockScanner.LogFileHandler.setMaxNumLines(int)
4293    9   4    1 org.apache.hadoop.hdfs.server.datanode.DataBlockScanner.LogFileHandler.appendLine(String)
4294    5   2    0 org.apache.hadoop.hdfs.server.datanode.DataBlockScanner.LogFileHandler.warn(String)
4295    3   1    0 org.apache.hadoop.hdfs.server.datanode.DataBlockScanner.LogFileHandler.openCurFile()
4296    8   3    0 org.apache.hadoop.hdfs.server.datanode.DataBlockScanner.LogFileHandler.updateCurNumLines()
4297   14  11    0 org.apache.hadoop.hdfs.server.datanode.DataBlockScanner.LogFileHandler.rollIfRequired()
4298    4   2    0 org.apache.hadoop.hdfs.server.datanode.DataBlockScanner.LogFileHandler.close()
4299    6   2    0 org.apache.hadoop.hdfs.server.datanode.DataBlockScanner.LogFileHandler.Reader.Reader(boolean)
4300   13   9    0 org.apache.hadoop.hdfs.server.datanode.DataBlockScanner.LogFileHandler.Reader.openFile()
4301   10   7    0 org.apache.hadoop.hdfs.server.datanode.DataBlockScanner.LogFileHandler.Reader.readNext()
4302    2   1    0 org.apache.hadoop.hdfs.server.datanode.DataBlockScanner.LogFileHandler.Reader.hasNext()
4303    6   2    0 org.apache.hadoop.hdfs.server.datanode.DataBlockScanner.LogFileHandler.Reader.next()
4304    2   2    0 org.apache.hadoop.hdfs.server.datanode.DataBlockScanner.LogFileHandler.Reader.remove()
4305   11   3    0 org.apache.hadoop.hdfs.server.datanode.DataBlockScanner.LogFileHandler.Reader.close()
4306   13   3    0 org.apache.hadoop.hdfs.server.datanode.DataBlockScanner.Servlet.doGet(HttpServletRequest,HttpServletResponse)
4307    2   1    0 org.apache.hadoop.hdfs.server.datanode.DataNode.createSocketAddr(String)
4308    2   1    1 org.apache.hadoop.hdfs.server.datanode.DataNode.now()
4309    7   3    1 org.apache.hadoop.hdfs.server.datanode.DataNode.DataNode(Configuration,AbstractList)
4310   82  12    1 org.apache.hadoop.hdfs.server.datanode.DataNode.startDataNode(Configuration,AbstractList)
4311    2   2    1 org.apache.hadoop.hdfs.server.datanode.DataNode.newSocket()
4312   19   7    0 org.apache.hadoop.hdfs.server.datanode.DataNode.handshake()
4313    2   1    1 org.apache.hadoop.hdfs.server.datanode.DataNode.getDataNode()
4314    5   2    0 org.apache.hadoop.hdfs.server.datanode.DataNode.createInterDataNodeProtocolProxy(DatanodeID,Configuration)
4315    2   1    0 org.apache.hadoop.hdfs.server.datanode.DataNode.getNameNodeAddr()
4316    2   1    0 org.apache.hadoop.hdfs.server.datanode.DataNode.getSelfAddr()
4317    2   1    0 org.apache.hadoop.hdfs.server.datanode.DataNode.getMetrics()
4318    2   1    1 org.apache.hadoop.hdfs.server.datanode.DataNode.getNamenode()
4319   11   3    0 org.apache.hadoop.hdfs.server.datanode.DataNode.setNewStorageID(DatanodeRegistration)
4320   19  10    1 org.apache.hadoop.hdfs.server.datanode.DataNode.register()
4321   36  18    1 org.apache.hadoop.hdfs.server.datanode.DataNode.shutdown()
4322    5   3    0 org.apache.hadoop.hdfs.server.datanode.DataNode.checkDiskError(IOException)
4323    4   2    0 org.apache.hadoop.hdfs.server.datanode.DataNode.checkDiskError()
4324    5   2    0 org.apache.hadoop.hdfs.server.datanode.DataNode.handleDiskError(String)
4325    2   2    1 org.apache.hadoop.hdfs.server.datanode.DataNode.getXceiverCount()
4326   60  24    1 org.apache.hadoop.hdfs.server.datanode.DataNode.offerService()
4327   38  15    1 org.apache.hadoop.hdfs.server.datanode.DataNode.processCommand(DatanodeCommand)
4328    3   1    0 org.apache.hadoop.hdfs.server.datanode.DataNode.processDistributedUpgradeCommand(UpgradeCommand)
4329    8   3    1 org.apache.hadoop.hdfs.server.datanode.DataNode.startDistributedUpgradeIfNeeded()
4330   19   7    0 org.apache.hadoop.hdfs.server.datanode.DataNode.transferBlocks(Block[],DatanodeInfo[][])
4331    8   5    0 org.apache.hadoop.hdfs.server.datanode.DataNode.notifyNamenodeReceivedBlock(Block,String)
4332    4   1    1 org.apache.hadoop.hdfs.server.datanode.DataNode.DataTransfer.DataTransfer(DatanodeInfo[],Block,DataNode)
4333   35   3    1 org.apache.hadoop.hdfs.server.datanode.DataNode.DataTransfer.run()
4334   15   6    1 org.apache.hadoop.hdfs.server.datanode.DataNode.run()
4335    6   2    1 org.apache.hadoop.hdfs.server.datanode.DataNode.runDatanodeDaemon(DataNode)
4336   12   5    1 org.apache.hadoop.hdfs.server.datanode.DataNode.instantiateDataNode(String[],Configuration)
4337    4   1    1 org.apache.hadoop.hdfs.server.datanode.DataNode.createDataNode(String[],Configuration)
4338    4   3    0 org.apache.hadoop.hdfs.server.datanode.DataNode.join()
4339   12   5    1 org.apache.hadoop.hdfs.server.datanode.DataNode.makeInstance(String[],Configuration)
4340    2   1    0 org.apache.hadoop.hdfs.server.datanode.DataNode.toString()
4341    3   1    0 org.apache.hadoop.hdfs.server.datanode.DataNode.printUsage()
4342   18   8    1 org.apache.hadoop.hdfs.server.datanode.DataNode.parseArguments(String[],Configuration)
4343    2   1    0 org.apache.hadoop.hdfs.server.datanode.DataNode.setStartupOption(Configuration,StartupOption)
4344    2   1    0 org.apache.hadoop.hdfs.server.datanode.DataNode.getStartupOption(Configuration)
4345    6   2    1 org.apache.hadoop.hdfs.server.datanode.DataNode.scheduleBlockReport(long)
4346    2   1    1 org.apache.hadoop.hdfs.server.datanode.DataNode.getFSDataset()
4347    8   3    1 org.apache.hadoop.hdfs.server.datanode.DataNode.main(String[])
4348   11   5    1 org.apache.hadoop.hdfs.server.datanode.DataNode.getBlockMetaDataInfo(Block)
4349    6   3    0 org.apache.hadoop.hdfs.server.datanode.DataNode.Runnable$1.run()
4350   10   1    0 org.apache.hadoop.hdfs.server.datanode.DataNode.recoverBlocks(Block[],DatanodeInfo[][])
4351    8   2    1 org.apache.hadoop.hdfs.server.datanode.DataNode.updateBlock(Block,Block,boolean)
4352    7   6    1 org.apache.hadoop.hdfs.server.datanode.DataNode.getProtocolVersion(String,long)
4353    4   1    0 org.apache.hadoop.hdfs.server.datanode.DataNode.BlockRecord.BlockRecord(DatanodeID,InterDatanodeProtocol,Block)
4354    2   1    1 org.apache.hadoop.hdfs.server.datanode.DataNode.BlockRecord.toString()
4355   34  16    1 org.apache.hadoop.hdfs.server.datanode.DataNode.recoverBlock(Block,boolean,DatanodeID[],boolean)
4356   25  11    1 org.apache.hadoop.hdfs.server.datanode.DataNode.syncBlock(Block,List,boolean)
4357    3   1    1 org.apache.hadoop.hdfs.server.datanode.DataNode.recoverBlock(Block,boolean,DatanodeInfo[])
4358    5   2    0 org.apache.hadoop.hdfs.server.datanode.DataNode.logRecoverBlock(String,Block,DatanodeID[])
4359    4   1    0 org.apache.hadoop.hdfs.server.datanode.DatanodeBlockInfo.DatanodeBlockInfo(FSVolume,File)
4360    4   1    0 org.apache.hadoop.hdfs.server.datanode.DatanodeBlockInfo.DatanodeBlockInfo(FSVolume)
4361    2   1    0 org.apache.hadoop.hdfs.server.datanode.DatanodeBlockInfo.getVolume()
4362    2   1    0 org.apache.hadoop.hdfs.server.datanode.DatanodeBlockInfo.getFile()
4363    2   1    1 org.apache.hadoop.hdfs.server.datanode.DatanodeBlockInfo.isDetached()
4364    2   1    1 org.apache.hadoop.hdfs.server.datanode.DatanodeBlockInfo.setDetached()
4365   11   6    1 org.apache.hadoop.hdfs.server.datanode.DatanodeBlockInfo.detachFile(File,Block)
4366   15  10    1 org.apache.hadoop.hdfs.server.datanode.DatanodeBlockInfo.detachBlock(Block,int)
4367    2   1    0 org.apache.hadoop.hdfs.server.datanode.DatanodeBlockInfo.toString()
4368    3   1    0 org.apache.hadoop.hdfs.server.datanode.DataStorage.DataStorage()
4369    3   1    0 org.apache.hadoop.hdfs.server.datanode.DataStorage.DataStorage(int,long,String)
4370    3   1    0 org.apache.hadoop.hdfs.server.datanode.DataStorage.DataStorage(StorageInfo,String)
4371    2   1    0 org.apache.hadoop.hdfs.server.datanode.DataStorage.getStorageID()
4372    2   1    0 org.apache.hadoop.hdfs.server.datanode.DataStorage.setStorageID(String)
4373   36  10    1 org.apache.hadoop.hdfs.server.datanode.DataStorage.recoverTransitionRead(NamespaceInfo,Collection,StartupOption)
4374    6   1    0 org.apache.hadoop.hdfs.server.datanode.DataStorage.format(StorageDirectory,NamespaceInfo)
4375    3   1    0 org.apache.hadoop.hdfs.server.datanode.DataStorage.setFields(Properties,StorageDirectory)
4376    7   7    0 org.apache.hadoop.hdfs.server.datanode.DataStorage.getFields(Properties,StorageDirectory)
4377   14   5    0 org.apache.hadoop.hdfs.server.datanode.DataStorage.isConversionNeeded(StorageDirectory)
4378   15  11    1 org.apache.hadoop.hdfs.server.datanode.DataStorage.doTransition(StorageDirectory,NamespaceInfo,StartupOption)
4379   17   2    1 org.apache.hadoop.hdfs.server.datanode.DataStorage.doUpgrade(StorageDirectory,NamespaceInfo)
4380   18   6    0 org.apache.hadoop.hdfs.server.datanode.DataStorage.doRollback(StorageDirectory,NamespaceInfo)
4381    5   2    0 org.apache.hadoop.hdfs.server.datanode.DataStorage.Runnable$1.run()
4382    2   1    0 org.apache.hadoop.hdfs.server.datanode.DataStorage.Runnable$1.toString()
4383   17   3    0 org.apache.hadoop.hdfs.server.datanode.DataStorage.doFinalize(StorageDirectory)
4384    3   2    0 org.apache.hadoop.hdfs.server.datanode.DataStorage.finalizeUpgrade()
4385    2   3    0 org.apache.hadoop.hdfs.server.datanode.DataStorage.java.io.FilenameFilter$2.accept(File,String)
4386   16   8    0 org.apache.hadoop.hdfs.server.datanode.DataStorage.linkBlocks(File,File,int)
4387   10   5    0 org.apache.hadoop.hdfs.server.datanode.DataStorage.corruptPreUpgradeStorage(File)
4388    5   1    0 org.apache.hadoop.hdfs.server.datanode.DataStorage.verifyDistributedUpgradeProgress(NamespaceInfo)
4389    5   3    1 org.apache.hadoop.hdfs.server.datanode.DataStorage.convertMetatadataFileName(String)
4390    7   1    0 org.apache.hadoop.hdfs.server.datanode.DataXceiver.DataXceiver(Socket,DataNode,DataXceiverServer)
4391   54  15    1 org.apache.hadoop.hdfs.server.datanode.DataXceiver.run()
4392   30  12    1 org.apache.hadoop.hdfs.server.datanode.DataXceiver.readBlock(DataInputStream)
4393   95  23    1 org.apache.hadoop.hdfs.server.datanode.DataXceiver.writeBlock(DataInputStream)
4394   18   4    1 org.apache.hadoop.hdfs.server.datanode.DataXceiver.readMetadata(DataInputStream)
4395   22   2    1 org.apache.hadoop.hdfs.server.datanode.DataXceiver.getBlockChecksum(DataInputStream)
4396   26   7    1 org.apache.hadoop.hdfs.server.datanode.DataXceiver.copyBlock(DataInputStream)
4397   45   8    1 org.apache.hadoop.hdfs.server.datanode.DataXceiver.replaceBlock(DataInputStream)
4398    6   1    1 org.apache.hadoop.hdfs.server.datanode.DataXceiver.sendResponse(Socket,short,long)
4399    3   1    1 org.apache.hadoop.hdfs.server.datanode.DataXceiverServer.BlockBalanceThrottler.BlockBalanceThrottler(long)
4400    5   3    1 org.apache.hadoop.hdfs.server.datanode.DataXceiverServer.BlockBalanceThrottler.acquire()
4401    2   1    1 org.apache.hadoop.hdfs.server.datanode.DataXceiverServer.BlockBalanceThrottler.release()
4402    6   1    0 org.apache.hadoop.hdfs.server.datanode.DataXceiverServer.DataXceiverServer(ServerSocket,Configuration,DataNode)
4403   13   5    1 org.apache.hadoop.hdfs.server.datanode.DataXceiverServer.run()
4404   10   4    0 org.apache.hadoop.hdfs.server.datanode.DataXceiverServer.kill()
4405   22  10    1 org.apache.hadoop.hdfs.server.datanode.FSDataset.FSDir.FSDir(File)
4406    3   2    0 org.apache.hadoop.hdfs.server.datanode.FSDataset.FSDir.addBlock(Block,File)
4407   30  19    0 org.apache.hadoop.hdfs.server.datanode.FSDataset.FSDir.addBlock(Block,File,boolean,boolean)
4408   15   6    1 org.apache.hadoop.hdfs.server.datanode.FSDataset.FSDir.getGenerationStampFromFile(File[],File)
4409    9   5    1 org.apache.hadoop.hdfs.server.datanode.FSDataset.FSDir.getBlockInfo(TreeSet)
4410    9   5    0 org.apache.hadoop.hdfs.server.datanode.FSDataset.FSDir.getVolumeMap(HashMap,FSVolume)
4411    5   3    1 org.apache.hadoop.hdfs.server.datanode.FSDataset.FSDir.checkDirTree()
4412    8   4    0 org.apache.hadoop.hdfs.server.datanode.FSDataset.FSDir.clearPath(File)
4413   17  18    0 org.apache.hadoop.hdfs.server.datanode.FSDataset.FSDir.clearPath(File,String[],int)
4414    2   2    0 org.apache.hadoop.hdfs.server.datanode.FSDataset.FSDir.toString()
4415   19   9    0 org.apache.hadoop.hdfs.server.datanode.FSDataset.FSVolume.FSVolume(File,Configuration)
4416    2   1    0 org.apache.hadoop.hdfs.server.datanode.FSDataset.FSVolume.decDfsUsed(long)
4417    2   1    0 org.apache.hadoop.hdfs.server.datanode.FSDataset.FSVolume.getDfsUsed()
4418    4   3    0 org.apache.hadoop.hdfs.server.datanode.FSDataset.FSVolume.getCapacity()
4419    6   3    0 org.apache.hadoop.hdfs.server.datanode.FSDataset.FSVolume.getAvailable()
4420    2   1    0 org.apache.hadoop.hdfs.server.datanode.FSDataset.FSVolume.getMount()
4421    2   1    0 org.apache.hadoop.hdfs.server.datanode.FSDataset.FSVolume.getDir()
4422    3   1    1 org.apache.hadoop.hdfs.server.datanode.FSDataset.FSVolume.createTmpFile(Block)
4423    3   1    1 org.apache.hadoop.hdfs.server.datanode.FSDataset.FSVolume.getTmpFile(Block)
4424    3   1    1 org.apache.hadoop.hdfs.server.datanode.FSDataset.FSVolume.createDetachFile(Block,String)
4425    6   5    0 org.apache.hadoop.hdfs.server.datanode.FSDataset.FSVolume.createTmpFile(Block,File)
4426    5   1    0 org.apache.hadoop.hdfs.server.datanode.FSDataset.FSVolume.addBlock(Block,File)
4427    3   1    0 org.apache.hadoop.hdfs.server.datanode.FSDataset.FSVolume.checkDirs()
4428    2   1    0 org.apache.hadoop.hdfs.server.datanode.FSDataset.FSVolume.getBlockInfo(TreeSet)
4429    2   1    0 org.apache.hadoop.hdfs.server.datanode.FSDataset.FSVolume.getVolumeMap(HashMap)
4430    2   1    0 org.apache.hadoop.hdfs.server.datanode.FSDataset.FSVolume.clearPath(File)
4431    2   1    0 org.apache.hadoop.hdfs.server.datanode.FSDataset.FSVolume.toString()
4432   14  11    1 org.apache.hadoop.hdfs.server.datanode.FSDataset.FSVolume.recoverDetachedBlocks(File,File)
4433    2   1    0 org.apache.hadoop.hdfs.server.datanode.FSDataset.FSVolumeSet.FSVolumeSet(FSVolume[])
4434    9   6    0 org.apache.hadoop.hdfs.server.datanode.FSDataset.FSVolumeSet.getNextVolume(long)
4435    5   2    0 org.apache.hadoop.hdfs.server.datanode.FSDataset.FSVolumeSet.getDfsUsed()
4436    5   2    0 org.apache.hadoop.hdfs.server.datanode.FSDataset.FSVolumeSet.getCapacity()
4437    5   2    0 org.apache.hadoop.hdfs.server.datanode.FSDataset.FSVolumeSet.getRemaining()
4438    3   2    0 org.apache.hadoop.hdfs.server.datanode.FSDataset.FSVolumeSet.getBlockInfo(TreeSet)
4439    3   2    0 org.apache.hadoop.hdfs.server.datanode.FSDataset.FSVolumeSet.getVolumeMap(HashMap)
4440    3   2    0 org.apache.hadoop.hdfs.server.datanode.FSDataset.FSVolumeSet.checkDirs()
4441    7   3    0 org.apache.hadoop.hdfs.server.datanode.FSDataset.FSVolumeSet.toString()
4442    5   2    0 org.apache.hadoop.hdfs.server.datanode.FSDataset.ActiveFile.ActiveFile(File,List)
4443    2   1    0 org.apache.hadoop.hdfs.server.datanode.FSDataset.ActiveFile.toString()
4444    2   1    0 org.apache.hadoop.hdfs.server.datanode.FSDataset.getMetaFileName(String,long)
4445    2   1    0 org.apache.hadoop.hdfs.server.datanode.FSDataset.getMetaFile(File,Block)
4446    2   1    0 org.apache.hadoop.hdfs.server.datanode.FSDataset.getMetaFile(Block)
4447    2   3    0 org.apache.hadoop.hdfs.server.datanode.FSDataset.FilenameFilter$1.accept(File,String)
4448   12   6    1 org.apache.hadoop.hdfs.server.datanode.FSDataset.findMetaFile(File)
4449    6   4    1 org.apache.hadoop.hdfs.server.datanode.FSDataset.parseGenerationStamp(File,File)
4450   13   5    1 org.apache.hadoop.hdfs.server.datanode.FSDataset.findBlockFile(long)
4451    6   3    1 org.apache.hadoop.hdfs.server.datanode.FSDataset.getStoredBlock(long)
4452    2   1    0 org.apache.hadoop.hdfs.server.datanode.FSDataset.metaFileExists(Block)
4453    3   1    0 org.apache.hadoop.hdfs.server.datanode.FSDataset.getMetaDataLength(Block)
4454    3   1    0 org.apache.hadoop.hdfs.server.datanode.FSDataset.getMetaDataInputStream(Block)
4455    9   2    1 org.apache.hadoop.hdfs.server.datanode.FSDataset.FSDataset(DataStorage,Configuration)
4456    2   1    1 org.apache.hadoop.hdfs.server.datanode.FSDataset.getDfsUsed()
4457    2   1    1 org.apache.hadoop.hdfs.server.datanode.FSDataset.getCapacity()
4458    2   1    1 org.apache.hadoop.hdfs.server.datanode.FSDataset.getRemaining()
4459    2   1    1 org.apache.hadoop.hdfs.server.datanode.FSDataset.getLength(Block)
4460    7   4    1 org.apache.hadoop.hdfs.server.datanode.FSDataset.getBlockFile(Block)
4461    2   1    0 org.apache.hadoop.hdfs.server.datanode.FSDataset.getBlockInputStream(Block)
4462    6   2    0 org.apache.hadoop.hdfs.server.datanode.FSDataset.getBlockInputStream(Block,long)
4463   14   5    1 org.apache.hadoop.hdfs.server.datanode.FSDataset.getTmpInputStreams(Block,long,long)
4464    2   1    0 org.apache.hadoop.hdfs.server.datanode.FSDataset.createBlockWriteStreams(File,File)
4465    5   1    1 org.apache.hadoop.hdfs.server.datanode.FSDataset.detachBlock(Block,int)
4466    4   2    0 org.apache.hadoop.hdfs.server.datanode.FSDataset.updateBlockMap(Map,Block,Block)
4467   13   9    1 org.apache.hadoop.hdfs.server.datanode.FSDataset.updateBlock(Block,Block)
4468   30  18    1 org.apache.hadoop.hdfs.server.datanode.FSDataset.tryUpdateBlock(Block,Block)
4469   27   5    0 org.apache.hadoop.hdfs.server.datanode.FSDataset.truncateBlock(File,File,long,long)
4470   59  23    1 org.apache.hadoop.hdfs.server.datanode.FSDataset.writeToBlock(Block,boolean)
4471    3   1    1 org.apache.hadoop.hdfs.server.datanode.FSDataset.getChannelPosition(Block,BlockWriteStreams)
4472   12   3    1 org.apache.hadoop.hdfs.server.datanode.FSDataset.setChannelPosition(Block,BlockWriteStreams,long,long)
4473    6   4    0 org.apache.hadoop.hdfs.server.datanode.FSDataset.createTmpFile(FSVolume,Block)
4474   14   8    1 org.apache.hadoop.hdfs.server.datanode.FSDataset.finalizeBlock(Block)
4475    4   1    1 org.apache.hadoop.hdfs.server.datanode.FSDataset.unfinalizeBlock(Block)
4476    8   2    1 org.apache.hadoop.hdfs.server.datanode.FSDataset.getBlockReport()
4477    2   1    1 org.apache.hadoop.hdfs.server.datanode.FSDataset.isValidBlock(Block)
4478    7   5    1 org.apache.hadoop.hdfs.server.datanode.FSDataset.validateBlockFile(Block)
4479   23  16    1 org.apache.hadoop.hdfs.server.datanode.FSDataset.validateBlockMetadata(Block)
4480   40  13    1 org.apache.hadoop.hdfs.server.datanode.FSDataset.invalidate(Block[])
4481    5   3    1 org.apache.hadoop.hdfs.server.datanode.FSDataset.getFile(Block)
4482    2   1    1 org.apache.hadoop.hdfs.server.datanode.FSDataset.checkDataDir()
4483    2   1    0 org.apache.hadoop.hdfs.server.datanode.FSDataset.toString()
4484   12   3    1 org.apache.hadoop.hdfs.server.datanode.FSDataset.registerMBean(String)
4485    7   5    0 org.apache.hadoop.hdfs.server.datanode.FSDataset.shutdown()
4486    2   1    0 org.apache.hadoop.hdfs.server.datanode.FSDataset.getStorageInfo()
4487    1   1    1 org.apache.hadoop.hdfs.server.datanode.FSDatasetInterface.getMetaDataLength(Block)
4488    3   1    0 org.apache.hadoop.hdfs.server.datanode.FSDatasetInterface.MetaDataInputStream.MetaDataInputStream(InputStream,long)
4489    2   1    0 org.apache.hadoop.hdfs.server.datanode.FSDatasetInterface.MetaDataInputStream.getLength()
4490    1   1    1 org.apache.hadoop.hdfs.server.datanode.FSDatasetInterface.getMetaDataInputStream(Block)
4491    1   1    1 org.apache.hadoop.hdfs.server.datanode.FSDatasetInterface.metaFileExists(Block)
4492    1   1    1 org.apache.hadoop.hdfs.server.datanode.FSDatasetInterface.getLength(Block)
4493    1   1    1 org.apache.hadoop.hdfs.server.datanode.FSDatasetInterface.getStoredBlock(long)
4494    1   1    1 org.apache.hadoop.hdfs.server.datanode.FSDatasetInterface.getBlockInputStream(Block)
4495    1   1    1 org.apache.hadoop.hdfs.server.datanode.FSDatasetInterface.getBlockInputStream(Block,long)
4496    1   1    1 org.apache.hadoop.hdfs.server.datanode.FSDatasetInterface.getTmpInputStreams(Block,long,long)
4497    3   1    0 org.apache.hadoop.hdfs.server.datanode.FSDatasetInterface.BlockWriteStreams.BlockWriteStreams(OutputStream,OutputStream)
4498    3   1    0 org.apache.hadoop.hdfs.server.datanode.FSDatasetInterface.BlockInputStreams.BlockInputStreams(InputStream,InputStream)
4499    3   1    1 org.apache.hadoop.hdfs.server.datanode.FSDatasetInterface.BlockInputStreams.close()
4500    1   1    1 org.apache.hadoop.hdfs.server.datanode.FSDatasetInterface.writeToBlock(Block,boolean)
4501    1   1    1 org.apache.hadoop.hdfs.server.datanode.FSDatasetInterface.updateBlock(Block,Block)
4502    1   1    1 org.apache.hadoop.hdfs.server.datanode.FSDatasetInterface.finalizeBlock(Block)
4503    1   1    1 org.apache.hadoop.hdfs.server.datanode.FSDatasetInterface.unfinalizeBlock(Block)
4504    1   1    1 org.apache.hadoop.hdfs.server.datanode.FSDatasetInterface.getBlockReport()
4505    1   1    1 org.apache.hadoop.hdfs.server.datanode.FSDatasetInterface.isValidBlock(Block)
4506    1   1    1 org.apache.hadoop.hdfs.server.datanode.FSDatasetInterface.invalidate(Block[])
4507    1   1    1 org.apache.hadoop.hdfs.server.datanode.FSDatasetInterface.checkDataDir()
4508    1   1    1 org.apache.hadoop.hdfs.server.datanode.FSDatasetInterface.toString()
4509    1   1    1 org.apache.hadoop.hdfs.server.datanode.FSDatasetInterface.shutdown()
4510    1   1    1 org.apache.hadoop.hdfs.server.datanode.FSDatasetInterface.getChannelPosition(Block,BlockWriteStreams)
4511    1   1    1 org.apache.hadoop.hdfs.server.datanode.FSDatasetInterface.setChannelPosition(Block,BlockWriteStreams,long,long)
4512    1   1    1 org.apache.hadoop.hdfs.server.datanode.FSDatasetInterface.validateBlockMetadata(Block)
4513    8   1    0 org.apache.hadoop.hdfs.server.datanode.metrics.DataNodeMetrics.DataNodeMetrics(Configuration,String)
4514    3   2    0 org.apache.hadoop.hdfs.server.datanode.metrics.DataNodeMetrics.shutdown()
4515   23   1    1 org.apache.hadoop.hdfs.server.datanode.metrics.DataNodeMetrics.doUpdates(MetricsContext)
4516    9   1    0 org.apache.hadoop.hdfs.server.datanode.metrics.DataNodeMetrics.resetAllMinMax()
4517    8   2    1 org.apache.hadoop.hdfs.server.datanode.metrics.DataNodeStatistics.DataNodeStatistics(DataNodeMetrics,String)
4518    3   2    1 org.apache.hadoop.hdfs.server.datanode.metrics.DataNodeStatistics.shutdown()
4519    2   1    1 org.apache.hadoop.hdfs.server.datanode.metrics.DataNodeStatistics.resetAllMinMax()
4520    2   1    1 org.apache.hadoop.hdfs.server.datanode.metrics.DataNodeStatistics.getBlocksRead()
4521    2   1    1 org.apache.hadoop.hdfs.server.datanode.metrics.DataNodeStatistics.getBlocksRemoved()
4522    2   1    1 org.apache.hadoop.hdfs.server.datanode.metrics.DataNodeStatistics.getBlocksReplicated()
4523    2   1    1 org.apache.hadoop.hdfs.server.datanode.metrics.DataNodeStatistics.getBlocksWritten()
4524    2   1    1 org.apache.hadoop.hdfs.server.datanode.metrics.DataNodeStatistics.getBytesRead()
4525    2   1    1 org.apache.hadoop.hdfs.server.datanode.metrics.DataNodeStatistics.getBytesWritten()
4526    2   1    1 org.apache.hadoop.hdfs.server.datanode.metrics.DataNodeStatistics.getBlockVerificationFailures()
4527    2   1    1 org.apache.hadoop.hdfs.server.datanode.metrics.DataNodeStatistics.getBlocksVerified()
4528    2   1    1 org.apache.hadoop.hdfs.server.datanode.metrics.DataNodeStatistics.getReadsFromLocalClient()
4529    2   1    1 org.apache.hadoop.hdfs.server.datanode.metrics.DataNodeStatistics.getReadsFromRemoteClient()
4530    2   1    1 org.apache.hadoop.hdfs.server.datanode.metrics.DataNodeStatistics.getWritesFromLocalClient()
4531    2   1    1 org.apache.hadoop.hdfs.server.datanode.metrics.DataNodeStatistics.getWritesFromRemoteClient()
4532    2   1    1 org.apache.hadoop.hdfs.server.datanode.metrics.DataNodeStatistics.getReadBlockOpAverageTime()
4533    2   1    1 org.apache.hadoop.hdfs.server.datanode.metrics.DataNodeStatistics.getReadBlockOpMaxTime()
4534    2   1    1 org.apache.hadoop.hdfs.server.datanode.metrics.DataNodeStatistics.getReadBlockOpMinTime()
4535    2   1    1 org.apache.hadoop.hdfs.server.datanode.metrics.DataNodeStatistics.getReadBlockOpNum()
4536    2   1    1 org.apache.hadoop.hdfs.server.datanode.metrics.DataNodeStatistics.getReadMetadataOpAverageTime()
4537    2   1    1 org.apache.hadoop.hdfs.server.datanode.metrics.DataNodeStatistics.getReadMetadataOpMaxTime()
4538    2   1    1 org.apache.hadoop.hdfs.server.datanode.metrics.DataNodeStatistics.getReadMetadataOpMinTime()
4539    2   1    1 org.apache.hadoop.hdfs.server.datanode.metrics.DataNodeStatistics.getReadMetadataOpNum()
4540    2   1    1 org.apache.hadoop.hdfs.server.datanode.metrics.DataNodeStatistics.getBlockChecksumOpAverageTime()
4541    2   1    1 org.apache.hadoop.hdfs.server.datanode.metrics.DataNodeStatistics.getBlockChecksumOpMaxTime()
4542    2   1    1 org.apache.hadoop.hdfs.server.datanode.metrics.DataNodeStatistics.getBlockChecksumOpMinTime()
4543    2   1    1 org.apache.hadoop.hdfs.server.datanode.metrics.DataNodeStatistics.getBlockChecksumOpNum()
4544    2   1    1 org.apache.hadoop.hdfs.server.datanode.metrics.DataNodeStatistics.getReplaceBlockOpAverageTime()
4545    2   1    1 org.apache.hadoop.hdfs.server.datanode.metrics.DataNodeStatistics.getReplaceBlockOpMaxTime()
4546    2   1    1 org.apache.hadoop.hdfs.server.datanode.metrics.DataNodeStatistics.getReplaceBlockOpMinTime()
4547    2   1    1 org.apache.hadoop.hdfs.server.datanode.metrics.DataNodeStatistics.getReplaceBlockOpNum()
4548    2   1    1 org.apache.hadoop.hdfs.server.datanode.metrics.DataNodeStatistics.getWriteBlockOpAverageTime()
4549    2   1    1 org.apache.hadoop.hdfs.server.datanode.metrics.DataNodeStatistics.getWriteBlockOpMaxTime()
4550    2   1    1 org.apache.hadoop.hdfs.server.datanode.metrics.DataNodeStatistics.getWriteBlockOpMinTime()
4551    2   1    1 org.apache.hadoop.hdfs.server.datanode.metrics.DataNodeStatistics.getWriteBlockOpNum()
4552    2   1    1 org.apache.hadoop.hdfs.server.datanode.metrics.DataNodeStatistics.getCopyBlockOpAverageTime()
4553    2   1    1 org.apache.hadoop.hdfs.server.datanode.metrics.DataNodeStatistics.getCopyBlockOpMaxTime()
4554    2   1    1 org.apache.hadoop.hdfs.server.datanode.metrics.DataNodeStatistics.getCopyBlockOpMinTime()
4555    2   1    1 org.apache.hadoop.hdfs.server.datanode.metrics.DataNodeStatistics.getCopyBlockOpNum()
4556    2   1    1 org.apache.hadoop.hdfs.server.datanode.metrics.DataNodeStatistics.getBlockReportsAverageTime()
4557    2   1    1 org.apache.hadoop.hdfs.server.datanode.metrics.DataNodeStatistics.getBlockReportsMaxTime()
4558    2   1    1 org.apache.hadoop.hdfs.server.datanode.metrics.DataNodeStatistics.getBlockReportsMinTime()
4559    2   1    1 org.apache.hadoop.hdfs.server.datanode.metrics.DataNodeStatistics.getBlockReportsNum()
4560    2   1    1 org.apache.hadoop.hdfs.server.datanode.metrics.DataNodeStatistics.getHeartbeatsAverageTime()
4561    2   1    1 org.apache.hadoop.hdfs.server.datanode.metrics.DataNodeStatistics.getHeartbeatsMaxTime()
4562    2   1    1 org.apache.hadoop.hdfs.server.datanode.metrics.DataNodeStatistics.getHeartbeatsMinTime()
4563    2   1    1 org.apache.hadoop.hdfs.server.datanode.metrics.DataNodeStatistics.getHeartbeatsNum()
4564    3   1    0 org.apache.hadoop.hdfs.server.datanode.UpgradeManagerDatanode.UpgradeManagerDatanode(DataNode)
4565    2   1    0 org.apache.hadoop.hdfs.server.datanode.UpgradeManagerDatanode.getType()
4566    7   3    0 org.apache.hadoop.hdfs.server.datanode.UpgradeManagerDatanode.initializeUpgrade(NamespaceInfo)
4567   27   9    1 org.apache.hadoop.hdfs.server.datanode.UpgradeManagerDatanode.startUpgrade()
4568    6   4    0 org.apache.hadoop.hdfs.server.datanode.UpgradeManagerDatanode.processUpgradeCommand(UpgradeCommand)
4569    8   1    0 org.apache.hadoop.hdfs.server.datanode.UpgradeManagerDatanode.completeUpgrade()
4570    3   2    0 org.apache.hadoop.hdfs.server.datanode.UpgradeManagerDatanode.shutdownUpgrade()
4571    2   1    0 org.apache.hadoop.hdfs.server.datanode.UpgradeObjectDatanode.getType()
4572    2   1    0 org.apache.hadoop.hdfs.server.datanode.UpgradeObjectDatanode.getDatanode()
4573    2   1    0 org.apache.hadoop.hdfs.server.datanode.UpgradeObjectDatanode.setDatanode(DataNode)
4574    1   1    1 org.apache.hadoop.hdfs.server.datanode.UpgradeObjectDatanode.doUpgrade()
4575   10   5    1 org.apache.hadoop.hdfs.server.datanode.UpgradeObjectDatanode.preUpgradeAction(NamespaceInfo)
4576   12   5    0 org.apache.hadoop.hdfs.server.datanode.UpgradeObjectDatanode.run()
4577    2   1    1 org.apache.hadoop.hdfs.server.datanode.UpgradeObjectDatanode.completeUpgrade()
4578    2   1    1 org.apache.hadoop.hdfs.server.namenode.INode.DirCounts.getNsCount()
4579    2   1    1 org.apache.hadoop.hdfs.server.namenode.INode.DirCounts.getDsCount()
4580    4   1    0 org.apache.hadoop.hdfs.server.namenode.INode.PermissionStatusFormat.PermissionStatusFormat(int,int)
4581    2   1    0 org.apache.hadoop.hdfs.server.namenode.INode.PermissionStatusFormat.retrieve(long)
4582    2   1    0 org.apache.hadoop.hdfs.server.namenode.INode.PermissionStatusFormat.combine(long,long)
4583    5   1    0 org.apache.hadoop.hdfs.server.namenode.INode.INode()
4584    6   1    0 org.apache.hadoop.hdfs.server.namenode.INode.INode(PermissionStatus,long,long)
4585    3   1    0 org.apache.hadoop.hdfs.server.namenode.INode.INode(String,PermissionStatus)
4586    6   1    1 org.apache.hadoop.hdfs.server.namenode.INode.INode(INode)
4587    2   1    1 org.apache.hadoop.hdfs.server.namenode.INode.isRoot()
4588    4   1    1 org.apache.hadoop.hdfs.server.namenode.INode.setPermissionStatus(PermissionStatus)
4589    2   1    1 org.apache.hadoop.hdfs.server.namenode.INode.getPermissionStatus()
4590    2   1    0 org.apache.hadoop.hdfs.server.namenode.INode.updatePermissionStatus(PermissionStatusFormat,long)
4591    3   1    1 org.apache.hadoop.hdfs.server.namenode.INode.getUserName()
4592    3   1    1 org.apache.hadoop.hdfs.server.namenode.INode.setUser(String)
4593    3   1    1 org.apache.hadoop.hdfs.server.namenode.INode.getGroupName()
4594    3   1    1 org.apache.hadoop.hdfs.server.namenode.INode.setGroup(String)
4595    2   1    1 org.apache.hadoop.hdfs.server.namenode.INode.getFsPermission()
4596    2   1    0 org.apache.hadoop.hdfs.server.namenode.INode.getFsPermissionShort()
4597    2   1    1 org.apache.hadoop.hdfs.server.namenode.INode.setPermission(FsPermission)
4598    1   1    1 org.apache.hadoop.hdfs.server.namenode.INode.isDirectory()
4599    1   1    1 org.apache.hadoop.hdfs.server.namenode.INode.collectSubtreeBlocksAndClear(List)
4600    3   1    1 org.apache.hadoop.hdfs.server.namenode.INode.computeContentSummary()
4601    1   1    1 org.apache.hadoop.hdfs.server.namenode.INode.computeContentSummary(long[])
4602    2   1    1 org.apache.hadoop.hdfs.server.namenode.INode.getNsQuota()
4603    2   1    0 org.apache.hadoop.hdfs.server.namenode.INode.getDsQuota()
4604    2   2    0 org.apache.hadoop.hdfs.server.namenode.INode.isQuotaSet()
4605    1   1    1 org.apache.hadoop.hdfs.server.namenode.INode.spaceConsumedInTree(DirCounts)
4606    2   1    1 org.apache.hadoop.hdfs.server.namenode.INode.getLocalName()
4607    2   1    0 org.apache.hadoop.hdfs.server.namenode.INode.getLocalNameBytes()
4608    2   1    1 org.apache.hadoop.hdfs.server.namenode.INode.setLocalName(String)
4609    2   1    1 org.apache.hadoop.hdfs.server.namenode.INode.setLocalName(byte[])
4610    2   1    1 org.apache.hadoop.hdfs.server.namenode.INode.toString()
4611    2   1    1 org.apache.hadoop.hdfs.server.namenode.INode.getParent()
4612    2   1    1 org.apache.hadoop.hdfs.server.namenode.INode.getModificationTime()
4613    4   2    1 org.apache.hadoop.hdfs.server.namenode.INode.setModificationTime(long)
4614    3   1    1 org.apache.hadoop.hdfs.server.namenode.INode.setModificationTimeForce(long)
4615    2   1    1 org.apache.hadoop.hdfs.server.namenode.INode.getAccessTime()
4616    2   1    1 org.apache.hadoop.hdfs.server.namenode.INode.setAccessTime(long)
4617    2   1    1 org.apache.hadoop.hdfs.server.namenode.INode.isUnderConstruction()
4618    2   1    1 org.apache.hadoop.hdfs.server.namenode.INode.getPathComponents(String)
4619    7   4    1 org.apache.hadoop.hdfs.server.namenode.INode.getPathComponents(String[])
4620    4   4    1 org.apache.hadoop.hdfs.server.namenode.INode.getPathNames(String)
4621    7   3    0 org.apache.hadoop.hdfs.server.namenode.INode.removeNode()
4622    2   1    0 org.apache.hadoop.hdfs.server.namenode.INode.compareTo(byte[])
4623    4   3    0 org.apache.hadoop.hdfs.server.namenode.INode.equals(Object)
4624    2   1    0 org.apache.hadoop.hdfs.server.namenode.INode.hashCode()
4625   13   8    1 org.apache.hadoop.hdfs.server.namenode.INode.compareBytes(byte[],byte[])
4626    5   3    1 org.apache.hadoop.hdfs.server.namenode.INode.bytes2String(byte[])
4627    5   3    1 org.apache.hadoop.hdfs.server.namenode.INode.string2Bytes(String)
4628    2   1    0 org.apache.hadoop.hdfs.server.namenode.INode.createLocatedBlocks(List)
4629    4   1    0 org.apache.hadoop.hdfs.server.namenode.BlocksMap.BlockInfo.BlockInfo(Block,int)
4630    2   1    0 org.apache.hadoop.hdfs.server.namenode.BlocksMap.BlockInfo.getINode()
4631    6   3    0 org.apache.hadoop.hdfs.server.namenode.BlocksMap.BlockInfo.getDatanode(int)
4632    6   3    0 org.apache.hadoop.hdfs.server.namenode.BlocksMap.BlockInfo.getPrevious(int)
4633    6   3    0 org.apache.hadoop.hdfs.server.namenode.BlocksMap.BlockInfo.getNext(int)
4634    4   2    0 org.apache.hadoop.hdfs.server.namenode.BlocksMap.BlockInfo.setDatanode(int,DatanodeDescriptor)
4635    4   2    0 org.apache.hadoop.hdfs.server.namenode.BlocksMap.BlockInfo.setPrevious(int,BlockInfo)
4636    4   2    0 org.apache.hadoop.hdfs.server.namenode.BlocksMap.BlockInfo.setNext(int,BlockInfo)
4637    4   1    0 org.apache.hadoop.hdfs.server.namenode.BlocksMap.BlockInfo.getCapacity()
4638   10   4    1 org.apache.hadoop.hdfs.server.namenode.BlocksMap.BlockInfo.ensureCapacity(int)
4639    7   4    1 org.apache.hadoop.hdfs.server.namenode.BlocksMap.BlockInfo.numNodes()
4640    8   3    1 org.apache.hadoop.hdfs.server.namenode.BlocksMap.BlockInfo.addNode(DatanodeDescriptor)
4641   13   4    1 org.apache.hadoop.hdfs.server.namenode.BlocksMap.BlockInfo.removeNode(DatanodeDescriptor)
4642    9   5    1 org.apache.hadoop.hdfs.server.namenode.BlocksMap.BlockInfo.findDatanode(DatanodeDescriptor)
4643    9   3    1 org.apache.hadoop.hdfs.server.namenode.BlocksMap.BlockInfo.listInsert(BlockInfo,DatanodeDescriptor)
4644   17   8    1 org.apache.hadoop.hdfs.server.namenode.BlocksMap.BlockInfo.listRemove(BlockInfo,DatanodeDescriptor)
4645    5   2    0 org.apache.hadoop.hdfs.server.namenode.BlocksMap.BlockInfo.listCount(DatanodeDescriptor)
4646   14   5    0 org.apache.hadoop.hdfs.server.namenode.BlocksMap.BlockInfo.listIsConsistent(DatanodeDescriptor)
4647    2   1    0 org.apache.hadoop.hdfs.server.namenode.BlocksMap.NodeIterator.NodeIterator(BlockInfo)
4648    2   3    0 org.apache.hadoop.hdfs.server.namenode.BlocksMap.NodeIterator.hasNext()
4649    2   1    0 org.apache.hadoop.hdfs.server.namenode.BlocksMap.NodeIterator.next()
4650    2   2    0 org.apache.hadoop.hdfs.server.namenode.BlocksMap.NodeIterator.remove()
4651    6   2    1 org.apache.hadoop.hdfs.server.namenode.BlocksMap.checkBlockInfo(Block,int)
4652    3   2    0 org.apache.hadoop.hdfs.server.namenode.BlocksMap.getINode(Block)
4653    4   1    1 org.apache.hadoop.hdfs.server.namenode.BlocksMap.addINode(Block,INodeFile)
4654    6   3    1 org.apache.hadoop.hdfs.server.namenode.BlocksMap.removeINode(Block)
4655    8   4    1 org.apache.hadoop.hdfs.server.namenode.BlocksMap.removeBlock(BlockInfo)
4656    2   1    1 org.apache.hadoop.hdfs.server.namenode.BlocksMap.getStoredBlock(Block)
4657    2   1    1 org.apache.hadoop.hdfs.server.namenode.BlocksMap.nodeIterator(Block)
4658    3   2    1 org.apache.hadoop.hdfs.server.namenode.BlocksMap.numNodes(Block)
4659    3   1    1 org.apache.hadoop.hdfs.server.namenode.BlocksMap.addNode(Block,DatanodeDescriptor,int)
4660    8   5    1 org.apache.hadoop.hdfs.server.namenode.BlocksMap.removeNode(Block,DatanodeDescriptor)
4661    2   1    0 org.apache.hadoop.hdfs.server.namenode.BlocksMap.size()
4662    2   1    0 org.apache.hadoop.hdfs.server.namenode.BlocksMap.getBlocks()
4663    2   1    1 org.apache.hadoop.hdfs.server.namenode.BlocksMap.contains(Block)
4664    7   5    1 org.apache.hadoop.hdfs.server.namenode.BlocksMap.contains(Block,DatanodeDescriptor)
4665    1   1    0 org.apache.hadoop.hdfs.server.namenode.CheckpointSignature.CheckpointSignature()
4666    4   1    0 org.apache.hadoop.hdfs.server.namenode.CheckpointSignature.CheckpointSignature(FSImage)
4667    8   1    0 org.apache.hadoop.hdfs.server.namenode.CheckpointSignature.CheckpointSignature(String)
4668    2   1    0 org.apache.hadoop.hdfs.server.namenode.CheckpointSignature.toString()
4669    3   5    0 org.apache.hadoop.hdfs.server.namenode.CheckpointSignature.validateStorageInfo(StorageInfo)
4670    2  11    0 org.apache.hadoop.hdfs.server.namenode.CheckpointSignature.compareTo(CheckpointSignature)
4671    4   3    0 org.apache.hadoop.hdfs.server.namenode.CheckpointSignature.equals(Object)
4672    2   1    0 org.apache.hadoop.hdfs.server.namenode.CheckpointSignature.hashCode()
4673    6   1    0 org.apache.hadoop.hdfs.server.namenode.CheckpointSignature.write(DataOutput)
4674    6   1    0 org.apache.hadoop.hdfs.server.namenode.CheckpointSignature.readFields(DataInput)
4675   12   4    1 org.apache.hadoop.hdfs.server.namenode.CorruptReplicasMap.addToCorruptReplicasMap(Block,DatanodeDescriptor)
4676    5   3    1 org.apache.hadoop.hdfs.server.namenode.CorruptReplicasMap.removeFromCorruptReplicasMap(Block)
4677    9   6    1 org.apache.hadoop.hdfs.server.namenode.CorruptReplicasMap.removeFromCorruptReplicasMap(Block,DatanodeDescriptor)
4678    2   1    1 org.apache.hadoop.hdfs.server.namenode.CorruptReplicasMap.getNodes(Block)
4679    3   2    1 org.apache.hadoop.hdfs.server.namenode.CorruptReplicasMap.isReplicaCorrupt(Block,DatanodeDescriptor)
4680    3   2    0 org.apache.hadoop.hdfs.server.namenode.CorruptReplicasMap.numCorruptReplicas(Block)
4681    3   1    0 org.apache.hadoop.hdfs.server.namenode.DatanodeDescriptor.BlockTargetPair.BlockTargetPair(Block,DatanodeDescriptor[])
4682    2   1    1 org.apache.hadoop.hdfs.server.namenode.DatanodeDescriptor.BlockQueue.size()
4683    2   1    1 org.apache.hadoop.hdfs.server.namenode.DatanodeDescriptor.BlockQueue.offer(Block,DatanodeDescriptor[])
4684   10   7    1 org.apache.hadoop.hdfs.server.namenode.DatanodeDescriptor.BlockQueue.poll(int)
4685    1   1    1 org.apache.hadoop.hdfs.server.namenode.DatanodeDescriptor.DatanodeDescriptor()
4686    2   1    1 org.apache.hadoop.hdfs.server.namenode.DatanodeDescriptor.DatanodeDescriptor(DatanodeID)
4687    2   1    1 org.apache.hadoop.hdfs.server.namenode.DatanodeDescriptor.DatanodeDescriptor(DatanodeID,String)
4688    2   1    1 org.apache.hadoop.hdfs.server.namenode.DatanodeDescriptor.DatanodeDescriptor(DatanodeID,String,String)
4689    3   1    1 org.apache.hadoop.hdfs.server.namenode.DatanodeDescriptor.DatanodeDescriptor(DatanodeID,long,long,long,int)
4690    3   1    1 org.apache.hadoop.hdfs.server.namenode.DatanodeDescriptor.DatanodeDescriptor(DatanodeID,String,String,long,long,long,int)
4691    5   3    1 org.apache.hadoop.hdfs.server.namenode.DatanodeDescriptor.addBlock(BlockInfo)
4692    3   1    1 org.apache.hadoop.hdfs.server.namenode.DatanodeDescriptor.removeBlock(BlockInfo)
4693    3   1    1 org.apache.hadoop.hdfs.server.namenode.DatanodeDescriptor.moveBlockToHead(BlockInfo)
4694    7   1    0 org.apache.hadoop.hdfs.server.namenode.DatanodeDescriptor.resetBlocks()
4695    2   2    0 org.apache.hadoop.hdfs.server.namenode.DatanodeDescriptor.numBlocks()
4696    7   1    1 org.apache.hadoop.hdfs.server.namenode.DatanodeDescriptor.updateHeartbeat(long,long,long,int)
4697    3   1    0 org.apache.hadoop.hdfs.server.namenode.DatanodeDescriptor.BlockIterator.BlockIterator(BlockInfo,DatanodeDescriptor)
4698    2   1    0 org.apache.hadoop.hdfs.server.namenode.DatanodeDescriptor.BlockIterator.hasNext()
4699    4   1    0 org.apache.hadoop.hdfs.server.namenode.DatanodeDescriptor.BlockIterator.next()
4700    2   2    0 org.apache.hadoop.hdfs.server.namenode.DatanodeDescriptor.BlockIterator.remove()
4701    2   1    0 org.apache.hadoop.hdfs.server.namenode.DatanodeDescriptor.getBlockIterator()
4702    3   3    1 org.apache.hadoop.hdfs.server.namenode.DatanodeDescriptor.addBlockToBeReplicated(Block,DatanodeDescriptor[])
4703    3   3    1 org.apache.hadoop.hdfs.server.namenode.DatanodeDescriptor.addBlockToBeRecovered(Block,DatanodeDescriptor[])
4704    5   3    1 org.apache.hadoop.hdfs.server.namenode.DatanodeDescriptor.addBlocksToBeInvalidated(List)
4705    2   1    1 org.apache.hadoop.hdfs.server.namenode.DatanodeDescriptor.getNumberOfBlocksToBeReplicated()
4706    3   1    1 org.apache.hadoop.hdfs.server.namenode.DatanodeDescriptor.getNumberOfBlocksToBeInvalidated()
4707    3   2    0 org.apache.hadoop.hdfs.server.namenode.DatanodeDescriptor.getReplicationCommand(int)
4708    3   2    0 org.apache.hadoop.hdfs.server.namenode.DatanodeDescriptor.getLeaseRecoveryCommand(int)
4709    3   2    1 org.apache.hadoop.hdfs.server.namenode.DatanodeDescriptor.getInvalidateBlocks(int)
4710   19   8    0 org.apache.hadoop.hdfs.server.namenode.DatanodeDescriptor.getBlockArray(Collection,int)
4711   24   7    0 org.apache.hadoop.hdfs.server.namenode.DatanodeDescriptor.reportDiff(BlocksMap,BlockListAsLongs,Collection,Collection,Collection)
4712   12   1    1 org.apache.hadoop.hdfs.server.namenode.DatanodeDescriptor.readFieldsFromFSEditLog(DataInput)
4713    2   1    1 org.apache.hadoop.hdfs.server.namenode.DatanodeDescriptor.getBlocksScheduled()
4714    2   1    1 org.apache.hadoop.hdfs.server.namenode.DatanodeDescriptor.incBlocksScheduled()
4715    6   3    1 org.apache.hadoop.hdfs.server.namenode.DatanodeDescriptor.decBlocksScheduled()
4716    5   2    1 org.apache.hadoop.hdfs.server.namenode.DatanodeDescriptor.rollBlocksScheduled(long)
4717    6   3    1 org.apache.hadoop.hdfs.server.namenode.DfsServlet.getUGI(HttpServletRequest)
4718    6   1    1 org.apache.hadoop.hdfs.server.namenode.DfsServlet.createNameNodeProxy(UnixUserGroupInformation)
4719    6   3    1 org.apache.hadoop.hdfs.server.namenode.DfsServlet.createRedirectUri(String,UserGroupInformation,DatanodeID,HttpServletRequest)
4720    5   4    1 org.apache.hadoop.hdfs.server.namenode.DfsServlet.getFilename(HttpServletRequest,HttpServletResponse)
4721    1   1    1 org.apache.hadoop.hdfs.server.namenode.EditLogInputStream.getName()
4722    1   1    1 org.apache.hadoop.hdfs.server.namenode.EditLogInputStream.available()
4723    1   1    1 org.apache.hadoop.hdfs.server.namenode.EditLogInputStream.read()
4724    1   1    1 org.apache.hadoop.hdfs.server.namenode.EditLogInputStream.read(byte[],int,int)
4725    1   1    1 org.apache.hadoop.hdfs.server.namenode.EditLogInputStream.close()
4726    1   1    1 org.apache.hadoop.hdfs.server.namenode.EditLogInputStream.length()
4727    2   1    0 org.apache.hadoop.hdfs.server.namenode.EditLogOutputStream.EditLogOutputStream()
4728    1   1    1 org.apache.hadoop.hdfs.server.namenode.EditLogOutputStream.getName()
4729    1   1    1 org.apache.hadoop.hdfs.server.namenode.EditLogOutputStream.write(int)
4730    1   1    1 org.apache.hadoop.hdfs.server.namenode.EditLogOutputStream.write(byte,Writable)
4731    1   1    1 org.apache.hadoop.hdfs.server.namenode.EditLogOutputStream.create()
4732    1   1    1 org.apache.hadoop.hdfs.server.namenode.EditLogOutputStream.close()
4733    1   1    1 org.apache.hadoop.hdfs.server.namenode.EditLogOutputStream.setReadyToFlush()
4734    1   1    1 org.apache.hadoop.hdfs.server.namenode.EditLogOutputStream.flushAndSync()
4735    6   1    1 org.apache.hadoop.hdfs.server.namenode.EditLogOutputStream.flush()
4736    1   1    1 org.apache.hadoop.hdfs.server.namenode.EditLogOutputStream.length()
4737    1   1    1 org.apache.hadoop.hdfs.server.namenode.EditLogOutputStream.lastModified()
4738    2   1    1 org.apache.hadoop.hdfs.server.namenode.EditLogOutputStream.getTotalSyncTime()
4739    2   1    1 org.apache.hadoop.hdfs.server.namenode.EditLogOutputStream.getNumSync()
4740   11   4    1 org.apache.hadoop.hdfs.server.namenode.FileChecksumServlets.RedirectServlet.doGet(HttpServletRequest,HttpServletResponse)
4741   16   2    1 org.apache.hadoop.hdfs.server.namenode.FileChecksumServlets.GetServlet.doGet(HttpServletRequest,HttpServletResponse)
4742    8   3    0 org.apache.hadoop.hdfs.server.namenode.FileDataServlet.createUri(FileStatus,UnixUserGroupInformation,ClientProtocol,String)
4743    5   4    1 org.apache.hadoop.hdfs.server.namenode.FileDataServlet.pickSrcDatanode(FileStatus,ClientProtocol)
4744   16   7    1 org.apache.hadoop.hdfs.server.namenode.FileDataServlet.doGet(HttpServletRequest,HttpServletResponse)
4745   13   3    0 org.apache.hadoop.hdfs.server.namenode.FsckServlet.doGet(HttpServletRequest,HttpServletResponse)
4746    3   1    1 org.apache.hadoop.hdfs.server.namenode.FSDirectory.FSDirectory(FSNamesystem,Configuration)
4747    5   1    0 org.apache.hadoop.hdfs.server.namenode.FSDirectory.FSDirectory(FSImage,FSNamesystem,Configuration)
4748    4   1    0 org.apache.hadoop.hdfs.server.namenode.FSDirectory.initialize(Configuration)
4749   18   6    0 org.apache.hadoop.hdfs.server.namenode.FSDirectory.loadFSImage(Collection,Collection,StartupOption)
4750    3   1    0 org.apache.hadoop.hdfs.server.namenode.FSDirectory.incrDeletedFileCount(int)
4751    2   1    1 org.apache.hadoop.hdfs.server.namenode.FSDirectory.close()
4752    6   4    1 org.apache.hadoop.hdfs.server.namenode.FSDirectory.waitForReady()
4753   14   5    1 org.apache.hadoop.hdfs.server.namenode.FSDirectory.addFile(String,PermissionStatus,short,long,String,String,DatanodeDescriptor,long)
4754   18   7    1 org.apache.hadoop.hdfs.server.namenode.FSDirectory.unprotectedAddFile(String,PermissionStatus,Block[],short,long,long,long)
4755   22  10    0 org.apache.hadoop.hdfs.server.namenode.FSDirectory.addToParent(String,INodeDirectory,PermissionStatus,Block[],short,long,long,long,long,long)
4756   10   1    1 org.apache.hadoop.hdfs.server.namenode.FSDirectory.addBlock(String,INode[],Block)
4757    5   1    1 org.apache.hadoop.hdfs.server.namenode.FSDirectory.persistBlocks(String,INodeFileUnderConstruction)
4758    6   2    1 org.apache.hadoop.hdfs.server.namenode.FSDirectory.closeFile(String,INodeFile)
4759    9   1    1 org.apache.hadoop.hdfs.server.namenode.FSDirectory.removeBlock(String,INodeFileUnderConstruction,Block)
4760    9   4    1 org.apache.hadoop.hdfs.server.namenode.FSDirectory.renameTo(String,String)
4761   49  18    1 org.apache.hadoop.hdfs.server.namenode.FSDirectory.unprotectedRenameTo(String,String,long)
4762    6   2    1 org.apache.hadoop.hdfs.server.namenode.FSDirectory.setReplication(String,short,int[])
4763   19   6    0 org.apache.hadoop.hdfs.server.namenode.FSDirectory.unprotectedSetReplication(String,short,int[])
4764    8   5    1 org.apache.hadoop.hdfs.server.namenode.FSDirectory.getPreferredBlockSize(String)
4765    7   4    0 org.apache.hadoop.hdfs.server.namenode.FSDirectory.exists(String)
4766    3   1    0 org.apache.hadoop.hdfs.server.namenode.FSDirectory.setPermission(String,FsPermission)
4767    6   3    0 org.apache.hadoop.hdfs.server.namenode.FSDirectory.unprotectedSetPermission(String,FsPermission)
4768    3   1    0 org.apache.hadoop.hdfs.server.namenode.FSDirectory.setOwner(String,String,String)
4769    9   5    0 org.apache.hadoop.hdfs.server.namenode.FSDirectory.unprotectedSetOwner(String,String,String)
4770    9   3    1 org.apache.hadoop.hdfs.server.namenode.FSDirectory.delete(String)
4771   10   4    1 org.apache.hadoop.hdfs.server.namenode.FSDirectory.isDirEmpty(String)
4772   25   8    1 org.apache.hadoop.hdfs.server.namenode.FSDirectory.unprotectedDelete(String,long)
4773    2   1    1 org.apache.hadoop.hdfs.server.namenode.FSDirectory.replaceNode(String,INodeFile,INodeFile)
4774   18   8    1 org.apache.hadoop.hdfs.server.namenode.FSDirectory.replaceNode(String,INodeFile,INodeFile,boolean)
4775   17   7    1 org.apache.hadoop.hdfs.server.namenode.FSDirectory.getListing(String)
4776    8   3    1 org.apache.hadoop.hdfs.server.namenode.FSDirectory.getFileInfo(String)
4777    9   5    1 org.apache.hadoop.hdfs.server.namenode.FSDirectory.getFileBlocks(String)
4778    6   4    1 org.apache.hadoop.hdfs.server.namenode.FSDirectory.getFileINode(String)
4779    3   1    1 org.apache.hadoop.hdfs.server.namenode.FSDirectory.getExistingPathINodes(String)
4780    7   5    1 org.apache.hadoop.hdfs.server.namenode.FSDirectory.isValidToCreate(String)
4781    4   2    1 org.apache.hadoop.hdfs.server.namenode.FSDirectory.isDir(String)
4782    7   3    1 org.apache.hadoop.hdfs.server.namenode.FSDirectory.updateSpaceConsumed(String,long,long)
4783   18  11    1 org.apache.hadoop.hdfs.server.namenode.FSDirectory.updateCount(INode[],int,long,long)
4784    5   2    1 org.apache.hadoop.hdfs.server.namenode.FSDirectory.getFullPathName(INode[],int)
4785   24  10    1 org.apache.hadoop.hdfs.server.namenode.FSDirectory.mkdirs(String,PermissionStatus,boolean,long)
4786    7   1    1 org.apache.hadoop.hdfs.server.namenode.FSDirectory.unprotectedMkdir(String,PermissionStatus,long)
4787    2   1    1 org.apache.hadoop.hdfs.server.namenode.FSDirectory.unprotectedMkdir(INode[],int,byte[],PermissionStatus,boolean,long)
4788    7   1    1 org.apache.hadoop.hdfs.server.namenode.FSDirectory.addNode(String,T,long,boolean)
4789    2   1    1 org.apache.hadoop.hdfs.server.namenode.FSDirectory.addChild(INode[],int,T,boolean)
4790   10   3    1 org.apache.hadoop.hdfs.server.namenode.FSDirectory.addChild(INode[],int,T,long,boolean)
4791    7   2    1 org.apache.hadoop.hdfs.server.namenode.FSDirectory.removeChild(INode[],int)
4792    4   3    1 org.apache.hadoop.hdfs.server.namenode.FSDirectory.normalizePath(String)
4793    8   3    0 org.apache.hadoop.hdfs.server.namenode.FSDirectory.getContentSummary(String)
4794    2   1    1 org.apache.hadoop.hdfs.server.namenode.FSDirectory.updateCountForINodeWithQuota()
4795   23   9    1 org.apache.hadoop.hdfs.server.namenode.FSDirectory.updateCountForINodeWithQuota(INodeDirectory,INode.DirCounts,ArrayList)
4796   27  17    1 org.apache.hadoop.hdfs.server.namenode.FSDirectory.unprotectedSetQuota(String,long,long)
4797    5   2    1 org.apache.hadoop.hdfs.server.namenode.FSDirectory.setQuota(String,long,long)
4798    3   1    0 org.apache.hadoop.hdfs.server.namenode.FSDirectory.totalInodes()
4799    3   2    1 org.apache.hadoop.hdfs.server.namenode.FSDirectory.setTimes(String,INodeFile,long,long,boolean)
4800    3   1    0 org.apache.hadoop.hdfs.server.namenode.FSDirectory.unprotectedSetTimes(String,long,long,boolean)
4801   13   5    0 org.apache.hadoop.hdfs.server.namenode.FSDirectory.unprotectedSetTimes(String,INodeFile,long,long,boolean)
4802    2   4    1 org.apache.hadoop.hdfs.server.namenode.FSDirectory.createFileStatus(String,INode)
4803    2   1    0 org.apache.hadoop.hdfs.server.namenode.FSEditLog.TransactionId.TransactionId(long)
4804    2   1    0 org.apache.hadoop.hdfs.server.namenode.FSEditLog.ThreadLocal$1.initialValue()
4805    9   1    0 org.apache.hadoop.hdfs.server.namenode.FSEditLog.EditLogFileOutputStream.EditLogFileOutputStream(File)
4806    2   1    0 org.apache.hadoop.hdfs.server.namenode.FSEditLog.EditLogFileOutputStream.getName()
4807    2   1    0 org.apache.hadoop.hdfs.server.namenode.FSEditLog.EditLogFileOutputStream.write(int)
4808    4   2    0 org.apache.hadoop.hdfs.server.namenode.FSEditLog.EditLogFileOutputStream.write(byte,Writable)
4809    6   1    0 org.apache.hadoop.hdfs.server.namenode.FSEditLog.EditLogFileOutputStream.create()
4810    9   3    0 org.apache.hadoop.hdfs.server.namenode.FSEditLog.EditLogFileOutputStream.close()
4811    6   1    0 org.apache.hadoop.hdfs.server.namenode.FSEditLog.EditLogFileOutputStream.setReadyToFlush()
4812    6   1    0 org.apache.hadoop.hdfs.server.namenode.FSEditLog.EditLogFileOutputStream.flushAndSync()
4813    2   1    0 org.apache.hadoop.hdfs.server.namenode.FSEditLog.EditLogFileOutputStream.length()
4814    2   1    0 org.apache.hadoop.hdfs.server.namenode.FSEditLog.EditLogFileOutputStream.lastModified()
4815    8   2    0 org.apache.hadoop.hdfs.server.namenode.FSEditLog.EditLogFileOutputStream.preallocate()
4816    2   1    1 org.apache.hadoop.hdfs.server.namenode.FSEditLog.EditLogFileOutputStream.getFile()
4817    3   1    0 org.apache.hadoop.hdfs.server.namenode.FSEditLog.EditLogFileInputStream.EditLogFileInputStream(File)
4818    2   1    0 org.apache.hadoop.hdfs.server.namenode.FSEditLog.EditLogFileInputStream.getName()
4819    2   1    0 org.apache.hadoop.hdfs.server.namenode.FSEditLog.EditLogFileInputStream.available()
4820    2   1    0 org.apache.hadoop.hdfs.server.namenode.FSEditLog.EditLogFileInputStream.read()
4821    2   1    0 org.apache.hadoop.hdfs.server.namenode.FSEditLog.EditLogFileInputStream.read(byte[],int,int)
4822    2   1    0 org.apache.hadoop.hdfs.server.namenode.FSEditLog.EditLogFileInputStream.close()
4823    2   1    0 org.apache.hadoop.hdfs.server.namenode.FSEditLog.EditLogFileInputStream.length()
4824    5   1    0 org.apache.hadoop.hdfs.server.namenode.FSEditLog.FSEditLog(FSImage)
4825    2   1    0 org.apache.hadoop.hdfs.server.namenode.FSEditLog.getEditFile(StorageDirectory)
4826    2   1    0 org.apache.hadoop.hdfs.server.namenode.FSEditLog.getEditNewFile(StorageDirectory)
4827    5   2    0 org.apache.hadoop.hdfs.server.namenode.FSEditLog.getNumStorageDirs()
4828    2   2    0 org.apache.hadoop.hdfs.server.namenode.FSEditLog.getNumEditStreams()
4829    2   1    0 org.apache.hadoop.hdfs.server.namenode.FSEditLog.isOpen()
4830   12   4    1 org.apache.hadoop.hdfs.server.namenode.FSEditLog.open()
4831    4   1    0 org.apache.hadoop.hdfs.server.namenode.FSEditLog.createEditLogFile(File)
4832    5   3    1 org.apache.hadoop.hdfs.server.namenode.FSEditLog.createNewIfMissing()
4833   17   7    1 org.apache.hadoop.hdfs.server.namenode.FSEditLog.close()
4834    9   3    1 org.apache.hadoop.hdfs.server.namenode.FSEditLog.processIOError(int)
4835   10   7    1 org.apache.hadoop.hdfs.server.namenode.FSEditLog.processIOError(StorageDirectory)
4836   14   7    1 org.apache.hadoop.hdfs.server.namenode.FSEditLog.processIOError(ArrayList)
4837    5   4    1 org.apache.hadoop.hdfs.server.namenode.FSEditLog.existsNew()
4838  186  69    1 org.apache.hadoop.hdfs.server.namenode.FSEditLog.loadFSEdits(EditLogInputStream)
4839    4   1    1 org.apache.hadoop.hdfs.server.namenode.FSEditLog.readLongWritable(DataInputStream)
4840    9   3    0 org.apache.hadoop.hdfs.server.namenode.FSEditLog.adjustReplication(short)
4841   16   4    1 org.apache.hadoop.hdfs.server.namenode.FSEditLog.logEdit(byte,Writable)
4842   35  11    0 org.apache.hadoop.hdfs.server.namenode.FSEditLog.logSync()
4843   16   7    0 org.apache.hadoop.hdfs.server.namenode.FSEditLog.printStatistics(boolean)
4844    3   1    1 org.apache.hadoop.hdfs.server.namenode.FSEditLog.logOpenFile(String,INodeFileUnderConstruction)
4845    3   1    1 org.apache.hadoop.hdfs.server.namenode.FSEditLog.logCloseFile(String,INodeFile)
4846    3   1    1 org.apache.hadoop.hdfs.server.namenode.FSEditLog.logMkDir(String,INode)
4847    3   1    1 org.apache.hadoop.hdfs.server.namenode.FSEditLog.logRename(String,String,long)
4848    2   1    1 org.apache.hadoop.hdfs.server.namenode.FSEditLog.logSetReplication(String,short)
4849    2   1    1 org.apache.hadoop.hdfs.server.namenode.FSEditLog.logSetQuota(String,long,long)
4850    2   1    1 org.apache.hadoop.hdfs.server.namenode.FSEditLog.logSetPermissions(String,FsPermission)
4851    4   3    1 org.apache.hadoop.hdfs.server.namenode.FSEditLog.logSetOwner(String,String,String)
4852    3   1    1 org.apache.hadoop.hdfs.server.namenode.FSEditLog.logDelete(String,long)
4853    2   1    1 org.apache.hadoop.hdfs.server.namenode.FSEditLog.logGenerationStamp(long)
4854    3   1    1 org.apache.hadoop.hdfs.server.namenode.FSEditLog.logTimes(String,long,long)
4855    2   1    0 org.apache.hadoop.hdfs.server.namenode.FSEditLog.toLogReplication(short)
4856    2   1    0 org.apache.hadoop.hdfs.server.namenode.FSEditLog.toLogLong(long)
4857    8   3    1 org.apache.hadoop.hdfs.server.namenode.FSEditLog.getEditLogSize()
4858   16   8    1 org.apache.hadoop.hdfs.server.namenode.FSEditLog.rollEditLog()
4859   11   6    1 org.apache.hadoop.hdfs.server.namenode.FSEditLog.purgeEditLog()
4860    5   2    1 org.apache.hadoop.hdfs.server.namenode.FSEditLog.getFsEditName()
4861    2   1    1 org.apache.hadoop.hdfs.server.namenode.FSEditLog.getFsEditTime()
4862    2   1    0 org.apache.hadoop.hdfs.server.namenode.FSEditLog.setBufferCapacity(int)
4863    2   1    0 org.apache.hadoop.hdfs.server.namenode.FSEditLog.BlockTwo.WritableFactory$2.newInstance()
4864    3   1    0 org.apache.hadoop.hdfs.server.namenode.FSEditLog.BlockTwo.BlockTwo()
4865    3   1    0 org.apache.hadoop.hdfs.server.namenode.FSEditLog.BlockTwo.write(DataOutput)
4866    3   1    0 org.apache.hadoop.hdfs.server.namenode.FSEditLog.BlockTwo.readFields(DataInput)
4867    6   2    1 org.apache.hadoop.hdfs.server.namenode.FSEditLog.readDatanodeDescriptorArray(DataInput)
4868    2   1    0 org.apache.hadoop.hdfs.server.namenode.FSEditLog.readShort(DataInputStream)
4869    2   1    0 org.apache.hadoop.hdfs.server.namenode.FSEditLog.readLong(DataInputStream)
4870    7   2    0 org.apache.hadoop.hdfs.server.namenode.FSEditLog.readBlocks(DataInputStream)
4871    2   1    0 org.apache.hadoop.hdfs.server.namenode.FSImage.NameNodeFile.NameNodeFile(String)
4872    2   1    0 org.apache.hadoop.hdfs.server.namenode.FSImage.NameNodeFile.getName()
4873    2   1    0 org.apache.hadoop.hdfs.server.namenode.FSImage.NameNodeDirType.getStorageDirType()
4874    4   5    0 org.apache.hadoop.hdfs.server.namenode.FSImage.NameNodeDirType.isOfType(StorageDirType)
4875    3   1    1 org.apache.hadoop.hdfs.server.namenode.FSImage.FSImage()
4876    3   1    1 org.apache.hadoop.hdfs.server.namenode.FSImage.FSImage(Collection,Collection)
4877    2   1    0 org.apache.hadoop.hdfs.server.namenode.FSImage.FSImage(StorageInfo)
4878    7   1    1 org.apache.hadoop.hdfs.server.namenode.FSImage.FSImage(File)
4879   13   6    0 org.apache.hadoop.hdfs.server.namenode.FSImage.setStorageDirectories(Collection,Collection)
4880    3   1    0 org.apache.hadoop.hdfs.server.namenode.FSImage.setCheckpointDirectories(Collection,Collection)
4881    2   1    0 org.apache.hadoop.hdfs.server.namenode.FSImage.getImageFile(StorageDirectory,NameNodeFile)
4882    2   1    0 org.apache.hadoop.hdfs.server.namenode.FSImage.getEditFile(StorageDirectory)
4883    2   1    0 org.apache.hadoop.hdfs.server.namenode.FSImage.getEditNewFile(StorageDirectory)
4884    6   3    0 org.apache.hadoop.hdfs.server.namenode.FSImage.getFileNames(NameNodeFile,NameNodeDirType)
4885    2   1    0 org.apache.hadoop.hdfs.server.namenode.FSImage.getImageFiles()
4886    2   1    0 org.apache.hadoop.hdfs.server.namenode.FSImage.getEditsFiles()
4887    2   1    0 org.apache.hadoop.hdfs.server.namenode.FSImage.getTimeFiles()
4888   66  42    1 org.apache.hadoop.hdfs.server.namenode.FSImage.recoverTransitionRead(Collection,Collection,StartupOption)
4889   35   9    0 org.apache.hadoop.hdfs.server.namenode.FSImage.doUpgrade()
4890   32   7    0 org.apache.hadoop.hdfs.server.namenode.FSImage.doRollback()
4891   13   4    0 org.apache.hadoop.hdfs.server.namenode.FSImage.doFinalize(StorageDirectory)
4892   12   1    1 org.apache.hadoop.hdfs.server.namenode.FSImage.doImportCheckpoint()
4893    3   2    0 org.apache.hadoop.hdfs.server.namenode.FSImage.finalizeUpgrade()
4894    2   1    0 org.apache.hadoop.hdfs.server.namenode.FSImage.isUpgradeFinalized()
4895    9   5    0 org.apache.hadoop.hdfs.server.namenode.FSImage.getFields(Properties,StorageDirectory)
4896    9   3    0 org.apache.hadoop.hdfs.server.namenode.FSImage.readCheckpointTime(StorageDirectory)
4897    8   3    1 org.apache.hadoop.hdfs.server.namenode.FSImage.setFields(Properties,StorageDirectory)
4898   10   4    1 org.apache.hadoop.hdfs.server.namenode.FSImage.writeCheckpointTime(StorageDirectory)
4899    9   4    1 org.apache.hadoop.hdfs.server.namenode.FSImage.incrementCheckpointTime()
4900    5   3    1 org.apache.hadoop.hdfs.server.namenode.FSImage.processIOError(File)
4901    2   1    0 org.apache.hadoop.hdfs.server.namenode.FSImage.getEditLog()
4902   15   7    0 org.apache.hadoop.hdfs.server.namenode.FSImage.isConversionNeeded(StorageDirectory)
4903   16   9    0 org.apache.hadoop.hdfs.server.namenode.FSImage.recoverInterruptedCheckpoint(StorageDirectory,StorageDirectory)
4904   47  22    1 org.apache.hadoop.hdfs.server.namenode.FSImage.loadFSImage()
4905   76  25    1 org.apache.hadoop.hdfs.server.namenode.FSImage.loadFSImage(File)
4906    2   1    1 org.apache.hadoop.hdfs.server.namenode.FSImage.getParent(String)
4907    2   4    0 org.apache.hadoop.hdfs.server.namenode.FSImage.isParent(String,String)
4908   12   3    1 org.apache.hadoop.hdfs.server.namenode.FSImage.loadFSEdits(StorageDirectory)
4909   18   1    1 org.apache.hadoop.hdfs.server.namenode.FSImage.saveFSImage(File)
4910   14   5    1 org.apache.hadoop.hdfs.server.namenode.FSImage.saveFSImage()
4911    7   2    1 org.apache.hadoop.hdfs.server.namenode.FSImage.newNamespaceID()
4912   12   3    1 org.apache.hadoop.hdfs.server.namenode.FSImage.format(StorageDirectory)
4913    8   2    0 org.apache.hadoop.hdfs.server.namenode.FSImage.format()
4914   26   3    0 org.apache.hadoop.hdfs.server.namenode.FSImage.saveINode2Image(ByteBuffer,INode,DataOutputStream)
4915   16   6    1 org.apache.hadoop.hdfs.server.namenode.FSImage.saveImage(ByteBuffer,int,INodeDirectory,DataOutputStream)
4916    9   6    0 org.apache.hadoop.hdfs.server.namenode.FSImage.loadDatanodes(int,DataInputStream)
4917   17   8    0 org.apache.hadoop.hdfs.server.namenode.FSImage.loadFilesUnderConstruction(int,DataInputStream,FSNamesystem)
4918   20   3    0 org.apache.hadoop.hdfs.server.namenode.FSImage.readINodeUnderConstruction(DataInputStream)
4919   13   2    0 org.apache.hadoop.hdfs.server.namenode.FSImage.writeINodeUnderConstruction(DataOutputStream,INodeFileUnderConstruction,String)
4920   38  17    1 org.apache.hadoop.hdfs.server.namenode.FSImage.rollFSImage()
4921    4   1    0 org.apache.hadoop.hdfs.server.namenode.FSImage.rollEditLog()
4922    8   5    1 org.apache.hadoop.hdfs.server.namenode.FSImage.validateCheckpointUpload(CheckpointSignature)
4923    2   1    1 org.apache.hadoop.hdfs.server.namenode.FSImage.checkpointUploadDone()
4924    3   1    0 org.apache.hadoop.hdfs.server.namenode.FSImage.close()
4925    5   2    1 org.apache.hadoop.hdfs.server.namenode.FSImage.getFsImageName()
4926    2   1    0 org.apache.hadoop.hdfs.server.namenode.FSImage.getFsEditName()
4927    5   2    0 org.apache.hadoop.hdfs.server.namenode.FSImage.getFsTimeName()
4928    5   2    1 org.apache.hadoop.hdfs.server.namenode.FSImage.getFsImageNameCheckpoint()
4929    6   1    1 org.apache.hadoop.hdfs.server.namenode.FSImage.DatanodeImage.write(DataOutput)
4930   13   1    1 org.apache.hadoop.hdfs.server.namenode.FSImage.DatanodeImage.readFields(DataInput)
4931   13   7    0 org.apache.hadoop.hdfs.server.namenode.FSImage.corruptPreUpgradeStorage(File)
4932    2   1    0 org.apache.hadoop.hdfs.server.namenode.FSImage.getDistributedUpgradeState()
4933    2   1    0 org.apache.hadoop.hdfs.server.namenode.FSImage.getDistributedUpgradeVersion()
4934    2   1    0 org.apache.hadoop.hdfs.server.namenode.FSImage.setDistributedUpgradeState(boolean,int)
4935   10   9    0 org.apache.hadoop.hdfs.server.namenode.FSImage.verifyDistributedUpgradeProgress(StartupOption)
4936    6   3    0 org.apache.hadoop.hdfs.server.namenode.FSImage.initializeDistributedUpgrade()
4937    8   4    0 org.apache.hadoop.hdfs.server.namenode.FSImage.getCheckpointDirs(Configuration,String)
4938    8   4    0 org.apache.hadoop.hdfs.server.namenode.FSImage.getCheckpointEditsDirs(Configuration,String)
4939    3   1    0 org.apache.hadoop.hdfs.server.namenode.FSImage.readString(DataInputStream)
4940    3   2    0 org.apache.hadoop.hdfs.server.namenode.FSImage.readString_EmptyAsNull(DataInputStream)
4941    6   1    0 org.apache.hadoop.hdfs.server.namenode.FSImage.readBytes(DataInputStream)
4942    3   1    0 org.apache.hadoop.hdfs.server.namenode.FSImage.writeString(String,DataOutputStream)
4943    2   1    0 org.apache.hadoop.hdfs.server.namenode.FSNamesystem.ThreadLocal$1.initialValue()
4944    4   2    0 org.apache.hadoop.hdfs.server.namenode.FSNamesystem.logAuditEvent(UserGroupInformation,InetAddress,String,String,String,FileStatus)
4945    6   3    1 org.apache.hadoop.hdfs.server.namenode.FSNamesystem.FSNamesystem(NameNode,Configuration)
4946   53   3    1 org.apache.hadoop.hdfs.server.namenode.FSNamesystem.initialize(NameNode,Configuration)
4947    8   3    0 org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getNamespaceDirs(Configuration)
4948    8   3    0 org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getNamespaceEditsDirs(Configuration)
4949    3   1    1 org.apache.hadoop.hdfs.server.namenode.FSNamesystem.FSNamesystem(FSImage,Configuration)
4950   32   9    1 org.apache.hadoop.hdfs.server.namenode.FSNamesystem.setConfigurationParameters(Configuration)
4951    2   1    1 org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getUpgradePermission()
4952    2   1    1 org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getFSNamesystem()
4953    2   1    0 org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getNamespaceInfo()
4954   24  11    1 org.apache.hadoop.hdfs.server.namenode.FSNamesystem.close()
4955   17   3    1 org.apache.hadoop.hdfs.server.namenode.FSNamesystem.metaSave(String)
4956    2   1    0 org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getDefaultBlockSize()
4957    2   1    0 org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getAccessTimePrecision()
4958    2   1    0 org.apache.hadoop.hdfs.server.namenode.FSNamesystem.isAccessTimeSupported()
4959    6   3    0 org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getReplication(Block)
4960    4   1    0 org.apache.hadoop.hdfs.server.namenode.FSNamesystem.updateNeededReplications(Block,int,int)
4961   22  11    1 org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getBlocks(DatanodeID,long)
4962   12   6    1 org.apache.hadoop.hdfs.server.namenode.FSNamesystem.addBlock(Block,List)
4963    7   2    1 org.apache.hadoop.hdfs.server.namenode.FSNamesystem.setPermission(String,FsPermission)
4964   12   9    1 org.apache.hadoop.hdfs.server.namenode.FSNamesystem.setOwner(String,String,String)
4965    9   4    1 org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getBlockLocations(String,String,long,long)
4966    2   1    1 org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getBlockLocations(String,long,long)
4967    9   6    1 org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getBlockLocations(String,long,long,boolean)
4968   44  24    0 org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getBlockLocationsInternal(String,INodeFile,long,long,int,boolean)
4969   13   8    1 org.apache.hadoop.hdfs.server.namenode.FSNamesystem.setTimes(String,long,long)
4970    6   3    1 org.apache.hadoop.hdfs.server.namenode.FSNamesystem.setReplication(String,short)
4971   22  11    0 org.apache.hadoop.hdfs.server.namenode.FSNamesystem.setReplicationInternal(String,short)
4972    4   2    0 org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getPreferredBlockSize(String)
4973    6   6    1 org.apache.hadoop.hdfs.server.namenode.FSNamesystem.verifyReplication(String,short,String)
4974    6   2    1 org.apache.hadoop.hdfs.server.namenode.FSNamesystem.startFile(String,PermissionStatus,String,String,boolean,short,long)
4975   58  34    0 org.apache.hadoop.hdfs.server.namenode.FSNamesystem.startFileInternal(String,PermissionStatus,String,String,boolean,boolean,short,long)
4976   28  11    1 org.apache.hadoop.hdfs.server.namenode.FSNamesystem.appendFile(String,String,String)
4977   32  10    1 org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getAdditionalBlock(String,String)
4978    6   1    1 org.apache.hadoop.hdfs.server.namenode.FSNamesystem.abandonBlock(Block,String,String)
4979    4   1    0 org.apache.hadoop.hdfs.server.namenode.FSNamesystem.checkLease(String,String)
4980   10  11    0 org.apache.hadoop.hdfs.server.namenode.FSNamesystem.checkLease(String,String,INode)
4981    4   1    0 org.apache.hadoop.hdfs.server.namenode.FSNamesystem.completeFile(String,String)
4982   20  11    0 org.apache.hadoop.hdfs.server.namenode.FSNamesystem.completeFileInternal(String,String)
4983    8   3    1 org.apache.hadoop.hdfs.server.namenode.FSNamesystem.checkReplicationFactor(INodeFile)
4984    7   2    1 org.apache.hadoop.hdfs.server.namenode.FSNamesystem.allocateBlock(String,INode[])
4985   11   8    1 org.apache.hadoop.hdfs.server.namenode.FSNamesystem.checkFileProgress(INodeFile,boolean)
4986    2   1    1 org.apache.hadoop.hdfs.server.namenode.FSNamesystem.removeFromInvalidates(DatanodeInfo)
4987    3   1    1 org.apache.hadoop.hdfs.server.namenode.FSNamesystem.addToInvalidates(Block,DatanodeInfo)
4988    6   2    1 org.apache.hadoop.hdfs.server.namenode.FSNamesystem.addToInvalidatesNoLog(Block,DatanodeInfo)
4989    4   2    1 org.apache.hadoop.hdfs.server.namenode.FSNamesystem.addToInvalidates(Block)
4990    9   5    1 org.apache.hadoop.hdfs.server.namenode.FSNamesystem.dumpRecentInvalidateSets(PrintWriter)
4991   14   5    1 org.apache.hadoop.hdfs.server.namenode.FSNamesystem.markBlockAsCorrupt(Block,DatanodeInfo)
4992   14   6    1 org.apache.hadoop.hdfs.server.namenode.FSNamesystem.invalidateBlock(Block,DatanodeInfo)
4993    7   3    1 org.apache.hadoop.hdfs.server.namenode.FSNamesystem.renameTo(String,String)
4994   15   9    0 org.apache.hadoop.hdfs.server.namenode.FSNamesystem.renameToInternal(String,String)
4995    8   6    1 org.apache.hadoop.hdfs.server.namenode.FSNamesystem.delete(String,boolean)
4996    4   1    1 org.apache.hadoop.hdfs.server.namenode.FSNamesystem.deleteInSafeMode(String)
4997    8   7    1 org.apache.hadoop.hdfs.server.namenode.FSNamesystem.deleteInternal(String,boolean,boolean)
4998    6   2    0 org.apache.hadoop.hdfs.server.namenode.FSNamesystem.removePathAndBlocks(String,List)
4999    4   2    1 org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getFileInfo(String)
5000    7   3    1 org.apache.hadoop.hdfs.server.namenode.FSNamesystem.mkdirs(String,PermissionStatus)
5001   16  11    1 org.apache.hadoop.hdfs.server.namenode.FSNamesystem.mkdirsInternal(String,PermissionStatus)
5002    4   2    0 org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getContentSummary(String)
5003    5   2    1 org.apache.hadoop.hdfs.server.namenode.FSNamesystem.setQuota(String,long,long)
5004    7   3    1 org.apache.hadoop.hdfs.server.namenode.FSNamesystem.fsync(String,String)
5005   26  12    1 org.apache.hadoop.hdfs.server.namenode.FSNamesystem.internalReleaseLease(Lease,String)
5006    6   1    0 org.apache.hadoop.hdfs.server.namenode.FSNamesystem.finalizeINodeFileUnderConstruction(String,INodeFileUnderConstruction)
5007   31  10    0 org.apache.hadoop.hdfs.server.namenode.FSNamesystem.commitBlockSynchronization(Block,long,long,boolean,boolean,DatanodeID[])
5008    4   3    1 org.apache.hadoop.hdfs.server.namenode.FSNamesystem.renewLease(String)
5009    9   4    1 org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getListing(String)
5010   44  11    1 org.apache.hadoop.hdfs.server.namenode.FSNamesystem.registerDatanode(DatanodeRegistration)
5011   17   4    0 org.apache.hadoop.hdfs.server.namenode.FSNamesystem.resolveNetworkLocation(DatanodeDescriptor)
5012    2   1    1 org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getRegistrationID()
5013    7   3    1 org.apache.hadoop.hdfs.server.namenode.FSNamesystem.newStorageID()
5014    2   1    0 org.apache.hadoop.hdfs.server.namenode.FSNamesystem.isDatanodeDead(DatanodeDescriptor)
5015    2   1    0 org.apache.hadoop.hdfs.server.namenode.FSNamesystem.setDatanodeDead(DatanodeDescriptor)
5016   25  13    1 org.apache.hadoop.hdfs.server.namenode.FSNamesystem.handleHeartbeat(DatanodeRegistration,long,long,long,int,int)
5017   12   2    0 org.apache.hadoop.hdfs.server.namenode.FSNamesystem.updateStats(DatanodeDescriptor,boolean)
5018    7   4    1 org.apache.hadoop.hdfs.server.namenode.FSNamesystem.HeartbeatMonitor.run()
5019   13   5    0 org.apache.hadoop.hdfs.server.namenode.FSNamesystem.ReplicationMonitor.run()
5020   16   4    1 org.apache.hadoop.hdfs.server.namenode.FSNamesystem.computeDatanodeWork()
5021    8   3    0 org.apache.hadoop.hdfs.server.namenode.FSNamesystem.computeInvalidateWork(int)
5022   57  17    1 org.apache.hadoop.hdfs.server.namenode.FSNamesystem.computeReplicationWork(int)
5023   41  19    1 org.apache.hadoop.hdfs.server.namenode.FSNamesystem.chooseSourceDatanode(Block,List,NumberReplicas)
5024   26  13    1 org.apache.hadoop.hdfs.server.namenode.FSNamesystem.invalidateWorkForOneNode()
5025    2   1    0 org.apache.hadoop.hdfs.server.namenode.FSNamesystem.setNodeReplicationLimit(int)
5026    7   3    1 org.apache.hadoop.hdfs.server.namenode.FSNamesystem.processPendingReplications()
5027    6   2    1 org.apache.hadoop.hdfs.server.namenode.FSNamesystem.removeDatanode(DatanodeID)
5028   10   3    1 org.apache.hadoop.hdfs.server.namenode.FSNamesystem.removeDatanode(DatanodeDescriptor)
5029    4   1    0 org.apache.hadoop.hdfs.server.namenode.FSNamesystem.unprotectedRemoveDatanode(DatanodeDescriptor)
5030    4   1    0 org.apache.hadoop.hdfs.server.namenode.FSNamesystem.unprotectedAddDatanode(DatanodeDescriptor)
5031    4   1    1 org.apache.hadoop.hdfs.server.namenode.FSNamesystem.wipeDatanode(DatanodeID)
5032    2   1    0 org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getFSImage()
5033    2   1    0 org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getEditLog()
5034   24   8    1 org.apache.hadoop.hdfs.server.namenode.FSNamesystem.heartbeatCheck()
5035   22   9    1 org.apache.hadoop.hdfs.server.namenode.FSNamesystem.processReport(DatanodeID,BlockListAsLongs)
5036   77  32    1 org.apache.hadoop.hdfs.server.namenode.FSNamesystem.addStoredBlock(Block,DatanodeDescriptor,DatanodeDescriptor)
5037   13   6    1 org.apache.hadoop.hdfs.server.namenode.FSNamesystem.invalidateCorruptReplicas(Block)
5038   22   5    1 org.apache.hadoop.hdfs.server.namenode.FSNamesystem.processMisReplicatedBlocks()
5039   11   7    1 org.apache.hadoop.hdfs.server.namenode.FSNamesystem.processOverReplicatedBlock(Block,short,DatanodeDescriptor,DatanodeDescriptor)
5040   54  19    1 org.apache.hadoop.hdfs.server.namenode.FSNamesystem.chooseExcessReplicates(Collection,Block,short,DatanodeDescriptor,DatanodeDescriptor)
5041   16   6    1 org.apache.hadoop.hdfs.server.namenode.FSNamesystem.removeStoredBlock(Block,DatanodeDescriptor)
5042   18   9    1 org.apache.hadoop.hdfs.server.namenode.FSNamesystem.blockReceived(DatanodeID,Block,String)
5043    4   1    0 org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getStats()
5044    3   1    1 org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getCapacityTotal()
5045    3   1    1 org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getCapacityUsed()
5046    5   3    1 org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getCapacityUsedPercent()
5047    5   2    1 org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getCapacityUsedNonDFS()
5048    3   1    1 org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getCapacityRemaining()
5049    5   3    1 org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getCapacityRemainingPercent()
5050    3   1    1 org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getTotalLoad()
5051    2   1    0 org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getNumberOfDatanodes(DatanodeReportType)
5052   26  13    0 org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getDatanodeListForReport(DatanodeReportType)
5053    7   2    0 org.apache.hadoop.hdfs.server.namenode.FSNamesystem.datanodeReport(DatanodeReportType)
5054    8   3    1 org.apache.hadoop.hdfs.server.namenode.FSNamesystem.DFSNodesStatus(ArrayList,ArrayList)
5055    6   2    1 org.apache.hadoop.hdfs.server.namenode.FSNamesystem.datanodeDump(PrintWriter)
5056    8   4    1 org.apache.hadoop.hdfs.server.namenode.FSNamesystem.startDecommission(DatanodeDescriptor)
5057    3   1    1 org.apache.hadoop.hdfs.server.namenode.FSNamesystem.stopDecommission(DatanodeDescriptor)
5058    2   1    1 org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getDataNodeInfo(String)
5059    2   1    1 org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getDFSNameNodeMachine()
5060    2   1    1 org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getDFSNameNodePort()
5061    2   1    1 org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getStartTime()
5062    2   1    0 org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getMaxReplication()
5063    2   1    0 org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getMinReplication()
5064    2   1    0 org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getDefaultReplication()
5065    2   1    0 org.apache.hadoop.hdfs.server.namenode.FSNamesystem.NumberReplicas.NumberReplicas()
5066    2   1    0 org.apache.hadoop.hdfs.server.namenode.FSNamesystem.NumberReplicas.NumberReplicas(int,int,int,int)
5067    5   1    0 org.apache.hadoop.hdfs.server.namenode.FSNamesystem.NumberReplicas.initialize(int,int,int,int)
5068    2   1    0 org.apache.hadoop.hdfs.server.namenode.FSNamesystem.NumberReplicas.liveReplicas()
5069    2   1    0 org.apache.hadoop.hdfs.server.namenode.FSNamesystem.NumberReplicas.decommissionedReplicas()
5070    2   1    0 org.apache.hadoop.hdfs.server.namenode.FSNamesystem.NumberReplicas.corruptReplicas()
5071    2   1    0 org.apache.hadoop.hdfs.server.namenode.FSNamesystem.NumberReplicas.excessReplicas()
5072   20   8    1 org.apache.hadoop.hdfs.server.namenode.FSNamesystem.countNodes(Block,Iterator)
5073    2   1    1 org.apache.hadoop.hdfs.server.namenode.FSNamesystem.countNodes(Block)
5074   15   6    1 org.apache.hadoop.hdfs.server.namenode.FSNamesystem.isReplicationInProgress(DatanodeDescriptor)
5075    8   5    1 org.apache.hadoop.hdfs.server.namenode.FSNamesystem.checkDecommissionStateInternal(DatanodeDescriptor)
5076    3   7    1 org.apache.hadoop.hdfs.server.namenode.FSNamesystem.inHostsList(DatanodeID,String)
5077    3   6    0 org.apache.hadoop.hdfs.server.namenode.FSNamesystem.inExcludedHostsList(DatanodeID,String)
5078   18   9    1 org.apache.hadoop.hdfs.server.namenode.FSNamesystem.refreshNodes(Configuration)
5079    3   1    0 org.apache.hadoop.hdfs.server.namenode.FSNamesystem.finalizeUpgrade()
5080   10   7    1 org.apache.hadoop.hdfs.server.namenode.FSNamesystem.verifyNodeRegistration(DatanodeRegistration,String)
5081    2   1    1 org.apache.hadoop.hdfs.server.namenode.FSNamesystem.shouldNodeShutdown(DatanodeDescriptor)
5082    4   2    1 org.apache.hadoop.hdfs.server.namenode.FSNamesystem.decommissionedDatanodeCheck()
5083    7   4    0 org.apache.hadoop.hdfs.server.namenode.FSNamesystem.DecommissionedMonitor.run()
5084   10   5    1 org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getDatanode(DatanodeID)
5085    7   4    0 org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getDatanodeByIndex(int)
5086   11   8    0 org.apache.hadoop.hdfs.server.namenode.FSNamesystem.randomDataNode()
5087    2   1    0 org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getRandomDatanode()
5088    2   1    0 org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getNameNodeInfoPort()
5089    6   1    1 org.apache.hadoop.hdfs.server.namenode.FSNamesystem.SafeModeInfo.SafeModeInfo(Configuration)
5090    9   1    1 org.apache.hadoop.hdfs.server.namenode.FSNamesystem.SafeModeInfo.SafeModeInfo()
5091    5   2    1 org.apache.hadoop.hdfs.server.namenode.FSNamesystem.SafeModeInfo.isOn()
5092    2   1    1 org.apache.hadoop.hdfs.server.namenode.FSNamesystem.SafeModeInfo.enter()
5093   19   6    1 org.apache.hadoop.hdfs.server.namenode.FSNamesystem.SafeModeInfo.leave(boolean)
5094    7   5    1 org.apache.hadoop.hdfs.server.namenode.FSNamesystem.SafeModeInfo.canLeave()
5095    2   1    1 org.apache.hadoop.hdfs.server.namenode.FSNamesystem.SafeModeInfo.needEnter()
5096    2   2    1 org.apache.hadoop.hdfs.server.namenode.FSNamesystem.SafeModeInfo.getSafeBlockRatio()
5097   15   9    1 org.apache.hadoop.hdfs.server.namenode.FSNamesystem.SafeModeInfo.checkMode()
5098    3   1    1 org.apache.hadoop.hdfs.server.namenode.FSNamesystem.SafeModeInfo.setBlockTotal(int)
5099    4   2    1 org.apache.hadoop.hdfs.server.namenode.FSNamesystem.SafeModeInfo.incrementSafeBlockCount(short)
5100    4   2    1 org.apache.hadoop.hdfs.server.namenode.FSNamesystem.SafeModeInfo.decrementSafeBlockCount(short)
5101    2   1    1 org.apache.hadoop.hdfs.server.namenode.FSNamesystem.SafeModeInfo.isManual()
5102   12  10    1 org.apache.hadoop.hdfs.server.namenode.FSNamesystem.SafeModeInfo.getTurnOffTip()
5103    6   4    1 org.apache.hadoop.hdfs.server.namenode.FSNamesystem.SafeModeInfo.reportStatus(String,boolean)
5104    5   2    1 org.apache.hadoop.hdfs.server.namenode.FSNamesystem.SafeModeInfo.toString()
5105    7   7    1 org.apache.hadoop.hdfs.server.namenode.FSNamesystem.SafeModeInfo.isConsistent()
5106    7   6    1 org.apache.hadoop.hdfs.server.namenode.FSNamesystem.SafeModeMonitor.run()
5107    2   1    1 org.apache.hadoop.hdfs.server.namenode.FSNamesystem.now()
5108   11   4    0 org.apache.hadoop.hdfs.server.namenode.FSNamesystem.setSafeMode(SafeModeAction)
5109    4   3    1 org.apache.hadoop.hdfs.server.namenode.FSNamesystem.isInSafeMode()
5110    4   3    1 org.apache.hadoop.hdfs.server.namenode.FSNamesystem.incrementSafeBlockCount(int)
5111    4   3    1 org.apache.hadoop.hdfs.server.namenode.FSNamesystem.decrementSafeBlockCount(Block)
5112    4   3    1 org.apache.hadoop.hdfs.server.namenode.FSNamesystem.setBlockTotal()
5113    2   1    1 org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getBlocksTotal()
5114    5   3    1 org.apache.hadoop.hdfs.server.namenode.FSNamesystem.enterSafeMode()
5115    7   5    1 org.apache.hadoop.hdfs.server.namenode.FSNamesystem.leaveSafeMode(boolean)
5116    4   3    0 org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getSafeModeTip()
5117    2   1    0 org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getEditLogSize()
5118    5   3    0 org.apache.hadoop.hdfs.server.namenode.FSNamesystem.rollEditLog()
5119    5   3    0 org.apache.hadoop.hdfs.server.namenode.FSNamesystem.rollFSImage()
5120    2   1    1 org.apache.hadoop.hdfs.server.namenode.FSNamesystem.isValidBlock(Block)
5121    2   1    0 org.apache.hadoop.hdfs.server.namenode.FSNamesystem.distributedUpgradeProgress(UpgradeAction)
5122    2   1    0 org.apache.hadoop.hdfs.server.namenode.FSNamesystem.processDistributedUpgradeCommand(UpgradeCommand)
5123    2   1    0 org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getDistributedUpgradeVersion()
5124    2   1    0 org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getDistributedUpgradeCommand()
5125    2   1    0 org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getDistributedUpgradeState()
5126    2   1    0 org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getDistributedUpgradeStatus()
5127    2   1    0 org.apache.hadoop.hdfs.server.namenode.FSNamesystem.startDistributedUpgradeIfNeeded()
5128    2   1    0 org.apache.hadoop.hdfs.server.namenode.FSNamesystem.createFsOwnerPermissions(FsPermission)
5129    2   1    0 org.apache.hadoop.hdfs.server.namenode.FSNamesystem.checkOwner(String)
5130    2   1    0 org.apache.hadoop.hdfs.server.namenode.FSNamesystem.checkPathAccess(String,FsAction)
5131    2   1    0 org.apache.hadoop.hdfs.server.namenode.FSNamesystem.checkParentAccess(String,FsAction)
5132    2   1    0 org.apache.hadoop.hdfs.server.namenode.FSNamesystem.checkAncestorAccess(String,FsAction)
5133    2   1    0 org.apache.hadoop.hdfs.server.namenode.FSNamesystem.checkTraverse(String)
5134    5   4    0 org.apache.hadoop.hdfs.server.namenode.FSNamesystem.checkSuperuserPrivilege()
5135    6   2    1 org.apache.hadoop.hdfs.server.namenode.FSNamesystem.checkPermission(String,boolean,FsAction,FsAction,FsAction,FsAction)
5136    3   4    1 org.apache.hadoop.hdfs.server.namenode.FSNamesystem.checkFsObjectLimit()
5137    2   1    1 org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getMaxObjects()
5138    2   1    0 org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getFilesTotal()
5139    2   1    0 org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getPendingReplicationBlocks()
5140    2   1    0 org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getUnderReplicatedBlocks()
5141    2   1    0 org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getScheduledReplicationBlocks()
5142    2   2    0 org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getFSState()
5143    8   2    1 org.apache.hadoop.hdfs.server.namenode.FSNamesystem.registerMBean(Configuration)
5144    2   1    1 org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getFSNamesystemMetrics()
5145    3   2    1 org.apache.hadoop.hdfs.server.namenode.FSNamesystem.shutdown()
5146    8   3    1 org.apache.hadoop.hdfs.server.namenode.FSNamesystem.numLiveDataNodes()
5147    8   3    1 org.apache.hadoop.hdfs.server.namenode.FSNamesystem.numDeadDataNodes()
5148    2   1    1 org.apache.hadoop.hdfs.server.namenode.FSNamesystem.setGenerationStamp(long)
5149    2   1    1 org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getGenerationStamp()
5150    4   1    1 org.apache.hadoop.hdfs.server.namenode.FSNamesystem.nextGenerationStamp()
5151   16   7    1 org.apache.hadoop.hdfs.server.namenode.FSNamesystem.nextGenerationStampForBlock(Block)
5152   14   4    0 org.apache.hadoop.hdfs.server.namenode.FSNamesystem.changeLease(String,String,FileStatus)
5153   14   7    1 org.apache.hadoop.hdfs.server.namenode.FSNamesystem.saveFilesUnderConstruction(DataOutputStream)
5154   21   6    0 org.apache.hadoop.hdfs.server.namenode.GetImageServlet.doGet(HttpServletRequest,HttpServletResponse)
5155   13   7    1 org.apache.hadoop.hdfs.server.namenode.Host2NodesMap.contains(DatanodeDescriptor)
5156   18   6    1 org.apache.hadoop.hdfs.server.namenode.Host2NodesMap.add(DatanodeDescriptor)
5157   29  14    1 org.apache.hadoop.hdfs.server.namenode.Host2NodesMap.remove(DatanodeDescriptor)
5158   12   8    1 org.apache.hadoop.hdfs.server.namenode.Host2NodesMap.getDatanodeByHost(String)
5159   19  10    1 org.apache.hadoop.hdfs.server.namenode.Host2NodesMap.getDatanodeByName(String)
5160    3   1    0 org.apache.hadoop.hdfs.server.namenode.INodeDirectory.INodeDirectory(String,PermissionStatus)
5161    3   1    0 org.apache.hadoop.hdfs.server.namenode.INodeDirectory.INodeDirectory(PermissionStatus,long)
5162    3   1    1 org.apache.hadoop.hdfs.server.namenode.INodeDirectory.INodeDirectory(byte[],PermissionStatus,long)
5163    3   1    1 org.apache.hadoop.hdfs.server.namenode.INodeDirectory.INodeDirectory(INodeDirectory)
5164    2   1    1 org.apache.hadoop.hdfs.server.namenode.INodeDirectory.isDirectory()
5165    7   3    0 org.apache.hadoop.hdfs.server.namenode.INodeDirectory.removeChild(INode)
5166    8   5    1 org.apache.hadoop.hdfs.server.namenode.INodeDirectory.replaceChild(INode)
5167    2   1    0 org.apache.hadoop.hdfs.server.namenode.INodeDirectory.getChild(String)
5168    7   5    0 org.apache.hadoop.hdfs.server.namenode.INodeDirectory.getChildINode(byte[])
5169    4   1    1 org.apache.hadoop.hdfs.server.namenode.INodeDirectory.getNode(byte[][])
5170    2   1    1 org.apache.hadoop.hdfs.server.namenode.INodeDirectory.getNode(String)
5171   17   7    1 org.apache.hadoop.hdfs.server.namenode.INodeDirectory.getExistingPathINodes(byte[][],INode[])
5172    5   1    1 org.apache.hadoop.hdfs.server.namenode.INodeDirectory.getExistingPathINodes(String)
5173   17   7    0 org.apache.hadoop.hdfs.server.namenode.INodeDirectory.addChild(T,boolean)
5174    2   1    0 org.apache.hadoop.hdfs.server.namenode.INodeDirectory.addNode(String,T)
5175    4   3    0 org.apache.hadoop.hdfs.server.namenode.INodeDirectory.addNode(String,T,boolean)
5176   19  10    0 org.apache.hadoop.hdfs.server.namenode.INodeDirectory.addToParent(String,T,INodeDirectory,boolean)
5177    6   3    1 org.apache.hadoop.hdfs.server.namenode.INodeDirectory.spaceConsumedInTree(DirCounts)
5178    6   3    0 org.apache.hadoop.hdfs.server.namenode.INodeDirectory.computeContentSummary(long[])
5179    2   2    1 org.apache.hadoop.hdfs.server.namenode.INodeDirectory.getChildren()
5180    2   1    0 org.apache.hadoop.hdfs.server.namenode.INodeDirectory.getChildrenRaw()
5181    9   4    0 org.apache.hadoop.hdfs.server.namenode.INodeDirectory.collectSubtreeBlocksAndClear(List)
5182    7   1    1 org.apache.hadoop.hdfs.server.namenode.INodeDirectoryWithQuota.INodeDirectoryWithQuota(long,long,INodeDirectory)
5183    4   1    1 org.apache.hadoop.hdfs.server.namenode.INodeDirectoryWithQuota.INodeDirectoryWithQuota(PermissionStatus,long,long,long)
5184    4   1    1 org.apache.hadoop.hdfs.server.namenode.INodeDirectoryWithQuota.INodeDirectoryWithQuota(String,PermissionStatus,long,long)
5185    2   1    1 org.apache.hadoop.hdfs.server.namenode.INodeDirectoryWithQuota.getNsQuota()
5186    2   1    1 org.apache.hadoop.hdfs.server.namenode.INodeDirectoryWithQuota.getDsQuota()
5187    5   8    1 org.apache.hadoop.hdfs.server.namenode.INodeDirectoryWithQuota.setQuota(long,long)
5188    4   1    0 org.apache.hadoop.hdfs.server.namenode.INodeDirectoryWithQuota.spaceConsumedInTree(DirCounts)
5189    2   1    1 org.apache.hadoop.hdfs.server.namenode.INodeDirectoryWithQuota.numItemsInTree()
5190    2   1    0 org.apache.hadoop.hdfs.server.namenode.INodeDirectoryWithQuota.diskspaceConsumed()
5191    7   3    1 org.apache.hadoop.hdfs.server.namenode.INodeDirectoryWithQuota.updateNumItemsInTree(long,long)
5192    3   1    1 org.apache.hadoop.hdfs.server.namenode.INodeDirectoryWithQuota.setSpaceConsumed(long,long)
5193    3   6    1 org.apache.hadoop.hdfs.server.namenode.INodeDirectoryWithQuota.verifyQuota(long,long,long,long)
5194    2   1    0 org.apache.hadoop.hdfs.server.namenode.INodeFile.INodeFile(PermissionStatus,int,short,long,long,long)
5195    4   1    0 org.apache.hadoop.hdfs.server.namenode.INodeFile.INodeFile()
5196    5   1    0 org.apache.hadoop.hdfs.server.namenode.INodeFile.INodeFile(PermissionStatus,BlockInfo[],short,long,long,long)
5197    2   1    1 org.apache.hadoop.hdfs.server.namenode.INodeFile.setPermission(FsPermission)
5198    2   1    0 org.apache.hadoop.hdfs.server.namenode.INodeFile.isDirectory()
5199    2   1    1 org.apache.hadoop.hdfs.server.namenode.INodeFile.getReplication()
5200    2   1    0 org.apache.hadoop.hdfs.server.namenode.INodeFile.setReplication(short)
5201    2   1    1 org.apache.hadoop.hdfs.server.namenode.INodeFile.getBlocks()
5202   11   3    1 org.apache.hadoop.hdfs.server.namenode.INodeFile.addBlock(BlockInfo)
5203    2   1    1 org.apache.hadoop.hdfs.server.namenode.INodeFile.setBlock(int,BlockInfo)
5204    6   2    0 org.apache.hadoop.hdfs.server.namenode.INodeFile.collectSubtreeBlocksAndClear(List)
5205    8   2    0 org.apache.hadoop.hdfs.server.namenode.INodeFile.computeContentSummary(long[])
5206    4   1    0 org.apache.hadoop.hdfs.server.namenode.INodeFile.spaceConsumedInTree(DirCounts)
5207    2   1    0 org.apache.hadoop.hdfs.server.namenode.INodeFile.diskspaceConsumed()
5208    8   6    0 org.apache.hadoop.hdfs.server.namenode.INodeFile.diskspaceConsumed(Block[])
5209    2   1    1 org.apache.hadoop.hdfs.server.namenode.INodeFile.getPreferredBlockSize()
5210    4   4    1 org.apache.hadoop.hdfs.server.namenode.INodeFile.getPenultimateBlock()
5211    4   3    0 org.apache.hadoop.hdfs.server.namenode.INodeFile.toINodeFileUnderConstruction(String,String,DatanodeDescriptor)
5212    1   1    0 org.apache.hadoop.hdfs.server.namenode.INodeFileUnderConstruction.INodeFileUnderConstruction()
5213    5   1    0 org.apache.hadoop.hdfs.server.namenode.INodeFileUnderConstruction.INodeFileUnderConstruction(PermissionStatus,short,long,long,String,String,DatanodeDescriptor)
5214    6   1    0 org.apache.hadoop.hdfs.server.namenode.INodeFileUnderConstruction.INodeFileUnderConstruction(byte[],short,long,long,BlockInfo[],PermissionStatus,String,String,DatanodeDescriptor)
5215    2   1    0 org.apache.hadoop.hdfs.server.namenode.INodeFileUnderConstruction.getClientName()
5216    2   1    0 org.apache.hadoop.hdfs.server.namenode.INodeFileUnderConstruction.getClientMachine()
5217    2   1    0 org.apache.hadoop.hdfs.server.namenode.INodeFileUnderConstruction.getClientNode()
5218    2   1    0 org.apache.hadoop.hdfs.server.namenode.INodeFileUnderConstruction.isUnderConstruction()
5219    2   1    0 org.apache.hadoop.hdfs.server.namenode.INodeFileUnderConstruction.getTargets()
5220    3   1    0 org.apache.hadoop.hdfs.server.namenode.INodeFileUnderConstruction.setTargets(DatanodeDescriptor[])
5221    3   1    0 org.apache.hadoop.hdfs.server.namenode.INodeFileUnderConstruction.convertToInodeFile()
5222   10   5    1 org.apache.hadoop.hdfs.server.namenode.INodeFileUnderConstruction.removeBlock(Block)
5223    6   3    0 org.apache.hadoop.hdfs.server.namenode.INodeFileUnderConstruction.setLastBlock(BlockInfo,DatanodeDescriptor[])
5224   11   4    1 org.apache.hadoop.hdfs.server.namenode.INodeFileUnderConstruction.assignPrimaryDatanode()
5225    5   2    1 org.apache.hadoop.hdfs.server.namenode.INodeFileUnderConstruction.setLastRecoveryTime(long)
5226    7   2    0 org.apache.hadoop.hdfs.server.namenode.JspHelper.JspHelper()
5227    2   1    0 org.apache.hadoop.hdfs.server.namenode.JspHelper.randomNode()
5228   27  10    0 org.apache.hadoop.hdfs.server.namenode.JspHelper.bestNode(LocatedBlock)
5229   24   7    0 org.apache.hadoop.hdfs.server.namenode.JspHelper.streamBlockInAscii(InetSocketAddress,long,long,long,long,long,JspWriter)
5230    3   2    0 org.apache.hadoop.hdfs.server.namenode.JspHelper.DFSNodesStatus(ArrayList,ArrayList)
5231    3   1    0 org.apache.hadoop.hdfs.server.namenode.JspHelper.addTableHeader(JspWriter)
5232    5   2    0 org.apache.hadoop.hdfs.server.namenode.JspHelper.addTableRow(JspWriter,String[])
5233    8   3    0 org.apache.hadoop.hdfs.server.namenode.JspHelper.addTableRow(JspWriter,String[],int)
5234    2   1    0 org.apache.hadoop.hdfs.server.namenode.JspHelper.addTableFooter(JspWriter)
5235    4   3    0 org.apache.hadoop.hdfs.server.namenode.JspHelper.getSafeModeText()
5236   13   2    0 org.apache.hadoop.hdfs.server.namenode.JspHelper.getInodeLimitText()
5237    7   3    0 org.apache.hadoop.hdfs.server.namenode.JspHelper.getUpgradeStatusText()
5238   30  10    0 org.apache.hadoop.hdfs.server.namenode.JspHelper.NodeComapare.NodeComapare(String,String)
5239   37  23    0 org.apache.hadoop.hdfs.server.namenode.JspHelper.NodeComapare.compare(DatanodeDescriptor,DatanodeDescriptor)
5240   73   1    0 org.apache.hadoop.hdfs.server.namenode.JspHelper.sortNodeList(ArrayList,String,String)
5241   15   5    0 org.apache.hadoop.hdfs.server.namenode.JspHelper.printPathWithLinks(String,JspWriter,int)
5242    7   1    0 org.apache.hadoop.hdfs.server.namenode.JspHelper.printGotoForm(JspWriter,int,String)
5243    7   3    0 org.apache.hadoop.hdfs.server.namenode.JspHelper.createTitle(JspWriter,HttpServletRequest,String)
5244    2   1    0 org.apache.hadoop.hdfs.server.namenode.LeaseExpiredException.LeaseExpiredException(String)
5245    2   1    0 org.apache.hadoop.hdfs.server.namenode.LeaseManager.LeaseManager(FSNamesystem)
5246    2   1    0 org.apache.hadoop.hdfs.server.namenode.LeaseManager.getLease(StringBytesWritable)
5247    2   1    0 org.apache.hadoop.hdfs.server.namenode.LeaseManager.getSortedLeases()
5248    2   1    1 org.apache.hadoop.hdfs.server.namenode.LeaseManager.getLeaseByPath(String)
5249    2   1    1 org.apache.hadoop.hdfs.server.namenode.LeaseManager.countLease()
5250    5   2    1 org.apache.hadoop.hdfs.server.namenode.LeaseManager.countPath()
5251   10   2    1 org.apache.hadoop.hdfs.server.namenode.LeaseManager.addLease(StringBytesWritable,String)
5252    8   4    1 org.apache.hadoop.hdfs.server.namenode.LeaseManager.removeLease(Lease,String)
5253    4   2    1 org.apache.hadoop.hdfs.server.namenode.LeaseManager.removeLease(StringBytesWritable,String)
5254    7   5    1 org.apache.hadoop.hdfs.server.namenode.LeaseManager.findPath(INodeFileUnderConstruction)
5255    2   1    1 org.apache.hadoop.hdfs.server.namenode.LeaseManager.renewLease(String)
5256    5   2    0 org.apache.hadoop.hdfs.server.namenode.LeaseManager.renewLease(Lease)
5257    3   1    1 org.apache.hadoop.hdfs.server.namenode.LeaseManager.Lease.Lease(StringBytesWritable)
5258    2   1    1 org.apache.hadoop.hdfs.server.namenode.LeaseManager.Lease.renew()
5259    2   1    1 org.apache.hadoop.hdfs.server.namenode.LeaseManager.Lease.expiredHardLimit()
5260    2   1    1 org.apache.hadoop.hdfs.server.namenode.LeaseManager.Lease.expiredSoftLimit()
5261    6   4    1 org.apache.hadoop.hdfs.server.namenode.LeaseManager.Lease.findPath(INodeFileUnderConstruction)
5262    2   1    1 org.apache.hadoop.hdfs.server.namenode.LeaseManager.Lease.hasPath()
5263    2   1    0 org.apache.hadoop.hdfs.server.namenode.LeaseManager.Lease.removePath(String)
5264    2   1    1 org.apache.hadoop.hdfs.server.namenode.LeaseManager.Lease.toString()
5265   12   5    1 org.apache.hadoop.hdfs.server.namenode.LeaseManager.Lease.compareTo(Lease)
5266    7   6    1 org.apache.hadoop.hdfs.server.namenode.LeaseManager.Lease.equals(Object)
5267    2   1    1 org.apache.hadoop.hdfs.server.namenode.LeaseManager.Lease.hashCode()
5268    2   1    0 org.apache.hadoop.hdfs.server.namenode.LeaseManager.Lease.getPaths()
5269    3   1    0 org.apache.hadoop.hdfs.server.namenode.LeaseManager.Lease.replacePath(String,String)
5270   12   4    0 org.apache.hadoop.hdfs.server.namenode.LeaseManager.changeLease(String,String,String,String)
5271    5   3    0 org.apache.hadoop.hdfs.server.namenode.LeaseManager.removeLeaseWithPrefixPath(String)
5272   12   7    0 org.apache.hadoop.hdfs.server.namenode.LeaseManager.findLeaseWithPrefixPath(String,SortedMap)
5273    3   1    0 org.apache.hadoop.hdfs.server.namenode.LeaseManager.setLeasePeriod(long,long)
5274    2   1    0 org.apache.hadoop.hdfs.server.namenode.LeaseManager.createMonitor()
5275   18   9    0 org.apache.hadoop.hdfs.server.namenode.LeaseManager.Monitor.run()
5276    2   1    1 org.apache.hadoop.hdfs.server.namenode.LeaseManager.toString()
5277   13   4    1 org.apache.hadoop.hdfs.server.namenode.ListPathsServlet.writeInfo(FileStatus,XMLOutputter)
5278   13   6    1 org.apache.hadoop.hdfs.server.namenode.ListPathsServlet.buildRoot(HttpServletRequest,XMLOutputter)
5279   36  14    1 org.apache.hadoop.hdfs.server.namenode.ListPathsServlet.doGet(HttpServletRequest,HttpServletResponse)
5280    7   1    0 org.apache.hadoop.hdfs.server.namenode.metrics.FSNamesystemMetrics.FSNamesystemMetrics(Configuration)
5281    2   1    0 org.apache.hadoop.hdfs.server.namenode.metrics.FSNamesystemMetrics.roundBytesToGBytes(long)
5282   22   1    1 org.apache.hadoop.hdfs.server.namenode.metrics.FSNamesystemMetrics.doUpdates(MetricsContext)
5283    9   1    0 org.apache.hadoop.hdfs.server.namenode.metrics.NameNodeMetrics.NameNodeMetrics(Configuration,NameNode)
5284    3   2    0 org.apache.hadoop.hdfs.server.namenode.metrics.NameNodeMetrics.shutdown()
5285   17   1    1 org.apache.hadoop.hdfs.server.namenode.metrics.NameNodeMetrics.doUpdates(MetricsContext)
5286    4   1    0 org.apache.hadoop.hdfs.server.namenode.metrics.NameNodeMetrics.resetAllMinMax()
5287    3   1    1 org.apache.hadoop.hdfs.server.namenode.metrics.NameNodeStatistics.NameNodeStatistics(NameNodeMetrics)
5288    3   2    1 org.apache.hadoop.hdfs.server.namenode.metrics.NameNodeStatistics.shutdown()
5289    2   1    1 org.apache.hadoop.hdfs.server.namenode.metrics.NameNodeStatistics.getBlockReportAverageTime()
5290    2   1    1 org.apache.hadoop.hdfs.server.namenode.metrics.NameNodeStatistics.getBlockReportMaxTime()
5291    2   1    1 org.apache.hadoop.hdfs.server.namenode.metrics.NameNodeStatistics.getBlockReportMinTime()
5292    2   1    1 org.apache.hadoop.hdfs.server.namenode.metrics.NameNodeStatistics.getBlockReportNum()
5293    2   1    1 org.apache.hadoop.hdfs.server.namenode.metrics.NameNodeStatistics.getJournalTransactionAverageTime()
5294    2   1    1 org.apache.hadoop.hdfs.server.namenode.metrics.NameNodeStatistics.getJournalTransactionNum()
5295    2   1    1 org.apache.hadoop.hdfs.server.namenode.metrics.NameNodeStatistics.getJournalTransactionMaxTime()
5296    2   1    1 org.apache.hadoop.hdfs.server.namenode.metrics.NameNodeStatistics.getJournalTransactionMinTime()
5297    2   1    1 org.apache.hadoop.hdfs.server.namenode.metrics.NameNodeStatistics.getJournalSyncAverageTime()
5298    2   1    1 org.apache.hadoop.hdfs.server.namenode.metrics.NameNodeStatistics.getJournalSyncMaxTime()
5299    2   1    1 org.apache.hadoop.hdfs.server.namenode.metrics.NameNodeStatistics.getJournalSyncMinTime()
5300    2   1    1 org.apache.hadoop.hdfs.server.namenode.metrics.NameNodeStatistics.getJournalSyncNum()
5301    2   1    1 org.apache.hadoop.hdfs.server.namenode.metrics.NameNodeStatistics.getSafemodeTime()
5302    2   1    1 org.apache.hadoop.hdfs.server.namenode.metrics.NameNodeStatistics.getFSImageLoadTime()
5303    2   1    1 org.apache.hadoop.hdfs.server.namenode.metrics.NameNodeStatistics.resetAllMinMax()
5304    2   1    1 org.apache.hadoop.hdfs.server.namenode.metrics.NameNodeStatistics.getNumFilesCreated()
5305    2   1    0 org.apache.hadoop.hdfs.server.namenode.metrics.NameNodeStatistics.getNumFilesListed()
5306    2   1    1 org.apache.hadoop.hdfs.server.namenode.metrics.NameNodeStatistics.getNumGetListingOps()
5307    2   1    1 org.apache.hadoop.hdfs.server.namenode.metrics.NameNodeStatistics.getNumCreateFileOps()
5308    2   1    1 org.apache.hadoop.hdfs.server.namenode.metrics.NameNodeStatistics.getNumDeleteFileOps()
5309    2   1    1 org.apache.hadoop.hdfs.server.namenode.metrics.NameNodeStatistics.getNumAddBlockOps()
5310    2   1    1 org.apache.hadoop.hdfs.server.namenode.metrics.NameNodeStatistics.getNumGetBlockLocations()
5311    2   1    1 org.apache.hadoop.hdfs.server.namenode.metrics.NameNodeStatistics.getNumFilesRenamed()
5312    2   1    1 org.apache.hadoop.hdfs.server.namenode.metrics.NameNodeStatistics.getNumFilesAppended()
5313   11   8    0 org.apache.hadoop.hdfs.server.namenode.NameNode.getProtocolVersion(String,long)
5314    2   1    1 org.apache.hadoop.hdfs.server.namenode.NameNode.format(Configuration)
5315    2   1    0 org.apache.hadoop.hdfs.server.namenode.NameNode.getNameNodeMetrics()
5316    2   1    0 org.apache.hadoop.hdfs.server.namenode.NameNode.getAddress(String)
5317    2   1    0 org.apache.hadoop.hdfs.server.namenode.NameNode.getAddress(Configuration)
5318    4   2    0 org.apache.hadoop.hdfs.server.namenode.NameNode.getUri(InetSocketAddress)
5319   14   1    1 org.apache.hadoop.hdfs.server.namenode.NameNode.initialize(String,Configuration)
5320    2   1    1 org.apache.hadoop.hdfs.server.namenode.NameNode.NameNode(Configuration)
5321    5   3    1 org.apache.hadoop.hdfs.server.namenode.NameNode.NameNode(String,Configuration)
5322    3   2    1 org.apache.hadoop.hdfs.server.namenode.NameNode.join()
5323   14   8    1 org.apache.hadoop.hdfs.server.namenode.NameNode.stop()
5324    4   3    1 org.apache.hadoop.hdfs.server.namenode.NameNode.getBlocks(DatanodeInfo,long)
5325    3   1    1 org.apache.hadoop.hdfs.server.namenode.NameNode.getBlockLocations(String,long,long)
5326    5   2    0 org.apache.hadoop.hdfs.server.namenode.NameNode.getClientMachine()
5327    9   4    1 org.apache.hadoop.hdfs.server.namenode.NameNode.create(String,FsPermission,String,boolean,short,long)
5328    9   4    1 org.apache.hadoop.hdfs.server.namenode.NameNode.append(String,String)
5329    2   1    1 org.apache.hadoop.hdfs.server.namenode.NameNode.setReplication(String,short)
5330    2   1    1 org.apache.hadoop.hdfs.server.namenode.NameNode.setPermission(String,FsPermission)
5331    2   1    1 org.apache.hadoop.hdfs.server.namenode.NameNode.setOwner(String,String,String)
5332    6   2    1 org.apache.hadoop.hdfs.server.namenode.NameNode.addBlock(String,String)
5333    4   3    1 org.apache.hadoop.hdfs.server.namenode.NameNode.abandonBlock(Block,String,String)
5334   10   6    1 org.apache.hadoop.hdfs.server.namenode.NameNode.complete(String,String)
5335    8   3    1 org.apache.hadoop.hdfs.server.namenode.NameNode.reportBadBlocks(LocatedBlock[])
5336    2   1    1 org.apache.hadoop.hdfs.server.namenode.NameNode.nextGenerationStamp(Block)
5337    2   1    1 org.apache.hadoop.hdfs.server.namenode.NameNode.commitBlockSynchronization(Block,long,long,boolean,boolean,DatanodeID[])
5338    2   1    0 org.apache.hadoop.hdfs.server.namenode.NameNode.getPreferredBlockSize(String)
5339    8   4    1 org.apache.hadoop.hdfs.server.namenode.NameNode.rename(String,String)
5340    2   1    0 org.apache.hadoop.hdfs.server.namenode.NameNode.delete(String)
5341    7   3    1 org.apache.hadoop.hdfs.server.namenode.NameNode.delete(String,boolean)
5342    3   2    1 org.apache.hadoop.hdfs.server.namenode.NameNode.checkPathLength(String)
5343    5   3    1 org.apache.hadoop.hdfs.server.namenode.NameNode.mkdirs(String,FsPermission)
5344    2   1    1 org.apache.hadoop.hdfs.server.namenode.NameNode.renewLease(String)
5345    5   2    1 org.apache.hadoop.hdfs.server.namenode.NameNode.getListing(String)
5346    2   1    1 org.apache.hadoop.hdfs.server.namenode.NameNode.getFileInfo(String)
5347    2   1    1 org.apache.hadoop.hdfs.server.namenode.NameNode.getStats()
5348    5   3    1 org.apache.hadoop.hdfs.server.namenode.NameNode.getDatanodeReport(DatanodeReportType)
5349    2   1    1 org.apache.hadoop.hdfs.server.namenode.NameNode.setSafeMode(SafeModeAction)
5350    2   1    1 org.apache.hadoop.hdfs.server.namenode.NameNode.isInSafeMode()
5351    2   1    0 org.apache.hadoop.hdfs.server.namenode.NameNode.refreshNodes()
5352    2   1    1 org.apache.hadoop.hdfs.server.namenode.NameNode.getEditLogSize()
5353    2   1    1 org.apache.hadoop.hdfs.server.namenode.NameNode.rollEditLog()
5354    2   1    1 org.apache.hadoop.hdfs.server.namenode.NameNode.rollFsImage()
5355    2   1    0 org.apache.hadoop.hdfs.server.namenode.NameNode.finalizeUpgrade()
5356    2   1    0 org.apache.hadoop.hdfs.server.namenode.NameNode.distributedUpgradeProgress(UpgradeAction)
5357    2   1    1 org.apache.hadoop.hdfs.server.namenode.NameNode.metaSave(String)
5358    2   1    1 org.apache.hadoop.hdfs.server.namenode.NameNode.getContentSummary(String)
5359    2   1    1 org.apache.hadoop.hdfs.server.namenode.NameNode.setQuota(String,long,long)
5360    2   1    1 org.apache.hadoop.hdfs.server.namenode.NameNode.fsync(String,String)
5361    2   1    1 org.apache.hadoop.hdfs.server.namenode.NameNode.setTimes(String,long,long)
5362    4   1    1 org.apache.hadoop.hdfs.server.namenode.NameNode.register(DatanodeRegistration)
5363    3   1    1 org.apache.hadoop.hdfs.server.namenode.NameNode.sendHeartbeat(DatanodeRegistration,long,long,long,int,int)
5364    8   3    0 org.apache.hadoop.hdfs.server.namenode.NameNode.blockReport(DatanodeRegistration,long[])
5365    5   2    0 org.apache.hadoop.hdfs.server.namenode.NameNode.blockReceived(DatanodeRegistration,Block[],String[])
5366    8   5    1 org.apache.hadoop.hdfs.server.namenode.NameNode.errorReport(DatanodeRegistration,int,String)
5367    2   1    0 org.apache.hadoop.hdfs.server.namenode.NameNode.versionRequest()
5368    2   1    0 org.apache.hadoop.hdfs.server.namenode.NameNode.processUpgradeCommand(UpgradeCommand)
5369    4   3    1 org.apache.hadoop.hdfs.server.namenode.NameNode.verifyRequest(DatanodeRegistration)
5370    3   3    1 org.apache.hadoop.hdfs.server.namenode.NameNode.verifyVersion(int)
5371    2   1    1 org.apache.hadoop.hdfs.server.namenode.NameNode.getFsImageName()
5372    2   1    0 org.apache.hadoop.hdfs.server.namenode.NameNode.getFSImage()
5373    2   1    1 org.apache.hadoop.hdfs.server.namenode.NameNode.getFsImageNameCheckpoint()
5374    2   1    1 org.apache.hadoop.hdfs.server.namenode.NameNode.getNameNodeAddress()
5375    2   1    0 org.apache.hadoop.hdfs.server.namenode.NameNode.getNetworkTopology()
5376   16   7    1 org.apache.hadoop.hdfs.server.namenode.NameNode.format(Configuration,boolean)
5377   13   5    0 org.apache.hadoop.hdfs.server.namenode.NameNode.finalize(Configuration,boolean)
5378    2   1    0 org.apache.hadoop.hdfs.server.namenode.NameNode.printUsage()
5379   26  10    0 org.apache.hadoop.hdfs.server.namenode.NameNode.parseArguments(String[],Configuration)
5380    2   1    0 org.apache.hadoop.hdfs.server.namenode.NameNode.setStartupOption(Configuration,StartupOption)
5381    2   1    0 org.apache.hadoop.hdfs.server.namenode.NameNode.getStartupOption(Configuration)
5382   17   8    0 org.apache.hadoop.hdfs.server.namenode.NameNode.createNameNode(String[],Configuration)
5383    8   3    1 org.apache.hadoop.hdfs.server.namenode.NameNode.main(String[])
5384   29  10    1 org.apache.hadoop.hdfs.server.namenode.NamenodeFsck.NamenodeFsck(Configuration,NameNode,Map,HttpServletResponse)
5385   18   4    1 org.apache.hadoop.hdfs.server.namenode.NamenodeFsck.fsck()
5386  119  46    0 org.apache.hadoop.hdfs.server.namenode.NamenodeFsck.check(FileStatus,FsckResult)
5387   44  15    0 org.apache.hadoop.hdfs.server.namenode.NamenodeFsck.lostFoundMove(FileStatus,LocatedBlocks)
5388   50  18    0 org.apache.hadoop.hdfs.server.namenode.NamenodeFsck.copyBlock(DFSClient,LocatedBlock,OutputStream)
5389    7   5    0 org.apache.hadoop.hdfs.server.namenode.NamenodeFsck.bestNode(DFSClient,DatanodeInfo[],TreeSet)
5390   19   5    0 org.apache.hadoop.hdfs.server.namenode.NamenodeFsck.lostFoundInit(DFSClient)
5391    2   1    1 org.apache.hadoop.hdfs.server.namenode.NamenodeFsck.run(String[])
5392    2   2    1 org.apache.hadoop.hdfs.server.namenode.NamenodeFsck.FsckResult.isHealthy()
5393    3   1    1 org.apache.hadoop.hdfs.server.namenode.NamenodeFsck.FsckResult.addMissing(String,long)
5394    2   1    1 org.apache.hadoop.hdfs.server.namenode.NamenodeFsck.FsckResult.getMissingIds()
5395    2   1    1 org.apache.hadoop.hdfs.server.namenode.NamenodeFsck.FsckResult.getMissingSize()
5396    2   1    0 org.apache.hadoop.hdfs.server.namenode.NamenodeFsck.FsckResult.setMissingSize(long)
5397    2   1    1 org.apache.hadoop.hdfs.server.namenode.NamenodeFsck.FsckResult.getExcessiveReplicas()
5398    2   1    0 org.apache.hadoop.hdfs.server.namenode.NamenodeFsck.FsckResult.setExcessiveReplicas(long)
5399    4   3    1 org.apache.hadoop.hdfs.server.namenode.NamenodeFsck.FsckResult.getReplicationFactor()
5400    2   1    1 org.apache.hadoop.hdfs.server.namenode.NamenodeFsck.FsckResult.getMissingReplicas()
5401    2   1    0 org.apache.hadoop.hdfs.server.namenode.NamenodeFsck.FsckResult.setMissingReplicas(long)
5402    2   1    1 org.apache.hadoop.hdfs.server.namenode.NamenodeFsck.FsckResult.getTotalDirs()
5403    2   1    0 org.apache.hadoop.hdfs.server.namenode.NamenodeFsck.FsckResult.setTotalDirs(long)
5404    2   1    1 org.apache.hadoop.hdfs.server.namenode.NamenodeFsck.FsckResult.getTotalFiles()
5405    2   1    0 org.apache.hadoop.hdfs.server.namenode.NamenodeFsck.FsckResult.setTotalFiles(long)
5406    2   1    1 org.apache.hadoop.hdfs.server.namenode.NamenodeFsck.FsckResult.getTotalOpenFiles()
5407    2   1    1 org.apache.hadoop.hdfs.server.namenode.NamenodeFsck.FsckResult.setTotalOpenFiles(long)
5408    2   1    1 org.apache.hadoop.hdfs.server.namenode.NamenodeFsck.FsckResult.getTotalSize()
5409    2   1    0 org.apache.hadoop.hdfs.server.namenode.NamenodeFsck.FsckResult.setTotalSize(long)
5410    2   1    1 org.apache.hadoop.hdfs.server.namenode.NamenodeFsck.FsckResult.getTotalOpenFilesSize()
5411    2   1    0 org.apache.hadoop.hdfs.server.namenode.NamenodeFsck.FsckResult.setTotalOpenFilesSize(long)
5412    2   1    1 org.apache.hadoop.hdfs.server.namenode.NamenodeFsck.FsckResult.getReplication()
5413    2   1    0 org.apache.hadoop.hdfs.server.namenode.NamenodeFsck.FsckResult.setReplication(int)
5414    2   1    1 org.apache.hadoop.hdfs.server.namenode.NamenodeFsck.FsckResult.getTotalBlocks()
5415    2   1    0 org.apache.hadoop.hdfs.server.namenode.NamenodeFsck.FsckResult.setTotalBlocks(long)
5416    2   1    1 org.apache.hadoop.hdfs.server.namenode.NamenodeFsck.FsckResult.getTotalOpenFilesBlocks()
5417    2   1    0 org.apache.hadoop.hdfs.server.namenode.NamenodeFsck.FsckResult.setTotalOpenFilesBlocks(long)
5418   45  14    0 org.apache.hadoop.hdfs.server.namenode.NamenodeFsck.FsckResult.toString()
5419    2   1    1 org.apache.hadoop.hdfs.server.namenode.NamenodeFsck.FsckResult.getCorruptFiles()
5420    2   1    0 org.apache.hadoop.hdfs.server.namenode.NamenodeFsck.FsckResult.setCorruptFiles(long)
5421    2   1    0 org.apache.hadoop.hdfs.server.namenode.NotReplicatedYetException.NotReplicatedYetException(String)
5422    4   2    0 org.apache.hadoop.hdfs.server.namenode.PendingReplicationBlocks.PendingReplicationBlocks(long)
5423    2   1    0 org.apache.hadoop.hdfs.server.namenode.PendingReplicationBlocks.PendingReplicationBlocks()
5424    5   1    0 org.apache.hadoop.hdfs.server.namenode.PendingReplicationBlocks.init()
5425    8   2    1 org.apache.hadoop.hdfs.server.namenode.PendingReplicationBlocks.add(Block,int)
5426    8   3    1 org.apache.hadoop.hdfs.server.namenode.PendingReplicationBlocks.remove(Block)
5427    2   1    1 org.apache.hadoop.hdfs.server.namenode.PendingReplicationBlocks.size()
5428    6   3    1 org.apache.hadoop.hdfs.server.namenode.PendingReplicationBlocks.getNumReplicas(Block)
5429    7   3    1 org.apache.hadoop.hdfs.server.namenode.PendingReplicationBlocks.getTimedOutBlocks()
5430    3   1    0 org.apache.hadoop.hdfs.server.namenode.PendingReplicationBlocks.PendingBlockInfo.PendingBlockInfo(int)
5431    2   1    0 org.apache.hadoop.hdfs.server.namenode.PendingReplicationBlocks.PendingBlockInfo.getTimeStamp()
5432    2   1    0 org.apache.hadoop.hdfs.server.namenode.PendingReplicationBlocks.PendingBlockInfo.setTimeStamp()
5433    2   1    0 org.apache.hadoop.hdfs.server.namenode.PendingReplicationBlocks.PendingBlockInfo.incrementReplicas(int)
5434    3   1    0 org.apache.hadoop.hdfs.server.namenode.PendingReplicationBlocks.PendingBlockInfo.decrementReplicas()
5435    2   1    0 org.apache.hadoop.hdfs.server.namenode.PendingReplicationBlocks.PendingBlockInfo.getNumReplicas()
5436    7   3    0 org.apache.hadoop.hdfs.server.namenode.PendingReplicationBlocks.PendingReplicationMonitor.run()
5437   14   3    1 org.apache.hadoop.hdfs.server.namenode.PendingReplicationBlocks.PendingReplicationMonitor.pendingReplicationCheck()
5438    5   2    0 org.apache.hadoop.hdfs.server.namenode.PendingReplicationBlocks.stop()
5439    9   2    1 org.apache.hadoop.hdfs.server.namenode.PendingReplicationBlocks.metaSave(PrintWriter)
5440   10   5    0 org.apache.hadoop.hdfs.server.namenode.PermissionChecker.PermissionChecker(String,String)
5441    2   1    0 org.apache.hadoop.hdfs.server.namenode.PermissionChecker.containsGroup(String)
5442   18  11    1 org.apache.hadoop.hdfs.server.namenode.PermissionChecker.checkPermission(String,INodeDirectory,boolean,FsAction,FsAction,FsAction,FsAction)
5443    4   5    0 org.apache.hadoop.hdfs.server.namenode.PermissionChecker.checkOwner(INode)
5444    3   2    0 org.apache.hadoop.hdfs.server.namenode.PermissionChecker.checkTraverse(INode[],int)
5445   10   7    0 org.apache.hadoop.hdfs.server.namenode.PermissionChecker.checkSubAccess(INode,FsAction)
5446    2   2    0 org.apache.hadoop.hdfs.server.namenode.PermissionChecker.check(INode[],int,FsAction)
5447   15  12    0 org.apache.hadoop.hdfs.server.namenode.PermissionChecker.check(INode,FsAction)
5448    4   1    0 org.apache.hadoop.hdfs.server.namenode.ReplicationTargetChooser.ReplicationTargetChooser(boolean,FSNamesystem,NetworkTopology)
5449    2   1    0 org.apache.hadoop.hdfs.server.namenode.ReplicationTargetChooser.NotEnoughReplicasException.NotEnoughReplicasException(String)
5450    4   2    1 org.apache.hadoop.hdfs.server.namenode.ReplicationTargetChooser.chooseTarget(int,DatanodeDescriptor,List,long)
5451   18   8    1 org.apache.hadoop.hdfs.server.namenode.ReplicationTargetChooser.chooseTarget(int,DatanodeDescriptor,List,List,long)
5452   31  15    0 org.apache.hadoop.hdfs.server.namenode.ReplicationTargetChooser.chooseTarget(int,DatanodeDescriptor,List,long,int,List)
5453    9   6    0 org.apache.hadoop.hdfs.server.namenode.ReplicationTargetChooser.chooseLocalNode(DatanodeDescriptor,List,long,int,List)
5454   17  11    0 org.apache.hadoop.hdfs.server.namenode.ReplicationTargetChooser.chooseLocalRack(DatanodeDescriptor,List,long,int,List)
5455    5   2    0 org.apache.hadoop.hdfs.server.namenode.ReplicationTargetChooser.chooseRemoteRack(int,DatanodeDescriptor,List,long,int,List)
5456    9   4    0 org.apache.hadoop.hdfs.server.namenode.ReplicationTargetChooser.chooseRandom(String,List,long,int,List)
5457   13   8    0 org.apache.hadoop.hdfs.server.namenode.ReplicationTargetChooser.chooseRandom(int,String,List,long,int,List)
5458   11   4    0 org.apache.hadoop.hdfs.server.namenode.ReplicationTargetChooser.chooseRandom(int,String,List)
5459    2   1    0 org.apache.hadoop.hdfs.server.namenode.ReplicationTargetChooser.isGoodTarget(DatanodeDescriptor,long,int,List)
5460   27  14    0 org.apache.hadoop.hdfs.server.namenode.ReplicationTargetChooser.isGoodTarget(DatanodeDescriptor,long,int,boolean,List)
5461   23   9    0 org.apache.hadoop.hdfs.server.namenode.ReplicationTargetChooser.getPipeline(DatanodeDescriptor,DatanodeDescriptor[])
5462    3   2    1 org.apache.hadoop.hdfs.server.namenode.ReplicationTargetChooser.verifyBlockPlacement(LocatedBlock,short,NetworkTopology)
5463   12   5    1 org.apache.hadoop.hdfs.server.namenode.ReplicationTargetChooser.verifyBlockPlacement(LocatedBlock,int,NetworkTopology)
5464    2   1    0 org.apache.hadoop.hdfs.server.namenode.SafeModeException.SafeModeException(String,FSNamesystem.SafeModeInfo)
5465    4   2    0 org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.ErrorSimulator.initializeErrorSimulationEvent(int)
5466    5   3    0 org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.ErrorSimulator.getErrorSimulation(int)
5467    3   1    0 org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.ErrorSimulator.setErrorSimulation(int)
5468    3   1    0 org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.ErrorSimulator.clearErrorSimulation(int)
5469    2   1    0 org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.getFSImage()
5470    5   3    1 org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.SecondaryNameNode(Configuration)
5471   27   1    1 org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.initialize(Configuration)
5472   10   5    1 org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.shutdown()
5473   24   9    0 org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.run()
5474   14   1    1 org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.downloadCheckpointFiles(CheckpointSignature)
5475    4   1    1 org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.putFSImage(CheckpointSignature)
5476    5   3    1 org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.getInfoServer()
5477   13   5    1 org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.doCheckpoint()
5478    5   1    0 org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.startCheckpoint()
5479    4   1    1 org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.doMerge(CheckpointSignature)
5480   47  21    1 org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.processArgs(String[])
5481    8   3    1 org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.printUsage(String)
5482    9   2    1 org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.main(String[])
5483    2   1    1 org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.CheckpointStorage.CheckpointStorage()
5484    2   1    0 org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.CheckpointStorage.isConversionNeeded(StorageDirectory)
5485   27  12    1 org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.CheckpointStorage.recoverCreate(Collection,Collection)
5486    9   5    1 org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.CheckpointStorage.startCheckpoint()
5487    8   4    0 org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.CheckpointStorage.endCheckpoint()
5488   17   6    1 org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.CheckpointStorage.doMerge(CheckpointSignature)
5489    1   1    0 org.apache.hadoop.hdfs.server.namenode.SerialNumberManager.SerialNumberManager()
5490    2   1    0 org.apache.hadoop.hdfs.server.namenode.SerialNumberManager.getUserSerialNumber(String)
5491    2   1    0 org.apache.hadoop.hdfs.server.namenode.SerialNumberManager.getGroupSerialNumber(String)
5492    2   1    0 org.apache.hadoop.hdfs.server.namenode.SerialNumberManager.getUser(int)
5493    2   1    0 org.apache.hadoop.hdfs.server.namenode.SerialNumberManager.getGroup(int)
5494    2   1    0 org.apache.hadoop.hdfs.server.namenode.SerialNumberManager.SerialNumberMap.nextSerialNumber()
5495    7   2    0 org.apache.hadoop.hdfs.server.namenode.SerialNumberManager.SerialNumberMap.get(T)
5496    4   3    0 org.apache.hadoop.hdfs.server.namenode.SerialNumberManager.SerialNumberMap.get(int)
5497    2   1    1 org.apache.hadoop.hdfs.server.namenode.SerialNumberManager.SerialNumberMap.toString()
5498   22   5    0 org.apache.hadoop.hdfs.server.namenode.StreamFile.doGet(HttpServletRequest,HttpServletResponse)
5499    2   1    0 org.apache.hadoop.hdfs.server.namenode.StringBytesWritable.StringBytesWritable()
5500    2   1    1 org.apache.hadoop.hdfs.server.namenode.StringBytesWritable.StringBytesWritable(String)
5501    2   1    1 org.apache.hadoop.hdfs.server.namenode.StringBytesWritable.getString()
5502    4   4    1 org.apache.hadoop.hdfs.server.namenode.StringBytesWritable.toString()
5503    2   1    1 org.apache.hadoop.hdfs.server.namenode.StringBytesWritable.equals(String)
5504   27  14    1 org.apache.hadoop.hdfs.server.namenode.TransferFsImage.TransferFsImage(Map,HttpServletRequest,HttpServletResponse)
5505    2   1    0 org.apache.hadoop.hdfs.server.namenode.TransferFsImage.getEdit()
5506    2   1    0 org.apache.hadoop.hdfs.server.namenode.TransferFsImage.getImage()
5507    2   1    0 org.apache.hadoop.hdfs.server.namenode.TransferFsImage.putImage()
5508    2   1    0 org.apache.hadoop.hdfs.server.namenode.TransferFsImage.getToken()
5509    4   4    0 org.apache.hadoop.hdfs.server.namenode.TransferFsImage.getInfoServer()
5510   15   7    1 org.apache.hadoop.hdfs.server.namenode.TransferFsImage.getFileServer(OutputStream,File)
5511   24  10    1 org.apache.hadoop.hdfs.server.namenode.TransferFsImage.getFileClient(String,String,File[])
5512    3   2    0 org.apache.hadoop.hdfs.server.namenode.UnderReplicatedBlocks.UnderReplicatedBlocks()
5513    3   2    1 org.apache.hadoop.hdfs.server.namenode.UnderReplicatedBlocks.clear()
5514    5   2    0 org.apache.hadoop.hdfs.server.namenode.UnderReplicatedBlocks.size()
5515    5   4    0 org.apache.hadoop.hdfs.server.namenode.UnderReplicatedBlocks.contains(Block)
5516   16  12    0 org.apache.hadoop.hdfs.server.namenode.UnderReplicatedBlocks.getPriority(Block,int,int,int)
5517    8   7    0 org.apache.hadoop.hdfs.server.namenode.UnderReplicatedBlocks.add(Block,int,int,int)
5518    3   1    0 org.apache.hadoop.hdfs.server.namenode.UnderReplicatedBlocks.remove(Block,int,int,int)
5519   10   9    0 org.apache.hadoop.hdfs.server.namenode.UnderReplicatedBlocks.remove(Block,int)
5520   10   6    0 org.apache.hadoop.hdfs.server.namenode.UnderReplicatedBlocks.update(Block,int,int,int,int,int)
5521    3   3    0 org.apache.hadoop.hdfs.server.namenode.UnderReplicatedBlocks.Iterator$1.update()
5522    3   1    0 org.apache.hadoop.hdfs.server.namenode.UnderReplicatedBlocks.Iterator$1.next()
5523    3   1    0 org.apache.hadoop.hdfs.server.namenode.UnderReplicatedBlocks.Iterator$1.hasNext()
5524    2   1    0 org.apache.hadoop.hdfs.server.namenode.UnderReplicatedBlocks.Iterator$1.remove()
5525   19   2    0 org.apache.hadoop.hdfs.server.namenode.UnderReplicatedBlocks.iterator()
5526    2   1    0 org.apache.hadoop.hdfs.server.namenode.UpgradeManagerNamenode.getType()
5527   10   4    1 org.apache.hadoop.hdfs.server.namenode.UpgradeManagerNamenode.startUpgrade()
5528   20   8    0 org.apache.hadoop.hdfs.server.namenode.UpgradeManagerNamenode.processUpgradeCommand(UpgradeCommand)
5529    6   1    0 org.apache.hadoop.hdfs.server.namenode.UpgradeManagerNamenode.completeUpgrade()
5530   19   8    0 org.apache.hadoop.hdfs.server.namenode.UpgradeManagerNamenode.distributedUpgradeProgress(UpgradeAction)
5531    6   1    0 org.apache.hadoop.hdfs.server.namenode.UpgradeManagerNamenode.main(String[])
5532    1   1    1 org.apache.hadoop.hdfs.server.namenode.UpgradeObjectNamenode.processUpgradeCommand(UpgradeCommand)
5533    2   1    0 org.apache.hadoop.hdfs.server.namenode.UpgradeObjectNamenode.getType()
5534    2   1    1 org.apache.hadoop.hdfs.server.namenode.UpgradeObjectNamenode.startUpgrade()
5535    2   1    0 org.apache.hadoop.hdfs.server.namenode.UpgradeObjectNamenode.getFSNamesystem()
5536    2   1    0 org.apache.hadoop.hdfs.server.namenode.UpgradeObjectNamenode.forceProceed()
5537    1   1    0 org.apache.hadoop.hdfs.server.protocol.BlockCommand.BlockCommand()
5538    8   2    1 org.apache.hadoop.hdfs.server.protocol.BlockCommand.BlockCommand(int,List)
5539    4   1    1 org.apache.hadoop.hdfs.server.protocol.BlockCommand.BlockCommand(int,Block[])
5540    2   1    0 org.apache.hadoop.hdfs.server.protocol.BlockCommand.getBlocks()
5541    2   1    0 org.apache.hadoop.hdfs.server.protocol.BlockCommand.getTargets()
5542    2   1    0 org.apache.hadoop.hdfs.server.protocol.BlockCommand.WritableFactory$1.newInstance()
5543   10   4    0 org.apache.hadoop.hdfs.server.protocol.BlockCommand.write(DataOutput)
5544   12   4    0 org.apache.hadoop.hdfs.server.protocol.BlockCommand.readFields(DataInput)
5545    2   1    0 org.apache.hadoop.hdfs.server.protocol.BlockMetaDataInfo.WritableFactory$1.newInstance()
5546    1   1    0 org.apache.hadoop.hdfs.server.protocol.BlockMetaDataInfo.BlockMetaDataInfo()
5547    3   1    0 org.apache.hadoop.hdfs.server.protocol.BlockMetaDataInfo.BlockMetaDataInfo(Block,long)
5548    2   1    0 org.apache.hadoop.hdfs.server.protocol.BlockMetaDataInfo.getLastScanTime()
5549    3   1    1 org.apache.hadoop.hdfs.server.protocol.BlockMetaDataInfo.write(DataOutput)
5550    3   1    1 org.apache.hadoop.hdfs.server.protocol.BlockMetaDataInfo.readFields(DataInput)
5551    3   1    1 org.apache.hadoop.hdfs.server.protocol.BlocksWithLocations.BlockWithLocations.BlockWithLocations()
5552    3   1    1 org.apache.hadoop.hdfs.server.protocol.BlocksWithLocations.BlockWithLocations.BlockWithLocations(Block,String[])
5553    2   1    1 org.apache.hadoop.hdfs.server.protocol.BlocksWithLocations.BlockWithLocations.getBlock()
5554    2   1    1 org.apache.hadoop.hdfs.server.protocol.BlocksWithLocations.BlockWithLocations.getDatanodes()
5555    6   2    1 org.apache.hadoop.hdfs.server.protocol.BlocksWithLocations.BlockWithLocations.readFields(DataInput)
5556    5   2    1 org.apache.hadoop.hdfs.server.protocol.BlocksWithLocations.BlockWithLocations.write(DataOutput)
5557    1   1    1 org.apache.hadoop.hdfs.server.protocol.BlocksWithLocations.BlocksWithLocations()
5558    2   1    1 org.apache.hadoop.hdfs.server.protocol.BlocksWithLocations.BlocksWithLocations(BlockWithLocations[])
5559    2   1    1 org.apache.hadoop.hdfs.server.protocol.BlocksWithLocations.getBlocks()
5560    4   2    1 org.apache.hadoop.hdfs.server.protocol.BlocksWithLocations.write(DataOutput)
5561    6   2    1 org.apache.hadoop.hdfs.server.protocol.BlocksWithLocations.readFields(DataInput)
5562    2   1    0 org.apache.hadoop.hdfs.server.protocol.DatanodeCommand.Register.Register()
5563    1   1    0 org.apache.hadoop.hdfs.server.protocol.DatanodeCommand.Register.readFields(DataInput)
5564    1   1    0 org.apache.hadoop.hdfs.server.protocol.DatanodeCommand.Register.write(DataOutput)
5565    2   1    0 org.apache.hadoop.hdfs.server.protocol.DatanodeCommand.Finalize.Finalize()
5566    1   1    0 org.apache.hadoop.hdfs.server.protocol.DatanodeCommand.Finalize.readFields(DataInput)
5567    1   1    0 org.apache.hadoop.hdfs.server.protocol.DatanodeCommand.Finalize.write(DataOutput)
5568    2   1    0 org.apache.hadoop.hdfs.server.protocol.DatanodeCommand.WritableFactory$1.newInstance()
5569    2   1    0 org.apache.hadoop.hdfs.server.protocol.DatanodeCommand.WritableFactory$2.newInstance()
5570    2   1    0 org.apache.hadoop.hdfs.server.protocol.DatanodeCommand.DatanodeCommand()
5571    2   1    0 org.apache.hadoop.hdfs.server.protocol.DatanodeCommand.DatanodeCommand(int)
5572    2   1    0 org.apache.hadoop.hdfs.server.protocol.DatanodeCommand.getAction()
5573    2   1    0 org.apache.hadoop.hdfs.server.protocol.DatanodeCommand.write(DataOutput)
5574    2   1    0 org.apache.hadoop.hdfs.server.protocol.DatanodeCommand.readFields(DataInput)
5575    1   1    1 org.apache.hadoop.hdfs.server.protocol.DatanodeProtocol.register(DatanodeRegistration)
5576    1   1    1 org.apache.hadoop.hdfs.server.protocol.DatanodeProtocol.sendHeartbeat(DatanodeRegistration,long,long,long,int,int)
5577    1   1    1 org.apache.hadoop.hdfs.server.protocol.DatanodeProtocol.blockReport(DatanodeRegistration,long[])
5578    1   1    1 org.apache.hadoop.hdfs.server.protocol.DatanodeProtocol.blockReceived(DatanodeRegistration,Block[],String[])
5579    1   1    1 org.apache.hadoop.hdfs.server.protocol.DatanodeProtocol.errorReport(DatanodeRegistration,int,String)
5580    1   1    0 org.apache.hadoop.hdfs.server.protocol.DatanodeProtocol.versionRequest()
5581    1   1    1 org.apache.hadoop.hdfs.server.protocol.DatanodeProtocol.processUpgradeCommand(UpgradeCommand)
5582    1   1    1 org.apache.hadoop.hdfs.server.protocol.DatanodeProtocol.reportBadBlocks(LocatedBlock[])
5583    1   1    1 org.apache.hadoop.hdfs.server.protocol.DatanodeProtocol.nextGenerationStamp(Block)
5584    1   1    1 org.apache.hadoop.hdfs.server.protocol.DatanodeProtocol.commitBlockSynchronization(Block,long,long,boolean,boolean,DatanodeID[])
5585    2   1    0 org.apache.hadoop.hdfs.server.protocol.DatanodeRegistration.WritableFactory$1.newInstance()
5586    2   1    1 org.apache.hadoop.hdfs.server.protocol.DatanodeRegistration.DatanodeRegistration()
5587    3   1    1 org.apache.hadoop.hdfs.server.protocol.DatanodeRegistration.DatanodeRegistration(String)
5588    2   1    0 org.apache.hadoop.hdfs.server.protocol.DatanodeRegistration.setInfoPort(int)
5589    2   1    0 org.apache.hadoop.hdfs.server.protocol.DatanodeRegistration.setIpcPort(int)
5590    3   1    0 org.apache.hadoop.hdfs.server.protocol.DatanodeRegistration.setStorageInfo(DataStorage)
5591    2   1    0 org.apache.hadoop.hdfs.server.protocol.DatanodeRegistration.setName(String)
5592    2   1    1 org.apache.hadoop.hdfs.server.protocol.DatanodeRegistration.getVersion()
5593    2   1    1 org.apache.hadoop.hdfs.server.protocol.DatanodeRegistration.getRegistrationID()
5594    2   1    0 org.apache.hadoop.hdfs.server.protocol.DatanodeRegistration.toString()
5595    6   1    1 org.apache.hadoop.hdfs.server.protocol.DatanodeRegistration.write(DataOutput)
5596    6   1    1 org.apache.hadoop.hdfs.server.protocol.DatanodeRegistration.readFields(DataInput)
5597    2   1    0 org.apache.hadoop.hdfs.server.protocol.DisallowedDatanodeException.DisallowedDatanodeException(DatanodeID)
5598    1   1    1 org.apache.hadoop.hdfs.server.protocol.InterDatanodeProtocol.getBlockMetaDataInfo(Block)
5599    1   1    1 org.apache.hadoop.hdfs.server.protocol.InterDatanodeProtocol.updateBlock(Block,Block,boolean)
5600    1   1    1 org.apache.hadoop.hdfs.server.protocol.NamenodeProtocol.getBlocks(DatanodeInfo,long)
5601    1   1    1 org.apache.hadoop.hdfs.server.protocol.NamenodeProtocol.getEditLogSize()
5602    1   1    1 org.apache.hadoop.hdfs.server.protocol.NamenodeProtocol.rollEditLog()
5603    1   1    1 org.apache.hadoop.hdfs.server.protocol.NamenodeProtocol.rollFsImage()
5604    3   1    0 org.apache.hadoop.hdfs.server.protocol.NamespaceInfo.NamespaceInfo()
5605    4   1    0 org.apache.hadoop.hdfs.server.protocol.NamespaceInfo.NamespaceInfo(int,long,int)
5606    2   1    0 org.apache.hadoop.hdfs.server.protocol.NamespaceInfo.getBuildVersion()
5607    2   1    0 org.apache.hadoop.hdfs.server.protocol.NamespaceInfo.getDistributedUpgradeVersion()
5608    2   1    0 org.apache.hadoop.hdfs.server.protocol.NamespaceInfo.WritableFactory$1.newInstance()
5609    6   1    0 org.apache.hadoop.hdfs.server.protocol.NamespaceInfo.write(DataOutput)
5610    6   1    0 org.apache.hadoop.hdfs.server.protocol.NamespaceInfo.readFields(DataInput)
5611    4   1    0 org.apache.hadoop.hdfs.server.protocol.UpgradeCommand.UpgradeCommand()
5612    4   1    0 org.apache.hadoop.hdfs.server.protocol.UpgradeCommand.UpgradeCommand(int,int,short)
5613    2   1    0 org.apache.hadoop.hdfs.server.protocol.UpgradeCommand.getVersion()
5614    2   1    0 org.apache.hadoop.hdfs.server.protocol.UpgradeCommand.getCurrentStatus()
5615    2   1    0 org.apache.hadoop.hdfs.server.protocol.UpgradeCommand.WritableFactory$1.newInstance()
5616    4   1    1 org.apache.hadoop.hdfs.server.protocol.UpgradeCommand.write(DataOutput)
5617    4   1    1 org.apache.hadoop.hdfs.server.protocol.UpgradeCommand.readFields(DataInput)
5618    5   3    1 org.apache.hadoop.hdfs.tools.DFSAdmin.DFSAdminCommand.DFSAdminCommand(FileSystem)
5619    5   1    1 org.apache.hadoop.hdfs.tools.DFSAdmin.ClearQuotaCommand.ClearQuotaCommand(String[],int,FileSystem)
5620    2   1    1 org.apache.hadoop.hdfs.tools.DFSAdmin.ClearQuotaCommand.matches(String)
5621    2   1    0 org.apache.hadoop.hdfs.tools.DFSAdmin.ClearQuotaCommand.getCommandName()
5622    2   1    0 org.apache.hadoop.hdfs.tools.DFSAdmin.ClearQuotaCommand.run(Path)
5623    6   1    1 org.apache.hadoop.hdfs.tools.DFSAdmin.SetQuotaCommand.SetQuotaCommand(String[],int,FileSystem)
5624    2   1    1 org.apache.hadoop.hdfs.tools.DFSAdmin.SetQuotaCommand.matches(String)
5625    2   1    0 org.apache.hadoop.hdfs.tools.DFSAdmin.SetQuotaCommand.getCommandName()
5626    2   1    0 org.apache.hadoop.hdfs.tools.DFSAdmin.SetQuotaCommand.run(Path)
5627    5   1    1 org.apache.hadoop.hdfs.tools.DFSAdmin.ClearSpaceQuotaCommand.ClearSpaceQuotaCommand(String[],int,FileSystem)
5628    2   1    1 org.apache.hadoop.hdfs.tools.DFSAdmin.ClearSpaceQuotaCommand.matches(String)
5629    2   1    0 org.apache.hadoop.hdfs.tools.DFSAdmin.ClearSpaceQuotaCommand.getCommandName()
5630    2   1    0 org.apache.hadoop.hdfs.tools.DFSAdmin.ClearSpaceQuotaCommand.run(Path)
5631    7   1    1 org.apache.hadoop.hdfs.tools.DFSAdmin.SetSpaceQuotaCommand.SetSpaceQuotaCommand(String[],int,FileSystem)
5632    2   1    1 org.apache.hadoop.hdfs.tools.DFSAdmin.SetSpaceQuotaCommand.matches(String)
5633    2   1    0 org.apache.hadoop.hdfs.tools.DFSAdmin.SetSpaceQuotaCommand.getCommandName()
5634    2   1    0 org.apache.hadoop.hdfs.tools.DFSAdmin.SetSpaceQuotaCommand.run(Path)
5635    2   1    1 org.apache.hadoop.hdfs.tools.DFSAdmin.DFSAdmin()
5636    2   1    1 org.apache.hadoop.hdfs.tools.DFSAdmin.DFSAdmin(Configuration)
5637   30   6    1 org.apache.hadoop.hdfs.tools.DFSAdmin.report()
5638   33  15    1 org.apache.hadoop.hdfs.tools.DFSAdmin.setSafeMode(String[],int)
5639    9   3    1 org.apache.hadoop.hdfs.tools.DFSAdmin.refreshNodes()
5640   56  12    0 org.apache.hadoop.hdfs.tools.DFSAdmin.printHelp(String)
5641    9   3    1 org.apache.hadoop.hdfs.tools.DFSAdmin.finalizeUpgrade()
5642   24  10    1 org.apache.hadoop.hdfs.tools.DFSAdmin.upgradeProgress(String[],int)
5643    6   1    1 org.apache.hadoop.hdfs.tools.DFSAdmin.metaSave(String[],int)
5644   45  11    1 org.apache.hadoop.hdfs.tools.DFSAdmin.printUsage(String)
5645   98  41    0 org.apache.hadoop.hdfs.tools.DFSAdmin.run(String[])
5646    3   1    1 org.apache.hadoop.hdfs.tools.DFSAdmin.main(String[])
5647    1   1    0 org.apache.hadoop.hdfs.tools.DFSck.DFSck()
5648    2   1    1 org.apache.hadoop.hdfs.tools.DFSck.DFSck(Configuration)
5649    2   1    0 org.apache.hadoop.hdfs.tools.DFSck.getInfoServer()
5650   12   1    1 org.apache.hadoop.hdfs.tools.DFSck.printUsage()
5651   46  15    1 org.apache.hadoop.hdfs.tools.DFSck.run(String[])
5652    7   3    0 org.apache.hadoop.hdfs.tools.DFSck.main(String[])
5653    2   1    0 org.apache.hadoop.mapred.BasicTypeSorterBase.configure(JobConf)
5654    2   1    0 org.apache.hadoop.mapred.BasicTypeSorterBase.setProgressable(Progressable)
5655   12   5    0 org.apache.hadoop.mapred.BasicTypeSorterBase.addKeyValue(int,int,int)
5656    2   1    0 org.apache.hadoop.mapred.BasicTypeSorterBase.setInputBuffer(OutputBuffer)
5657    5   3    0 org.apache.hadoop.mapred.BasicTypeSorterBase.getMemoryUtilized()
5658    1   1    0 org.apache.hadoop.mapred.BasicTypeSorterBase.sort()
5659    9   1    0 org.apache.hadoop.mapred.BasicTypeSorterBase.close()
5660    9   2    0 org.apache.hadoop.mapred.BasicTypeSorterBase.grow()
5661    5   2    0 org.apache.hadoop.mapred.BasicTypeSorterBase.grow(int[],int)
5662    7   1    0 org.apache.hadoop.mapred.MRSortResultIterator.MRSortResultIterator(OutputBuffer,int[],int[],int[],int[])
5663    2   1    0 org.apache.hadoop.mapred.MRSortResultIterator.getProgress()
5664    6   1    0 org.apache.hadoop.mapred.MRSortResultIterator.getKey()
5665    3   1    0 org.apache.hadoop.mapred.MRSortResultIterator.getValue()
5666    6   3    0 org.apache.hadoop.mapred.MRSortResultIterator.next()
5667    2   1    0 org.apache.hadoop.mapred.MRSortResultIterator.close()
5668    4   1    0 org.apache.hadoop.mapred.MRSortResultIterator.InMemUncompressedBytes.reset(OutputBuffer,int,int)
5669    2   1    0 org.apache.hadoop.mapred.MRSortResultIterator.InMemUncompressedBytes.getSize()
5670    2   1    0 org.apache.hadoop.mapred.MRSortResultIterator.InMemUncompressedBytes.writeUncompressedBytes(DataOutputStream)
5671    2   2    0 org.apache.hadoop.mapred.MRSortResultIterator.InMemUncompressedBytes.writeCompressedBytes(DataOutputStream)
5672    9   5    0 org.apache.hadoop.mapred.Child.Thread$1.run()
5673   81  14    0 org.apache.hadoop.mapred.Child.main(String[])
5674    1   1    0 org.apache.hadoop.mapred.ClusterStatus.ClusterStatus()
5675    7   1    1 org.apache.hadoop.mapred.ClusterStatus.ClusterStatus(int,int,int,int,int,JobTracker.State)
5676    2   1    1 org.apache.hadoop.mapred.ClusterStatus.getTaskTrackers()
5677    2   1    1 org.apache.hadoop.mapred.ClusterStatus.getMapTasks()
5678    2   1    1 org.apache.hadoop.mapred.ClusterStatus.getReduceTasks()
5679    2   1    1 org.apache.hadoop.mapred.ClusterStatus.getMaxMapTasks()
5680    2   1    1 org.apache.hadoop.mapred.ClusterStatus.getMaxReduceTasks()
5681    2   1    1 org.apache.hadoop.mapred.ClusterStatus.getJobTrackerState()
5682    7   1    0 org.apache.hadoop.mapred.ClusterStatus.write(DataOutput)
5683    7   1    0 org.apache.hadoop.mapred.ClusterStatus.readFields(DataInput)
5684    2   1    0 org.apache.hadoop.mapred.CommitTaskAction.CommitTaskAction()
5685    3   1    0 org.apache.hadoop.mapred.CommitTaskAction.CommitTaskAction(TaskAttemptID)
5686    2   1    0 org.apache.hadoop.mapred.CommitTaskAction.getTaskID()
5687    2   1    0 org.apache.hadoop.mapred.CommitTaskAction.write(DataOutput)
5688    2   1    0 org.apache.hadoop.mapred.CommitTaskAction.readFields(DataInput)
5689   11   4    0 org.apache.hadoop.mapred.CompletedJobStatusStore.CompletedJobStatusStore(Configuration,FileSystem)
5690    2   1    1 org.apache.hadoop.mapred.CompletedJobStatusStore.isActive()
5691    7   4    0 org.apache.hadoop.mapred.CompletedJobStatusStore.run()
5692   10   5    0 org.apache.hadoop.mapred.CompletedJobStatusStore.deleteJobStatusDirs()
5693    2   1    0 org.apache.hadoop.mapred.CompletedJobStatusStore.getInfoFilePath(JobID)
5694   17   6    1 org.apache.hadoop.mapred.CompletedJobStatusStore.store(JobInProgress)
5695    3   2    0 org.apache.hadoop.mapred.CompletedJobStatusStore.getJobInfoFile(JobID)
5696    4   1    0 org.apache.hadoop.mapred.CompletedJobStatusStore.readJobStatus(FSDataInputStream)
5697    4   1    0 org.apache.hadoop.mapred.CompletedJobStatusStore.readJobProfile(FSDataInputStream)
5698    4   1    0 org.apache.hadoop.mapred.CompletedJobStatusStore.readCounters(FSDataInputStream)
5699   13   6    0 org.apache.hadoop.mapred.CompletedJobStatusStore.readEvents(FSDataInputStream,int,int)
5700   10   4    1 org.apache.hadoop.mapred.CompletedJobStatusStore.readJobStatus(JobID)
5701   11   4    1 org.apache.hadoop.mapred.CompletedJobStatusStore.readJobProfile(JobID)
5702   12   4    1 org.apache.hadoop.mapred.CompletedJobStatusStore.readCounters(JobID)
5703   13   4    1 org.apache.hadoop.mapred.CompletedJobStatusStore.readJobTaskCompletionEvents(JobID,int,int)
5704    2   1    0 org.apache.hadoop.mapred.Counters.Counter.Counter()
5705    4   1    0 org.apache.hadoop.mapred.Counters.Counter.Counter(String,String,long)
5706    7   2    1 org.apache.hadoop.mapred.Counters.Counter.readFields(DataInput)
5707    7   2    1 org.apache.hadoop.mapred.Counters.Counter.write(DataOutput)
5708    2   1    1 org.apache.hadoop.mapred.Counters.Counter.getName()
5709    2   1    1 org.apache.hadoop.mapred.Counters.Counter.getDisplayName()
5710    2   1    1 org.apache.hadoop.mapred.Counters.Counter.setDisplayName(String)
5711   14   1    1 org.apache.hadoop.mapred.Counters.Counter.makeEscapedCompactString()
5712    2   3    0 org.apache.hadoop.mapred.Counters.Counter.contentEquals(Counter)
5713    2   1    1 org.apache.hadoop.mapred.Counters.Counter.getCounter()
5714    2   1    1 org.apache.hadoop.mapred.Counters.Counter.increment(long)
5715    6   3    0 org.apache.hadoop.mapred.Counters.Group.Group(String)
5716    3   1    1 org.apache.hadoop.mapred.Counters.Group.getResourceBundle(String)
5717    2   1    1 org.apache.hadoop.mapred.Counters.Group.getName()
5718    2   1    1 org.apache.hadoop.mapred.Counters.Group.getDisplayName()
5719    2   1    1 org.apache.hadoop.mapred.Counters.Group.setDisplayName(String)
5720   13   2    1 org.apache.hadoop.mapred.Counters.Group.makeEscapedCompactString()
5721   13   5    1 org.apache.hadoop.mapred.Counters.Group.contentEquals(Group)
5722    5   5    1 org.apache.hadoop.mapred.Counters.Group.getCounter(String)
5723    2   1    0 org.apache.hadoop.mapred.Counters.Group.getCounter(int,String)
5724    7   2    1 org.apache.hadoop.mapred.Counters.Group.getCounterForName(String)
5725    2   1    1 org.apache.hadoop.mapred.Counters.Group.size()
5726    6   3    1 org.apache.hadoop.mapred.Counters.Group.localize(String,String)
5727    5   2    0 org.apache.hadoop.mapred.Counters.Group.write(DataOutput)
5728    8   2    0 org.apache.hadoop.mapred.Counters.Group.readFields(DataInput)
5729    2   1    0 org.apache.hadoop.mapred.Counters.Group.iterator()
5730    2   1    1 org.apache.hadoop.mapred.Counters.getGroupNames()
5731    2   1    0 org.apache.hadoop.mapred.Counters.iterator()
5732    6   2    1 org.apache.hadoop.mapred.Counters.getGroup(String)
5733    7   2    1 org.apache.hadoop.mapred.Counters.findCounter(Enum)
5734    2   1    1 org.apache.hadoop.mapred.Counters.findCounter(String,String)
5735    2   1    0 org.apache.hadoop.mapred.Counters.findCounter(String,int,String)
5736    2   1    1 org.apache.hadoop.mapred.Counters.incrCounter(Enum,long)
5737    2   1    1 org.apache.hadoop.mapred.Counters.incrCounter(String,String,long)
5738    2   1    1 org.apache.hadoop.mapred.Counters.getCounter(Enum)
5739    8   3    1 org.apache.hadoop.mapred.Counters.incrAllCounters(Counters)
5740    5   1    1 org.apache.hadoop.mapred.Counters.sum(Counters,Counters)
5741    5   2    1 org.apache.hadoop.mapred.Counters.size()
5742    5   2    1 org.apache.hadoop.mapred.Counters.write(DataOutput)
5743    8   2    1 org.apache.hadoop.mapred.Counters.readFields(DataInput)
5744    6   3    1 org.apache.hadoop.mapred.Counters.log(Log)
5745    7   3    1 org.apache.hadoop.mapred.Counters.toString()
5746   15   4    1 org.apache.hadoop.mapred.Counters.makeCompactString()
5747    5   2    1 org.apache.hadoop.mapred.Counters.makeEscapedCompactString()
5748   14   5    0 org.apache.hadoop.mapred.Counters.getBlock(String,char,char,IntWritable)
5749   26   3    1 org.apache.hadoop.mapred.Counters.fromEscapedCompactString(String)
5750    2   1    0 org.apache.hadoop.mapred.Counters.escape(String)
5751    2   1    0 org.apache.hadoop.mapred.Counters.unescape(String)
5752   13   5    0 org.apache.hadoop.mapred.Counters.contentEquals(Counters)
5753    2   1    1 org.apache.hadoop.mapred.DefaultJobHistoryParser.parseJobTasks(String,JobHistory.JobInfo,FileSystem)
5754    2   1    0 org.apache.hadoop.mapred.DefaultJobHistoryParser.JobTasksParseListener.JobTasksParseListener(JobHistory.JobInfo)
5755    7   2    0 org.apache.hadoop.mapred.DefaultJobHistoryParser.JobTasksParseListener.getTask(String)
5756    8   2    0 org.apache.hadoop.mapred.DefaultJobHistoryParser.JobTasksParseListener.getMapAttempt(String,String,String,String)
5757    8   2    0 org.apache.hadoop.mapred.DefaultJobHistoryParser.JobTasksParseListener.getReduceAttempt(String,String,String,String)
5758   18   5    0 org.apache.hadoop.mapred.DefaultJobHistoryParser.JobTasksParseListener.handle(JobHistory.RecordTypes,Map)
5759    2   1    0 org.apache.hadoop.mapred.DefaultJobHistoryParser.NodesFilter.getValues()
5760   12   5    0 org.apache.hadoop.mapred.DefaultJobHistoryParser.NodesFilter.handle(JobHistory.RecordTypes,Map)
5761    1   1    0 org.apache.hadoop.mapred.DefaultJobHistoryParser.NodesFilter.setFailureType()
5762    2   1    0 org.apache.hadoop.mapred.DefaultJobHistoryParser.NodesFilter.getFailureType()
5763    2   1    0 org.apache.hadoop.mapred.DefaultJobHistoryParser.NodesFilter.NodesFilter()
5764    2   1    0 org.apache.hadoop.mapred.DefaultJobHistoryParser.FailedOnNodesFilter.setFailureType()
5765    2   1    0 org.apache.hadoop.mapred.DefaultJobHistoryParser.KilledOnNodesFilter.setFailureType()
5766    2   1    0 org.apache.hadoop.mapred.DisallowedTaskTrackerException.DisallowedTaskTrackerException(TaskTrackerStatus)
5767   15   6    0 org.apache.hadoop.mapred.EagerTaskInitializationListener.JobInitThread.run()
5768    3   1    0 org.apache.hadoop.mapred.EagerTaskInitializationListener.start()
5769    7   4    0 org.apache.hadoop.mapred.EagerTaskInitializationListener.terminate()
5770    5   1    0 org.apache.hadoop.mapred.EagerTaskInitializationListener.jobAdded(JobInProgress)
5771    8   4    0 org.apache.hadoop.mapred.EagerTaskInitializationListener.Comparator$1.compare(JobInProgress,JobInProgress)
5772   12   1    1 org.apache.hadoop.mapred.EagerTaskInitializationListener.resortInitQueue()
5773    3   1    0 org.apache.hadoop.mapred.EagerTaskInitializationListener.jobRemoved(JobInProgress)
5774    3   2    0 org.apache.hadoop.mapred.EagerTaskInitializationListener.jobUpdated(JobChangeEvent)
5775    4   3    0 org.apache.hadoop.mapred.EagerTaskInitializationListener.jobStateChanged(JobStatusChangeEvent)
5776    2   1    0 org.apache.hadoop.mapred.FileAlreadyExistsException.FileAlreadyExistsException()
5777    2   1    0 org.apache.hadoop.mapred.FileAlreadyExistsException.FileAlreadyExistsException(String)
5778    3   2    0 org.apache.hadoop.mapred.FileInputFormat.PathFilter$1.accept(Path)
5779    2   1    0 org.apache.hadoop.mapred.FileInputFormat.setMinSplitSize(long)
5780    2   1    0 org.apache.hadoop.mapred.FileInputFormat.MultiPathFilter.MultiPathFilter(List)
5781    5   4    0 org.apache.hadoop.mapred.FileInputFormat.MultiPathFilter.accept(Path)
5782    2   1    1 org.apache.hadoop.mapred.FileInputFormat.isSplitable(FileSystem,Path)
5783    1   1    0 org.apache.hadoop.mapred.FileInputFormat.getRecordReader(InputSplit,JobConf,Reporter)
5784    2   1    1 org.apache.hadoop.mapred.FileInputFormat.setInputPathFilter(JobConf,Class)
5785    3   2    1 org.apache.hadoop.mapred.FileInputFormat.getInputPathFilter(JobConf)
5786   31  12    1 org.apache.hadoop.mapred.FileInputFormat.listStatus(JobConf)
5787   32  11    0 org.apache.hadoop.mapred.FileInputFormat.getSplits(JobConf,int)
5788    2   1    0 org.apache.hadoop.mapred.FileInputFormat.computeSplitSize(long,long,long)
5789    7   6    0 org.apache.hadoop.mapred.FileInputFormat.getBlockIndex(BlockLocation[],long)
5790    2   1    1 org.apache.hadoop.mapred.FileInputFormat.setInputPaths(JobConf,String)
5791    3   2    1 org.apache.hadoop.mapred.FileInputFormat.addInputPaths(JobConf,String)
5792    8   2    1 org.apache.hadoop.mapred.FileInputFormat.setInputPaths(JobConf,Path)
5793    5   2    1 org.apache.hadoop.mapred.FileInputFormat.addInputPath(JobConf,Path)
5794   26   9    0 org.apache.hadoop.mapred.FileInputFormat.getPathStrings(String)
5795    7   2    1 org.apache.hadoop.mapred.FileInputFormat.getInputPaths(JobConf)
5796    8   3    0 org.apache.hadoop.mapred.FileOutputCommitter.setupJob(JobContext)
5797    9   3    0 org.apache.hadoop.mapred.FileOutputCommitter.cleanupJob(JobContext)
5798    1   1    0 org.apache.hadoop.mapred.FileOutputCommitter.setupTask(TaskAttemptContext)
5799   13   4    0 org.apache.hadoop.mapred.FileOutputCommitter.commitTask(TaskAttemptContext)
5800   19  10    0 org.apache.hadoop.mapred.FileOutputCommitter.moveTaskOutputs(TaskAttemptContext,FileSystem,Path,Path)
5801    7   2    0 org.apache.hadoop.mapred.FileOutputCommitter.abortTask(TaskAttemptContext)
5802    6   3    0 org.apache.hadoop.mapred.FileOutputCommitter.getFinalPath(Path,Path,Path)
5803   10   6    0 org.apache.hadoop.mapred.FileOutputCommitter.needsTaskCommit(TaskAttemptContext)
5804   11   5    0 org.apache.hadoop.mapred.FileOutputCommitter.getTempTaskOutputPath(TaskAttemptContext)
5805   10   5    0 org.apache.hadoop.mapred.FileOutputCommitter.getWorkPath(TaskAttemptContext,Path)
5806    2   1    1 org.apache.hadoop.mapred.FileOutputFormat.setCompressOutput(JobConf,boolean)
5807    2   1    1 org.apache.hadoop.mapred.FileOutputFormat.getCompressOutput(JobConf)
5808    3   1    1 org.apache.hadoop.mapred.FileOutputFormat.setOutputCompressorClass(JobConf,Class)
5809    8   4    1 org.apache.hadoop.mapred.FileOutputFormat.getOutputCompressorClass(JobConf,Class)
5810    1   1    0 org.apache.hadoop.mapred.FileOutputFormat.getRecordWriter(FileSystem,JobConf,String,Progressable)
5811    6   7    0 org.apache.hadoop.mapred.FileOutputFormat.checkOutputSpecs(FileSystem,JobConf)
5812    3   1    1 org.apache.hadoop.mapred.FileOutputFormat.setOutputPath(JobConf,Path)
5813    3   1    1 org.apache.hadoop.mapred.FileOutputFormat.setWorkOutputPath(JobConf,Path)
5814    3   2    1 org.apache.hadoop.mapred.FileOutputFormat.getOutputPath(JobConf)
5815    3   2    1 org.apache.hadoop.mapred.FileOutputFormat.getWorkOutputPath(JobConf)
5816   10   4    1 org.apache.hadoop.mapred.FileOutputFormat.getTaskOutputPath(JobConf,String)
5817    9   4    1 org.apache.hadoop.mapred.FileOutputFormat.getUniqueName(JobConf,String)
5818    2   1    1 org.apache.hadoop.mapred.FileOutputFormat.getPathForCustomFile(JobConf,String)
5819    1   1    0 org.apache.hadoop.mapred.FileSplit.FileSplit()
5820    2   1    0 org.apache.hadoop.mapred.FileSplit.FileSplit(Path,long,long,JobConf)
5821    5   1    1 org.apache.hadoop.mapred.FileSplit.FileSplit(Path,long,long,String[])
5822    2   1    1 org.apache.hadoop.mapred.FileSplit.getPath()
5823    2   1    1 org.apache.hadoop.mapred.FileSplit.getStart()
5824    2   1    1 org.apache.hadoop.mapred.FileSplit.getLength()
5825    2   1    0 org.apache.hadoop.mapred.FileSplit.toString()
5826    4   1    0 org.apache.hadoop.mapred.FileSplit.write(DataOutput)
5827    5   1    0 org.apache.hadoop.mapred.FileSplit.readFields(DataInput)
5828    5   3    0 org.apache.hadoop.mapred.FileSplit.getLocations()
5829    1   1    0 org.apache.hadoop.mapred.HeartbeatResponse.HeartbeatResponse()
5830    4   1    0 org.apache.hadoop.mapred.HeartbeatResponse.HeartbeatResponse(short,TaskTrackerAction[])
5831    2   1    0 org.apache.hadoop.mapred.HeartbeatResponse.setResponseId(short)
5832    2   1    0 org.apache.hadoop.mapred.HeartbeatResponse.getResponseId()
5833    2   1    0 org.apache.hadoop.mapred.HeartbeatResponse.setLastKnownIndices(Map)
5834    2   1    0 org.apache.hadoop.mapred.HeartbeatResponse.getLastKnownIndex()
5835    2   1    0 org.apache.hadoop.mapred.HeartbeatResponse.setActions(TaskTrackerAction[])
5836    2   1    0 org.apache.hadoop.mapred.HeartbeatResponse.getActions()
5837    2   1    0 org.apache.hadoop.mapred.HeartbeatResponse.setConf(Configuration)
5838    2   1    0 org.apache.hadoop.mapred.HeartbeatResponse.getConf()
5839    2   1    0 org.apache.hadoop.mapred.HeartbeatResponse.setHeartbeatInterval(int)
5840    2   1    0 org.apache.hadoop.mapred.HeartbeatResponse.getHeartbeatInterval()
5841   17   5    0 org.apache.hadoop.mapred.HeartbeatResponse.write(DataOutput)
5842   19   5    0 org.apache.hadoop.mapred.HeartbeatResponse.readFields(DataInput)
5843    2   1    1 org.apache.hadoop.mapred.ID.ID(int)
5844    1   1    0 org.apache.hadoop.mapred.ID.ID()
5845    2   1    1 org.apache.hadoop.mapred.ID.getId()
5846    2   1    0 org.apache.hadoop.mapred.ID.toString()
5847    2   1    0 org.apache.hadoop.mapred.ID.hashCode()
5848    8   5    0 org.apache.hadoop.mapred.ID.equals(Object)
5849    2   1    1 org.apache.hadoop.mapred.ID.compareTo(ID)
5850    2   1    0 org.apache.hadoop.mapred.ID.readFields(DataInput)
5851    2   1    0 org.apache.hadoop.mapred.ID.write(DataOutput)
5852    4   1    0 org.apache.hadoop.mapred.ID.read(DataInput)
5853    7   6    1 org.apache.hadoop.mapred.ID.forName(String)
5854    3   1    0 org.apache.hadoop.mapred.IFile.Writer.Writer(Configuration,FileSystem,Path,Class,Class,CompressionCodec)
5855   19   2    0 org.apache.hadoop.mapred.IFile.Writer.Writer(Configuration,FSDataOutputStream,Class,Class,CompressionCodec)
5856   17   3    0 org.apache.hadoop.mapred.IFile.Writer.close()
5857   18   9    0 org.apache.hadoop.mapred.IFile.Writer.append(K,V)
5858   12   5    0 org.apache.hadoop.mapred.IFile.Writer.append(DataInputBuffer,DataInputBuffer)
5859    2   1    0 org.apache.hadoop.mapred.IFile.Writer.getRawLength()
5860    2   1    0 org.apache.hadoop.mapred.IFile.Writer.getCompressedLength()
5861    2   1    1 org.apache.hadoop.mapred.IFile.Reader.Reader(Configuration,FileSystem,Path,CompressionCodec)
5862   10   3    1 org.apache.hadoop.mapred.IFile.Reader.Reader(Configuration,FSDataInputStream,long,CompressionCodec)
5863    2   1    0 org.apache.hadoop.mapred.IFile.Reader.getLength()
5864    2   1    0 org.apache.hadoop.mapred.IFile.Reader.getPosition()
5865    8   4    1 org.apache.hadoop.mapred.IFile.Reader.readData(byte[],int,int)
5866    6   3    0 org.apache.hadoop.mapred.IFile.Reader.readNextBlock(int)
5867    7   2    0 org.apache.hadoop.mapred.IFile.Reader.rejigData(byte[],byte[])
5868   32  16    0 org.apache.hadoop.mapred.IFile.Reader.next(DataInputBuffer,DataInputBuffer)
5869    8   2    0 org.apache.hadoop.mapred.IFile.Reader.close()
5870    7   1    0 org.apache.hadoop.mapred.IFile.InMemoryReader.InMemoryReader(RamManager,TaskAttemptID,byte[],int,int)
5871    2   1    0 org.apache.hadoop.mapred.IFile.InMemoryReader.getPosition()
5872    2   1    0 org.apache.hadoop.mapred.IFile.InMemoryReader.getLength()
5873    8   2    0 org.apache.hadoop.mapred.IFile.InMemoryReader.dumpOnError()
5874   29  15    0 org.apache.hadoop.mapred.IFile.InMemoryReader.next(DataInputBuffer,DataInputBuffer)
5875    4   1    0 org.apache.hadoop.mapred.IFile.InMemoryReader.close()
5876    7   1    1 org.apache.hadoop.mapred.IFileInputStream.IFileInputStream(InputStream,long)
5877    2   1    0 org.apache.hadoop.mapred.IFileInputStream.close()
5878    2   2    0 org.apache.hadoop.mapred.IFileInputStream.skip(long)
5879    2   2    0 org.apache.hadoop.mapred.IFileInputStream.getPosition()
5880    2   1    0 org.apache.hadoop.mapred.IFileInputStream.getSize()
5881    4   3    1 org.apache.hadoop.mapred.IFileInputStream.read(byte[],int,int)
5882   18   8    1 org.apache.hadoop.mapred.IFileInputStream.readWithChecksum(byte[],int,int)
5883   14   7    0 org.apache.hadoop.mapred.IFileInputStream.doRead(byte[],int,int)
5884    7   3    0 org.apache.hadoop.mapred.IFileInputStream.read()
5885    2   1    0 org.apache.hadoop.mapred.IFileInputStream.getChecksum()
5886    4   1    1 org.apache.hadoop.mapred.IFileOutputStream.IFileOutputStream(OutputStream)
5887    7   3    0 org.apache.hadoop.mapred.IFileOutputStream.close()
5888    3   1    0 org.apache.hadoop.mapred.IFileOutputStream.write(byte[],int,int)
5889    3   1    0 org.apache.hadoop.mapred.IFileOutputStream.write(int)
5890    4   1    0 org.apache.hadoop.mapred.IndexCache.IndexCache(JobConf)
5891   15   8    1 org.apache.hadoop.mapred.IndexCache.getIndexInformation(String,int,Path)
5892   26   9    0 org.apache.hadoop.mapred.IndexCache.readIndexFileToCache(Path,String)
5893    8   3    1 org.apache.hadoop.mapred.IndexCache.removeMap(String)
5894    6   3    1 org.apache.hadoop.mapred.IndexCache.freeIndexInformation()
5895    2   2    0 org.apache.hadoop.mapred.IndexCache.IndexInformation.getSize()
5896    4   1    0 org.apache.hadoop.mapred.IndexRecord.IndexRecord(long,long,long)
5897   17   2    0 org.apache.hadoop.mapred.IndexRecord.readIndexFile(Path,JobConf)
5898    1   1    1 org.apache.hadoop.mapred.InputFormat.getSplits(JobConf,int)
5899    1   1    1 org.apache.hadoop.mapred.InputFormat.getRecordReader(InputSplit,JobConf,Reporter)
5900    1   1    1 org.apache.hadoop.mapred.InputSplit.getLength()
5901    1   1    1 org.apache.hadoop.mapred.InputSplit.getLocations()
5902    1   1    1 org.apache.hadoop.mapred.InterTrackerProtocol.heartbeat(TaskTrackerStatus,boolean,boolean,short)
5903    1   1    1 org.apache.hadoop.mapred.InterTrackerProtocol.getFilesystemName()
5904    1   1    1 org.apache.hadoop.mapred.InterTrackerProtocol.reportTaskTrackerError(String,String,String)
5905    1   1    1 org.apache.hadoop.mapred.InterTrackerProtocol.getTaskCompletionEvents(JobID,int,int)
5906    1   1    1 org.apache.hadoop.mapred.InterTrackerProtocol.getSystemDir()
5907    1   1    1 org.apache.hadoop.mapred.InterTrackerProtocol.getBuildVersion()
5908    2   1    0 org.apache.hadoop.mapred.InvalidFileTypeException.InvalidFileTypeException()
5909    2   1    0 org.apache.hadoop.mapred.InvalidFileTypeException.InvalidFileTypeException(String)
5910    2   1    1 org.apache.hadoop.mapred.InvalidInputException.InvalidInputException(List)
5911    2   1    1 org.apache.hadoop.mapred.InvalidInputException.getProblems()
5912    8   3    1 org.apache.hadoop.mapred.InvalidInputException.getMessage()
5913    2   1    0 org.apache.hadoop.mapred.InvalidJobConfException.InvalidJobConfException()
5914    2   1    0 org.apache.hadoop.mapred.InvalidJobConfException.InvalidJobConfException(String)
5915    2   1    0 org.apache.hadoop.mapred.IsolationRunner.FakeUmbilical.getProtocolVersion(String,long)
5916    2   1    0 org.apache.hadoop.mapred.IsolationRunner.FakeUmbilical.done(TaskAttemptID)
5917    2   1    0 org.apache.hadoop.mapred.IsolationRunner.FakeUmbilical.fsError(TaskAttemptID,String)
5918    2   1    0 org.apache.hadoop.mapred.IsolationRunner.FakeUmbilical.shuffleError(TaskAttemptID,String)
5919    2   1    0 org.apache.hadoop.mapred.IsolationRunner.FakeUmbilical.getTask(JVMId)
5920    2   1    0 org.apache.hadoop.mapred.IsolationRunner.FakeUmbilical.ping(TaskAttemptID)
5921    2   1    0 org.apache.hadoop.mapred.IsolationRunner.FakeUmbilical.commitPending(TaskAttemptID,TaskStatus)
5922    2   1    0 org.apache.hadoop.mapred.IsolationRunner.FakeUmbilical.canCommit(TaskAttemptID)
5923   11   2    0 org.apache.hadoop.mapred.IsolationRunner.FakeUmbilical.statusUpdate(TaskAttemptID,TaskStatus)
5924    2   1    0 org.apache.hadoop.mapred.IsolationRunner.FakeUmbilical.reportDiagnosticInfo(TaskAttemptID,String)
5925    2   1    0 org.apache.hadoop.mapred.IsolationRunner.FakeUmbilical.getMapCompletionEvents(JobID,int,int,TaskAttemptID)
5926    2   1    0 org.apache.hadoop.mapred.IsolationRunner.FakeUmbilical.reportNextRecordRange(TaskAttemptID,SortedRanges.Range)
5927   11   4    0 org.apache.hadoop.mapred.IsolationRunner.makeClassLoader(JobConf,File)
5928   11   3    1 org.apache.hadoop.mapred.IsolationRunner.fillInMissingMapOutputs(FileSystem,TaskAttemptID,int,JobConf)
5929   35   5    1 org.apache.hadoop.mapred.IsolationRunner.main(String[])
5930    2   1    0 org.apache.hadoop.mapred.JobChangeEvent.JobChangeEvent(JobInProgress)
5931    2   1    1 org.apache.hadoop.mapred.JobChangeEvent.getJobInProgress()
5932    4   1    1 org.apache.hadoop.mapred.JobClient.NetworkedJob.NetworkedJob(JobStatus)
5933    3   2    1 org.apache.hadoop.mapred.JobClient.NetworkedJob.ensureFreshStatus()
5934    3   1    1 org.apache.hadoop.mapred.JobClient.NetworkedJob.updateStatus()
5935    2   1    1 org.apache.hadoop.mapred.JobClient.NetworkedJob.getID()
5936    2   1    0 org.apache.hadoop.mapred.JobClient.NetworkedJob.getJobID()
5937    2   1    1 org.apache.hadoop.mapred.JobClient.NetworkedJob.getJobName()
5938    2   1    1 org.apache.hadoop.mapred.JobClient.NetworkedJob.getJobFile()
5939    2   1    1 org.apache.hadoop.mapred.JobClient.NetworkedJob.getTrackingURL()
5940    3   1    1 org.apache.hadoop.mapred.JobClient.NetworkedJob.mapProgress()
5941    3   1    1 org.apache.hadoop.mapred.JobClient.NetworkedJob.reduceProgress()
5942    3   1    1 org.apache.hadoop.mapred.JobClient.NetworkedJob.cleanupProgress()
5943    3   1    1 org.apache.hadoop.mapred.JobClient.NetworkedJob.setupProgress()
5944    3   3    1 org.apache.hadoop.mapred.JobClient.NetworkedJob.isComplete()
5945    3   1    1 org.apache.hadoop.mapred.JobClient.NetworkedJob.isSuccessful()
5946    4   3    1 org.apache.hadoop.mapred.JobClient.NetworkedJob.waitForCompletion()
5947    3   1    1 org.apache.hadoop.mapred.JobClient.NetworkedJob.getJobState()
5948    2   1    1 org.apache.hadoop.mapred.JobClient.NetworkedJob.killJob()
5949    2   1    1 org.apache.hadoop.mapred.JobClient.NetworkedJob.setJobPriority(String)
5950    2   1    1 org.apache.hadoop.mapred.JobClient.NetworkedJob.killTask(TaskAttemptID,boolean)
5951    2   1    0 org.apache.hadoop.mapred.JobClient.NetworkedJob.killTask(String,boolean)
5952    2   1    1 org.apache.hadoop.mapred.JobClient.NetworkedJob.getTaskCompletionEvents(int)
5953    4   2    0 org.apache.hadoop.mapred.JobClient.NetworkedJob.toString()
5954    2   1    1 org.apache.hadoop.mapred.JobClient.NetworkedJob.getCounters()
5955    1   1    1 org.apache.hadoop.mapred.JobClient.JobClient()
5956    3   1    1 org.apache.hadoop.mapred.JobClient.JobClient(JobConf)
5957    2   1    1 org.apache.hadoop.mapred.JobClient.setCommandLineConfig(Configuration)
5958    2   1    1 org.apache.hadoop.mapred.JobClient.getCommandLineConfig()
5959    6   2    1 org.apache.hadoop.mapred.JobClient.init(JobConf)
5960    2   1    0 org.apache.hadoop.mapred.JobClient.createRPCProxy(InetSocketAddress,Configuration)
5961    2   1    1 org.apache.hadoop.mapred.JobClient.JobClient(InetSocketAddress,Configuration)
5962    3   2    1 org.apache.hadoop.mapred.JobClient.close()
5963    5   2    1 org.apache.hadoop.mapred.JobClient.getFs()
5964   25  19    0 org.apache.hadoop.mapred.JobClient.compareFs(FileSystem,FileSystem)
5965    9   3    0 org.apache.hadoop.mapred.JobClient.copyRemoteFiles(FileSystem,Path,Path,JobConf,short)
5966   96  29    1 org.apache.hadoop.mapred.JobClient.configureCommandLineOptions(JobConf,Path,Path)
5967    6   3    0 org.apache.hadoop.mapred.JobClient.getUGI(Configuration)
5968    3   1    1 org.apache.hadoop.mapred.JobClient.submitJob(String)
5969   12   8    0 org.apache.hadoop.mapred.JobClient.Comparator$1.compare(InputSplit,InputSplit)
5970   38   4    1 org.apache.hadoop.mapred.JobClient.submitJob(JobConf)
5971   14   9    1 org.apache.hadoop.mapred.JobClient.isJobDirValid(Path,FileSystem)
5972    2   1    0 org.apache.hadoop.mapred.JobClient.RawSplit.setBytes(byte[],int,int)
5973    2   1    0 org.apache.hadoop.mapred.JobClient.RawSplit.setClassName(String)
5974    2   1    0 org.apache.hadoop.mapred.JobClient.RawSplit.getClassName()
5975    2   1    0 org.apache.hadoop.mapred.JobClient.RawSplit.getBytes()
5976    2   1    0 org.apache.hadoop.mapred.JobClient.RawSplit.clearBytes()
5977    2   1    0 org.apache.hadoop.mapred.JobClient.RawSplit.setLocations(String[])
5978    2   1    0 org.apache.hadoop.mapred.JobClient.RawSplit.getLocations()
5979    8   2    0 org.apache.hadoop.mapred.JobClient.RawSplit.readFields(DataInput)
5980    7   2    0 org.apache.hadoop.mapred.JobClient.RawSplit.write(DataOutput)
5981    2   1    0 org.apache.hadoop.mapred.JobClient.RawSplit.getDataLength()
5982    2   1    0 org.apache.hadoop.mapred.JobClient.RawSplit.setDataLength(long)
5983   14   2    1 org.apache.hadoop.mapred.JobClient.writeSplitsFile(InputSplit[],FSDataOutputStream)
5984   14   6    1 org.apache.hadoop.mapred.JobClient.readSplitFile(DataInput)
5985    6   3    1 org.apache.hadoop.mapred.JobClient.getJob(JobID)
5986    2   1    0 org.apache.hadoop.mapred.JobClient.getJob(String)
5987    2   1    1 org.apache.hadoop.mapred.JobClient.getMapTaskReports(JobID)
5988    2   1    0 org.apache.hadoop.mapred.JobClient.getMapTaskReports(String)
5989    2   1    1 org.apache.hadoop.mapred.JobClient.getReduceTaskReports(JobID)
5990    2   1    1 org.apache.hadoop.mapred.JobClient.getCleanupTaskReports(JobID)
5991    2   1    1 org.apache.hadoop.mapred.JobClient.getSetupTaskReports(JobID)
5992    2   1    0 org.apache.hadoop.mapred.JobClient.getReduceTaskReports(String)
5993    2   1    1 org.apache.hadoop.mapred.JobClient.getClusterStatus()
5994    2   1    1 org.apache.hadoop.mapred.JobClient.jobsToComplete()
5995    5   1    0 org.apache.hadoop.mapred.JobClient.downloadProfile(TaskCompletionEvent)
5996    2   1    1 org.apache.hadoop.mapred.JobClient.getAllJobs()
5997   79  32    1 org.apache.hadoop.mapred.JobClient.runJob(JobConf)
5998    2   1    0 org.apache.hadoop.mapred.JobClient.getTaskLogURL(TaskAttemptID,String)
5999    5   2    0 org.apache.hadoop.mapred.JobClient.displayTaskLogs(TaskAttemptID,String)
6000   13   4    0 org.apache.hadoop.mapred.JobClient.getTaskLogs(TaskAttemptID,URL,OutputStream)
6001   12   5    0 org.apache.hadoop.mapred.JobClient.getConfiguration(String)
6002    2   1    0 org.apache.hadoop.mapred.JobClient.setTaskOutputFilter(TaskStatusFilter)
6003    2   1    1 org.apache.hadoop.mapred.JobClient.getTaskOutputFilter(JobConf)
6004    2   1    1 org.apache.hadoop.mapred.JobClient.setTaskOutputFilter(JobConf,TaskStatusFilter)
6005    2   1    0 org.apache.hadoop.mapred.JobClient.getTaskOutputFilter()
6006    5   2    0 org.apache.hadoop.mapred.JobClient.getJobPriorityNames()
6007   39  11    1 org.apache.hadoop.mapred.JobClient.displayUsage(String)
6008  196  62    0 org.apache.hadoop.mapred.JobClient.run(String[])
6009    3   1    0 org.apache.hadoop.mapred.JobClient.viewHistory(String,boolean)
6010    6   2    1 org.apache.hadoop.mapred.JobClient.listEvents(JobID,int,int)
6011    6   2    1 org.apache.hadoop.mapred.JobClient.listJobs()
6012    7   2    1 org.apache.hadoop.mapred.JobClient.listAllJobs()
6013    4   2    0 org.apache.hadoop.mapred.JobClient.displayJobList(JobStatus[])
6014    2   1    1 org.apache.hadoop.mapred.JobClient.getDefaultMaps()
6015    2   1    1 org.apache.hadoop.mapred.JobClient.getDefaultReduces()
6016    4   2    1 org.apache.hadoop.mapred.JobClient.getSystemDir()
6017    2   1    1 org.apache.hadoop.mapred.JobClient.getQueues()
6018    2   1    1 org.apache.hadoop.mapred.JobClient.getJobsFromQueue(String)
6019    2   1    1 org.apache.hadoop.mapred.JobClient.getQueueInfo(String)
6020    3   1    1 org.apache.hadoop.mapred.JobClient.main(String[])
6021    1   1    1 org.apache.hadoop.mapred.JobConf.JobConf()
6022    2   1    1 org.apache.hadoop.mapred.JobConf.JobConf(Class)
6023    2   1    1 org.apache.hadoop.mapred.JobConf.JobConf(Configuration)
6024    3   1    1 org.apache.hadoop.mapred.JobConf.JobConf(Configuration,Class)
6025    2   1    1 org.apache.hadoop.mapred.JobConf.JobConf(String)
6026    3   1    1 org.apache.hadoop.mapred.JobConf.JobConf(Path)
6027    2   1    1 org.apache.hadoop.mapred.JobConf.JobConf(boolean)
6028    2   1    1 org.apache.hadoop.mapred.JobConf.getJar()
6029    2   1    1 org.apache.hadoop.mapred.JobConf.setJar(String)
6030    4   2    1 org.apache.hadoop.mapred.JobConf.setJarByClass(Class)
6031    2   1    0 org.apache.hadoop.mapred.JobConf.getLocalDirs()
6032    4   2    0 org.apache.hadoop.mapred.JobConf.deleteLocalFiles()
6033    4   2    0 org.apache.hadoop.mapred.JobConf.deleteLocalFiles(String)
6034    2   1    1 org.apache.hadoop.mapred.JobConf.getLocalPath(String)
6035    2   1    1 org.apache.hadoop.mapred.JobConf.getUser()
6036    2   1    1 org.apache.hadoop.mapred.JobConf.setUser(String)
6037    2   1    1 org.apache.hadoop.mapred.JobConf.setKeepFailedTaskFiles(boolean)
6038    2   1    1 org.apache.hadoop.mapred.JobConf.getKeepFailedTaskFiles()
6039    2   1    1 org.apache.hadoop.mapred.JobConf.setKeepTaskFilesPattern(String)
6040    2   1    1 org.apache.hadoop.mapred.JobConf.getKeepTaskFilesPattern()
6041    3   1    1 org.apache.hadoop.mapred.JobConf.setWorkingDirectory(Path)
6042   10   6    1 org.apache.hadoop.mapred.JobConf.getWorkingDirectory()
6043    2   1    1 org.apache.hadoop.mapred.JobConf.setNumTasksToExecutePerJvm(int)
6044    2   1    1 org.apache.hadoop.mapred.JobConf.getNumTasksToExecutePerJvm()
6045    2   1    1 org.apache.hadoop.mapred.JobConf.getInputFormat()
6046    2   1    1 org.apache.hadoop.mapred.JobConf.setInputFormat(Class)
6047    2   1    1 org.apache.hadoop.mapred.JobConf.getOutputFormat()
6048    2   1    1 org.apache.hadoop.mapred.JobConf.getOutputCommitter()
6049    2   1    1 org.apache.hadoop.mapred.JobConf.setOutputCommitter(Class)
6050    2   1    1 org.apache.hadoop.mapred.JobConf.setOutputFormat(Class)
6051    2   1    1 org.apache.hadoop.mapred.JobConf.setCompressMapOutput(boolean)
6052    2   1    1 org.apache.hadoop.mapred.JobConf.getCompressMapOutput()
6053    3   1    1 org.apache.hadoop.mapred.JobConf.setMapOutputCompressorClass(Class)
6054    8   4    1 org.apache.hadoop.mapred.JobConf.getMapOutputCompressorClass(Class)
6055    5   2    1 org.apache.hadoop.mapred.JobConf.getMapOutputKeyClass()
6056    2   1    1 org.apache.hadoop.mapred.JobConf.setMapOutputKeyClass(Class)
6057    5   2    1 org.apache.hadoop.mapred.JobConf.getMapOutputValueClass()
6058    2   1    1 org.apache.hadoop.mapred.JobConf.setMapOutputValueClass(Class)
6059    2   1    1 org.apache.hadoop.mapred.JobConf.getOutputKeyClass()
6060    2   1    1 org.apache.hadoop.mapred.JobConf.setOutputKeyClass(Class)
6061    5   3    1 org.apache.hadoop.mapred.JobConf.getOutputKeyComparator()
6062    2   1    1 org.apache.hadoop.mapred.JobConf.setOutputKeyComparatorClass(Class)
6063    3   1    1 org.apache.hadoop.mapred.JobConf.setKeyFieldComparatorOptions(String)
6064    2   1    1 org.apache.hadoop.mapred.JobConf.getKeyFieldComparatorOption()
6065    3   1    1 org.apache.hadoop.mapred.JobConf.setKeyFieldPartitionerOptions(String)
6066    2   1    1 org.apache.hadoop.mapred.JobConf.getKeyFieldPartitionerOption()
6067    5   3    1 org.apache.hadoop.mapred.JobConf.getOutputValueGroupingComparator()
6068    2   1    1 org.apache.hadoop.mapred.JobConf.setOutputValueGroupingComparator(Class)
6069    2   1    1 org.apache.hadoop.mapred.JobConf.getOutputValueClass()
6070    2   1    1 org.apache.hadoop.mapred.JobConf.setOutputValueClass(Class)
6071    2   1    1 org.apache.hadoop.mapred.JobConf.getMapperClass()
6072    2   1    1 org.apache.hadoop.mapred.JobConf.setMapperClass(Class)
6073    2   1    1 org.apache.hadoop.mapred.JobConf.getMapRunnerClass()
6074    2   1    1 org.apache.hadoop.mapred.JobConf.setMapRunnerClass(Class)
6075    2   1    1 org.apache.hadoop.mapred.JobConf.getPartitionerClass()
6076    2   1    1 org.apache.hadoop.mapred.JobConf.setPartitionerClass(Class)
6077    2   1    1 org.apache.hadoop.mapred.JobConf.getReducerClass()
6078    2   1    1 org.apache.hadoop.mapred.JobConf.setReducerClass(Class)
6079    2   1    1 org.apache.hadoop.mapred.JobConf.getCombinerClass()
6080    2   1    1 org.apache.hadoop.mapred.JobConf.setCombinerClass(Class)
6081    2   2    1 org.apache.hadoop.mapred.JobConf.getSpeculativeExecution()
6082    3   1    1 org.apache.hadoop.mapred.JobConf.setSpeculativeExecution(boolean)
6083    2   1    1 org.apache.hadoop.mapred.JobConf.getMapSpeculativeExecution()
6084    2   1    1 org.apache.hadoop.mapred.JobConf.setMapSpeculativeExecution(boolean)
6085    2   1    1 org.apache.hadoop.mapred.JobConf.getReduceSpeculativeExecution()
6086    2   1    1 org.apache.hadoop.mapred.JobConf.setReduceSpeculativeExecution(boolean)
6087    2   1    1 org.apache.hadoop.mapred.JobConf.getNumMapTasks()
6088    2   1    1 org.apache.hadoop.mapred.JobConf.setNumMapTasks(int)
6089    2   1    1 org.apache.hadoop.mapred.JobConf.getNumReduceTasks()
6090    2   1    1 org.apache.hadoop.mapred.JobConf.setNumReduceTasks(int)
6091    2   1    1 org.apache.hadoop.mapred.JobConf.getMaxMapAttempts()
6092    2   1    1 org.apache.hadoop.mapred.JobConf.setMaxMapAttempts(int)
6093    2   1    1 org.apache.hadoop.mapred.JobConf.getMaxReduceAttempts()
6094    2   1    1 org.apache.hadoop.mapred.JobConf.setMaxReduceAttempts(int)
6095    2   1    1 org.apache.hadoop.mapred.JobConf.getJobName()
6096    2   1    1 org.apache.hadoop.mapred.JobConf.setJobName(String)
6097    2   1    1 org.apache.hadoop.mapred.JobConf.getSessionId()
6098    2   1    1 org.apache.hadoop.mapred.JobConf.setSessionId(String)
6099    2   1    1 org.apache.hadoop.mapred.JobConf.setMaxTaskFailuresPerTracker(int)
6100    2   1    1 org.apache.hadoop.mapred.JobConf.getMaxTaskFailuresPerTracker()
6101    2   1    1 org.apache.hadoop.mapred.JobConf.getMaxMapTaskFailuresPercent()
6102    2   1    1 org.apache.hadoop.mapred.JobConf.setMaxMapTaskFailuresPercent(int)
6103    2   1    1 org.apache.hadoop.mapred.JobConf.getMaxReduceTaskFailuresPercent()
6104    2   1    1 org.apache.hadoop.mapred.JobConf.setMaxReduceTaskFailuresPercent(int)
6105    2   1    1 org.apache.hadoop.mapred.JobConf.setJobPriority(JobPriority)
6106    5   3    1 org.apache.hadoop.mapred.JobConf.getJobPriority()
6107    2   1    1 org.apache.hadoop.mapred.JobConf.getProfileEnabled()
6108    2   1    1 org.apache.hadoop.mapred.JobConf.setProfileEnabled(boolean)
6109    2   1    1 org.apache.hadoop.mapred.JobConf.getProfileParams()
6110    2   1    1 org.apache.hadoop.mapred.JobConf.setProfileParams(String)
6111    2   2    1 org.apache.hadoop.mapred.JobConf.getProfileTaskRange(boolean)
6112    3   2    1 org.apache.hadoop.mapred.JobConf.setProfileTaskRange(boolean,String)
6113    2   1    1 org.apache.hadoop.mapred.JobConf.setMapDebugScript(String)
6114    2   1    1 org.apache.hadoop.mapred.JobConf.getMapDebugScript()
6115    2   1    1 org.apache.hadoop.mapred.JobConf.setReduceDebugScript(String)
6116    2   1    1 org.apache.hadoop.mapred.JobConf.getReduceDebugScript()
6117    2   1    1 org.apache.hadoop.mapred.JobConf.getJobEndNotificationURI()
6118    2   1    1 org.apache.hadoop.mapred.JobConf.setJobEndNotificationURI(String)
6119    2   1    1 org.apache.hadoop.mapred.JobConf.getJobLocalDir()
6120    2   1    1 org.apache.hadoop.mapred.JobConf.getMaxVirtualMemoryForTask()
6121    2   1    1 org.apache.hadoop.mapred.JobConf.setMaxVirtualMemoryForTask(long)
6122    2   1    1 org.apache.hadoop.mapred.JobConf.getQueueName()
6123    2   1    1 org.apache.hadoop.mapred.JobConf.setQueueName(String)
6124   14   7    1 org.apache.hadoop.mapred.JobConf.findContainingJar(Class)
6125    3   1    0 org.apache.hadoop.mapred.JobContext.JobContext(JobConf,Progressable)
6126    2   1    0 org.apache.hadoop.mapred.JobContext.JobContext(JobConf)
6127    2   1    1 org.apache.hadoop.mapred.JobContext.getJobConf()
6128    2   1    1 org.apache.hadoop.mapred.JobContext.getProgressible()
6129    9   1    1 org.apache.hadoop.mapred.jobcontrol.Job.Job(JobConf,ArrayList)
6130    2   1    1 org.apache.hadoop.mapred.jobcontrol.Job.Job(JobConf)
6131   15   5    0 org.apache.hadoop.mapred.jobcontrol.Job.toString()
6132    2   1    1 org.apache.hadoop.mapred.jobcontrol.Job.getJobName()
6133    2   1    1 org.apache.hadoop.mapred.jobcontrol.Job.setJobName(String)
6134    2   1    1 org.apache.hadoop.mapred.jobcontrol.Job.getJobID()
6135    2   1    1 org.apache.hadoop.mapred.jobcontrol.Job.setJobID(String)
6136    2   1    0 org.apache.hadoop.mapred.jobcontrol.Job.getMapredJobID()
6137    2   1    0 org.apache.hadoop.mapred.jobcontrol.Job.setMapredJobID(String)
6138    2   1    1 org.apache.hadoop.mapred.jobcontrol.Job.getAssignedJobID()
6139    2   1    1 org.apache.hadoop.mapred.jobcontrol.Job.setAssignedJobID(JobID)
6140    2   1    1 org.apache.hadoop.mapred.jobcontrol.Job.getJobConf()
6141    2   1    1 org.apache.hadoop.mapred.jobcontrol.Job.setJobConf(JobConf)
6142    2   1    1 org.apache.hadoop.mapred.jobcontrol.Job.getState()
6143    2   1    1 org.apache.hadoop.mapred.jobcontrol.Job.setState(int)
6144    2   1    1 org.apache.hadoop.mapred.jobcontrol.Job.getMessage()
6145    2   1    1 org.apache.hadoop.mapred.jobcontrol.Job.setMessage(String)
6146    2   1    1 org.apache.hadoop.mapred.jobcontrol.Job.getJobClient()
6147    2   1    1 org.apache.hadoop.mapred.jobcontrol.Job.getDependingJobs()
6148    7   4    1 org.apache.hadoop.mapred.jobcontrol.Job.addDependingJob(Job)
6149    2   3    1 org.apache.hadoop.mapred.jobcontrol.Job.isCompleted()
6150    2   1    1 org.apache.hadoop.mapred.jobcontrol.Job.isReady()
6151   21   9    1 org.apache.hadoop.mapred.jobcontrol.Job.checkRunningState()
6152   22  14    1 org.apache.hadoop.mapred.jobcontrol.Job.checkState()
6153   14   6    1 org.apache.hadoop.mapred.jobcontrol.Job.submit()
6154    9   1    1 org.apache.hadoop.mapred.jobcontrol.JobControl.JobControl(String)
6155    6   2    0 org.apache.hadoop.mapred.jobcontrol.JobControl.toArrayList(Map)
6156    2   1    1 org.apache.hadoop.mapred.jobcontrol.JobControl.getWaitingJobs()
6157    2   1    1 org.apache.hadoop.mapred.jobcontrol.JobControl.getRunningJobs()
6158    2   1    1 org.apache.hadoop.mapred.jobcontrol.JobControl.getReadyJobs()
6159    2   1    1 org.apache.hadoop.mapred.jobcontrol.JobControl.getSuccessfulJobs()
6160    2   1    0 org.apache.hadoop.mapred.jobcontrol.JobControl.getFailedJobs()
6161    3   1    0 org.apache.hadoop.mapred.jobcontrol.JobControl.getNextJobID()
6162    3   1    0 org.apache.hadoop.mapred.jobcontrol.JobControl.addToQueue(Job,Map)
6163    3   1    0 org.apache.hadoop.mapred.jobcontrol.JobControl.addToQueue(Job)
6164   17   7    0 org.apache.hadoop.mapred.jobcontrol.JobControl.getQueue(int)
6165    6   1    1 org.apache.hadoop.mapred.jobcontrol.JobControl.addJob(Job)
6166    3   2    1 org.apache.hadoop.mapred.jobcontrol.JobControl.addJobs(Collection)
6167    2   1    1 org.apache.hadoop.mapred.jobcontrol.JobControl.getState()
6168    2   1    1 org.apache.hadoop.mapred.jobcontrol.JobControl.stop()
6169    3   2    1 org.apache.hadoop.mapred.jobcontrol.JobControl.suspend()
6170    3   2    1 org.apache.hadoop.mapred.jobcontrol.JobControl.resume()
6171    7   2    0 org.apache.hadoop.mapred.jobcontrol.JobControl.checkRunningJobs()
6172    7   2    0 org.apache.hadoop.mapred.jobcontrol.JobControl.checkWaitingJobs()
6173    7   2    0 org.apache.hadoop.mapred.jobcontrol.JobControl.startReadyJobs()
6174    2   3    0 org.apache.hadoop.mapred.jobcontrol.JobControl.allFinished()
6175   16   9    1 org.apache.hadoop.mapred.jobcontrol.JobControl.run()
6176    6   4    0 org.apache.hadoop.mapred.JobEndNotifier.Runnable$1.run()
6177   12   7    0 org.apache.hadoop.mapred.JobEndNotifier.Runnable$1.sendNotification(JobEndStatusInfo)
6178   22   1    0 org.apache.hadoop.mapred.JobEndNotifier.startNotifier()
6179    3   1    0 org.apache.hadoop.mapred.JobEndNotifier.stopNotifier()
6180   13   5    0 org.apache.hadoop.mapred.JobEndNotifier.createNotification(JobConf,JobStatus)
6181    6   3    0 org.apache.hadoop.mapred.JobEndNotifier.registerNotification(JobConf,JobStatus)
6182    6   1    0 org.apache.hadoop.mapred.JobEndNotifier.httpNotification(String)
6183   17   8    0 org.apache.hadoop.mapred.JobEndNotifier.localRunnerNotification(JobConf,JobStatus)
6184    5   1    0 org.apache.hadoop.mapred.JobEndNotifier.JobEndStatusInfo.JobEndStatusInfo(String,int,long)
6185    2   1    0 org.apache.hadoop.mapred.JobEndNotifier.JobEndStatusInfo.getUri()
6186    2   1    0 org.apache.hadoop.mapred.JobEndNotifier.JobEndStatusInfo.getRetryAttempts()
6187    2   1    0 org.apache.hadoop.mapred.JobEndNotifier.JobEndStatusInfo.getRetryInterval()
6188    2   1    0 org.apache.hadoop.mapred.JobEndNotifier.JobEndStatusInfo.getDelayTime()
6189    7   2    0 org.apache.hadoop.mapred.JobEndNotifier.JobEndStatusInfo.configureForRetry()
6190    3   1    0 org.apache.hadoop.mapred.JobEndNotifier.JobEndStatusInfo.getDelay(TimeUnit)
6191    2   1    0 org.apache.hadoop.mapred.JobEndNotifier.JobEndStatusInfo.compareTo(Delayed)
6192    6   5    0 org.apache.hadoop.mapred.JobEndNotifier.JobEndStatusInfo.equals(Object)
6193    2   1    0 org.apache.hadoop.mapred.JobEndNotifier.JobEndStatusInfo.hashCode()
6194    2   1    0 org.apache.hadoop.mapred.JobEndNotifier.JobEndStatusInfo.toString()
6195   16   5    1 org.apache.hadoop.mapred.JobHistory.init(JobConf,String,long)
6196    3   2    0 org.apache.hadoop.mapred.JobHistory.MetaInfoManager.MetaInfoManager(String)
6197    5   3    0 org.apache.hadoop.mapred.JobHistory.MetaInfoManager.getLineDelim()
6198    2   1    0 org.apache.hadoop.mapred.JobHistory.MetaInfoManager.isValueEscaped()
6199    4   2    0 org.apache.hadoop.mapred.JobHistory.MetaInfoManager.handle(RecordTypes,Map)
6200    4   3    1 org.apache.hadoop.mapred.JobHistory.MetaInfoManager.logMetaInfo(ArrayList)
6201    2   1    1 org.apache.hadoop.mapred.JobHistory.escapeString(String)
6202   22   7    1 org.apache.hadoop.mapred.JobHistory.parseHistoryFromFS(String,Listener,FileSystem)
6203   14   3    1 org.apache.hadoop.mapred.JobHistory.parseLine(String,Listener,boolean)
6204    3   1    1 org.apache.hadoop.mapred.JobHistory.log(PrintWriter,RecordTypes,Keys,String)
6205   13   3    1 org.apache.hadoop.mapred.JobHistory.log(ArrayList,RecordTypes,Keys[],String[])
6206    2   1    1 org.apache.hadoop.mapred.JobHistory.isDisableHistory()
6207    2   1    1 org.apache.hadoop.mapred.JobHistory.setDisableHistory(boolean)
6208    3   2    1 org.apache.hadoop.mapred.JobHistory.KeyValuePair.get(Keys)
6209    5   3    1 org.apache.hadoop.mapred.JobHistory.KeyValuePair.getInt(Keys)
6210    5   3    1 org.apache.hadoop.mapred.JobHistory.KeyValuePair.getLong(Keys)
6211    2   1    1 org.apache.hadoop.mapred.JobHistory.KeyValuePair.set(Keys,String)
6212    2   1    1 org.apache.hadoop.mapred.JobHistory.KeyValuePair.set(Map)
6213    2   1    1 org.apache.hadoop.mapred.JobHistory.KeyValuePair.handle(Map)
6214    2   1    1 org.apache.hadoop.mapred.JobHistory.KeyValuePair.getValues()
6215    2   1    1 org.apache.hadoop.mapred.JobHistory.JobInfo.JobInfo(String)
6216    2   1    1 org.apache.hadoop.mapred.JobHistory.JobInfo.getAllTasks()
6217    2   1    1 org.apache.hadoop.mapred.JobHistory.JobInfo.getLocalJobFilePath(JobID)
6218   11   3    1 org.apache.hadoop.mapred.JobHistory.JobInfo.encodeJobHistoryFilePath(String)
6219    9   3    1 org.apache.hadoop.mapred.JobHistory.JobInfo.encodeJobHistoryFileName(String)
6220    9   3    1 org.apache.hadoop.mapred.JobHistory.JobInfo.decodeJobHistoryFileName(String)
6221    5   3    1 org.apache.hadoop.mapred.JobHistory.JobInfo.getJobName(JobConf)
6222    5   3    1 org.apache.hadoop.mapred.JobHistory.JobInfo.getUserName(JobConf)
6223    2   2    1 org.apache.hadoop.mapred.JobHistory.JobInfo.getJobHistoryLogLocation(String)
6224   10   4    1 org.apache.hadoop.mapred.JobHistory.JobInfo.getJobHistoryLogLocationForUser(String,JobConf)
6225    2   1    1 org.apache.hadoop.mapred.JobHistory.JobInfo.getNewJobHistoryFileName(JobConf,JobID)
6226    4   2    1 org.apache.hadoop.mapred.JobHistory.JobInfo.trimJobName(String)
6227    2   1    0 org.apache.hadoop.mapred.JobHistory.JobInfo.escapeRegexChars(String)
6228    7   3    0 org.apache.hadoop.mapred.JobHistory.JobInfo.PathFilter$1.accept(Path)
6229   27   5    1 org.apache.hadoop.mapred.JobHistory.JobInfo.getJobHistoryFileName(JobConf,JobID)
6230    9   3    1 org.apache.hadoop.mapred.JobHistory.JobInfo.checkpointRecovery(String,JobConf)
6231    2   1    0 org.apache.hadoop.mapred.JobHistory.JobInfo.getSecondaryJobHistoryFile(String)
6232   15   6    1 org.apache.hadoop.mapred.JobHistory.JobInfo.recoverJobHistoryFile(JobConf,Path)
6233   15   5    1 org.apache.hadoop.mapred.JobHistory.JobInfo.finalizeRecovery(JobID,JobConf)
6234   73  18    1 org.apache.hadoop.mapred.JobHistory.JobInfo.logSubmitted(JobID,JobConf,String,long)
6235    6   3    1 org.apache.hadoop.mapred.JobHistory.JobInfo.logInited(JobID,long,int,int)
6236    2   1    0 org.apache.hadoop.mapred.JobHistory.JobInfo.logStarted(JobID,long,int,int)
6237    6   3    1 org.apache.hadoop.mapred.JobHistory.JobInfo.logStarted(JobID)
6238   11   4    1 org.apache.hadoop.mapred.JobHistory.JobInfo.logFinished(JobID,long,int,int,int,int,Counters)
6239    9   4    1 org.apache.hadoop.mapred.JobHistory.JobInfo.logFailed(JobID,long,int,int)
6240    9   4    1 org.apache.hadoop.mapred.JobHistory.JobInfo.logKilled(JobID,long,int,int)
6241    6   3    1 org.apache.hadoop.mapred.JobHistory.JobInfo.logJobPriority(JobID,JobPriority)
6242    6   3    1 org.apache.hadoop.mapred.JobHistory.JobInfo.logJobInfo(JobID,long,long,int)
6243    5   3    1 org.apache.hadoop.mapred.JobHistory.Task.logStarted(TaskID,String,long,String)
6244    5   3    1 org.apache.hadoop.mapred.JobHistory.Task.logFinished(TaskID,String,long,Counters)
6245    2   1    1 org.apache.hadoop.mapred.JobHistory.Task.logFailed(TaskID,String,long,String)
6246    6   4    1 org.apache.hadoop.mapred.JobHistory.Task.logFailed(TaskID,String,long,String,TaskAttemptID)
6247    2   1    1 org.apache.hadoop.mapred.JobHistory.Task.getTaskAttempts()
6248    2   1    0 org.apache.hadoop.mapred.JobHistory.MapAttempt.logStarted(TaskAttemptID,long,String)
6249    5   3    1 org.apache.hadoop.mapred.JobHistory.MapAttempt.logStarted(TaskAttemptID,long,String,int,String)
6250    2   1    0 org.apache.hadoop.mapred.JobHistory.MapAttempt.logFinished(TaskAttemptID,long,String)
6251    5   3    1 org.apache.hadoop.mapred.JobHistory.MapAttempt.logFinished(TaskAttemptID,long,String,String,String,Counters)
6252    2   1    0 org.apache.hadoop.mapred.JobHistory.MapAttempt.logFailed(TaskAttemptID,long,String,String)
6253    5   3    1 org.apache.hadoop.mapred.JobHistory.MapAttempt.logFailed(TaskAttemptID,long,String,String,String)
6254    2   1    0 org.apache.hadoop.mapred.JobHistory.MapAttempt.logKilled(TaskAttemptID,long,String,String)
6255    5   3    1 org.apache.hadoop.mapred.JobHistory.MapAttempt.logKilled(TaskAttemptID,long,String,String,String)
6256    2   1    0 org.apache.hadoop.mapred.JobHistory.ReduceAttempt.logStarted(TaskAttemptID,long,String)
6257    5   3    1 org.apache.hadoop.mapred.JobHistory.ReduceAttempt.logStarted(TaskAttemptID,long,String,int,String)
6258    2   1    0 org.apache.hadoop.mapred.JobHistory.ReduceAttempt.logFinished(TaskAttemptID,long,long,long,String)
6259    5   3    1 org.apache.hadoop.mapred.JobHistory.ReduceAttempt.logFinished(TaskAttemptID,long,long,long,String,String,String,Counters)
6260    2   1    0 org.apache.hadoop.mapred.JobHistory.ReduceAttempt.logFailed(TaskAttemptID,long,String,String)
6261    5   3    1 org.apache.hadoop.mapred.JobHistory.ReduceAttempt.logFailed(TaskAttemptID,long,String,String,String)
6262    2   1    0 org.apache.hadoop.mapred.JobHistory.ReduceAttempt.logKilled(TaskAttemptID,long,String,String)
6263    5   3    1 org.apache.hadoop.mapred.JobHistory.ReduceAttempt.logKilled(TaskAttemptID,long,String,String,String)
6264    1   1    1 org.apache.hadoop.mapred.JobHistory.Listener.handle(RecordTypes,Map)
6265    4   3    0 org.apache.hadoop.mapred.JobHistory.HistoryCleaner.FileFilter$2.accept(File)
6266   17   7    1 org.apache.hadoop.mapred.JobHistory.HistoryCleaner.run()
6267    5   5    1 org.apache.hadoop.mapred.JobHistory.getTaskLogsUrl(JobHistory.TaskAttempt)
6268    3   1    1 org.apache.hadoop.mapred.JobID.JobID(String,int)
6269    1   1    0 org.apache.hadoop.mapred.JobID.JobID()
6270    2   1    0 org.apache.hadoop.mapred.JobID.getJtIdentifier()
6271    8   6    0 org.apache.hadoop.mapred.JobID.equals(Object)
6272    7   3    0 org.apache.hadoop.mapred.JobID.compareTo(ID)
6273    3   1    0 org.apache.hadoop.mapred.JobID.toString()
6274    4   1    1 org.apache.hadoop.mapred.JobID.toStringWOPrefix()
6275    2   1    0 org.apache.hadoop.mapred.JobID.hashCode()
6276    3   1    0 org.apache.hadoop.mapred.JobID.readFields(DataInput)
6277    3   1    0 org.apache.hadoop.mapred.JobID.write(DataOutput)
6278    4   1    0 org.apache.hadoop.mapred.JobID.read(DataInput)
6279    9   8    1 org.apache.hadoop.mapred.JobID.forName(String)
6280    4   1    1 org.apache.hadoop.mapred.JobID.getJobIDsPattern(String,Integer)
6281    3   3    0 org.apache.hadoop.mapred.JobID.getJobIDsPatternWOPrefix(String,Integer)
6282    5   1    1 org.apache.hadoop.mapred.JobInProgress.JobInProgress(JobID,JobConf)
6283   44   2    1 org.apache.hadoop.mapred.JobInProgress.JobInProgress(JobID,JobTracker,JobConf)
6284    8   3    1 org.apache.hadoop.mapred.JobInProgress.updateMetrics()
6285    4   1    1 org.apache.hadoop.mapred.JobInProgress.cleanUpMetrics()
6286    7   3    0 org.apache.hadoop.mapred.JobInProgress.printCache(Map)
6287   20   7    0 org.apache.hadoop.mapred.JobInProgress.createCache(JobClient.RawSplit[],int)
6288    2   1    1 org.apache.hadoop.mapred.JobInProgress.inited()
6289   63  15    1 org.apache.hadoop.mapred.JobInProgress.initTasks()
6290    2   1    0 org.apache.hadoop.mapred.JobInProgress.getProfile()
6291    2   1    0 org.apache.hadoop.mapred.JobInProgress.getStatus()
6292    2   1    0 org.apache.hadoop.mapred.JobInProgress.getLaunchTime()
6293    2   1    0 org.apache.hadoop.mapred.JobInProgress.getStartTime()
6294    2   1    0 org.apache.hadoop.mapred.JobInProgress.getFinishTime()
6295    2   1    0 org.apache.hadoop.mapred.JobInProgress.desiredMaps()
6296    2   1    0 org.apache.hadoop.mapred.JobInProgress.finishedMaps()
6297    2   1    0 org.apache.hadoop.mapred.JobInProgress.desiredReduces()
6298    2   1    0 org.apache.hadoop.mapred.JobInProgress.runningMaps()
6299    2   1    0 org.apache.hadoop.mapred.JobInProgress.runningReduces()
6300    2   1    0 org.apache.hadoop.mapred.JobInProgress.finishedReduces()
6301    2   1    0 org.apache.hadoop.mapred.JobInProgress.pendingMaps()
6302    2   1    0 org.apache.hadoop.mapred.JobInProgress.pendingReduces()
6303    2   1    0 org.apache.hadoop.mapred.JobInProgress.getPriority()
6304    8   2    0 org.apache.hadoop.mapred.JobInProgress.setPriority(JobPriority)
6305    2   1    0 org.apache.hadoop.mapred.JobInProgress.getMaxVirtualMemoryForTask()
6306    5   1    0 org.apache.hadoop.mapred.JobInProgress.updateJobInfo(long,long,int)
6307    2   1    1 org.apache.hadoop.mapred.JobInProgress.numRestarts()
6308    2   1    0 org.apache.hadoop.mapred.JobInProgress.getInputLength()
6309    2   1    1 org.apache.hadoop.mapred.JobInProgress.getMapTasks()
6310    2   1    1 org.apache.hadoop.mapred.JobInProgress.getCleanupTasks()
6311    2   1    1 org.apache.hadoop.mapred.JobInProgress.getSetupTasks()
6312    2   1    1 org.apache.hadoop.mapred.JobInProgress.getReduceTasks()
6313    2   1    1 org.apache.hadoop.mapred.JobInProgress.getNonLocalRunningMaps()
6314    2   1    1 org.apache.hadoop.mapred.JobInProgress.getRunningMapCache()
6315    2   1    1 org.apache.hadoop.mapred.JobInProgress.getRunningReduces()
6316    2   1    1 org.apache.hadoop.mapred.JobInProgress.getJobConf()
6317   11   4    1 org.apache.hadoop.mapred.JobInProgress.reportTasksInProgress(boolean,boolean)
6318    6   3    1 org.apache.hadoop.mapred.JobInProgress.reportCleanupTIPs(boolean)
6319    6   3    1 org.apache.hadoop.mapred.JobInProgress.reportSetupTIPs(boolean)
6320   61  29    0 org.apache.hadoop.mapred.JobInProgress.updateTaskStatus(TaskInProgress,TaskStatus,JobTrackerInstrumentation)
6321    2   1    1 org.apache.hadoop.mapred.JobInProgress.getJobCounters()
6322    2   1    1 org.apache.hadoop.mapred.JobInProgress.getMapCounters()
6323    2   1    1 org.apache.hadoop.mapred.JobInProgress.getReduceCounters()
6324    5   1    1 org.apache.hadoop.mapred.JobInProgress.getCounters()
6325    4   2    1 org.apache.hadoop.mapred.JobInProgress.incrementTaskCounters(Counters,TaskInProgress[])
6326   11   6    1 org.apache.hadoop.mapred.JobInProgress.obtainNewMapTask(TaskTrackerStatus,int,int)
6327   22  11    1 org.apache.hadoop.mapred.JobInProgress.obtainCleanupTask(TaskTrackerStatus,int,int,boolean)
6328   13  12    1 org.apache.hadoop.mapred.JobInProgress.canLaunchCleanupTask()
6329   22  11    1 org.apache.hadoop.mapred.JobInProgress.obtainSetupTask(TaskTrackerStatus,int,int,boolean)
6330    2   5    1 org.apache.hadoop.mapred.JobInProgress.canLaunchSetupTask()
6331   11   6    1 org.apache.hadoop.mapred.JobInProgress.obtainNewReduceTask(TaskTrackerStatus,int,int)
6332    9   4    0 org.apache.hadoop.mapred.JobInProgress.getMatchingLevelForNodes(Node,Node)
6333   57  21    1 org.apache.hadoop.mapred.JobInProgress.addRunningTaskToTIP(TaskInProgress,TaskAttemptID,TaskTrackerStatus,boolean)
6334    4   2    0 org.apache.hadoop.mapred.JobInProgress.convertTrackerNameToHostName(String)
6335   10   4    1 org.apache.hadoop.mapred.JobInProgress.addTrackerTaskFailure(String)
6336    4   2    0 org.apache.hadoop.mapred.JobInProgress.getTrackerTaskFailures(String)
6337    2   1    1 org.apache.hadoop.mapred.JobInProgress.getNoOfBlackListedTrackers()
6338    3   1    1 org.apache.hadoop.mapred.JobInProgress.getTaskTrackerErrors()
6339   18  10    1 org.apache.hadoop.mapred.JobInProgress.retireMap(TaskInProgress)
6340    6   4    1 org.apache.hadoop.mapred.JobInProgress.retireReduce(TaskInProgress)
6341   18   9    1 org.apache.hadoop.mapred.JobInProgress.scheduleMap(TaskInProgress)
6342    6   4    1 org.apache.hadoop.mapred.JobInProgress.scheduleReduce(TaskInProgress)
6343   17   8    1 org.apache.hadoop.mapred.JobInProgress.failMap(TaskInProgress)
6344    5   3    1 org.apache.hadoop.mapred.JobInProgress.failReduce(TaskInProgress)
6345   14   8    1 org.apache.hadoop.mapred.JobInProgress.findTaskFromList(Collection,TaskTrackerStatus,int,boolean)
6346   16   8    1 org.apache.hadoop.mapred.JobInProgress.findSpeculativeTask(Collection,TaskTrackerStatus,double,long,boolean)
6347   73  34    1 org.apache.hadoop.mapred.JobInProgress.findNewMapTask(TaskTrackerStatus,int,int,double)
6348   21  10    1 org.apache.hadoop.mapred.JobInProgress.findNewReduceTask(TaskTrackerStatus,int,int,double)
6349    8   5    0 org.apache.hadoop.mapred.JobInProgress.shouldRunOnTaskTracker(String)
6350   60  18    1 org.apache.hadoop.mapred.JobInProgress.completedTask(TaskInProgress,TaskStatus,JobTrackerInstrumentation)
6351    9   2    1 org.apache.hadoop.mapred.JobInProgress.jobComplete(JobTrackerInstrumentation)
6352   11   4    0 org.apache.hadoop.mapred.JobInProgress.terminateJob(int)
6353   21  14    1 org.apache.hadoop.mapred.JobInProgress.terminate(int)
6354   10   6    1 org.apache.hadoop.mapred.JobInProgress.kill()
6355    2   1    1 org.apache.hadoop.mapred.JobInProgress.fail()
6356   69  33    1 org.apache.hadoop.mapred.JobInProgress.failedTask(TaskInProgress,TaskAttemptID,TaskStatus,TaskTrackerStatus,boolean,boolean,JobTrackerInstrumentation)
6357    5   2    0 org.apache.hadoop.mapred.JobInProgress.killSetupTip(boolean)
6358    5   4    1 org.apache.hadoop.mapred.JobInProgress.failedTask(TaskInProgress,TaskAttemptID,String,TaskStatus.Phase,TaskStatus.State,String,JobTrackerInstrumentation)
6359   20   5    1 org.apache.hadoop.mapred.JobInProgress.garbageCollect()
6360   18  16    1 org.apache.hadoop.mapred.JobInProgress.getTaskInProgress(TaskID)
6361    8   5    1 org.apache.hadoop.mapred.JobInProgress.findFinishedMap(int)
6362    2   1    0 org.apache.hadoop.mapred.JobInProgress.getNumTaskCompletionEvents()
6363    6   2    0 org.apache.hadoop.mapred.JobInProgress.getTaskCompletionEvents(int,int)
6364   11   6    0 org.apache.hadoop.mapred.JobInProgress.fetchFailureNotification(TaskInProgress,TaskAttemptID,String,JobTrackerInstrumentation)
6365    2   1    1 org.apache.hadoop.mapred.JobInProgress.getJobID()
6366    2   1    0 org.apache.hadoop.mapred.JobInProgress.getSchedulingInfo()
6367    3   1    0 org.apache.hadoop.mapred.JobInProgress.setSchedulingInfo(Object)
6368    3   3    0 org.apache.hadoop.mapred.JobInProgress.isComplete()
6369    1   1    1 org.apache.hadoop.mapred.JobInProgressListener.jobAdded(JobInProgress)
6370    1   1    1 org.apache.hadoop.mapred.JobInProgressListener.jobRemoved(JobInProgress)
6371    1   1    1 org.apache.hadoop.mapred.JobInProgressListener.jobUpdated(JobChangeEvent)
6372    2   1    0 org.apache.hadoop.mapred.JobProfile.WritableFactory$1.newInstance()
6373    1   1    1 org.apache.hadoop.mapred.JobProfile.JobProfile()
6374    2   1    1 org.apache.hadoop.mapred.JobProfile.JobProfile(String,JobID,String,String,String)
6375    7   1    1 org.apache.hadoop.mapred.JobProfile.JobProfile(String,JobID,String,String,String,String)
6376    2   1    0 org.apache.hadoop.mapred.JobProfile.JobProfile(String,String,String,String,String)
6377    2   1    1 org.apache.hadoop.mapred.JobProfile.getUser()
6378    2   1    1 org.apache.hadoop.mapred.JobProfile.getJobID()
6379    2   1    0 org.apache.hadoop.mapred.JobProfile.getJobId()
6380    2   1    1 org.apache.hadoop.mapred.JobProfile.getJobFile()
6381    4   3    1 org.apache.hadoop.mapred.JobProfile.getURL()
6382    2   1    1 org.apache.hadoop.mapred.JobProfile.getJobName()
6383    2   1    1 org.apache.hadoop.mapred.JobProfile.getQueueName()
6384    7   1    0 org.apache.hadoop.mapred.JobProfile.write(DataOutput)
6385    7   1    0 org.apache.hadoop.mapred.JobProfile.readFields(DataInput)
6386    1   1    0 org.apache.hadoop.mapred.JobQueueClient.JobQueueClient()
6387    2   1    0 org.apache.hadoop.mapred.JobQueueClient.JobQueueClient(JobConf)
6388    3   1    0 org.apache.hadoop.mapred.JobQueueClient.init(JobConf)
6389   42  15    0 org.apache.hadoop.mapred.JobQueueClient.run(String[])
6390   13   4    1 org.apache.hadoop.mapred.JobQueueClient.displayQueueInfo(String,boolean)
6391    8   3    1 org.apache.hadoop.mapred.JobQueueClient.displayQueueList()
6392    9   2    0 org.apache.hadoop.mapred.JobQueueClient.displayUsage(String)
6393    3   1    0 org.apache.hadoop.mapred.JobQueueClient.main(String[])
6394    1   1    1 org.apache.hadoop.mapred.JobQueueInfo.JobQueueInfo()
6395    3   1    1 org.apache.hadoop.mapred.JobQueueInfo.JobQueueInfo(String,String)
6396    2   1    1 org.apache.hadoop.mapred.JobQueueInfo.setQueueName(String)
6397    2   1    1 org.apache.hadoop.mapred.JobQueueInfo.getQueueName()
6398    2   1    1 org.apache.hadoop.mapred.JobQueueInfo.setSchedulingInfo(String)
6399    5   3    1 org.apache.hadoop.mapred.JobQueueInfo.getSchedulingInfo()
6400    3   1    0 org.apache.hadoop.mapred.JobQueueInfo.readFields(DataInput)
6401    6   2    0 org.apache.hadoop.mapred.JobQueueInfo.write(DataOutput)
6402    2   1    0 org.apache.hadoop.mapred.JobQueueJobInProgressListener.JobSchedulingInfo.JobSchedulingInfo(JobInProgress)
6403    4   1    0 org.apache.hadoop.mapred.JobQueueJobInProgressListener.JobSchedulingInfo.JobSchedulingInfo(JobStatus)
6404    2   1    0 org.apache.hadoop.mapred.JobQueueJobInProgressListener.JobSchedulingInfo.getPriority()
6405    2   1    0 org.apache.hadoop.mapred.JobQueueJobInProgressListener.JobSchedulingInfo.getStartTime()
6406    2   1    0 org.apache.hadoop.mapred.JobQueueJobInProgressListener.JobSchedulingInfo.getJobID()
6407   10   5    0 org.apache.hadoop.mapred.JobQueueJobInProgressListener.Comparator$1.compare(JobSchedulingInfo,JobSchedulingInfo)
6408    2   1    0 org.apache.hadoop.mapred.JobQueueJobInProgressListener.JobQueueJobInProgressListener()
6409    2   1    1 org.apache.hadoop.mapred.JobQueueJobInProgressListener.JobQueueJobInProgressListener(Map)
6410    2   1    1 org.apache.hadoop.mapred.JobQueueJobInProgressListener.getJobQueue()
6411    2   1    0 org.apache.hadoop.mapred.JobQueueJobInProgressListener.jobAdded(JobInProgress)
6412    1   1    0 org.apache.hadoop.mapred.JobQueueJobInProgressListener.jobRemoved(JobInProgress)
6413    2   1    0 org.apache.hadoop.mapred.JobQueueJobInProgressListener.jobCompleted(JobSchedulingInfo)
6414   12   8    0 org.apache.hadoop.mapred.JobQueueJobInProgressListener.jobUpdated(JobChangeEvent)
6415    4   1    0 org.apache.hadoop.mapred.JobQueueJobInProgressListener.reorderJobs(JobInProgress,JobSchedulingInfo)
6416    3   1    0 org.apache.hadoop.mapred.JobQueueTaskScheduler.JobQueueTaskScheduler()
6417    5   1    0 org.apache.hadoop.mapred.JobQueueTaskScheduler.start()
6418    7   3    0 org.apache.hadoop.mapred.JobQueueTaskScheduler.terminate()
6419    3   1    0 org.apache.hadoop.mapred.JobQueueTaskScheduler.setConf(Configuration)
6420   57  19    0 org.apache.hadoop.mapred.JobQueueTaskScheduler.assignTasks(TaskTrackerStatus)
6421    2   1    0 org.apache.hadoop.mapred.JobQueueTaskScheduler.getJobs(String)
6422    2   1    0 org.apache.hadoop.mapred.JobShell.JobShell()
6423    2   1    0 org.apache.hadoop.mapred.JobShell.JobShell(Configuration)
6424    2   1    0 org.apache.hadoop.mapred.JobShell.init()
6425   12   3    1 org.apache.hadoop.mapred.JobShell.run(String[])
6426    4   1    0 org.apache.hadoop.mapred.JobShell.main(String[])
6427    2   1    0 org.apache.hadoop.mapred.JobStatus.WritableFactory$1.newInstance()
6428    1   1    1 org.apache.hadoop.mapred.JobStatus.JobStatus()
6429    2   1    1 org.apache.hadoop.mapred.JobStatus.JobStatus(JobID,float,float,float,int)
6430    2   1    1 org.apache.hadoop.mapred.JobStatus.JobStatus(JobID,float,float,int)
6431    2   1    1 org.apache.hadoop.mapred.JobStatus.JobStatus(JobID,float,float,float,int,JobPriority)
6432   11   3    1 org.apache.hadoop.mapred.JobStatus.JobStatus(JobID,float,float,float,float,int,JobPriority)
6433    2   1    0 org.apache.hadoop.mapred.JobStatus.getJobId()
6434    2   1    1 org.apache.hadoop.mapred.JobStatus.getJobID()
6435    2   1    1 org.apache.hadoop.mapred.JobStatus.mapProgress()
6436    2   1    1 org.apache.hadoop.mapred.JobStatus.setMapProgress(float)
6437    2   1    1 org.apache.hadoop.mapred.JobStatus.cleanupProgress()
6438    2   1    1 org.apache.hadoop.mapred.JobStatus.setCleanupProgress(float)
6439    2   1    1 org.apache.hadoop.mapred.JobStatus.setupProgress()
6440    2   1    1 org.apache.hadoop.mapred.JobStatus.setSetupProgress(float)
6441    2   1    1 org.apache.hadoop.mapred.JobStatus.reduceProgress()
6442    2   1    1 org.apache.hadoop.mapred.JobStatus.setReduceProgress(float)
6443    2   1    1 org.apache.hadoop.mapred.JobStatus.getRunState()
6444    2   1    1 org.apache.hadoop.mapred.JobStatus.setRunState(int)
6445    2   1    1 org.apache.hadoop.mapred.JobStatus.setStartTime(long)
6446    2   1    1 org.apache.hadoop.mapred.JobStatus.getStartTime()
6447    4   4    0 org.apache.hadoop.mapred.JobStatus.clone()
6448    2   1    1 org.apache.hadoop.mapred.JobStatus.setUsername(String)
6449    2   1    1 org.apache.hadoop.mapred.JobStatus.getUsername()
6450    2   1    1 org.apache.hadoop.mapred.JobStatus.getSchedulingInfo()
6451    2   1    1 org.apache.hadoop.mapred.JobStatus.setSchedulingInfo(String)
6452    2   1    1 org.apache.hadoop.mapred.JobStatus.getJobPriority()
6453    4   3    1 org.apache.hadoop.mapred.JobStatus.setJobPriority(JobPriority)
6454   11   1    0 org.apache.hadoop.mapred.JobStatus.write(DataOutput)
6455   11   1    0 org.apache.hadoop.mapred.JobStatus.readFields(DataInput)
6456    5   1    0 org.apache.hadoop.mapred.JobStatusChangeEvent.JobStatusChangeEvent(JobInProgress,EventType,JobStatus,JobStatus)
6457    2   1    1 org.apache.hadoop.mapred.JobStatusChangeEvent.JobStatusChangeEvent(JobInProgress,EventType,JobStatus)
6458    2   1    1 org.apache.hadoop.mapred.JobStatusChangeEvent.getEventType()
6459    2   1    1 org.apache.hadoop.mapred.JobStatusChangeEvent.getOldStatus()
6460    2   1    1 org.apache.hadoop.mapred.JobStatusChangeEvent.getNewStatus()
6461    1   1    1 org.apache.hadoop.mapred.JobSubmissionProtocol.getNewJobId()
6462    1   1    1 org.apache.hadoop.mapred.JobSubmissionProtocol.submitJob(JobID)
6463    1   1    1 org.apache.hadoop.mapred.JobSubmissionProtocol.getClusterStatus()
6464    1   1    1 org.apache.hadoop.mapred.JobSubmissionProtocol.killJob(JobID)
6465    1   1    1 org.apache.hadoop.mapred.JobSubmissionProtocol.setJobPriority(JobID,String)
6466    1   1    1 org.apache.hadoop.mapred.JobSubmissionProtocol.killTask(TaskAttemptID,boolean)
6467    1   1    1 org.apache.hadoop.mapred.JobSubmissionProtocol.getJobProfile(JobID)
6468    1   1    1 org.apache.hadoop.mapred.JobSubmissionProtocol.getJobStatus(JobID)
6469    1   1    1 org.apache.hadoop.mapred.JobSubmissionProtocol.getJobCounters(JobID)
6470    1   1    1 org.apache.hadoop.mapred.JobSubmissionProtocol.getMapTaskReports(JobID)
6471    1   1    1 org.apache.hadoop.mapred.JobSubmissionProtocol.getReduceTaskReports(JobID)
6472    1   1    1 org.apache.hadoop.mapred.JobSubmissionProtocol.getCleanupTaskReports(JobID)
6473    1   1    1 org.apache.hadoop.mapred.JobSubmissionProtocol.getSetupTaskReports(JobID)
6474    1   1    1 org.apache.hadoop.mapred.JobSubmissionProtocol.getFilesystemName()
6475    1   1    1 org.apache.hadoop.mapred.JobSubmissionProtocol.jobsToComplete()
6476    1   1    1 org.apache.hadoop.mapred.JobSubmissionProtocol.getAllJobs()
6477    1   1    1 org.apache.hadoop.mapred.JobSubmissionProtocol.getTaskCompletionEvents(JobID,int,int)
6478    1   1    1 org.apache.hadoop.mapred.JobSubmissionProtocol.getTaskDiagnostics(TaskAttemptID)
6479    1   1    1 org.apache.hadoop.mapred.JobSubmissionProtocol.getSystemDir()
6480    1   1    1 org.apache.hadoop.mapred.JobSubmissionProtocol.getQueues()
6481    1   1    1 org.apache.hadoop.mapred.JobSubmissionProtocol.getQueueInfo(String)
6482    1   1    1 org.apache.hadoop.mapred.JobSubmissionProtocol.getJobsFromQueue(String)
6483    2   1    0 org.apache.hadoop.mapred.JobTrackerInstrumentation.JobTrackerInstrumentation(JobTracker,JobConf)
6484    1   1    0 org.apache.hadoop.mapred.JobTrackerInstrumentation.launchMap(TaskAttemptID)
6485    1   1    0 org.apache.hadoop.mapred.JobTrackerInstrumentation.completeMap(TaskAttemptID)
6486    1   1    0 org.apache.hadoop.mapred.JobTrackerInstrumentation.launchReduce(TaskAttemptID)
6487    1   1    0 org.apache.hadoop.mapred.JobTrackerInstrumentation.completeReduce(TaskAttemptID)
6488    1   1    0 org.apache.hadoop.mapred.JobTrackerInstrumentation.submitJob(JobConf,JobID)
6489    1   1    0 org.apache.hadoop.mapred.JobTrackerInstrumentation.completeJob(JobConf,JobID)
6490    8   1    0 org.apache.hadoop.mapred.JobTrackerMetricsInst.JobTrackerMetricsInst(JobTracker,JobConf)
6491   18   3    1 org.apache.hadoop.mapred.JobTrackerMetricsInst.doUpdates(MetricsContext)
6492    2   1    0 org.apache.hadoop.mapred.JobTrackerMetricsInst.launchMap(TaskAttemptID)
6493    2   1    0 org.apache.hadoop.mapred.JobTrackerMetricsInst.completeMap(TaskAttemptID)
6494    2   1    0 org.apache.hadoop.mapred.JobTrackerMetricsInst.launchReduce(TaskAttemptID)
6495    2   1    0 org.apache.hadoop.mapred.JobTrackerMetricsInst.completeReduce(TaskAttemptID)
6496    2   1    0 org.apache.hadoop.mapred.JobTrackerMetricsInst.submitJob(JobConf,JobID)
6497    2   1    0 org.apache.hadoop.mapred.JobTrackerMetricsInst.completeJob(JobConf,JobID)
6498    2   1    0 org.apache.hadoop.mapred.join.ArrayListBackedIterator.ArrayListBackedIterator()
6499    3   1    0 org.apache.hadoop.mapred.join.ArrayListBackedIterator.ArrayListBackedIterator(ArrayList)
6500    2   1    0 org.apache.hadoop.mapred.join.ArrayListBackedIterator.hasNext()
6501    9   4    0 org.apache.hadoop.mapred.join.ArrayListBackedIterator.next(X)
6502    3   1    0 org.apache.hadoop.mapred.join.ArrayListBackedIterator.replay(X)
6503    2   1    0 org.apache.hadoop.mapred.join.ArrayListBackedIterator.reset()
6504    2   1    0 org.apache.hadoop.mapred.join.ArrayListBackedIterator.add(X)
6505    3   1    0 org.apache.hadoop.mapred.join.ArrayListBackedIterator.close()
6506    3   1    0 org.apache.hadoop.mapred.join.ArrayListBackedIterator.clear()
6507    1   1    1 org.apache.hadoop.mapred.join.ComposableRecordReader.id()
6508    1   1    1 org.apache.hadoop.mapred.join.ComposableRecordReader.key()
6509    1   1    1 org.apache.hadoop.mapred.join.ComposableRecordReader.key(K)
6510    1   1    1 org.apache.hadoop.mapred.join.ComposableRecordReader.hasNext()
6511    1   1    1 org.apache.hadoop.mapred.join.ComposableRecordReader.skip(K)
6512    1   1    1 org.apache.hadoop.mapred.join.ComposableRecordReader.accept(CompositeRecordReader.JoinCollector,K)
6513    1   1    0 org.apache.hadoop.mapred.join.CompositeInputFormat.CompositeInputFormat()
6514    4   1    1 org.apache.hadoop.mapred.join.CompositeInputFormat.setFormat(JobConf)
6515    7   3    1 org.apache.hadoop.mapred.join.CompositeInputFormat.addDefaults()
6516    8   5    1 org.apache.hadoop.mapred.join.CompositeInputFormat.addUserIdentifiers(JobConf)
6517    4   1    1 org.apache.hadoop.mapred.join.CompositeInputFormat.getSplits(JobConf,int)
6518    3   1    0 org.apache.hadoop.mapred.join.CompositeInputFormat.getRecordReader(InputSplit,JobConf,Reporter)
6519    2   1    1 org.apache.hadoop.mapred.join.CompositeInputFormat.compose(Class,String)
6520    8   2    1 org.apache.hadoop.mapred.join.CompositeInputFormat.compose(String,Class,String)
6521    5   2    1 org.apache.hadoop.mapred.join.CompositeInputFormat.compose(String,Class,Path)
6522    5   1    0 org.apache.hadoop.mapred.join.CompositeInputFormat.compose(String,String,StringBuffer)
6523    1   1    0 org.apache.hadoop.mapred.join.CompositeInputSplit.CompositeInputSplit()
6524    2   1    0 org.apache.hadoop.mapred.join.CompositeInputSplit.CompositeInputSplit(int)
6525    7   5    1 org.apache.hadoop.mapred.join.CompositeInputSplit.add(InputSplit)
6526    2   1    1 org.apache.hadoop.mapred.join.CompositeInputSplit.get(int)
6527    2   1    1 org.apache.hadoop.mapred.join.CompositeInputSplit.getLength()
6528    2   1    1 org.apache.hadoop.mapred.join.CompositeInputSplit.getLength(int)
6529    8   5    1 org.apache.hadoop.mapred.join.CompositeInputSplit.getLocations()
6530    2   1    1 org.apache.hadoop.mapred.join.CompositeInputSplit.getLocation(int)
6531    6   3    1 org.apache.hadoop.mapred.join.CompositeInputSplit.write(DataOutput)
6532   12   7    0 org.apache.hadoop.mapred.join.CompositeInputSplit.readFields(DataInput)
6533    1   1    0 org.apache.hadoop.mapred.join.CompositeRecordReader.combine(Object[],TupleWritable)
6534    2   1    0 org.apache.hadoop.mapred.join.CompositeRecordReader.Comparator$1.compare(ComposableRecordReader,ComposableRecordReader)
6535   10   2    0 org.apache.hadoop.mapred.join.CompositeRecordReader.CompositeRecordReader(int,int,Class)
6536    2   1    1 org.apache.hadoop.mapred.join.CompositeRecordReader.id()
6537    2   1    1 org.apache.hadoop.mapred.join.CompositeRecordReader.setConf(Configuration)
6538    2   1    1 org.apache.hadoop.mapred.join.CompositeRecordReader.getConf()
6539    2   1    1 org.apache.hadoop.mapred.join.CompositeRecordReader.getRecordReaderQueue()
6540    2   1    1 org.apache.hadoop.mapred.join.CompositeRecordReader.getComparator()
6541    2   1    0 org.apache.hadoop.mapred.join.CompositeRecordReader.Comparator$2.compare(ComposableRecordReader,ComposableRecordReader)
6542    9   3    1 org.apache.hadoop.mapred.join.CompositeRecordReader.add(ComposableRecordReader)
6543    4   2    0 org.apache.hadoop.mapred.join.CompositeRecordReader.JoinCollector.JoinCollector(int)
6544    2   1    1 org.apache.hadoop.mapred.join.CompositeRecordReader.JoinCollector.add(int,ResetableIterator)
6545    2   1    1 org.apache.hadoop.mapred.join.CompositeRecordReader.JoinCollector.key()
6546    6   2    1 org.apache.hadoop.mapred.join.CompositeRecordReader.JoinCollector.reset(K)
6547    6   2    1 org.apache.hadoop.mapred.join.CompositeRecordReader.JoinCollector.clear()
6548    2   1    1 org.apache.hadoop.mapred.join.CompositeRecordReader.JoinCollector.hasNext()
6549   28  18    0 org.apache.hadoop.mapred.join.CompositeRecordReader.JoinCollector.next(TupleWritable)
6550    8   3    0 org.apache.hadoop.mapred.join.CompositeRecordReader.JoinCollector.replay(TupleWritable)
6551    3   2    1 org.apache.hadoop.mapred.join.CompositeRecordReader.JoinCollector.close()
6552    6   5    1 org.apache.hadoop.mapred.join.CompositeRecordReader.JoinCollector.flush(TupleWritable)
6553    6   5    1 org.apache.hadoop.mapred.join.CompositeRecordReader.key()
6554    2   1    1 org.apache.hadoop.mapred.join.CompositeRecordReader.key(K)
6555    2   2    1 org.apache.hadoop.mapred.join.CompositeRecordReader.hasNext()
6556    8   5    1 org.apache.hadoop.mapred.join.CompositeRecordReader.skip(K)
6557    1   1    1 org.apache.hadoop.mapred.join.CompositeRecordReader.getDelegate()
6558    6   4    0 org.apache.hadoop.mapred.join.CompositeRecordReader.accept(CompositeRecordReader.JoinCollector,K)
6559   11   5    1 org.apache.hadoop.mapred.join.CompositeRecordReader.fillJoinCollector(K)
6560    2   1    1 org.apache.hadoop.mapred.join.CompositeRecordReader.compareTo(ComposableRecordReader)
6561    8   5    0 org.apache.hadoop.mapred.join.CompositeRecordReader.createKey()
6562    5   2    1 org.apache.hadoop.mapred.join.CompositeRecordReader.createInternalValue()
6563    2   1    1 org.apache.hadoop.mapred.join.CompositeRecordReader.getPos()
6564    6   4    1 org.apache.hadoop.mapred.join.CompositeRecordReader.close()
6565    5   2    1 org.apache.hadoop.mapred.join.CompositeRecordReader.getProgress()
6566    2   1    0 org.apache.hadoop.mapred.join.InnerJoinRecordReader.InnerJoinRecordReader(int,JobConf,int,Class)
6567    6   4    1 org.apache.hadoop.mapred.join.InnerJoinRecordReader.combine(Object[],TupleWritable)
6568    3   1    0 org.apache.hadoop.mapred.join.JoinRecordReader.JoinRecordReader(int,JobConf,int,Class)
6569   15   6    1 org.apache.hadoop.mapred.join.JoinRecordReader.next(K,TupleWritable)
6570    2   1    1 org.apache.hadoop.mapred.join.JoinRecordReader.createValue()
6571    2   1    1 org.apache.hadoop.mapred.join.JoinRecordReader.getDelegate()
6572    2   1    0 org.apache.hadoop.mapred.join.JoinRecordReader.JoinDelegationIterator.hasNext()
6573    2   1    0 org.apache.hadoop.mapred.join.JoinRecordReader.JoinDelegationIterator.next(TupleWritable)
6574    2   1    0 org.apache.hadoop.mapred.join.JoinRecordReader.JoinDelegationIterator.replay(TupleWritable)
6575    2   1    0 org.apache.hadoop.mapred.join.JoinRecordReader.JoinDelegationIterator.reset()
6576    2   2    0 org.apache.hadoop.mapred.join.JoinRecordReader.JoinDelegationIterator.add(TupleWritable)
6577    2   1    0 org.apache.hadoop.mapred.join.JoinRecordReader.JoinDelegationIterator.close()
6578    2   1    0 org.apache.hadoop.mapred.join.JoinRecordReader.JoinDelegationIterator.clear()
6579    3   1    0 org.apache.hadoop.mapred.join.MultiFilterRecordReader.MultiFilterRecordReader(int,JobConf,int,Class)
6580    1   1    1 org.apache.hadoop.mapred.join.MultiFilterRecordReader.emit(TupleWritable)
6581    2   1    1 org.apache.hadoop.mapred.join.MultiFilterRecordReader.combine(Object[],TupleWritable)
6582   17   6    1 org.apache.hadoop.mapred.join.MultiFilterRecordReader.next(K,V)
6583    9   5    0 org.apache.hadoop.mapred.join.MultiFilterRecordReader.createValue()
6584    2   1    1 org.apache.hadoop.mapred.join.MultiFilterRecordReader.getDelegate()
6585    2   1    0 org.apache.hadoop.mapred.join.MultiFilterRecordReader.MultiFilterDelegationIterator.hasNext()
6586    5   2    0 org.apache.hadoop.mapred.join.MultiFilterRecordReader.MultiFilterDelegationIterator.next(V)
6587    3   1    0 org.apache.hadoop.mapred.join.MultiFilterRecordReader.MultiFilterDelegationIterator.replay(V)
6588    2   1    0 org.apache.hadoop.mapred.join.MultiFilterRecordReader.MultiFilterDelegationIterator.reset()
6589    2   2    0 org.apache.hadoop.mapred.join.MultiFilterRecordReader.MultiFilterDelegationIterator.add(V)
6590    2   1    0 org.apache.hadoop.mapred.join.MultiFilterRecordReader.MultiFilterDelegationIterator.close()
6591    2   1    0 org.apache.hadoop.mapred.join.MultiFilterRecordReader.MultiFilterDelegationIterator.clear()
6592    2   1    0 org.apache.hadoop.mapred.join.OuterJoinRecordReader.OuterJoinRecordReader(int,JobConf,int,Class)
6593    3   1    1 org.apache.hadoop.mapred.join.OuterJoinRecordReader.combine(Object[],TupleWritable)
6594    2   1    0 org.apache.hadoop.mapred.join.OverrideRecordReader.OverrideRecordReader(int,JobConf,int,Class)
6595    2   1    0 org.apache.hadoop.mapred.join.OverrideRecordReader.emit(TupleWritable)
6596   22   9    1 org.apache.hadoop.mapred.join.OverrideRecordReader.fillJoinCollector(K)
6597    2   1    0 org.apache.hadoop.mapred.join.Parser.Token.Token(TType)
6598    2   1    0 org.apache.hadoop.mapred.join.Parser.Token.getType()
6599    2   2    0 org.apache.hadoop.mapred.join.Parser.Token.getNode()
6600    2   2    0 org.apache.hadoop.mapred.join.Parser.Token.getNum()
6601    2   2    0 org.apache.hadoop.mapred.join.Parser.Token.getStr()
6602    3   1    0 org.apache.hadoop.mapred.join.Parser.NumToken.NumToken(double)
6603    2   1    0 org.apache.hadoop.mapred.join.Parser.NumToken.getNum()
6604    3   1    0 org.apache.hadoop.mapred.join.Parser.NodeToken.NodeToken(Node)
6605    2   1    0 org.apache.hadoop.mapred.join.Parser.NodeToken.getNode()
6606    3   1    0 org.apache.hadoop.mapred.join.Parser.StrToken.StrToken(TType,String)
6607    2   1    0 org.apache.hadoop.mapred.join.Parser.StrToken.getStr()
6608    9   1    0 org.apache.hadoop.mapred.join.Parser.Lexer.Lexer(String)
6609   22  17    0 org.apache.hadoop.mapred.join.Parser.Lexer.next()
6610   10  10    1 org.apache.hadoop.mapred.join.Parser.Node.forIdent(String)
6611    7   1    1 org.apache.hadoop.mapred.join.Parser.Node.addIdentifier(String,Class[],Class,Class)
6612    2   1    0 org.apache.hadoop.mapred.join.Parser.Node.Node(String)
6613    2   1    0 org.apache.hadoop.mapred.join.Parser.Node.setID(int)
6614    2   1    0 org.apache.hadoop.mapred.join.Parser.Node.setKeyComparator(Class)
6615    1   1    0 org.apache.hadoop.mapred.join.Parser.Node.parse(List,JobConf)
6616    2   1    0 org.apache.hadoop.mapred.join.Parser.WNode.addIdentifier(String,Class)
6617    2   1    0 org.apache.hadoop.mapred.join.Parser.WNode.WNode(String)
6618   19  11    1 org.apache.hadoop.mapred.join.Parser.WNode.parse(List,JobConf)
6619    4   1    0 org.apache.hadoop.mapred.join.Parser.WNode.getConf(JobConf)
6620    2   1    0 org.apache.hadoop.mapred.join.Parser.WNode.getSplits(JobConf,int)
6621   10  10    0 org.apache.hadoop.mapred.join.Parser.WNode.getRecordReader(InputSplit,JobConf,Reporter)
6622    2   1    0 org.apache.hadoop.mapred.join.Parser.WNode.toString()
6623    2   1    0 org.apache.hadoop.mapred.join.Parser.CNode.addIdentifier(String,Class)
6624    2   1    0 org.apache.hadoop.mapred.join.Parser.CNode.CNode(String)
6625    4   2    0 org.apache.hadoop.mapred.join.Parser.CNode.setKeyComparator(Class)
6626   16   9    1 org.apache.hadoop.mapred.join.Parser.CNode.getSplits(JobConf,int)
6627   18  12    0 org.apache.hadoop.mapred.join.Parser.CNode.getRecordReader(InputSplit,JobConf,Reporter)
6628    8   5    1 org.apache.hadoop.mapred.join.Parser.CNode.parse(List,JobConf)
6629    7   2    0 org.apache.hadoop.mapred.join.Parser.CNode.toString()
6630   12   8    0 org.apache.hadoop.mapred.join.Parser.reduce(Stack,JobConf)
6631   18  10    1 org.apache.hadoop.mapred.join.Parser.parse(String,JobConf)
6632    2   1    0 org.apache.hadoop.mapred.join.ResetableIterator.EMPTY.hasNext()
6633    1   1    0 org.apache.hadoop.mapred.join.ResetableIterator.EMPTY.reset()
6634    1   1    0 org.apache.hadoop.mapred.join.ResetableIterator.EMPTY.close()
6635    1   1    0 org.apache.hadoop.mapred.join.ResetableIterator.EMPTY.clear()
6636    2   1    0 org.apache.hadoop.mapred.join.ResetableIterator.EMPTY.next(U)
6637    2   1    0 org.apache.hadoop.mapred.join.ResetableIterator.EMPTY.replay(U)
6638    2   2    0 org.apache.hadoop.mapred.join.ResetableIterator.EMPTY.add(U)
6639    1   1    1 org.apache.hadoop.mapred.join.ResetableIterator.hasNext()
6640    1   1    1 org.apache.hadoop.mapred.join.ResetableIterator.next(T)
6641    1   1    1 org.apache.hadoop.mapred.join.ResetableIterator.replay(T)
6642    1   1    1 org.apache.hadoop.mapred.join.ResetableIterator.reset()
6643    1   1    1 org.apache.hadoop.mapred.join.ResetableIterator.add(T)
6644    1   1    1 org.apache.hadoop.mapred.join.ResetableIterator.close()
6645    1   1    1 org.apache.hadoop.mapred.join.ResetableIterator.clear()
6646    2   1    0 org.apache.hadoop.mapred.join.StreamBackedIterator.ReplayableByteInputStream.ReplayableByteInputStream(byte[])
6647    3   1    0 org.apache.hadoop.mapred.join.StreamBackedIterator.ReplayableByteInputStream.resetStream()
6648    1   1    0 org.apache.hadoop.mapred.join.StreamBackedIterator.StreamBackedIterator()
6649    2   2    0 org.apache.hadoop.mapred.join.StreamBackedIterator.hasNext()
6650    6   3    0 org.apache.hadoop.mapred.join.StreamBackedIterator.next(X)
6651    6   3    0 org.apache.hadoop.mapred.join.StreamBackedIterator.replay(X)
6652    6   2    0 org.apache.hadoop.mapred.join.StreamBackedIterator.reset()
6653    2   1    0 org.apache.hadoop.mapred.join.StreamBackedIterator.add(X)
6654    5   3    0 org.apache.hadoop.mapred.join.StreamBackedIterator.close()
6655    5   2    0 org.apache.hadoop.mapred.join.StreamBackedIterator.clear()
6656    1   1    1 org.apache.hadoop.mapred.join.TupleWritable.TupleWritable()
6657    3   1    1 org.apache.hadoop.mapred.join.TupleWritable.TupleWritable(Writable[])
6658    2   1    1 org.apache.hadoop.mapred.join.TupleWritable.has(int)
6659    2   1    1 org.apache.hadoop.mapred.join.TupleWritable.get(int)
6660    2   1    1 org.apache.hadoop.mapred.join.TupleWritable.size()
6661   12  10    1 org.apache.hadoop.mapred.join.TupleWritable.equals(Object)
6662    3   1    0 org.apache.hadoop.mapred.join.TupleWritable.hashCode()
6663    2   1    0 org.apache.hadoop.mapred.join.TupleWritable.Iterator$1.hasNext()
6664    6   3    0 org.apache.hadoop.mapred.join.TupleWritable.Iterator$1.next()
6665    4   3    0 org.apache.hadoop.mapred.join.TupleWritable.Iterator$1.remove()
6666   17   1    1 org.apache.hadoop.mapred.join.TupleWritable.iterator()
6667   10   4    1 org.apache.hadoop.mapred.join.TupleWritable.toString()
6668    8   4    1 org.apache.hadoop.mapred.join.TupleWritable.write(DataOutput)
6669   17  10    0 org.apache.hadoop.mapred.join.TupleWritable.readFields(DataInput)
6670    2   1    1 org.apache.hadoop.mapred.join.TupleWritable.setWritten(int)
6671    2   1    1 org.apache.hadoop.mapred.join.TupleWritable.clearWritten(int)
6672    2   1    1 org.apache.hadoop.mapred.join.TupleWritable.clearWritten()
6673   12   6    1 org.apache.hadoop.mapred.join.WrappedRecordReader.WrappedRecordReader(int,RecordReader,Class)
6674    2   1    1 org.apache.hadoop.mapred.join.WrappedRecordReader.id()
6675    2   1    1 org.apache.hadoop.mapred.join.WrappedRecordReader.key()
6676    2   1    1 org.apache.hadoop.mapred.join.WrappedRecordReader.key(K)
6677    2   1    1 org.apache.hadoop.mapred.join.WrappedRecordReader.hasNext()
6678    3   4    1 org.apache.hadoop.mapred.join.WrappedRecordReader.skip(K)
6679    3   1    1 org.apache.hadoop.mapred.join.WrappedRecordReader.next()
6680    6   4    0 org.apache.hadoop.mapred.join.WrappedRecordReader.accept(CompositeRecordReader.JoinCollector,K)
6681    7   3    1 org.apache.hadoop.mapred.join.WrappedRecordReader.next(K,U)
6682    2   1    1 org.apache.hadoop.mapred.join.WrappedRecordReader.createKey()
6683    2   1    1 org.apache.hadoop.mapred.join.WrappedRecordReader.createValue()
6684    2   1    1 org.apache.hadoop.mapred.join.WrappedRecordReader.getProgress()
6685    2   1    1 org.apache.hadoop.mapred.join.WrappedRecordReader.getPos()
6686    2   1    1 org.apache.hadoop.mapred.join.WrappedRecordReader.close()
6687    2   1    1 org.apache.hadoop.mapred.join.WrappedRecordReader.compareTo(ComposableRecordReader)
6688    2   2    0 org.apache.hadoop.mapred.join.WrappedRecordReader.equals(Object)
6689    3   1    0 org.apache.hadoop.mapred.join.WrappedRecordReader.hashCode()
6690   12   9    1 org.apache.hadoop.mapred.JSPUtil.processButtons(HttpServletRequest,HttpServletResponse,JobTracker)
6691   59  11    1 org.apache.hadoop.mapred.JSPUtil.generateJobTable(String,Collection,int,int)
6692    4   1    0 org.apache.hadoop.mapred.JVMId.JVMId(JobID,boolean,int)
6693    2   1    0 org.apache.hadoop.mapred.JVMId.JVMId(String,int,boolean,int)
6694    1   1    0 org.apache.hadoop.mapred.JVMId.JVMId()
6695    2   1    0 org.apache.hadoop.mapred.JVMId.isMapJVM()
6696    2   1    0 org.apache.hadoop.mapred.JVMId.getJobId()
6697    8   7    0 org.apache.hadoop.mapred.JVMId.equals(Object)
6698   10   6    0 org.apache.hadoop.mapred.JVMId.compareTo(ID)
6699    3   1    0 org.apache.hadoop.mapred.JVMId.toString()
6700    4   2    0 org.apache.hadoop.mapred.JVMId.toStringWOPrefix()
6701    2   1    0 org.apache.hadoop.mapred.JVMId.hashCode()
6702    4   1    0 org.apache.hadoop.mapred.JVMId.readFields(DataInput)
6703    4   1    0 org.apache.hadoop.mapred.JVMId.write(DataOutput)
6704    4   1    0 org.apache.hadoop.mapred.JVMId.read(DataInput)
6705   17  11    1 org.apache.hadoop.mapred.JVMId.forName(String)
6706    2   1    0 org.apache.hadoop.mapred.JvmManager.constructJvmEnv(List,Vector,File,File,long,File,Map,String,JobConf)
6707    4   1    0 org.apache.hadoop.mapred.JvmManager.JvmManager(TaskTracker)
6708    3   1    0 org.apache.hadoop.mapred.JvmManager.stop()
6709    5   3    0 org.apache.hadoop.mapred.JvmManager.isJvmKnown(JVMId)
6710    5   2    0 org.apache.hadoop.mapred.JvmManager.launchJvm(TaskRunner,JvmEnv)
6711    5   3    0 org.apache.hadoop.mapred.JvmManager.getTaskForJvm(JVMId)
6712    5   2    0 org.apache.hadoop.mapred.JvmManager.taskFinished(TaskRunner)
6713    5   2    0 org.apache.hadoop.mapred.JvmManager.taskKilled(TaskRunner)
6714    5   2    0 org.apache.hadoop.mapred.JvmManager.killJvm(JVMId)
6715    4   1    0 org.apache.hadoop.mapred.JvmManager.JvmManagerForType.JvmManagerForType(int,boolean,TaskTracker)
6716    4   1    0 org.apache.hadoop.mapred.JvmManager.JvmManagerForType.setRunningTaskForJvm(JVMId,TaskRunner)
6717    4   3    0 org.apache.hadoop.mapred.JvmManager.JvmManagerForType.getTaskForJvm(JVMId)
6718    2   1    0 org.apache.hadoop.mapred.JvmManager.JvmManagerForType.isJvmknown(JVMId)
6719    7   3    0 org.apache.hadoop.mapred.JvmManager.JvmManagerForType.taskFinished(TaskRunner)
6720    5   2    0 org.apache.hadoop.mapred.JvmManager.JvmManagerForType.taskKilled(TaskRunner)
6721    4   2    0 org.apache.hadoop.mapred.JvmManager.JvmManagerForType.killJvm(JVMId)
6722    5   2    0 org.apache.hadoop.mapred.JvmManager.JvmManagerForType.stop()
6723    2   1    0 org.apache.hadoop.mapred.JvmManager.JvmManagerForType.removeJvm(JVMId)
6724   26  15    0 org.apache.hadoop.mapred.JvmManager.JvmManagerForType.reapJvm(TaskRunner,TaskTracker,JvmEnv)
6725    8   2    0 org.apache.hadoop.mapred.JvmManager.JvmManagerForType.getDetails()
6726   10   2    0 org.apache.hadoop.mapred.JvmManager.JvmManagerForType.spawnNewJvm(JobID,JvmEnv,TaskTracker,TaskRunner)
6727    8   4    0 org.apache.hadoop.mapred.JvmManager.JvmManagerForType.updateOnJvmExit(JVMId,int,boolean)
6728    5   1    0 org.apache.hadoop.mapred.JvmManager.JvmManagerForType.JvmRunner.JvmRunner(JvmEnv,JobID)
6729    2   1    0 org.apache.hadoop.mapred.JvmManager.JvmManagerForType.JvmRunner.run()
6730   17   7    0 org.apache.hadoop.mapred.JvmManager.JvmManagerForType.JvmRunner.runChild(JvmEnv)
6731    6   3    0 org.apache.hadoop.mapred.JvmManager.JvmManagerForType.JvmRunner.kill()
6732    3   1    0 org.apache.hadoop.mapred.JvmManager.JvmManagerForType.JvmRunner.taskRan()
6733    2   1    0 org.apache.hadoop.mapred.JvmManager.JvmManagerForType.JvmRunner.ranAll()
6734    2   1    0 org.apache.hadoop.mapred.JvmManager.JvmManagerForType.JvmRunner.setBusy(boolean)
6735    2   1    0 org.apache.hadoop.mapred.JvmManager.JvmManagerForType.JvmRunner.isBusy()
6736    9   1    0 org.apache.hadoop.mapred.JvmManager.JvmEnv.JvmEnv(List,Vector,File,File,long,File,Map,String,JobConf)
6737    3   1    0 org.apache.hadoop.mapred.JvmTask.JvmTask(Task,boolean)
6738    1   1    0 org.apache.hadoop.mapred.JvmTask.JvmTask()
6739    2   1    0 org.apache.hadoop.mapred.JvmTask.getTask()
6740    2   1    0 org.apache.hadoop.mapred.JvmTask.shouldDie()
6741    8   2    0 org.apache.hadoop.mapred.JvmTask.write(DataOutput)
6742   10   3    0 org.apache.hadoop.mapred.JvmTask.readFields(DataInput)
6743    2   1    0 org.apache.hadoop.mapred.KeyValueLineRecordReader.getKeyClass()
6744    2   1    0 org.apache.hadoop.mapred.KeyValueLineRecordReader.createKey()
6745    2   1    0 org.apache.hadoop.mapred.KeyValueLineRecordReader.createValue()
6746    6   1    0 org.apache.hadoop.mapred.KeyValueLineRecordReader.KeyValueLineRecordReader(Configuration,FileSplit)
6747    5   4    0 org.apache.hadoop.mapred.KeyValueLineRecordReader.findSeparator(byte[],int,int,byte)
6748   26   6    1 org.apache.hadoop.mapred.KeyValueLineRecordReader.next(Text,Text)
6749    2   1    0 org.apache.hadoop.mapred.KeyValueLineRecordReader.getProgress()
6750    2   1    0 org.apache.hadoop.mapred.KeyValueLineRecordReader.getPos()
6751    2   1    0 org.apache.hadoop.mapred.KeyValueLineRecordReader.close()
6752    2   1    0 org.apache.hadoop.mapred.KeyValueTextInputFormat.configure(JobConf)
6753    2   1    0 org.apache.hadoop.mapred.KeyValueTextInputFormat.isSplitable(FileSystem,Path)
6754    3   1    0 org.apache.hadoop.mapred.KeyValueTextInputFormat.getRecordReader(InputSplit,JobConf,Reporter)
6755    2   1    0 org.apache.hadoop.mapred.KillJobAction.KillJobAction()
6756    3   1    0 org.apache.hadoop.mapred.KillJobAction.KillJobAction(JobID)
6757    2   1    0 org.apache.hadoop.mapred.KillJobAction.getJobID()
6758    2   1    0 org.apache.hadoop.mapred.KillJobAction.write(DataOutput)
6759    2   1    0 org.apache.hadoop.mapred.KillJobAction.readFields(DataInput)
6760    2   1    0 org.apache.hadoop.mapred.KillTaskAction.KillTaskAction()
6761    3   1    0 org.apache.hadoop.mapred.KillTaskAction.KillTaskAction(TaskAttemptID)
6762    2   1    0 org.apache.hadoop.mapred.KillTaskAction.getTaskID()
6763    2   1    0 org.apache.hadoop.mapred.KillTaskAction.write(DataOutput)
6764    2   1    0 org.apache.hadoop.mapred.KillTaskAction.readFields(DataInput)
6765    2   1    0 org.apache.hadoop.mapred.LaunchTaskAction.LaunchTaskAction()
6766    3   1    0 org.apache.hadoop.mapred.LaunchTaskAction.LaunchTaskAction(Task)
6767    2   1    0 org.apache.hadoop.mapred.LaunchTaskAction.getTask()
6768    3   1    0 org.apache.hadoop.mapred.LaunchTaskAction.write(DataOutput)
6769    7   2    0 org.apache.hadoop.mapred.LaunchTaskAction.readFields(DataInput)
6770    2   1    1 org.apache.hadoop.mapred.lib.aggregate.DoubleValueSum.DoubleValueSum()
6771    2   1    1 org.apache.hadoop.mapred.lib.aggregate.DoubleValueSum.addNextValue(Object)
6772    2   1    1 org.apache.hadoop.mapred.lib.aggregate.DoubleValueSum.addNextValue(double)
6773    2   1    1 org.apache.hadoop.mapred.lib.aggregate.DoubleValueSum.getReport()
6774    2   1    1 org.apache.hadoop.mapred.lib.aggregate.DoubleValueSum.getSum()
6775    2   1    1 org.apache.hadoop.mapred.lib.aggregate.DoubleValueSum.reset()
6776    4   1    1 org.apache.hadoop.mapred.lib.aggregate.DoubleValueSum.getCombinerOutput()
6777    2   1    1 org.apache.hadoop.mapred.lib.aggregate.LongValueMax.LongValueMax()
6778    4   2    1 org.apache.hadoop.mapred.lib.aggregate.LongValueMax.addNextValue(Object)
6779    3   2    1 org.apache.hadoop.mapred.lib.aggregate.LongValueMax.addNextValue(long)
6780    2   1    1 org.apache.hadoop.mapred.lib.aggregate.LongValueMax.getVal()
6781    2   1    1 org.apache.hadoop.mapred.lib.aggregate.LongValueMax.getReport()
6782    2   1    1 org.apache.hadoop.mapred.lib.aggregate.LongValueMax.reset()
6783    4   1    1 org.apache.hadoop.mapred.lib.aggregate.LongValueMax.getCombinerOutput()
6784    2   1    1 org.apache.hadoop.mapred.lib.aggregate.LongValueMin.LongValueMin()
6785    4   2    1 org.apache.hadoop.mapred.lib.aggregate.LongValueMin.addNextValue(Object)
6786    3   2    1 org.apache.hadoop.mapred.lib.aggregate.LongValueMin.addNextValue(long)
6787    2   1    1 org.apache.hadoop.mapred.lib.aggregate.LongValueMin.getVal()
6788    2   1    1 org.apache.hadoop.mapred.lib.aggregate.LongValueMin.getReport()
6789    2   1    1 org.apache.hadoop.mapred.lib.aggregate.LongValueMin.reset()
6790    4   1    1 org.apache.hadoop.mapred.lib.aggregate.LongValueMin.getCombinerOutput()
6791    2   1    1 org.apache.hadoop.mapred.lib.aggregate.LongValueSum.LongValueSum()
6792    2   1    1 org.apache.hadoop.mapred.lib.aggregate.LongValueSum.addNextValue(Object)
6793    2   1    1 org.apache.hadoop.mapred.lib.aggregate.LongValueSum.addNextValue(long)
6794    2   1    1 org.apache.hadoop.mapred.lib.aggregate.LongValueSum.getSum()
6795    2   1    1 org.apache.hadoop.mapred.lib.aggregate.LongValueSum.getReport()
6796    2   1    1 org.apache.hadoop.mapred.lib.aggregate.LongValueSum.reset()
6797    4   1    1 org.apache.hadoop.mapred.lib.aggregate.LongValueSum.getCombinerOutput()
6798    2   1    1 org.apache.hadoop.mapred.lib.aggregate.StringValueMax.StringValueMax()
6799    4   3    1 org.apache.hadoop.mapred.lib.aggregate.StringValueMax.addNextValue(Object)
6800    2   1    1 org.apache.hadoop.mapred.lib.aggregate.StringValueMax.getVal()
6801    2   1    1 org.apache.hadoop.mapred.lib.aggregate.StringValueMax.getReport()
6802    2   1    1 org.apache.hadoop.mapred.lib.aggregate.StringValueMax.reset()
6803    4   1    1 org.apache.hadoop.mapred.lib.aggregate.StringValueMax.getCombinerOutput()
6804    2   1    1 org.apache.hadoop.mapred.lib.aggregate.StringValueMin.StringValueMin()
6805    4   3    1 org.apache.hadoop.mapred.lib.aggregate.StringValueMin.addNextValue(Object)
6806    2   1    1 org.apache.hadoop.mapred.lib.aggregate.StringValueMin.getVal()
6807    2   1    1 org.apache.hadoop.mapred.lib.aggregate.StringValueMin.getReport()
6808    2   1    1 org.apache.hadoop.mapred.lib.aggregate.StringValueMin.reset()
6809    4   1    1 org.apache.hadoop.mapred.lib.aggregate.StringValueMin.getCombinerOutput()
6810    2   1    1 org.apache.hadoop.mapred.lib.aggregate.UniqValueCount.UniqValueCount()
6811    6   2    1 org.apache.hadoop.mapred.lib.aggregate.UniqValueCount.UniqValueCount(long)
6812    7   3    1 org.apache.hadoop.mapred.lib.aggregate.UniqValueCount.setMaxItems(long)
6813    4   2    1 org.apache.hadoop.mapred.lib.aggregate.UniqValueCount.addNextValue(Object)
6814    2   1    1 org.apache.hadoop.mapred.lib.aggregate.UniqValueCount.getReport()
6815    2   1    1 org.apache.hadoop.mapred.lib.aggregate.UniqValueCount.getUniqueItems()
6816    2   1    1 org.apache.hadoop.mapred.lib.aggregate.UniqValueCount.reset()
6817    8   2    1 org.apache.hadoop.mapred.lib.aggregate.UniqValueCount.getCombinerOutput()
6818   10   3    1 org.apache.hadoop.mapred.lib.aggregate.UserDefinedValueAggregatorDescriptor.createInstance(String)
6819    4   2    0 org.apache.hadoop.mapred.lib.aggregate.UserDefinedValueAggregatorDescriptor.createAggregator(JobConf)
6820    3   1    1 org.apache.hadoop.mapred.lib.aggregate.UserDefinedValueAggregatorDescriptor.UserDefinedValueAggregatorDescriptor(String,JobConf)
6821    5   2    1 org.apache.hadoop.mapred.lib.aggregate.UserDefinedValueAggregatorDescriptor.generateKeyValPairs(Object,Object)
6822    2   1    1 org.apache.hadoop.mapred.lib.aggregate.UserDefinedValueAggregatorDescriptor.toString()
6823    1   1    1 org.apache.hadoop.mapred.lib.aggregate.UserDefinedValueAggregatorDescriptor.configure(JobConf)
6824    1   1    1 org.apache.hadoop.mapred.lib.aggregate.ValueAggregator.addNextValue(Object)
6825    1   1    1 org.apache.hadoop.mapred.lib.aggregate.ValueAggregator.reset()
6826    1   1    1 org.apache.hadoop.mapred.lib.aggregate.ValueAggregator.getReport()
6827    1   1    1 org.apache.hadoop.mapred.lib.aggregate.ValueAggregator.getCombinerOutput()
6828    2   1    0 org.apache.hadoop.mapred.lib.aggregate.ValueAggregatorBaseDescriptor.MyEntry.getKey()
6829    2   1    0 org.apache.hadoop.mapred.lib.aggregate.ValueAggregatorBaseDescriptor.MyEntry.getValue()
6830    3   1    0 org.apache.hadoop.mapred.lib.aggregate.ValueAggregatorBaseDescriptor.MyEntry.setValue(Text)
6831    3   1    0 org.apache.hadoop.mapred.lib.aggregate.ValueAggregatorBaseDescriptor.MyEntry.MyEntry(Text,Text)
6832    3   1    1 org.apache.hadoop.mapred.lib.aggregate.ValueAggregatorBaseDescriptor.generateEntry(String,String,Text)
6833   25   9    1 org.apache.hadoop.mapred.lib.aggregate.ValueAggregatorBaseDescriptor.generateValueAggregator(String)
6834   12   4    1 org.apache.hadoop.mapred.lib.aggregate.ValueAggregatorBaseDescriptor.generateKeyValPairs(Object,Object)
6835    3   1    1 org.apache.hadoop.mapred.lib.aggregate.ValueAggregatorBaseDescriptor.configure(JobConf)
6836    1   1    1 org.apache.hadoop.mapred.lib.aggregate.ValueAggregatorCombiner.configure(JobConf)
6837   14   4    1 org.apache.hadoop.mapred.lib.aggregate.ValueAggregatorCombiner.reduce(Text,Iterator,OutputCollector,Reporter)
6838    1   1    1 org.apache.hadoop.mapred.lib.aggregate.ValueAggregatorCombiner.close()
6839    2   2    1 org.apache.hadoop.mapred.lib.aggregate.ValueAggregatorCombiner.map(K1,V1,OutputCollector,Reporter)
6840    1   1    1 org.apache.hadoop.mapred.lib.aggregate.ValueAggregatorDescriptor.generateKeyValPairs(Object,Object)
6841    1   1    1 org.apache.hadoop.mapred.lib.aggregate.ValueAggregatorDescriptor.configure(JobConf)
6842    9   2    0 org.apache.hadoop.mapred.lib.aggregate.ValueAggregatorJob.createValueAggregatorJobs(String[],Class[])
6843    2   1    0 org.apache.hadoop.mapred.lib.aggregate.ValueAggregatorJob.createValueAggregatorJobs(String[])
6844   47   9    1 org.apache.hadoop.mapred.lib.aggregate.ValueAggregatorJob.createValueAggregatorJob(String[])
6845    4   1    0 org.apache.hadoop.mapred.lib.aggregate.ValueAggregatorJob.createValueAggregatorJob(String[],Class[])
6846    4   2    0 org.apache.hadoop.mapred.lib.aggregate.ValueAggregatorJob.setAggregatorDescriptors(JobConf,Class[])
6847    3   1    1 org.apache.hadoop.mapred.lib.aggregate.ValueAggregatorJob.main(String[])
6848    3   1    0 org.apache.hadoop.mapred.lib.aggregate.ValueAggregatorJobBase.configure(JobConf)
6849    9   5    0 org.apache.hadoop.mapred.lib.aggregate.ValueAggregatorJobBase.getValueAggregatorDescriptor(String,JobConf)
6850   10   3    0 org.apache.hadoop.mapred.lib.aggregate.ValueAggregatorJobBase.getAggregatorDescriptors(JobConf)
6851    4   2    0 org.apache.hadoop.mapred.lib.aggregate.ValueAggregatorJobBase.initializeMySpec(JobConf)
6852    1   1    0 org.apache.hadoop.mapred.lib.aggregate.ValueAggregatorJobBase.logSpec()
6853    1   1    0 org.apache.hadoop.mapred.lib.aggregate.ValueAggregatorJobBase.close()
6854    8   3    1 org.apache.hadoop.mapred.lib.aggregate.ValueAggregatorMapper.map(K1,V1,OutputCollector,Reporter)
6855    2   2    1 org.apache.hadoop.mapred.lib.aggregate.ValueAggregatorMapper.reduce(Text,Iterator,OutputCollector,Reporter)
6856   11   2    1 org.apache.hadoop.mapred.lib.aggregate.ValueAggregatorReducer.reduce(Text,Iterator,OutputCollector,Reporter)
6857    2   2    1 org.apache.hadoop.mapred.lib.aggregate.ValueAggregatorReducer.map(K1,V1,OutputCollector,Reporter)
6858    2   1    0 org.apache.hadoop.mapred.lib.aggregate.ValueHistogram.ValueHistogram()
6859   15   3    1 org.apache.hadoop.mapred.lib.aggregate.ValueHistogram.addNextValue(Object)
6860   36   7    1 org.apache.hadoop.mapred.lib.aggregate.ValueHistogram.getReport()
6861    9   2    1 org.apache.hadoop.mapred.lib.aggregate.ValueHistogram.getReportDetails()
6862    9   2    1 org.apache.hadoop.mapred.lib.aggregate.ValueHistogram.getCombinerOutput()
6863    2   1    1 org.apache.hadoop.mapred.lib.aggregate.ValueHistogram.getReportItems()
6864    2   1    1 org.apache.hadoop.mapred.lib.aggregate.ValueHistogram.reset()
6865    2   1    1 org.apache.hadoop.mapred.lib.Chain.Chain(boolean)
6866    2   2    1 org.apache.hadoop.mapred.lib.Chain.getPrefix(boolean)
6867   10   4    1 org.apache.hadoop.mapred.lib.Chain.getChainElementConf(JobConf,String)
6868   32  18    1 org.apache.hadoop.mapred.lib.Chain.addMapper(boolean,JobConf,Class,Class,Class,Class,Class,boolean,JobConf)
6869   16   6    1 org.apache.hadoop.mapred.lib.Chain.setReducer(JobConf,Class,Class,Class,Class,Class,boolean,JobConf)
6870   26   5    1 org.apache.hadoop.mapred.lib.Chain.configure(JobConf)
6871    2   1    1 org.apache.hadoop.mapred.lib.Chain.getChainJobConf()
6872    2   2    1 org.apache.hadoop.mapred.lib.Chain.getFirstMap()
6873    2   1    1 org.apache.hadoop.mapred.lib.Chain.getReducer()
6874    4   1    0 org.apache.hadoop.mapred.lib.Chain.getMapperCollector(int,OutputCollector,Reporter)
6875    2   1    0 org.apache.hadoop.mapred.lib.Chain.getReducerCollector(OutputCollector,Reporter)
6876    5   3    1 org.apache.hadoop.mapred.lib.Chain.close()
6877    2   1    0 org.apache.hadoop.mapred.lib.Chain.ThreadLocal$1.initialValue()
6878    6   1    0 org.apache.hadoop.mapred.lib.Chain.ChainOutputCollector.ChainOutputCollector(int,Serialization,Serialization,OutputCollector,Reporter)
6879    6   1    0 org.apache.hadoop.mapred.lib.Chain.ChainOutputCollector.ChainOutputCollector(Serialization,Serialization,OutputCollector,Reporter)
6880   11   3    0 org.apache.hadoop.mapred.lib.Chain.ChainOutputCollector.collect(K,V)
6881   14   1    0 org.apache.hadoop.mapred.lib.Chain.ChainOutputCollector.makeCopyForPassByValue(Serialization,E)
6882    5   1    1 org.apache.hadoop.mapred.lib.ChainMapper.addMapper(JobConf,Class,Class,Class,Class,Class,boolean,JobConf)
6883    2   1    1 org.apache.hadoop.mapred.lib.ChainMapper.ChainMapper()
6884    2   1    1 org.apache.hadoop.mapred.lib.ChainMapper.configure(JobConf)
6885    4   2    0 org.apache.hadoop.mapred.lib.ChainMapper.map(Object,Object,OutputCollector,Reporter)
6886    2   1    1 org.apache.hadoop.mapred.lib.ChainMapper.close()
6887    5   1    1 org.apache.hadoop.mapred.lib.ChainReducer.setReducer(JobConf,Class,Class,Class,Class,Class,boolean,JobConf)
6888    4   1    1 org.apache.hadoop.mapred.lib.ChainReducer.addMapper(JobConf,Class,Class,Class,Class,Class,boolean,JobConf)
6889    2   1    1 org.apache.hadoop.mapred.lib.ChainReducer.ChainReducer()
6890    2   1    1 org.apache.hadoop.mapred.lib.ChainReducer.configure(JobConf)
6891    4   2    0 org.apache.hadoop.mapred.lib.ChainReducer.reduce(Object,Iterator,OutputCollector,Reporter)
6892    2   1    1 org.apache.hadoop.mapred.lib.ChainReducer.close()
6893    7   3    1 org.apache.hadoop.mapred.lib.db.DBConfiguration.configureDB(JobConf,String,String,String,String)
6894    2   1    1 org.apache.hadoop.mapred.lib.db.DBConfiguration.configureDB(JobConf,String,String)
6895    2   1    0 org.apache.hadoop.mapred.lib.db.DBConfiguration.DBConfiguration(JobConf)
6896    6   3    1 org.apache.hadoop.mapred.lib.db.DBConfiguration.getConnection()
6897    2   1    0 org.apache.hadoop.mapred.lib.db.DBConfiguration.getInputTableName()
6898    2   1    0 org.apache.hadoop.mapred.lib.db.DBConfiguration.setInputTableName(String)
6899    2   1    0 org.apache.hadoop.mapred.lib.db.DBConfiguration.getInputFieldNames()
6900    2   1    0 org.apache.hadoop.mapred.lib.db.DBConfiguration.setInputFieldNames(String)
6901    2   1    0 org.apache.hadoop.mapred.lib.db.DBConfiguration.getInputConditions()
6902    3   3    0 org.apache.hadoop.mapred.lib.db.DBConfiguration.setInputConditions(String)
6903    2   1    0 org.apache.hadoop.mapred.lib.db.DBConfiguration.getInputOrderBy()
6904    3   3    0 org.apache.hadoop.mapred.lib.db.DBConfiguration.setInputOrderBy(String)
6905    2   1    0 org.apache.hadoop.mapred.lib.db.DBConfiguration.getInputQuery()
6906    3   3    0 org.apache.hadoop.mapred.lib.db.DBConfiguration.setInputQuery(String)
6907    2   1    0 org.apache.hadoop.mapred.lib.db.DBConfiguration.getInputCountQuery()
6908    3   3    0 org.apache.hadoop.mapred.lib.db.DBConfiguration.setInputCountQuery(String)
6909    2   1    0 org.apache.hadoop.mapred.lib.db.DBConfiguration.getInputClass()
6910    2   1    0 org.apache.hadoop.mapred.lib.db.DBConfiguration.setInputClass(Class)
6911    2   1    0 org.apache.hadoop.mapred.lib.db.DBConfiguration.getOutputTableName()
6912    2   1    0 org.apache.hadoop.mapred.lib.db.DBConfiguration.setOutputTableName(String)
6913    2   1    0 org.apache.hadoop.mapred.lib.db.DBConfiguration.getOutputFieldNames()
6914    2   1    0 org.apache.hadoop.mapred.lib.db.DBConfiguration.setOutputFieldNames(String)
6915    6   1    1 org.apache.hadoop.mapred.lib.db.DBInputFormat.DBRecordReader.DBRecordReader(DBInputSplit,Class,JobConf)
6916   21   9    1 org.apache.hadoop.mapred.lib.db.DBInputFormat.DBRecordReader.getSelectQuery()
6917    6   3    1 org.apache.hadoop.mapred.lib.db.DBInputFormat.DBRecordReader.close()
6918    2   1    1 org.apache.hadoop.mapred.lib.db.DBInputFormat.DBRecordReader.createKey()
6919    2   1    1 org.apache.hadoop.mapred.lib.db.DBInputFormat.DBRecordReader.createValue()
6920    2   1    1 org.apache.hadoop.mapred.lib.db.DBInputFormat.DBRecordReader.getPos()
6921    2   1    1 org.apache.hadoop.mapred.lib.db.DBInputFormat.DBRecordReader.getProgress()
6922    9   5    1 org.apache.hadoop.mapred.lib.db.DBInputFormat.DBRecordReader.next(LongWritable,T)
6923    1   1    0 org.apache.hadoop.mapred.lib.db.DBInputFormat.NullDBWritable.readFields(DataInput)
6924    1   1    0 org.apache.hadoop.mapred.lib.db.DBInputFormat.NullDBWritable.readFields(ResultSet)
6925    1   1    0 org.apache.hadoop.mapred.lib.db.DBInputFormat.NullDBWritable.write(DataOutput)
6926    1   1    0 org.apache.hadoop.mapred.lib.db.DBInputFormat.NullDBWritable.write(PreparedStatement)
6927    1   1    1 org.apache.hadoop.mapred.lib.db.DBInputFormat.DBInputSplit.DBInputSplit()
6928    3   1    1 org.apache.hadoop.mapred.lib.db.DBInputFormat.DBInputSplit.DBInputSplit(long,long)
6929    2   1    1 org.apache.hadoop.mapred.lib.db.DBInputFormat.DBInputSplit.getLocations()
6930    2   1    1 org.apache.hadoop.mapred.lib.db.DBInputFormat.DBInputSplit.getStart()
6931    2   1    1 org.apache.hadoop.mapred.lib.db.DBInputFormat.DBInputSplit.getEnd()
6932    2   1    1 org.apache.hadoop.mapred.lib.db.DBInputFormat.DBInputSplit.getLength()
6933    3   1    1 org.apache.hadoop.mapred.lib.db.DBInputFormat.DBInputSplit.readFields(DataInput)
6934    3   1    1 org.apache.hadoop.mapred.lib.db.DBInputFormat.DBInputSplit.write(DataOutput)
6935   10   3    1 org.apache.hadoop.mapred.lib.db.DBInputFormat.configure(JobConf)
6936    5   4    0 org.apache.hadoop.mapred.lib.db.DBInputFormat.getRecordReader(InputSplit,JobConf,Reporter)
6937   19   6    1 org.apache.hadoop.mapred.lib.db.DBInputFormat.getSplits(JobConf,int)
6938    8   5    1 org.apache.hadoop.mapred.lib.db.DBInputFormat.getCountQuery()
6939    8   1    1 org.apache.hadoop.mapred.lib.db.DBInputFormat.setInput(JobConf,Class,String,String,String,String)
6940    6   1    1 org.apache.hadoop.mapred.lib.db.DBInputFormat.setInput(JobConf,Class,String,String)
6941    4   1    0 org.apache.hadoop.mapred.lib.db.DBOutputFormat.DBRecordWriter.DBRecordWriter(Connection,PreparedStatement)
6942   13   6    1 org.apache.hadoop.mapred.lib.db.DBOutputFormat.DBRecordWriter.close(Reporter)
6943    5   2    1 org.apache.hadoop.mapred.lib.db.DBOutputFormat.DBRecordWriter.write(K,V)
6944   10   3    1 org.apache.hadoop.mapred.lib.db.DBOutputFormat.constructQuery(String,String[])
6945    1   1    1 org.apache.hadoop.mapred.lib.db.DBOutputFormat.checkOutputSpecs(FileSystem,JobConf)
6946   10   4    1 org.apache.hadoop.mapred.lib.db.DBOutputFormat.getRecordWriter(FileSystem,JobConf,String,Progressable)
6947    6   1    1 org.apache.hadoop.mapred.lib.db.DBOutputFormat.setOutput(JobConf,String,String)
6948    1   1    1 org.apache.hadoop.mapred.lib.db.DBWritable.write(PreparedStatement)
6949    1   1    1 org.apache.hadoop.mapred.lib.db.DBWritable.readFields(ResultSet)
6950   30   9    0 org.apache.hadoop.mapred.lib.DelegatingInputFormat.getSplits(JobConf,int)
6951    4   1    0 org.apache.hadoop.mapred.lib.DelegatingInputFormat.getRecordReader(InputSplit,JobConf,Reporter)
6952    5   2    0 org.apache.hadoop.mapred.lib.DelegatingMapper.map(K1,V1,OutputCollector,Reporter)
6953    2   1    0 org.apache.hadoop.mapred.lib.DelegatingMapper.configure(JobConf)
6954    3   2    0 org.apache.hadoop.mapred.lib.DelegatingMapper.close()
6955   21   5    0 org.apache.hadoop.mapred.lib.FieldSelectionMapReduce.specToString()
6956   27   7    1 org.apache.hadoop.mapred.lib.FieldSelectionMapReduce.map(K,V,OutputCollector,Reporter)
6957   27   7    1 org.apache.hadoop.mapred.lib.FieldSelectionMapReduce.extractFields(String[],ArrayList)
6958   32   7    0 org.apache.hadoop.mapred.lib.FieldSelectionMapReduce.parseOutputKeyValueSpec()
6959    7   1    0 org.apache.hadoop.mapred.lib.FieldSelectionMapReduce.configure(JobConf)
6960    1   1    0 org.apache.hadoop.mapred.lib.FieldSelectionMapReduce.close()
6961   21  11    0 org.apache.hadoop.mapred.lib.FieldSelectionMapReduce.selectFields(String[],int[],int,String)
6962   15   4    0 org.apache.hadoop.mapred.lib.FieldSelectionMapReduce.reduce(Text,Iterator,OutputCollector,Reporter)
6963    1   1    0 org.apache.hadoop.mapred.lib.HashPartitioner.configure(JobConf)
6964    2   1    1 org.apache.hadoop.mapred.lib.HashPartitioner.getPartition(K2,V2,int)
6965    2   1    1 org.apache.hadoop.mapred.lib.IdentityMapper.map(K,V,OutputCollector,Reporter)
6966    3   2    1 org.apache.hadoop.mapred.lib.IdentityReducer.reduce(K,Iterator,OutputCollector,Reporter)
6967    5   1    0 org.apache.hadoop.mapred.lib.InputSampler.printUsage()
6968    2   1    0 org.apache.hadoop.mapred.lib.InputSampler.InputSampler(JobConf)
6969    2   1    0 org.apache.hadoop.mapred.lib.InputSampler.getConf()
6970    5   2    0 org.apache.hadoop.mapred.lib.InputSampler.setConf(Configuration)
6971    1   1    1 org.apache.hadoop.mapred.lib.InputSampler.Sampler.getSample(InputFormat,JobConf)
6972    2   1    1 org.apache.hadoop.mapred.lib.InputSampler.SplitSampler.SplitSampler(int)
6973    3   1    1 org.apache.hadoop.mapred.lib.InputSampler.SplitSampler.SplitSampler(int,int)
6974   19   4    0 org.apache.hadoop.mapred.lib.InputSampler.SplitSampler.getSample(InputFormat,JobConf)
6975    2   1    1 org.apache.hadoop.mapred.lib.InputSampler.RandomSampler.RandomSampler(double,int)
6976    4   1    1 org.apache.hadoop.mapred.lib.InputSampler.RandomSampler.RandomSampler(double,int,int)
6977   29   9    0 org.apache.hadoop.mapred.lib.InputSampler.RandomSampler.getSample(InputFormat,JobConf)
6978    2   1    1 org.apache.hadoop.mapred.lib.InputSampler.IntervalSampler.IntervalSampler(double)
6979    3   1    1 org.apache.hadoop.mapred.lib.InputSampler.IntervalSampler.IntervalSampler(double,int)
6980   19   4    0 org.apache.hadoop.mapred.lib.InputSampler.IntervalSampler.getSample(InputFormat,JobConf)
6981   22   5    0 org.apache.hadoop.mapred.lib.InputSampler.writePartitionFile(JobConf,Sampler)
6982   57  21    1 org.apache.hadoop.mapred.lib.InputSampler.run(String[])
6983    5   1    0 org.apache.hadoop.mapred.lib.InputSampler.main(String[])
6984    2   1    1 org.apache.hadoop.mapred.lib.InverseMapper.map(K,V,OutputCollector,Reporter)
6985    5   1    0 org.apache.hadoop.mapred.lib.KeyFieldBasedComparator.configure(JobConf)
6986    2   1    0 org.apache.hadoop.mapred.lib.KeyFieldBasedComparator.KeyFieldBasedComparator()
6987   17   6    0 org.apache.hadoop.mapred.lib.KeyFieldBasedComparator.compare(byte[],int,int,byte[],int,int)
6988   17  13    0 org.apache.hadoop.mapred.lib.KeyFieldBasedComparator.compareByteSequence(byte[],int,int,byte[],int,int,KeyDescription)
6989   53  35    0 org.apache.hadoop.mapred.lib.KeyFieldBasedComparator.numericalCompare(byte[],int,int,byte[],int,int)
6990    4   4    0 org.apache.hadoop.mapred.lib.KeyFieldBasedComparator.isdigit(byte)
6991   30  29    0 org.apache.hadoop.mapred.lib.KeyFieldBasedComparator.decimalCompare(byte[],int,int,byte[],int,int)
6992   10   6    0 org.apache.hadoop.mapred.lib.KeyFieldBasedComparator.decimalCompare1(byte[],int,int)
6993    6   5    0 org.apache.hadoop.mapred.lib.KeyFieldBasedComparator.oneNegativeCompare(byte[],int,int,byte[],int,int)
6994   16  12    0 org.apache.hadoop.mapred.lib.KeyFieldBasedComparator.isZero(byte[],int,int)
6995   10   2    0 org.apache.hadoop.mapred.lib.KeyFieldBasedPartitioner.configure(JobConf)
6996   15   6    0 org.apache.hadoop.mapred.lib.KeyFieldBasedPartitioner.getPartition(K2,V2,int)
6997    4   2    0 org.apache.hadoop.mapred.lib.KeyFieldBasedPartitioner.hashCode(byte[],int,int,int)
6998    4   3    0 org.apache.hadoop.mapred.lib.KeyFieldHelper.setKeyFieldSeparator(String)
6999    7   2    1 org.apache.hadoop.mapred.lib.KeyFieldHelper.setKeyFieldSpec(int,int)
7000    2   1    0 org.apache.hadoop.mapred.lib.KeyFieldHelper.keySpecs()
7001   18   6    0 org.apache.hadoop.mapred.lib.KeyFieldHelper.getWordLengths(byte[],int,int)
7002    8   5    0 org.apache.hadoop.mapred.lib.KeyFieldHelper.getStartOffset(byte[],int,int,int[],KeyDescription)
7003   14   9    0 org.apache.hadoop.mapred.lib.KeyFieldHelper.getEndOffset(byte[],int,int,int[],KeyDescription)
7004   25  13    0 org.apache.hadoop.mapred.lib.KeyFieldHelper.parseOption(String)
7005   63  28    0 org.apache.hadoop.mapred.lib.KeyFieldHelper.parseKey(String,StringTokenizer)
7006    8   1    0 org.apache.hadoop.mapred.lib.KeyFieldHelper.printKey(KeyDescription)
7007    5   2    0 org.apache.hadoop.mapred.lib.LongSumReducer.reduce(K,Iterator,OutputCollector,Reporter)
7008    5   2    1 org.apache.hadoop.mapred.lib.MultipleInputs.addInputPath(JobConf,Path,Class)
7009    6   2    1 org.apache.hadoop.mapred.lib.MultipleInputs.addInputPath(JobConf,Path,Class,Class)
7010   11   4    1 org.apache.hadoop.mapred.lib.MultipleInputs.getInputFormatMap(JobConf)
7011   13   6    0 org.apache.hadoop.mapred.lib.MultipleInputs.getMapperTypeMap(JobConf)
7012   10   2    0 org.apache.hadoop.mapred.lib.MultipleOutputFormat.RecordWriter$1.write(K,V)
7013    6   2    0 org.apache.hadoop.mapred.lib.MultipleOutputFormat.RecordWriter$1.close(Reporter)
7014   23   1    1 org.apache.hadoop.mapred.lib.MultipleOutputFormat.getRecordWriter(FileSystem,JobConf,String,Progressable)
7015    2   1    1 org.apache.hadoop.mapred.lib.MultipleOutputFormat.generateLeafFileName(String)
7016    2   1    1 org.apache.hadoop.mapred.lib.MultipleOutputFormat.generateFileNameForKeyValue(K,V,String)
7017    2   1    1 org.apache.hadoop.mapred.lib.MultipleOutputFormat.generateActualKey(K,V)
7018    2   1    1 org.apache.hadoop.mapred.lib.MultipleOutputFormat.generateActualValue(K,V)
7019   20   8    1 org.apache.hadoop.mapred.lib.MultipleOutputFormat.getInputFileBasedOutputFileName(JobConf,String)
7020    1   1    1 org.apache.hadoop.mapred.lib.MultipleOutputFormat.getBaseRecordWriter(FileSystem,JobConf,String,Progressable)
7021    7   7    1 org.apache.hadoop.mapred.lib.MultipleOutputs.checkNamedOutput(JobConf,String,boolean)
7022   11  12    1 org.apache.hadoop.mapred.lib.MultipleOutputs.checkTokenName(String)
7023    4   3    1 org.apache.hadoop.mapred.lib.MultipleOutputs.checkNamedOutputName(String)
7024    6   2    1 org.apache.hadoop.mapred.lib.MultipleOutputs.getNamedOutputsList(JobConf)
7025    3   1    1 org.apache.hadoop.mapred.lib.MultipleOutputs.isMultiNamedOutput(JobConf,String)
7026    3   1    1 org.apache.hadoop.mapred.lib.MultipleOutputs.getNamedOutputFormatClass(JobConf,String)
7027    3   1    1 org.apache.hadoop.mapred.lib.MultipleOutputs.getNamedOutputKeyClass(JobConf,String)
7028    3   1    1 org.apache.hadoop.mapred.lib.MultipleOutputs.getNamedOutputValueClass(JobConf,String)
7029    2   1    1 org.apache.hadoop.mapred.lib.MultipleOutputs.addNamedOutput(JobConf,String,Class,Class,Class)
7030    2   1    1 org.apache.hadoop.mapred.lib.MultipleOutputs.addMultiNamedOutput(JobConf,String,Class,Class,Class)
7031    8   1    1 org.apache.hadoop.mapred.lib.MultipleOutputs.addNamedOutput(JobConf,String,boolean,Class,Class,Class)
7032    2   1    1 org.apache.hadoop.mapred.lib.MultipleOutputs.setCountersEnabled(JobConf,boolean)
7033    2   1    1 org.apache.hadoop.mapred.lib.MultipleOutputs.getCountersEnabled(JobConf)
7034    6   1    1 org.apache.hadoop.mapred.lib.MultipleOutputs.MultipleOutputs(JobConf)
7035    2   1    1 org.apache.hadoop.mapred.lib.MultipleOutputs.getNamedOutputs()
7036   15   8    0 org.apache.hadoop.mapred.lib.MultipleOutputs.getRecordWriter(String,String,Reporter)
7037    4   1    0 org.apache.hadoop.mapred.lib.MultipleOutputs.RecordWriterWithCounter.RecordWriterWithCounter(RecordWriter,String,Reporter)
7038    3   1    0 org.apache.hadoop.mapred.lib.MultipleOutputs.RecordWriterWithCounter.write(Object,Object)
7039    2   1    0 org.apache.hadoop.mapred.lib.MultipleOutputs.RecordWriterWithCounter.close(Reporter)
7040    2   1    0 org.apache.hadoop.mapred.lib.MultipleOutputs.getCollector(String,Reporter)
7041    2   1    0 org.apache.hadoop.mapred.lib.MultipleOutputs.OutputCollector$1.collect(Object,Object)
7042   14   8    0 org.apache.hadoop.mapred.lib.MultipleOutputs.getCollector(String,String,Reporter)
7043    3   2    1 org.apache.hadoop.mapred.lib.MultipleOutputs.close()
7044    9   1    0 org.apache.hadoop.mapred.lib.MultipleOutputs.InternalFileOutputFormat.getRecordWriter(FileSystem,JobConf,String,Progressable)
7045    4   2    0 org.apache.hadoop.mapred.lib.MultipleSequenceFileOutputFormat.getBaseRecordWriter(FileSystem,JobConf,String,Progressable)
7046    4   2    0 org.apache.hadoop.mapred.lib.MultipleTextOutputFormat.getBaseRecordWriter(FileSystem,JobConf,String,Progressable)
7047    8   3    0 org.apache.hadoop.mapred.lib.MultithreadedMapRunner.configure(JobConf)
7048    2   1    0 org.apache.hadoop.mapred.lib.MultithreadedMapRunner.BlockingArrayQueue.BlockingArrayQueue(int)
7049    2   1    0 org.apache.hadoop.mapred.lib.MultithreadedMapRunner.BlockingArrayQueue.offer(Runnable)
7050    5   2    0 org.apache.hadoop.mapred.lib.MultithreadedMapRunner.BlockingArrayQueue.add(Runnable)
7051    5   5    0 org.apache.hadoop.mapred.lib.MultithreadedMapRunner.checkForExceptionsFromProcessingThreads()
7052   23   9    0 org.apache.hadoop.mapred.lib.MultithreadedMapRunner.run(RecordReader,OutputCollector,Reporter)
7053    5   1    1 org.apache.hadoop.mapred.lib.MultithreadedMapRunner.MapperInvokeRunable.MapperInvokeRunable(K1,V1,OutputCollector,Reporter)
7054   12   6    1 org.apache.hadoop.mapred.lib.MultithreadedMapRunner.MapperInvokeRunable.run()
7055    3   1    0 org.apache.hadoop.mapred.lib.NLineInputFormat.getRecordReader(InputSplit,JobConf,Reporter)
7056   29   8    1 org.apache.hadoop.mapred.lib.NLineInputFormat.getSplits(JobConf,int)
7057    2   1    0 org.apache.hadoop.mapred.lib.NLineInputFormat.configure(JobConf)
7058    1   1    0 org.apache.hadoop.mapred.lib.NullOutputFormat.RecordWriter$1.write(K,V)
7059    1   1    0 org.apache.hadoop.mapred.lib.NullOutputFormat.RecordWriter$1.close(Reporter)
7060    4   1    0 org.apache.hadoop.mapred.lib.NullOutputFormat.getRecordWriter(FileSystem,JobConf,String,Progressable)
7061    1   1    0 org.apache.hadoop.mapred.lib.NullOutputFormat.checkOutputSpecs(FileSystem,JobConf)
7062    3   1    0 org.apache.hadoop.mapred.lib.RegexMapper.configure(JobConf)
7063    5   2    0 org.apache.hadoop.mapred.lib.RegexMapper.map(K,Text,OutputCollector,Reporter)
7064    1   1    0 org.apache.hadoop.mapred.lib.TaggedInputSplit.TaggedInputSplit()
7065    6   1    1 org.apache.hadoop.mapred.lib.TaggedInputSplit.TaggedInputSplit(InputSplit,Configuration,Class,Class)
7066    2   1    1 org.apache.hadoop.mapred.lib.TaggedInputSplit.getInputSplit()
7067    2   1    1 org.apache.hadoop.mapred.lib.TaggedInputSplit.getInputFormatClass()
7068    2   1    1 org.apache.hadoop.mapred.lib.TaggedInputSplit.getMapperClass()
7069    2   1    0 org.apache.hadoop.mapred.lib.TaggedInputSplit.getLength()
7070    2   1    0 org.apache.hadoop.mapred.lib.TaggedInputSplit.getLocations()
7071    6   1    0 org.apache.hadoop.mapred.lib.TaggedInputSplit.readFields(DataInput)
7072    5   4    0 org.apache.hadoop.mapred.lib.TaggedInputSplit.readClass(DataInput)
7073    5   1    0 org.apache.hadoop.mapred.lib.TaggedInputSplit.write(DataOutput)
7074    2   1    0 org.apache.hadoop.mapred.lib.TaggedInputSplit.getConf()
7075    2   1    0 org.apache.hadoop.mapred.lib.TaggedInputSplit.setConf(Configuration)
7076    5   2    0 org.apache.hadoop.mapred.lib.TokenCountMapper.map(K,Text,OutputCollector,Reporter)
7077    1   1    0 org.apache.hadoop.mapred.lib.TotalOrderPartitioner.TotalOrderPartitioner()
7078   19  11    0 org.apache.hadoop.mapred.lib.TotalOrderPartitioner.configure(JobConf)
7079    2   1    0 org.apache.hadoop.mapred.lib.TotalOrderPartitioner.getPartition(K,V,int)
7080    2   1    1 org.apache.hadoop.mapred.lib.TotalOrderPartitioner.setPartitionFile(JobConf,Path)
7081    2   1    1 org.apache.hadoop.mapred.lib.TotalOrderPartitioner.getPartitionFile(JobConf)
7082    1   1    1 org.apache.hadoop.mapred.lib.TotalOrderPartitioner.Node.findPartition(T)
7083    2   1    0 org.apache.hadoop.mapred.lib.TotalOrderPartitioner.TrieNode.TrieNode(int)
7084    2   1    0 org.apache.hadoop.mapred.lib.TotalOrderPartitioner.TrieNode.getLevel()
7085    3   1    0 org.apache.hadoop.mapred.lib.TotalOrderPartitioner.BinarySearchNode.BinarySearchNode(K[],RawComparator)
7086    3   2    0 org.apache.hadoop.mapred.lib.TotalOrderPartitioner.BinarySearchNode.findPartition(K)
7087    2   1    0 org.apache.hadoop.mapred.lib.TotalOrderPartitioner.InnerTrieNode.InnerTrieNode(int)
7088    5   3    0 org.apache.hadoop.mapred.lib.TotalOrderPartitioner.InnerTrieNode.findPartition(BinaryComparable)
7089    5   1    0 org.apache.hadoop.mapred.lib.TotalOrderPartitioner.LeafTrieNode.LeafTrieNode(int,BinaryComparable[],int,int)
7090    3   2    0 org.apache.hadoop.mapred.lib.TotalOrderPartitioner.LeafTrieNode.findPartition(BinaryComparable)
7091   10   2    0 org.apache.hadoop.mapred.lib.TotalOrderPartitioner.readPartitions(FileSystem,Path,Class,JobConf)
7092   19   7    1 org.apache.hadoop.mapred.lib.TotalOrderPartitioner.buildTrie(BinaryComparable[],int,int,byte[],int)
7093    2   1    0 org.apache.hadoop.mapred.LimitTasksPerJobTaskScheduler.LimitTasksPerJobTaskScheduler()
7094    5   1    0 org.apache.hadoop.mapred.LimitTasksPerJobTaskScheduler.start()
7095    7   3    0 org.apache.hadoop.mapred.LimitTasksPerJobTaskScheduler.setConf(Configuration)
7096   32  18    0 org.apache.hadoop.mapred.LimitTasksPerJobTaskScheduler.assignTasks(TaskTrackerStatus)
7097   16   4    1 org.apache.hadoop.mapred.LimitTasksPerJobTaskScheduler.getMaxMapAndReduceLoad(int,int)
7098    2   1    0 org.apache.hadoop.mapred.LineRecordReader.LineReader.LineReader(InputStream)
7099    2   1    0 org.apache.hadoop.mapred.LineRecordReader.LineReader.LineReader(InputStream,int)
7100    2   1    0 org.apache.hadoop.mapred.LineRecordReader.LineReader.LineReader(InputStream,Configuration)
7101   22   4    0 org.apache.hadoop.mapred.LineRecordReader.LineRecordReader(Configuration,FileSplit)
7102    6   1    0 org.apache.hadoop.mapred.LineRecordReader.LineRecordReader(InputStream,long,long,int)
7103    6   1    0 org.apache.hadoop.mapred.LineRecordReader.LineRecordReader(InputStream,long,long,Configuration)
7104    2   1    0 org.apache.hadoop.mapred.LineRecordReader.createKey()
7105    2   1    0 org.apache.hadoop.mapred.LineRecordReader.createValue()
7106   11   6    1 org.apache.hadoop.mapred.LineRecordReader.next(LongWritable,Text)
7107    5   3    1 org.apache.hadoop.mapred.LineRecordReader.getProgress()
7108    2   1    0 org.apache.hadoop.mapred.LineRecordReader.getPos()
7109    3   2    0 org.apache.hadoop.mapred.LineRecordReader.close()
7110    2   1    0 org.apache.hadoop.mapred.LocalJobRunner.getProtocolVersion(String,long)
7111    2   1    0 org.apache.hadoop.mapred.LocalJobRunner.Job.getProtocolVersion(String,long)
7112   13   1    0 org.apache.hadoop.mapred.LocalJobRunner.Job.Job(JobID,JobConf)
7113    2   1    0 org.apache.hadoop.mapred.LocalJobRunner.Job.getProfile()
7114   70  14    0 org.apache.hadoop.mapred.LocalJobRunner.Job.run()
7115    2   1    0 org.apache.hadoop.mapred.LocalJobRunner.Job.getTask(JVMId)
7116   10   2    0 org.apache.hadoop.mapred.LocalJobRunner.Job.statusUpdate(TaskAttemptID,TaskStatus)
7117    2   1    1 org.apache.hadoop.mapred.LocalJobRunner.Job.commitPending(TaskAttemptID,TaskStatus)
7118    2   1    1 org.apache.hadoop.mapred.LocalJobRunner.Job.updateCounters(Task)
7119    1   1    0 org.apache.hadoop.mapred.LocalJobRunner.Job.reportDiagnosticInfo(TaskAttemptID,String)
7120    2   1    0 org.apache.hadoop.mapred.LocalJobRunner.Job.reportNextRecordRange(TaskAttemptID,SortedRanges.Range)
7121    2   1    0 org.apache.hadoop.mapred.LocalJobRunner.Job.ping(TaskAttemptID)
7122    2   1    0 org.apache.hadoop.mapred.LocalJobRunner.Job.canCommit(TaskAttemptID)
7123    6   2    0 org.apache.hadoop.mapred.LocalJobRunner.Job.done(TaskAttemptID)
7124    2   1    0 org.apache.hadoop.mapred.LocalJobRunner.Job.fsError(TaskAttemptID,String)
7125    2   1    0 org.apache.hadoop.mapred.LocalJobRunner.Job.shuffleError(TaskAttemptID,String)
7126    2   1    0 org.apache.hadoop.mapred.LocalJobRunner.Job.getMapCompletionEvents(JobID,int,int,TaskAttemptID)
7127    4   1    0 org.apache.hadoop.mapred.LocalJobRunner.LocalJobRunner(JobConf)
7128    2   1    0 org.apache.hadoop.mapred.LocalJobRunner.getNewJobId()
7129    2   1    0 org.apache.hadoop.mapred.LocalJobRunner.submitJob(JobID)
7130    2   1    0 org.apache.hadoop.mapred.LocalJobRunner.killJob(JobID)
7131    2   2    0 org.apache.hadoop.mapred.LocalJobRunner.setJobPriority(JobID,String)
7132    2   2    1 org.apache.hadoop.mapred.LocalJobRunner.killTask(TaskAttemptID,boolean)
7133    6   3    0 org.apache.hadoop.mapred.LocalJobRunner.getJobProfile(JobID)
7134    2   1    0 org.apache.hadoop.mapred.LocalJobRunner.getMapTaskReports(JobID)
7135    2   1    0 org.apache.hadoop.mapred.LocalJobRunner.getReduceTaskReports(JobID)
7136    2   1    0 org.apache.hadoop.mapred.LocalJobRunner.getCleanupTaskReports(JobID)
7137    2   1    0 org.apache.hadoop.mapred.LocalJobRunner.getSetupTaskReports(JobID)
7138    6   3    0 org.apache.hadoop.mapred.LocalJobRunner.getJobStatus(JobID)
7139    3   1    0 org.apache.hadoop.mapred.LocalJobRunner.getJobCounters(JobID)
7140    2   1    0 org.apache.hadoop.mapred.LocalJobRunner.getFilesystemName()
7141    2   1    0 org.apache.hadoop.mapred.LocalJobRunner.getClusterStatus()
7142    2   1    0 org.apache.hadoop.mapred.LocalJobRunner.jobsToComplete()
7143    2   1    0 org.apache.hadoop.mapred.LocalJobRunner.getTaskCompletionEvents(JobID,int,int)
7144    2   1    0 org.apache.hadoop.mapred.LocalJobRunner.getAllJobs()
7145    2   1    1 org.apache.hadoop.mapred.LocalJobRunner.getTaskDiagnostics(TaskAttemptID)
7146    3   1    1 org.apache.hadoop.mapred.LocalJobRunner.getSystemDir()
7147    2   1    0 org.apache.hadoop.mapred.LocalJobRunner.getJobsFromQueue(String)
7148    2   1    0 org.apache.hadoop.mapred.LocalJobRunner.getQueues()
7149    2   1    0 org.apache.hadoop.mapred.LocalJobRunner.getQueueInfo(String)
7150    2   1    0 org.apache.hadoop.mapred.MapFileOutputFormat.RecordWriter$1.write(WritableComparable,Writable)
7151    2   1    0 org.apache.hadoop.mapred.MapFileOutputFormat.RecordWriter$1.close(Reporter)
7152   15   2    0 org.apache.hadoop.mapred.MapFileOutputFormat.getRecordWriter(FileSystem,JobConf,String,Progressable)
7153    8   2    1 org.apache.hadoop.mapred.MapFileOutputFormat.getReaders(FileSystem,Path,Configuration)
7154    3   1    1 org.apache.hadoop.mapred.MapFileOutputFormat.getEntry(MapFile.Reader[],Partitioner,K,V)
7155    1   1    0 org.apache.hadoop.mapred.MapOutputFile.MapOutputFile()
7156    2   1    0 org.apache.hadoop.mapred.MapOutputFile.MapOutputFile(JobID)
7157    2   1    1 org.apache.hadoop.mapred.MapOutputFile.getOutputFile(TaskAttemptID)
7158    2   1    1 org.apache.hadoop.mapred.MapOutputFile.getOutputFileForWrite(TaskAttemptID,long)
7159    2   1    1 org.apache.hadoop.mapred.MapOutputFile.getOutputIndexFile(TaskAttemptID)
7160    2   1    1 org.apache.hadoop.mapred.MapOutputFile.getOutputIndexFileForWrite(TaskAttemptID,long)
7161    2   1    1 org.apache.hadoop.mapred.MapOutputFile.getSpillFile(TaskAttemptID,int)
7162    2   1    1 org.apache.hadoop.mapred.MapOutputFile.getSpillFileForWrite(TaskAttemptID,int,long)
7163    2   1    1 org.apache.hadoop.mapred.MapOutputFile.getSpillIndexFile(TaskAttemptID,int)
7164    2   1    1 org.apache.hadoop.mapred.MapOutputFile.getSpillIndexFileForWrite(TaskAttemptID,int,long)
7165    2   1    1 org.apache.hadoop.mapred.MapOutputFile.getInputFile(int,TaskAttemptID)
7166    2   1    1 org.apache.hadoop.mapred.MapOutputFile.getInputFileForWrite(TaskID,TaskAttemptID,long)
7167    2   1    1 org.apache.hadoop.mapred.MapOutputFile.removeAll(TaskAttemptID)
7168    5   2    0 org.apache.hadoop.mapred.MapOutputFile.setConf(Configuration)
7169    2   1    0 org.apache.hadoop.mapred.MapOutputFile.setJobId(JobID)
7170    2   1    0 org.apache.hadoop.mapred.HistoryViewer.PathFilter$1.accept(Path)
7171   20   7    0 org.apache.hadoop.mapred.HistoryViewer.HistoryViewer(String,Configuration,boolean)
7172   25   2    0 org.apache.hadoop.mapred.HistoryViewer.print()
7173   15   2    0 org.apache.hadoop.mapred.HistoryViewer.printJobDetails()
7174   20   7    0 org.apache.hadoop.mapred.HistoryViewer.printTasks(String,String)
7175   29   7    0 org.apache.hadoop.mapred.HistoryViewer.printAllTaskAttempts(String)
7176  116  29    0 org.apache.hadoop.mapred.HistoryViewer.printTaskSummary()
7177   18   4    0 org.apache.hadoop.mapred.HistoryViewer.printFailedAttempts(NodesFilter)
7178   42  11    0 org.apache.hadoop.mapred.HistoryViewer.printJobAnalysis()
7179   14   2    0 org.apache.hadoop.mapred.HistoryViewer.printLast(JobHistory.Task[],String,Comparator)
7180   32   7    0 org.apache.hadoop.mapred.HistoryViewer.printAnalysis(JobHistory.Task[],Comparator,String,long,int)
7181    4   3    0 org.apache.hadoop.mapred.HistoryViewer.Comparator$2.compare(JobHistory.Task,JobHistory.Task)
7182    4   3    0 org.apache.hadoop.mapred.HistoryViewer.Comparator$3.compare(JobHistory.Task,JobHistory.Task)
7183    4   3    0 org.apache.hadoop.mapred.HistoryViewer.Comparator$4.compare(JobHistory.Task,JobHistory.Task)
7184    4   3    0 org.apache.hadoop.mapred.HistoryViewer.Comparator$5.compare(JobHistory.Task,JobHistory.Task)
7185    4   3    0 org.apache.hadoop.mapred.HistoryViewer.Comparator$6.compare(JobHistory.Task,JobHistory.Task)
7186    1   1    1 org.apache.hadoop.mapred.JobConfigurable.configure(JobConf)
7187    2   1    0 org.apache.hadoop.mapred.JobTracker.IllegalStateException.IllegalStateException(String)
7188   18  10    1 org.apache.hadoop.mapred.JobTracker.startTracker(JobConf)
7189    3   1    0 org.apache.hadoop.mapred.JobTracker.stopTracker()
7190    8   6    0 org.apache.hadoop.mapred.JobTracker.getProtocolVersion(String,long)
7191   30   9    0 org.apache.hadoop.mapred.JobTracker.ExpireLaunchingTasks.run()
7192    3   1    0 org.apache.hadoop.mapred.JobTracker.ExpireLaunchingTasks.addNewTask(TaskAttemptID)
7193    3   1    0 org.apache.hadoop.mapred.JobTracker.ExpireLaunchingTasks.removeTask(TaskAttemptID)
7194    1   1    0 org.apache.hadoop.mapred.JobTracker.ExpireTrackers.ExpireTrackers()
7195   22   9    1 org.apache.hadoop.mapred.JobTracker.ExpireTrackers.run()
7196    1   1    0 org.apache.hadoop.mapred.JobTracker.RetireJobs.RetireJobs()
7197   31  13    1 org.apache.hadoop.mapred.JobTracker.RetireJobs.run()
7198    3   1    0 org.apache.hadoop.mapred.JobTracker.RecoveryManager.JobRecoveryListener.JobRecoveryListener(JobInProgress)
7199    7   2    1 org.apache.hadoop.mapred.JobTracker.RecoveryManager.JobRecoveryListener.processTask(String,JobHistory.Task)
7200   12   3    1 org.apache.hadoop.mapred.JobTracker.RecoveryManager.JobRecoveryListener.processTaskAttempt(String,JobHistory.TaskAttempt)
7201   28  11    0 org.apache.hadoop.mapred.JobTracker.RecoveryManager.JobRecoveryListener.handle(JobHistory.RecordTypes,Map)
7202    3   1    0 org.apache.hadoop.mapred.JobTracker.RecoveryManager.JobRecoveryListener.isCleanup(JobHistory.Task)
7203    6   2    0 org.apache.hadoop.mapred.JobTracker.RecoveryManager.JobRecoveryListener.checkAndInit()
7204    4   2    0 org.apache.hadoop.mapred.JobTracker.RecoveryManager.JobRecoveryListener.close()
7205    2   1    0 org.apache.hadoop.mapred.JobTracker.RecoveryManager.JobRecoveryListener.getNumEventsRecovered()
7206    2   1    0 org.apache.hadoop.mapred.JobTracker.RecoveryManager.RecoveryManager()
7207    2   1    0 org.apache.hadoop.mapred.JobTracker.RecoveryManager.contains(JobID)
7208    2   1    0 org.apache.hadoop.mapred.JobTracker.RecoveryManager.addJobForRecovery(JobID)
7209    2   1    0 org.apache.hadoop.mapred.JobTracker.RecoveryManager.shouldRecover()
7210    8   7    1 org.apache.hadoop.mapred.JobTracker.RecoveryManager.isJobNameValid(String)
7211   11   3    0 org.apache.hadoop.mapred.JobTracker.RecoveryManager.checkAndAddJob(FileStatus)
7212    8   1    0 org.apache.hadoop.mapred.JobTracker.RecoveryManager.updateJob(JobInProgress,JobHistory.JobInfo)
7213   12   4    0 org.apache.hadoop.mapred.JobTracker.RecoveryManager.updateTip(TaskInProgress,JobHistory.Task)
7214   27   3    0 org.apache.hadoop.mapred.JobTracker.RecoveryManager.createTaskAttempt(JobInProgress,TaskAttemptID,JobHistory.TaskAttempt)
7215   24   3    0 org.apache.hadoop.mapred.JobTracker.RecoveryManager.addSuccessfulAttempt(JobInProgress,TaskAttemptID,JobHistory.TaskAttempt)
7216   18   2    0 org.apache.hadoop.mapred.JobTracker.RecoveryManager.addUnsuccessfulAttempt(JobInProgress,TaskAttemptID,JobHistory.TaskAttempt)
7217   36   5    0 org.apache.hadoop.mapred.JobTracker.RecoveryManager.recover()
7218    2   1    0 org.apache.hadoop.mapred.JobTracker.RecoveryManager.totalEventsRecovered()
7219    8   5    0 org.apache.hadoop.mapred.JobTracker.Comparator$1.compare(TaskTrackerStatus,TaskTrackerStatus)
7220   84  19    1 org.apache.hadoop.mapred.JobTracker.JobTracker(JobConf)
7221    2   1    0 org.apache.hadoop.mapred.JobTracker.getDateFormat()
7222    5   3    0 org.apache.hadoop.mapred.JobTracker.validateIdentifier(String)
7223    5   3    0 org.apache.hadoop.mapred.JobTracker.validateJobNumber(String)
7224    2   1    1 org.apache.hadoop.mapred.JobTracker.hasRestarted()
7225    2   1    1 org.apache.hadoop.mapred.JobTracker.hasRecovered()
7226    2   2    1 org.apache.hadoop.mapred.JobTracker.getRecoveryDuration()
7227    2   1    0 org.apache.hadoop.mapred.JobTracker.getInstrumentationClass(Configuration)
7228    2   1    0 org.apache.hadoop.mapred.JobTracker.setInstrumentationClass(Configuration,Class)
7229    3   1    0 org.apache.hadoop.mapred.JobTracker.getAddress(Configuration)
7230   17   2    1 org.apache.hadoop.mapred.JobTracker.offerService()
7231   37  17    0 org.apache.hadoop.mapred.JobTracker.close()
7232   13   3    0 org.apache.hadoop.mapred.JobTracker.createTaskEntry(TaskAttemptID,String,TaskInProgress)
7233    8   3    0 org.apache.hadoop.mapred.JobTracker.removeTaskEntry(TaskAttemptID)
7234    7   2    1 org.apache.hadoop.mapred.JobTracker.markCompletedTaskAttempt(String,TaskAttemptID)
7235    9  11    1 org.apache.hadoop.mapred.JobTracker.markCompletedJob(JobInProgress)
7236    7   3    1 org.apache.hadoop.mapred.JobTracker.removeMarkedTasks(String)
7237    7   5    1 org.apache.hadoop.mapred.JobTracker.removeJobTasks(JobInProgress)
7238   36  11    1 org.apache.hadoop.mapred.JobTracker.finalizeJob(JobInProgress)
7239    2   1    0 org.apache.hadoop.mapred.JobTracker.getTotalSubmissions()
7240    2   1    0 org.apache.hadoop.mapred.JobTracker.getJobTrackerMachine()
7241    2   1    1 org.apache.hadoop.mapred.JobTracker.getTrackerIdentifier()
7242    2   1    0 org.apache.hadoop.mapred.JobTracker.getTrackerPort()
7243    2   1    0 org.apache.hadoop.mapred.JobTracker.getInfoPort()
7244    2   1    0 org.apache.hadoop.mapred.JobTracker.getStartTime()
7245    8   3    0 org.apache.hadoop.mapred.JobTracker.runningJobs()
7246    3   1    1 org.apache.hadoop.mapred.JobTracker.getRunningJobs()
7247    8   4    0 org.apache.hadoop.mapred.JobTracker.failedJobs()
7248    8   3    0 org.apache.hadoop.mapred.JobTracker.completedJobs()
7249    3   1    0 org.apache.hadoop.mapred.JobTracker.taskTrackers()
7250    3   1    0 org.apache.hadoop.mapred.JobTracker.getTaskTracker(String)
7251    4   2    1 org.apache.hadoop.mapred.JobTracker.addNewTracker(TaskTrackerStatus)
7252    7   1    0 org.apache.hadoop.mapred.JobTracker.resolveAndAddToTopology(String)
7253   14   4    0 org.apache.hadoop.mapred.JobTracker.addHostToNodeMapping(String,String)
7254    2   1    1 org.apache.hadoop.mapred.JobTracker.getNodesAtMaxLevel()
7255    4   2    0 org.apache.hadoop.mapred.JobTracker.getParentNode(Node,int)
7256    2   1    1 org.apache.hadoop.mapred.JobTracker.getNode(String)
7257    2   1    0 org.apache.hadoop.mapred.JobTracker.getNumTaskCacheLevels()
7258    2   1    0 org.apache.hadoop.mapred.JobTracker.getNumResolvedTaskTrackers()
7259    2   1    0 org.apache.hadoop.mapred.JobTracker.getNumberOfUniqueHosts()
7260    2   1    0 org.apache.hadoop.mapred.JobTracker.addJobInProgressListener(JobInProgressListener)
7261    2   1    0 org.apache.hadoop.mapred.JobTracker.removeJobInProgressListener(JobInProgressListener)
7262    3   2    0 org.apache.hadoop.mapred.JobTracker.updateJobInProgressListeners(JobChangeEvent)
7263    2   1    1 org.apache.hadoop.mapred.JobTracker.getQueueManager()
7264    2   1    0 org.apache.hadoop.mapred.JobTracker.getBuildVersion()
7265   52  20    1 org.apache.hadoop.mapred.JobTracker.heartbeat(TaskTrackerStatus,boolean,boolean,short)
7266    4   1    1 org.apache.hadoop.mapred.JobTracker.getNextHeartbeatInterval()
7267    3   2    1 org.apache.hadoop.mapred.JobTracker.inHostsList(TaskTrackerStatus)
7268    3   1    1 org.apache.hadoop.mapred.JobTracker.inExcludedHostsList(TaskTrackerStatus)
7269    2   2    1 org.apache.hadoop.mapred.JobTracker.acceptTaskTracker(TaskTrackerStatus)
7270   31   8    1 org.apache.hadoop.mapred.JobTracker.updateTaskTrackerStatus(String,TaskTrackerStatus)
7271   18   6    1 org.apache.hadoop.mapred.JobTracker.processHeartbeat(TaskTrackerStatus,boolean)
7272   19   8    1 org.apache.hadoop.mapred.JobTracker.getTasksToKill(String)
7273   13   6    1 org.apache.hadoop.mapred.JobTracker.getTasksToSave(TaskTrackerStatus)
7274   32  15    0 org.apache.hadoop.mapred.JobTracker.getSetupAndCleanupTasks(TaskTrackerStatus)
7275    4   3    1 org.apache.hadoop.mapred.JobTracker.getFilesystemName()
7276    2   1    0 org.apache.hadoop.mapred.JobTracker.reportTaskTrackerError(String,String,String)
7277    2   1    1 org.apache.hadoop.mapred.JobTracker.getJobUniqueString(String)
7278    2   1    1 org.apache.hadoop.mapred.JobTracker.getNewJobId()
7279    5   3    1 org.apache.hadoop.mapred.JobTracker.submitJob(JobID)
7280   10   2    1 org.apache.hadoop.mapred.JobTracker.addJob(JobID,JobInProgress)
7281    5   3    0 org.apache.hadoop.mapred.JobTracker.checkAccess(JobInProgress,QueueManager.QueueOperation)
7282    3   1    0 org.apache.hadoop.mapred.JobTracker.getClusterStatus()
7283    9   3    0 org.apache.hadoop.mapred.JobTracker.killJob(JobID)
7284    5   1    1 org.apache.hadoop.mapred.JobTracker.setJobPriority(JobID,String)
7285    6   3    0 org.apache.hadoop.mapred.JobTracker.getJobProfile(JobID)
7286    6   3    0 org.apache.hadoop.mapred.JobTracker.getJobStatus(JobID)
7287    6   3    0 org.apache.hadoop.mapred.JobTracker.getJobCounters(JobID)
7288   15   5    0 org.apache.hadoop.mapred.JobTracker.getMapTaskReports(JobID)
7289   15   5    0 org.apache.hadoop.mapred.JobTracker.getReduceTaskReports(JobID)
7290   15   5    0 org.apache.hadoop.mapred.JobTracker.getCleanupTaskReports(JobID)
7291   15   5    0 org.apache.hadoop.mapred.JobTracker.getSetupTaskReports(JobID)
7292    9   3    0 org.apache.hadoop.mapred.JobTracker.getTaskCompletionEvents(JobID,int,int)
7293   11   6    1 org.apache.hadoop.mapred.JobTracker.getTaskDiagnostics(TaskAttemptID)
7294    3   2    1 org.apache.hadoop.mapred.JobTracker.getTaskStatuses(TaskID)
7295    3   2    1 org.apache.hadoop.mapred.JobTracker.getTaskStatus(TaskAttemptID)
7296    3   2    1 org.apache.hadoop.mapred.JobTracker.getTipCounters(TaskID)
7297    2   1    1 org.apache.hadoop.mapred.JobTracker.getTaskScheduler()
7298    3   2    1 org.apache.hadoop.mapred.JobTracker.getTip(TaskID)
7299    8   3    1 org.apache.hadoop.mapred.JobTracker.killTask(TaskAttemptID,boolean)
7300    2   1    1 org.apache.hadoop.mapred.JobTracker.getAssignedTracker(TaskAttemptID)
7301    2   1    0 org.apache.hadoop.mapred.JobTracker.jobsToComplete()
7302    2   1    0 org.apache.hadoop.mapred.JobTracker.getAllJobs()
7303    3   1    1 org.apache.hadoop.mapred.JobTracker.getSystemDir()
7304    2   1    0 org.apache.hadoop.mapred.JobTracker.getJob(JobID)
7305   11   2    1 org.apache.hadoop.mapred.JobTracker.setJobPriority(JobID,JobPriority)
7306   29  10    1 org.apache.hadoop.mapred.JobTracker.updateTaskStatuses(TaskTrackerStatus)
7307   18  11    1 org.apache.hadoop.mapred.JobTracker.lostTaskTracker(String)
7308    2   1    1 org.apache.hadoop.mapred.JobTracker.getLocalJobFilePath(JobID)
7309   10   3    1 org.apache.hadoop.mapred.JobTracker.main(String[])
7310    2   1    0 org.apache.hadoop.mapred.JobTracker.getQueues()
7311    2   1    0 org.apache.hadoop.mapred.JobTracker.getQueueInfo(String)
7312    3   1    0 org.apache.hadoop.mapred.JobTracker.getJobsFromQueue(String)
7313   14   8    0 org.apache.hadoop.mapred.JobTracker.getJobStatus(Collection,boolean)
7314    2   1    1 org.apache.hadoop.mapred.JobTracker.getMaxTasksPerJob()
7315    1   1    1 org.apache.hadoop.mapred.Mapper.map(K1,V1,OutputCollector,Reporter)
7316    3   1    0 org.apache.hadoop.mapred.QueueManager.QueueOperation.QueueOperation(String,boolean)
7317    2   1    0 org.apache.hadoop.mapred.QueueManager.QueueOperation.getAclName()
7318    2   1    0 org.apache.hadoop.mapred.QueueManager.QueueOperation.isJobOwnerAllowed()
7319   15   6    1 org.apache.hadoop.mapred.QueueManager.ACL.ACL(String)
7320    2   1    0 org.apache.hadoop.mapred.QueueManager.ACL.allUsersAllowed()
7321    2   1    0 org.apache.hadoop.mapred.QueueManager.ACL.isUserAllowed(String)
7322    5   4    0 org.apache.hadoop.mapred.QueueManager.ACL.isAnyGroupAllowed(String[])
7323    5   1    1 org.apache.hadoop.mapred.QueueManager.QueueManager(Configuration)
7324    2   1    1 org.apache.hadoop.mapred.QueueManager.getQueues()
7325    2   1    1 org.apache.hadoop.mapred.QueueManager.hasAccess(String,QueueOperation,UserGroupInformation)
7326   12  11    1 org.apache.hadoop.mapred.QueueManager.hasAccess(String,JobInProgress,QueueOperation,UserGroupInformation)
7327    2   1    1 org.apache.hadoop.mapred.QueueManager.setSchedulerInfo(String,Object)
7328    2   1    1 org.apache.hadoop.mapred.QueueManager.getSchedulerInfo(String)
7329    5   1    1 org.apache.hadoop.mapred.QueueManager.refresh(Configuration)
7330    9   3    0 org.apache.hadoop.mapred.QueueManager.initialize(Configuration)
7331    2   1    0 org.apache.hadoop.mapred.QueueManager.toFullPropertyName(String,String)
7332    3   2    0 org.apache.hadoop.mapred.QueueManager.addToSet(Set,String[])
7333    9   3    0 org.apache.hadoop.mapred.QueueManager.getJobQueueInfos()
7334    6   3    0 org.apache.hadoop.mapred.QueueManager.getJobQueueInfo(String)
7335    4   1    0 org.apache.hadoop.mapred.SequenceFileAsTextRecordReader.SequenceFileAsTextRecordReader(Configuration,FileSplit)
7336    2   1    0 org.apache.hadoop.mapred.SequenceFileAsTextRecordReader.createKey()
7337    2   1    0 org.apache.hadoop.mapred.SequenceFileAsTextRecordReader.createValue()
7338    8   3    1 org.apache.hadoop.mapred.SequenceFileAsTextRecordReader.next(Text,Text)
7339    2   1    0 org.apache.hadoop.mapred.SequenceFileAsTextRecordReader.getProgress()
7340    2   1    0 org.apache.hadoop.mapred.SequenceFileAsTextRecordReader.getPos()
7341    2   1    0 org.apache.hadoop.mapred.SequenceFileAsTextRecordReader.close()
7342    1   1    1 org.apache.hadoop.mapred.MapReduceBase.close()
7343    1   1    1 org.apache.hadoop.mapred.MapReduceBase.configure(JobConf)
7344    1   1    1 org.apache.hadoop.mapred.MapRunnable.run(RecordReader,OutputCollector,Reporter)
7345    3   2    0 org.apache.hadoop.mapred.MapRunner.configure(JobConf)
7346    9   3    0 org.apache.hadoop.mapred.MapRunner.run(RecordReader,OutputCollector,Reporter)
7347    1   1    0 org.apache.hadoop.mapred.MapTaskCompletionEventsUpdate.MapTaskCompletionEventsUpdate()
7348    3   1    0 org.apache.hadoop.mapred.MapTaskCompletionEventsUpdate.MapTaskCompletionEventsUpdate(TaskCompletionEvent[],boolean)
7349    2   1    0 org.apache.hadoop.mapred.MapTaskCompletionEventsUpdate.shouldReset()
7350    2   1    0 org.apache.hadoop.mapred.MapTaskCompletionEventsUpdate.getMapTaskCompletionEvents()
7351    5   2    0 org.apache.hadoop.mapred.MapTaskCompletionEventsUpdate.write(DataOutput)
7352    6   2    0 org.apache.hadoop.mapred.MapTaskCompletionEventsUpdate.readFields(DataInput)
7353    2   1    0 org.apache.hadoop.mapred.MapTaskRunner.MapTaskRunner(TaskInProgress,TaskTracker,JobConf)
7354    5   3    1 org.apache.hadoop.mapred.MapTaskRunner.prepare()
7355    3   1    1 org.apache.hadoop.mapred.MapTaskRunner.close()
7356    1   1    0 org.apache.hadoop.mapred.MapTaskStatus.MapTaskStatus()
7357    2   1    0 org.apache.hadoop.mapred.MapTaskStatus.MapTaskStatus(TaskAttemptID,float,State,String,String,String,Phase,Counters)
7358    2   1    0 org.apache.hadoop.mapred.MapTaskStatus.getIsMap()
7359    2   2    0 org.apache.hadoop.mapred.MapTaskStatus.getShuffleFinishTime()
7360    2   2    0 org.apache.hadoop.mapred.MapTaskStatus.setShuffleFinishTime(long)
7361    2   2    0 org.apache.hadoop.mapred.MapTaskStatus.getSortFinishTime()
7362    2   2    0 org.apache.hadoop.mapred.MapTaskStatus.setSortFinishTime(long)
7363    2   1    0 org.apache.hadoop.mapred.Merger.merge(Configuration,FileSystem,Class,Class,CompressionCodec,Path[],boolean,int,Path,RawComparator,Progressable)
7364    2   1    0 org.apache.hadoop.mapred.Merger.merge(Configuration,FileSystem,Class,Class,List,int,Path,RawComparator,Progressable)
7365    2   1    0 org.apache.hadoop.mapred.Merger.merge(Configuration,FileSystem,Class,Class,List,int,Path,RawComparator,Progressable,boolean)
7366    2   1    0 org.apache.hadoop.mapred.Merger.merge(Configuration,FileSystem,Class,Class,List,int,int,Path,RawComparator,Progressable,boolean)
7367    6   3    0 org.apache.hadoop.mapred.Merger.writeFile(RawKeyValueIterator,Writer,Progressable)
7368    7   1    0 org.apache.hadoop.mapred.Merger.Segment.Segment(Configuration,FileSystem,Path,CompressionCodec,boolean)
7369    4   1    0 org.apache.hadoop.mapred.Merger.Segment.Segment(Reader,boolean)
7370    3   2    0 org.apache.hadoop.mapred.Merger.Segment.init()
7371    2   1    0 org.apache.hadoop.mapred.Merger.Segment.getKey()
7372    2   1    0 org.apache.hadoop.mapred.Merger.Segment.getValue()
7373    2   2    0 org.apache.hadoop.mapred.Merger.Segment.getLength()
7374    2   1    0 org.apache.hadoop.mapred.Merger.Segment.next()
7375    4   3    0 org.apache.hadoop.mapred.Merger.Segment.close()
7376    2   1    0 org.apache.hadoop.mapred.Merger.Segment.getPosition()
7377    4   4    0 org.apache.hadoop.mapred.Merger.MergeQueue.Comparator$1.compare(Segment,Segment)
7378    9   2    0 org.apache.hadoop.mapred.Merger.MergeQueue.MergeQueue(Configuration,FileSystem,Path[],boolean,CompressionCodec,RawComparator,Progressable)
7379    2   1    0 org.apache.hadoop.mapred.Merger.MergeQueue.MergeQueue(Configuration,FileSystem,List,RawComparator,Progressable)
7380    8   2    0 org.apache.hadoop.mapred.Merger.MergeQueue.MergeQueue(Configuration,FileSystem,List,RawComparator,Progressable,boolean)
7381    4   2    0 org.apache.hadoop.mapred.Merger.MergeQueue.close()
7382    2   1    0 org.apache.hadoop.mapred.Merger.MergeQueue.getKey()
7383    2   1    0 org.apache.hadoop.mapred.Merger.MergeQueue.getValue()
7384   11   2    0 org.apache.hadoop.mapred.Merger.MergeQueue.adjustPriorityQueue(Segment)
7385   12   6    0 org.apache.hadoop.mapred.Merger.MergeQueue.next()
7386    8   1    0 org.apache.hadoop.mapred.Merger.MergeQueue.lessThan(Object,Object)
7387    2   1    0 org.apache.hadoop.mapred.Merger.MergeQueue.merge(Class,Class,int,Path)
7388   63  15    0 org.apache.hadoop.mapred.Merger.MergeQueue.merge(Class,Class,int,int,Path)
7389    7   7    1 org.apache.hadoop.mapred.Merger.MergeQueue.getPassFactor(int,int,int)
7390    9   4    1 org.apache.hadoop.mapred.Merger.MergeQueue.getSegmentDescriptors(int)
7391    2   1    0 org.apache.hadoop.mapred.Merger.MergeQueue.getProgress()
7392   10   3    1 org.apache.hadoop.mapred.MergeSorter.sort()
7393    7   2    1 org.apache.hadoop.mapred.MergeSorter.compare(IntWritable,IntWritable)
7394    2   1    1 org.apache.hadoop.mapred.MergeSorter.getMemoryUtilized()
7395   25   6    0 org.apache.hadoop.mapred.MultiFileInputFormat.getSplits(JobConf,int)
7396   10   6    0 org.apache.hadoop.mapred.MultiFileInputFormat.findSize(int,double,long,int,long[])
7397    1   1    0 org.apache.hadoop.mapred.MultiFileInputFormat.getRecordReader(InputSplit,JobConf,Reporter)
7398    1   1    0 org.apache.hadoop.mapred.MultiFileSplit.MultiFileSplit()
7399    7   2    0 org.apache.hadoop.mapred.MultiFileSplit.MultiFileSplit(JobConf,Path[],long[])
7400    2   1    0 org.apache.hadoop.mapred.MultiFileSplit.getLength()
7401    2   1    1 org.apache.hadoop.mapred.MultiFileSplit.getLengths()
7402    2   1    1 org.apache.hadoop.mapred.MultiFileSplit.getLength(int)
7403    2   1    1 org.apache.hadoop.mapred.MultiFileSplit.getNumPaths()
7404    2   1    1 org.apache.hadoop.mapred.MultiFileSplit.getPath(int)
7405    2   1    1 org.apache.hadoop.mapred.MultiFileSplit.getPaths()
7406    9   4    0 org.apache.hadoop.mapred.MultiFileSplit.getLocations()
7407    3   2    0 org.apache.hadoop.mapred.MultiFileSplit.addToSet(Set,String[])
7408   10   3    0 org.apache.hadoop.mapred.MultiFileSplit.readFields(DataInput)
7409    8   3    0 org.apache.hadoop.mapred.MultiFileSplit.write(DataOutput)
7410    7   3    0 org.apache.hadoop.mapred.MultiFileSplit.toString()
7411    1   1    1 org.apache.hadoop.mapred.OutputCollector.collect(K,V)
7412    1   1    1 org.apache.hadoop.mapred.OutputCommitter.setupJob(JobContext)
7413    1   1    1 org.apache.hadoop.mapred.OutputCommitter.cleanupJob(JobContext)
7414    1   1    1 org.apache.hadoop.mapred.OutputCommitter.setupTask(TaskAttemptContext)
7415    1   1    1 org.apache.hadoop.mapred.OutputCommitter.needsTaskCommit(TaskAttemptContext)
7416    1   1    1 org.apache.hadoop.mapred.OutputCommitter.commitTask(TaskAttemptContext)
7417    1   1    1 org.apache.hadoop.mapred.OutputCommitter.abortTask(TaskAttemptContext)
7418    1   1    1 org.apache.hadoop.mapred.OutputFormat.getRecordWriter(FileSystem,JobConf,String,Progressable)
7419    1   1    1 org.apache.hadoop.mapred.OutputFormat.checkOutputSpecs(FileSystem,JobConf)
7420    2   1    0 org.apache.hadoop.mapred.OutputLogFilter.accept(Path)
7421    1   1    1 org.apache.hadoop.mapred.Partitioner.getPartition(K2,V2,int)
7422   22   1    1 org.apache.hadoop.mapred.pipes.Application.Application(JobConf,RecordReader,OutputCollector,Reporter,Class,Class)
7423    2   1    1 org.apache.hadoop.mapred.pipes.Application.getDownlink()
7424    3   1    1 org.apache.hadoop.mapred.pipes.Application.waitForFinish()
7425   11   4    1 org.apache.hadoop.mapred.pipes.Application.abort(Throwable)
7426    5   2    1 org.apache.hadoop.mapred.pipes.Application.cleanup()
7427    6   2    1 org.apache.hadoop.mapred.pipes.Application.runClient(List,Map)
7428    2   1    0 org.apache.hadoop.mapred.pipes.BinaryProtocol.MessageType.MessageType(int)
7429    5   1    0 org.apache.hadoop.mapred.pipes.BinaryProtocol.UplinkReaderThread.UplinkReaderThread(InputStream,UpwardProtocol,K2,V2)
7430    2   1    0 org.apache.hadoop.mapred.pipes.BinaryProtocol.UplinkReaderThread.closeConnection()
7431   46  16    0 org.apache.hadoop.mapred.pipes.BinaryProtocol.UplinkReaderThread.run()
7432   14   3    0 org.apache.hadoop.mapred.pipes.BinaryProtocol.UplinkReaderThread.readObject(Writable)
7433    3   1    0 org.apache.hadoop.mapred.pipes.BinaryProtocol.TeeOutputStream.TeeOutputStream(String,OutputStream)
7434    3   1    0 org.apache.hadoop.mapred.pipes.BinaryProtocol.TeeOutputStream.write(byte[],int,int)
7435    3   1    0 org.apache.hadoop.mapred.pipes.BinaryProtocol.TeeOutputStream.write(int)
7436    3   1    0 org.apache.hadoop.mapred.pipes.BinaryProtocol.TeeOutputStream.flush()
7437    4   1    0 org.apache.hadoop.mapred.pipes.BinaryProtocol.TeeOutputStream.close()
7438    8   2    1 org.apache.hadoop.mapred.pipes.BinaryProtocol.BinaryProtocol(Socket,UpwardProtocol,K2,V2,JobConf)
7439    6   1    1 org.apache.hadoop.mapred.pipes.BinaryProtocol.close()
7440    4   1    0 org.apache.hadoop.mapred.pipes.BinaryProtocol.start()
7441    9   3    0 org.apache.hadoop.mapred.pipes.BinaryProtocol.setJobConf(JobConf)
7442    4   1    0 org.apache.hadoop.mapred.pipes.BinaryProtocol.setInputTypes(String,String)
7443    5   2    0 org.apache.hadoop.mapred.pipes.BinaryProtocol.runMap(InputSplit,int,boolean)
7444    4   1    0 org.apache.hadoop.mapred.pipes.BinaryProtocol.mapItem(WritableComparable,Writable)
7445    4   2    0 org.apache.hadoop.mapred.pipes.BinaryProtocol.runReduce(int,boolean)
7446    3   1    0 org.apache.hadoop.mapred.pipes.BinaryProtocol.reduceKey(WritableComparable)
7447    3   1    0 org.apache.hadoop.mapred.pipes.BinaryProtocol.reduceValue(Writable)
7448    3   1    0 org.apache.hadoop.mapred.pipes.BinaryProtocol.endOfInput()
7449    3   1    0 org.apache.hadoop.mapred.pipes.BinaryProtocol.abort()
7450    2   1    0 org.apache.hadoop.mapred.pipes.BinaryProtocol.flush()
7451   18   3    1 org.apache.hadoop.mapred.pipes.BinaryProtocol.writeObject(Writable)
7452    1   1    1 org.apache.hadoop.mapred.pipes.DownwardProtocol.start()
7453    1   1    1 org.apache.hadoop.mapred.pipes.DownwardProtocol.setJobConf(JobConf)
7454    1   1    1 org.apache.hadoop.mapred.pipes.DownwardProtocol.setInputTypes(String,String)
7455    1   1    1 org.apache.hadoop.mapred.pipes.DownwardProtocol.runMap(InputSplit,int,boolean)
7456    1   1    1 org.apache.hadoop.mapred.pipes.DownwardProtocol.mapItem(K,V)
7457    1   1    1 org.apache.hadoop.mapred.pipes.DownwardProtocol.runReduce(int,boolean)
7458    1   1    1 org.apache.hadoop.mapred.pipes.DownwardProtocol.reduceKey(K)
7459    1   1    1 org.apache.hadoop.mapred.pipes.DownwardProtocol.reduceValue(V)
7460    1   1    1 org.apache.hadoop.mapred.pipes.DownwardProtocol.endOfInput()
7461    1   1    1 org.apache.hadoop.mapred.pipes.DownwardProtocol.abort()
7462    1   1    1 org.apache.hadoop.mapred.pipes.DownwardProtocol.flush()
7463    1   1    1 org.apache.hadoop.mapred.pipes.DownwardProtocol.close()
7464    4   1    1 org.apache.hadoop.mapred.pipes.OutputHandler.OutputHandler(OutputCollector,Reporter,RecordReader)
7465    2   1    1 org.apache.hadoop.mapred.pipes.OutputHandler.output(K,V)
7466    3   1    1 org.apache.hadoop.mapred.pipes.OutputHandler.partitionedOutput(int,K,V)
7467    2   1    1 org.apache.hadoop.mapred.pipes.OutputHandler.status(String)
7468    6   2    1 org.apache.hadoop.mapred.pipes.OutputHandler.progress(float)
7469    4   1    1 org.apache.hadoop.mapred.pipes.OutputHandler.done()
7470    2   1    1 org.apache.hadoop.mapred.pipes.OutputHandler.getProgress()
7471    4   1    1 org.apache.hadoop.mapred.pipes.OutputHandler.failed(Throwable)
7472    6   5    1 org.apache.hadoop.mapred.pipes.OutputHandler.waitForFinish()
7473    3   1    0 org.apache.hadoop.mapred.pipes.OutputHandler.registerCounter(int,String,String)
7474    6   3    0 org.apache.hadoop.mapred.pipes.OutputHandler.incrementCounter(int,long)
7475    3   1    1 org.apache.hadoop.mapred.pipes.PipesMapRunner.configure(JobConf)
7476   24   9    0 org.apache.hadoop.mapred.pipes.PipesMapRunner.run(RecordReader,OutputCollector,Reporter)
7477    2   1    0 org.apache.hadoop.mapred.pipes.PipesNonJavaInputFormat.getRecordReader(InputSplit,JobConf,Reporter)
7478    2   1    0 org.apache.hadoop.mapred.pipes.PipesNonJavaInputFormat.getSplits(JobConf,int)
7479    1   1    0 org.apache.hadoop.mapred.pipes.PipesNonJavaInputFormat.PipesDummyRecordReader.PipesDummyRecordReader(Configuration,InputSplit)
7480    2   1    0 org.apache.hadoop.mapred.pipes.PipesNonJavaInputFormat.PipesDummyRecordReader.createKey()
7481    2   1    0 org.apache.hadoop.mapred.pipes.PipesNonJavaInputFormat.PipesDummyRecordReader.createValue()
7482    1   1    0 org.apache.hadoop.mapred.pipes.PipesNonJavaInputFormat.PipesDummyRecordReader.close()
7483    2   1    0 org.apache.hadoop.mapred.pipes.PipesNonJavaInputFormat.PipesDummyRecordReader.getPos()
7484    2   1    0 org.apache.hadoop.mapred.pipes.PipesNonJavaInputFormat.PipesDummyRecordReader.getProgress()
7485    3   1    0 org.apache.hadoop.mapred.pipes.PipesNonJavaInputFormat.PipesDummyRecordReader.next(FloatWritable,NullWritable)
7486    2   1    0 org.apache.hadoop.mapred.pipes.PipesPartitioner.configure(JobConf)
7487    2   1    1 org.apache.hadoop.mapred.pipes.PipesPartitioner.setNextPartition(int)
7488    6   3    1 org.apache.hadoop.mapred.pipes.PipesPartitioner.getPartition(K,V,int)
7489    4   1    0 org.apache.hadoop.mapred.pipes.PipesReducer.configure(JobConf)
7490    9   3    1 org.apache.hadoop.mapred.pipes.PipesReducer.reduce(K2,Iterator,OutputCollector,Reporter)
7491    9   4    0 org.apache.hadoop.mapred.pipes.PipesReducer.startApplication(OutputCollector,Reporter)
7492    1   1    0 org.apache.hadoop.mapred.pipes.PipesReducer.OutputCollector$1.collect(K3,V3)
7493   16   4    1 org.apache.hadoop.mapred.pipes.PipesReducer.close()
7494    2   1    0 org.apache.hadoop.mapred.pipes.Submitter.Submitter()
7495    2   1    0 org.apache.hadoop.mapred.pipes.Submitter.Submitter(Configuration)
7496    2   1    1 org.apache.hadoop.mapred.pipes.Submitter.getExecutable(JobConf)
7497    2   1    1 org.apache.hadoop.mapred.pipes.Submitter.setExecutable(JobConf,String)
7498    2   1    1 org.apache.hadoop.mapred.pipes.Submitter.setIsJavaRecordReader(JobConf,boolean)
7499    2   1    1 org.apache.hadoop.mapred.pipes.Submitter.getIsJavaRecordReader(JobConf)
7500    2   1    1 org.apache.hadoop.mapred.pipes.Submitter.setIsJavaMapper(JobConf,boolean)
7501    2   1    1 org.apache.hadoop.mapred.pipes.Submitter.getIsJavaMapper(JobConf)
7502    2   1    1 org.apache.hadoop.mapred.pipes.Submitter.setIsJavaReducer(JobConf,boolean)
7503    2   1    1 org.apache.hadoop.mapred.pipes.Submitter.getIsJavaReducer(JobConf)
7504    2   1    1 org.apache.hadoop.mapred.pipes.Submitter.setIsJavaRecordWriter(JobConf,boolean)
7505    2   1    1 org.apache.hadoop.mapred.pipes.Submitter.getIsJavaRecordWriter(JobConf)
7506    3   2    1 org.apache.hadoop.mapred.pipes.Submitter.setIfUnset(JobConf,String,String)
7507    2   1    1 org.apache.hadoop.mapred.pipes.Submitter.setJavaPartitioner(JobConf,Class)
7508    2   1    1 org.apache.hadoop.mapred.pipes.Submitter.getJavaPartitioner(JobConf)
7509    2   1    1 org.apache.hadoop.mapred.pipes.Submitter.getKeepCommandFile(JobConf)
7510    2   1    1 org.apache.hadoop.mapred.pipes.Submitter.setKeepCommandFile(JobConf,boolean)
7511    2   1    0 org.apache.hadoop.mapred.pipes.Submitter.submitJob(JobConf)
7512    3   1    1 org.apache.hadoop.mapred.pipes.Submitter.runJob(JobConf)
7513    3   1    1 org.apache.hadoop.mapred.pipes.Submitter.jobSubmit(JobConf)
7514   38  12    0 org.apache.hadoop.mapred.pipes.Submitter.setupPipesJob(JobConf)
7515    3   1    0 org.apache.hadoop.mapred.pipes.Submitter.CommandLineParser.addOption(String,boolean,String,String)
7516    3   1    0 org.apache.hadoop.mapred.pipes.Submitter.CommandLineParser.addArgument(String,boolean,String)
7517    4   1    0 org.apache.hadoop.mapred.pipes.Submitter.CommandLineParser.createParser()
7518   14   1    0 org.apache.hadoop.mapred.pipes.Submitter.CommandLineParser.printUsage()
7519    2   1    0 org.apache.hadoop.mapred.pipes.Submitter.getClass(CommandLine,String,JobConf,Class)
7520    2   1    0 org.apache.hadoop.mapred.pipes.Submitter.PrivilegedAction$1.run()
7521   66  19    0 org.apache.hadoop.mapred.pipes.Submitter.run(String[])
7522    2   1    1 org.apache.hadoop.mapred.pipes.Submitter.main(String[])
7523    1   1    1 org.apache.hadoop.mapred.pipes.UpwardProtocol.output(K,V)
7524    1   1    1 org.apache.hadoop.mapred.pipes.UpwardProtocol.partitionedOutput(int,K,V)
7525    1   1    1 org.apache.hadoop.mapred.pipes.UpwardProtocol.status(String)
7526    1   1    1 org.apache.hadoop.mapred.pipes.UpwardProtocol.progress(float)
7527    1   1    1 org.apache.hadoop.mapred.pipes.UpwardProtocol.done()
7528    1   1    1 org.apache.hadoop.mapred.pipes.UpwardProtocol.failed(Throwable)
7529    1   1    1 org.apache.hadoop.mapred.pipes.UpwardProtocol.registerCounter(int,String,String)
7530    1   1    1 org.apache.hadoop.mapred.pipes.UpwardProtocol.incrementCounter(int,long)
7531    1   1    1 org.apache.hadoop.mapred.RamManager.reserve(int,InputStream)
7532    1   1    1 org.apache.hadoop.mapred.RamManager.unreserve(int)
7533    1   1    1 org.apache.hadoop.mapred.RawKeyValueIterator.getKey()
7534    1   1    1 org.apache.hadoop.mapred.RawKeyValueIterator.getValue()
7535    1   1    1 org.apache.hadoop.mapred.RawKeyValueIterator.next()
7536    1   1    1 org.apache.hadoop.mapred.RawKeyValueIterator.close()
7537    1   1    1 org.apache.hadoop.mapred.RawKeyValueIterator.getProgress()
7538    1   1    1 org.apache.hadoop.mapred.RecordReader.next(K,V)
7539    1   1    1 org.apache.hadoop.mapred.RecordReader.createKey()
7540    1   1    1 org.apache.hadoop.mapred.RecordReader.createValue()
7541    1   1    1 org.apache.hadoop.mapred.RecordReader.getPos()
7542    1   1    1 org.apache.hadoop.mapred.RecordReader.close()
7543    1   1    1 org.apache.hadoop.mapred.RecordReader.getProgress()
7544    1   1    1 org.apache.hadoop.mapred.RecordWriter.write(K,V)
7545    1   1    1 org.apache.hadoop.mapred.RecordWriter.close(Reporter)
7546    1   1    1 org.apache.hadoop.mapred.Reducer.reduce(K2,Iterator,OutputCollector,Reporter)
7547    2   1    0 org.apache.hadoop.mapred.ReduceTask.WritableFactory$1.newInstance()
7548   11   7    0 org.apache.hadoop.mapred.ReduceTask.Comparator$2.compare(FileStatus,FileStatus)
7549    2   1    0 org.apache.hadoop.mapred.ReduceTask.ReduceTask()
7550    3   1    0 org.apache.hadoop.mapred.ReduceTask.ReduceTask(String,TaskAttemptID,int,int)
7551    5   3    0 org.apache.hadoop.mapred.ReduceTask.initCodec()
7552    2   1    0 org.apache.hadoop.mapred.ReduceTask.createRunner(TaskTracker,TaskInProgress)
7553    2   1    0 org.apache.hadoop.mapred.ReduceTask.isMapTask()
7554    2   1    0 org.apache.hadoop.mapred.ReduceTask.getNumMaps()
7555    3   1    0 org.apache.hadoop.mapred.ReduceTask.localizeConfiguration(JobConf)
7556    3   1    0 org.apache.hadoop.mapred.ReduceTask.write(DataOutput)
7557    3   1    0 org.apache.hadoop.mapred.ReduceTask.readFields(DataInput)
7558    9   4    0 org.apache.hadoop.mapred.ReduceTask.getMapFiles(FileSystem,boolean)
7559    2   1    0 org.apache.hadoop.mapred.ReduceTask.ReduceValuesIterator.ReduceValuesIterator(RawKeyValueIterator,RawComparator,Class,Class,Configuration,Progressable)
7560    3   1    0 org.apache.hadoop.mapred.ReduceTask.ReduceValuesIterator.next()
7561    2   1    0 org.apache.hadoop.mapred.ReduceTask.ReduceValuesIterator.moveToNext()
7562    3   1    0 org.apache.hadoop.mapred.ReduceTask.ReduceValuesIterator.informReduceProgress()
7563   10   2    0 org.apache.hadoop.mapred.ReduceTask.SkippingReduceValuesIterator.SkippingReduceValuesIterator(RawKeyValueIterator,RawComparator,Class,Class,Configuration,Progressable,TaskUmbilicalProtocol)
7564    3   1    0 org.apache.hadoop.mapred.ReduceTask.SkippingReduceValuesIterator.nextKey()
7565    2   2    0 org.apache.hadoop.mapred.ReduceTask.SkippingReduceValuesIterator.more()
7566   23  10    0 org.apache.hadoop.mapred.ReduceTask.SkippingReduceValuesIterator.mayBeSkip()
7567    6   2    0 org.apache.hadoop.mapred.ReduceTask.SkippingReduceValuesIterator.writeSkippedRec(KEY,VALUE)
7568    4   1    0 org.apache.hadoop.mapred.ReduceTask.OutputCollector$3.collect(Object,Object)
7569   61  20    0 org.apache.hadoop.mapred.ReduceTask.run(JobConf,TaskUmbilicalProtocol)
7570    9   1    0 org.apache.hadoop.mapred.ReduceTask.ReduceCopier.ShuffleClientMetrics.ShuffleClientMetrics(JobConf)
7571    2   1    0 org.apache.hadoop.mapred.ReduceTask.ReduceCopier.ShuffleClientMetrics.inputBytes(long)
7572    2   1    0 org.apache.hadoop.mapred.ReduceTask.ReduceCopier.ShuffleClientMetrics.failedFetch()
7573    2   1    0 org.apache.hadoop.mapred.ReduceTask.ReduceCopier.ShuffleClientMetrics.successFetch()
7574    2   1    0 org.apache.hadoop.mapred.ReduceTask.ReduceCopier.ShuffleClientMetrics.threadBusy()
7575    2   1    0 org.apache.hadoop.mapred.ReduceTask.ReduceCopier.ShuffleClientMetrics.threadFree()
7576   13   2    0 org.apache.hadoop.mapred.ReduceTask.ReduceCopier.ShuffleClientMetrics.doUpdates(MetricsContext)
7577    3   1    0 org.apache.hadoop.mapred.ReduceTask.ReduceCopier.CopyResult.CopyResult(MapOutputLocation,long)
7578    2   1    0 org.apache.hadoop.mapred.ReduceTask.ReduceCopier.CopyResult.getSuccess()
7579    2   1    0 org.apache.hadoop.mapred.ReduceTask.ReduceCopier.CopyResult.isObsolete()
7580    2   1    0 org.apache.hadoop.mapred.ReduceTask.ReduceCopier.CopyResult.getSize()
7581    2   1    0 org.apache.hadoop.mapred.ReduceTask.ReduceCopier.CopyResult.getHost()
7582    2   1    0 org.apache.hadoop.mapred.ReduceTask.ReduceCopier.CopyResult.getLocation()
7583    5   1    0 org.apache.hadoop.mapred.ReduceTask.ReduceCopier.MapOutputLocation.MapOutputLocation(TaskAttemptID,String,URL)
7584    2   1    0 org.apache.hadoop.mapred.ReduceTask.ReduceCopier.MapOutputLocation.getTaskAttemptId()
7585    2   1    0 org.apache.hadoop.mapred.ReduceTask.ReduceCopier.MapOutputLocation.getTaskId()
7586    2   1    0 org.apache.hadoop.mapred.ReduceTask.ReduceCopier.MapOutputLocation.getHost()
7587    2   1    0 org.apache.hadoop.mapred.ReduceTask.ReduceCopier.MapOutputLocation.getOutputLocation()
7588    8   1    0 org.apache.hadoop.mapred.ReduceTask.ReduceCopier.MapOutput.MapOutput(TaskID,TaskAttemptID,Configuration,Path,long)
7589    8   1    0 org.apache.hadoop.mapred.ReduceTask.ReduceCopier.MapOutput.MapOutput(TaskID,TaskAttemptID,byte[])
7590    6   2    0 org.apache.hadoop.mapred.ReduceTask.ReduceCopier.MapOutput.discard()
7591    7   4    0 org.apache.hadoop.mapred.ReduceTask.ReduceCopier.ShuffleRamManager.ShuffleRamManager(Configuration)
7592   16   4    0 org.apache.hadoop.mapred.ReduceTask.ReduceCopier.ShuffleRamManager.reserve(int,InputStream)
7593    6   1    0 org.apache.hadoop.mapred.ReduceTask.ReduceCopier.ShuffleRamManager.unreserve(int)
7594    7   9    0 org.apache.hadoop.mapred.ReduceTask.ReduceCopier.ShuffleRamManager.waitForDataToMerge()
7595    5   1    0 org.apache.hadoop.mapred.ReduceTask.ReduceCopier.ShuffleRamManager.closeInMemoryFile(int)
7596    4   1    0 org.apache.hadoop.mapred.ReduceTask.ReduceCopier.ShuffleRamManager.setNumCopiedMapOutputs(int)
7597    5   1    0 org.apache.hadoop.mapred.ReduceTask.ReduceCopier.ShuffleRamManager.close()
7598    2   1    0 org.apache.hadoop.mapred.ReduceTask.ReduceCopier.ShuffleRamManager.getPercentUsed()
7599    2   1    0 org.apache.hadoop.mapred.ReduceTask.ReduceCopier.ShuffleRamManager.getMemoryLimit()
7600    2   2    0 org.apache.hadoop.mapred.ReduceTask.ReduceCopier.ShuffleRamManager.canFitInMemory(long)
7601    8   2    0 org.apache.hadoop.mapred.ReduceTask.ReduceCopier.MapOutputCopier.MapOutputCopier(JobConf,Reporter)
7602    6   3    1 org.apache.hadoop.mapred.ReduceTask.ReduceCopier.MapOutputCopier.fail()
7603    2   1    1 org.apache.hadoop.mapred.ReduceTask.ReduceCopier.MapOutputCopier.getLocation()
7604    2   1    0 org.apache.hadoop.mapred.ReduceTask.ReduceCopier.MapOutputCopier.start(MapOutputLocation)
7605    7   2    0 org.apache.hadoop.mapred.ReduceTask.ReduceCopier.MapOutputCopier.finish(long)
7606   29   9    0 org.apache.hadoop.mapred.ReduceTask.ReduceCopier.MapOutputCopier.run()
7607   33  14    1 org.apache.hadoop.mapred.ReduceTask.ReduceCopier.MapOutputCopier.copyOutput(MapOutputLocation)
7608    3   1    1 org.apache.hadoop.mapred.ReduceTask.ReduceCopier.MapOutputCopier.noteCopiedMapOutput(TaskID)
7609   14   2    1 org.apache.hadoop.mapred.ReduceTask.ReduceCopier.MapOutputCopier.getMapOutput(MapOutputLocation,Path)
7610   18  11    1 org.apache.hadoop.mapred.ReduceTask.ReduceCopier.MapOutputCopier.getInputStream(URLConnection,int,int)
7611   49  13    0 org.apache.hadoop.mapred.ReduceTask.ReduceCopier.MapOutputCopier.shuffleInMemory(MapOutputLocation,URLConnection,InputStream,int,int)
7612   32   8    0 org.apache.hadoop.mapred.ReduceTask.ReduceCopier.MapOutputCopier.shuffleToDisk(MapOutputLocation,InputStream,Path,long)
7613   18   4    0 org.apache.hadoop.mapred.ReduceTask.ReduceCopier.configureClasspath(JobConf)
7614   30   5    0 org.apache.hadoop.mapred.ReduceTask.ReduceCopier.ReduceCopier(TaskUmbilicalProtocol,JobConf)
7615    2   1    0 org.apache.hadoop.mapred.ReduceTask.ReduceCopier.busyEnough(int)
7616  170  52    0 org.apache.hadoop.mapred.ReduceTask.ReduceCopier.fetchOutputs()
7617   14   3    0 org.apache.hadoop.mapred.ReduceTask.ReduceCopier.createInMemorySegments(List,long)
7618    4   4    0 org.apache.hadoop.mapred.ReduceTask.ReduceCopier.Comparator$4.compare(Segment,Segment)
7619   56  13    0 org.apache.hadoop.mapred.ReduceTask.ReduceCopier.createKVIterator(JobConf,FileSystem,Reporter)
7620    3   1    0 org.apache.hadoop.mapred.ReduceTask.ReduceCopier.RawKVIteratorReader.RawKVIteratorReader(RawKeyValueIterator,long)
7621   13   3    0 org.apache.hadoop.mapred.ReduceTask.ReduceCopier.RawKVIteratorReader.next(DataInputBuffer,DataInputBuffer)
7622    2   1    0 org.apache.hadoop.mapred.ReduceTask.ReduceCopier.RawKVIteratorReader.getPosition()
7623    2   1    0 org.apache.hadoop.mapred.ReduceTask.ReduceCopier.RawKVIteratorReader.close()
7624    9   5    0 org.apache.hadoop.mapred.ReduceTask.ReduceCopier.getCopyResult(int)
7625    4   1    0 org.apache.hadoop.mapred.ReduceTask.ReduceCopier.addToMapOutputFilesOnDisk(FileStatus)
7626   45  12    1 org.apache.hadoop.mapred.ReduceTask.ReduceCopier.getMapCompletionEvents(IntWritable,HashMap,List)
7627    4   1    0 org.apache.hadoop.mapred.ReduceTask.ReduceCopier.LocalFSMerger.LocalFSMerger(LocalFileSystem)
7628   40  12    0 org.apache.hadoop.mapred.ReduceTask.ReduceCopier.LocalFSMerger.run()
7629    3   1    0 org.apache.hadoop.mapred.ReduceTask.ReduceCopier.InMemFSMergeThread.InMemFSMergeThread()
7630   10   4    0 org.apache.hadoop.mapred.ReduceTask.ReduceCopier.InMemFSMergeThread.run()
7631   26   6    0 org.apache.hadoop.mapred.ReduceTask.ReduceCopier.InMemFSMergeThread.doInMemMerge()
7632   12   2    0 org.apache.hadoop.mapred.ReduceTask.ReduceCopier.combineAndSpill(RawKeyValueIterator,Counters.Counter)
7633    5   4    1 org.apache.hadoop.mapred.ReduceTask.getClosestPowerOf2(int)
7634    2   1    0 org.apache.hadoop.mapred.ReduceTaskRunner.ReduceTaskRunner(TaskInProgress,TaskTracker,JobConf)
7635    5   3    1 org.apache.hadoop.mapred.ReduceTaskRunner.prepare()
7636    4   1    1 org.apache.hadoop.mapred.ReduceTaskRunner.close()
7637    1   1    0 org.apache.hadoop.mapred.ReduceTaskStatus.ReduceTaskStatus()
7638    2   1    0 org.apache.hadoop.mapred.ReduceTaskStatus.ReduceTaskStatus(TaskAttemptID,float,State,String,String,String,Phase,Counters)
7639    4   1    0 org.apache.hadoop.mapred.ReduceTaskStatus.clone()
7640    2   1    0 org.apache.hadoop.mapred.ReduceTaskStatus.getIsMap()
7641    6   3    0 org.apache.hadoop.mapred.ReduceTaskStatus.setFinishTime(long)
7642    2   1    0 org.apache.hadoop.mapred.ReduceTaskStatus.getShuffleFinishTime()
7643    2   1    0 org.apache.hadoop.mapred.ReduceTaskStatus.setShuffleFinishTime(long)
7644    2   1    0 org.apache.hadoop.mapred.ReduceTaskStatus.getSortFinishTime()
7645    4   2    0 org.apache.hadoop.mapred.ReduceTaskStatus.setSortFinishTime(long)
7646    2   1    0 org.apache.hadoop.mapred.ReduceTaskStatus.getFetchFailedMaps()
7647    2   1    0 org.apache.hadoop.mapred.ReduceTaskStatus.addFetchFailedMap(TaskAttemptID)
7648   12   5    0 org.apache.hadoop.mapred.ReduceTaskStatus.statusUpdate(TaskStatus)
7649    3   1    0 org.apache.hadoop.mapred.ReduceTaskStatus.clearStatus()
7650    8   2    0 org.apache.hadoop.mapred.ReduceTaskStatus.readFields(DataInput)
7651    7   2    0 org.apache.hadoop.mapred.ReduceTaskStatus.write(DataOutput)
7652    2   1    0 org.apache.hadoop.mapred.ReinitTrackerAction.ReinitTrackerAction()
7653    1   1    0 org.apache.hadoop.mapred.ReinitTrackerAction.write(DataOutput)
7654    1   1    0 org.apache.hadoop.mapred.ReinitTrackerAction.readFields(DataInput)
7655    1   1    0 org.apache.hadoop.mapred.Reporter.Reporter$1.setStatus(String)
7656    1   1    0 org.apache.hadoop.mapred.Reporter.Reporter$1.progress()
7657    2   1    0 org.apache.hadoop.mapred.Reporter.Reporter$1.getCounter(String,String)
7658    1   1    0 org.apache.hadoop.mapred.Reporter.Reporter$1.incrCounter(Enum,long)
7659    1   1    0 org.apache.hadoop.mapred.Reporter.Reporter$1.incrCounter(String,String,long)
7660    2   2    0 org.apache.hadoop.mapred.Reporter.Reporter$1.getInputSplit()
7661    1   1    1 org.apache.hadoop.mapred.Reporter.setStatus(String)
7662    1   1    1 org.apache.hadoop.mapred.Reporter.getCounter(String,String)
7663    1   1    1 org.apache.hadoop.mapred.Reporter.incrCounter(Enum,long)
7664    1   1    1 org.apache.hadoop.mapred.Reporter.incrCounter(String,String,long)
7665    1   1    1 org.apache.hadoop.mapred.Reporter.getInputSplit()
7666    5   1    0 org.apache.hadoop.mapred.ResourceEstimator.ResourceEstimator(JobInProgress)
7667    2   1    1 org.apache.hadoop.mapred.ResourceEstimator.getBlowupRatio()
7668    2   1    0 org.apache.hadoop.mapred.ResourceEstimator.setBlowupRatio(double)
7669   10   3    0 org.apache.hadoop.mapred.ResourceEstimator.updateWithCompletedTask(TaskStatus,TaskInProgress)
7670   12   3    1 org.apache.hadoop.mapred.ResourceEstimator.getEstimatedTotalMapOutputSize()
7671    3   1    1 org.apache.hadoop.mapred.ResourceEstimator.getEstimatedMapOutputSize()
7672    5   3    1 org.apache.hadoop.mapred.ResourceEstimator.getEstimatedReduceInputSize()
7673    1   1    1 org.apache.hadoop.mapred.RunningJob.getID()
7674    1   1    0 org.apache.hadoop.mapred.RunningJob.getJobID()
7675    1   1    1 org.apache.hadoop.mapred.RunningJob.getJobName()
7676    1   1    1 org.apache.hadoop.mapred.RunningJob.getJobFile()
7677    1   1    1 org.apache.hadoop.mapred.RunningJob.getTrackingURL()
7678    1   1    1 org.apache.hadoop.mapred.RunningJob.mapProgress()
7679    1   1    1 org.apache.hadoop.mapred.RunningJob.reduceProgress()
7680    1   1    1 org.apache.hadoop.mapred.RunningJob.cleanupProgress()
7681    1   1    1 org.apache.hadoop.mapred.RunningJob.setupProgress()
7682    1   1    1 org.apache.hadoop.mapred.RunningJob.isComplete()
7683    1   1    1 org.apache.hadoop.mapred.RunningJob.isSuccessful()
7684    1   1    1 org.apache.hadoop.mapred.RunningJob.waitForCompletion()
7685    1   1    1 org.apache.hadoop.mapred.RunningJob.getJobState()
7686    1   1    1 org.apache.hadoop.mapred.RunningJob.killJob()
7687    1   1    1 org.apache.hadoop.mapred.RunningJob.setJobPriority(String)
7688    1   1    1 org.apache.hadoop.mapred.RunningJob.getTaskCompletionEvents(int)
7689    1   1    1 org.apache.hadoop.mapred.RunningJob.killTask(TaskAttemptID,boolean)
7690    1   1    0 org.apache.hadoop.mapred.RunningJob.killTask(String,boolean)
7691    1   1    1 org.apache.hadoop.mapred.RunningJob.getCounters()
7692    2   1    0 org.apache.hadoop.mapred.SequenceFileAsBinaryInputFormat.SequenceFileAsBinaryInputFormat()
7693    2   1    0 org.apache.hadoop.mapred.SequenceFileAsBinaryInputFormat.getRecordReader(InputSplit,JobConf,Reporter)
7694   10   2    0 org.apache.hadoop.mapred.SequenceFileAsBinaryInputFormat.SequenceFileAsBinaryRecordReader.SequenceFileAsBinaryRecordReader(Configuration,FileSplit)
7695    2   1    0 org.apache.hadoop.mapred.SequenceFileAsBinaryInputFormat.SequenceFileAsBinaryRecordReader.createKey()
7696    2   1    0 org.apache.hadoop.mapred.SequenceFileAsBinaryInputFormat.SequenceFileAsBinaryRecordReader.createValue()
7697    2   1    1 org.apache.hadoop.mapred.SequenceFileAsBinaryInputFormat.SequenceFileAsBinaryRecordReader.getKeyClassName()
7698    2   1    1 org.apache.hadoop.mapred.SequenceFileAsBinaryInputFormat.SequenceFileAsBinaryRecordReader.getValueClassName()
7699   13   6    1 org.apache.hadoop.mapred.SequenceFileAsBinaryInputFormat.SequenceFileAsBinaryRecordReader.next(BytesWritable,BytesWritable)
7700    2   1    0 org.apache.hadoop.mapred.SequenceFileAsBinaryInputFormat.SequenceFileAsBinaryRecordReader.getPos()
7701    2   1    0 org.apache.hadoop.mapred.SequenceFileAsBinaryInputFormat.SequenceFileAsBinaryRecordReader.close()
7702    5   3    1 org.apache.hadoop.mapred.SequenceFileAsBinaryInputFormat.SequenceFileAsBinaryRecordReader.getProgress()
7703    2   1    0 org.apache.hadoop.mapred.SequenceFileAsBinaryOutputFormat.WritableValueBytes.WritableValueBytes()
7704    2   1    0 org.apache.hadoop.mapred.SequenceFileAsBinaryOutputFormat.WritableValueBytes.WritableValueBytes(BytesWritable)
7705    2   1    0 org.apache.hadoop.mapred.SequenceFileAsBinaryOutputFormat.WritableValueBytes.reset(BytesWritable)
7706    2   1    0 org.apache.hadoop.mapred.SequenceFileAsBinaryOutputFormat.WritableValueBytes.writeUncompressedBytes(DataOutputStream)
7707    2   2    0 org.apache.hadoop.mapred.SequenceFileAsBinaryOutputFormat.WritableValueBytes.writeCompressedBytes(DataOutputStream)
7708    2   1    0 org.apache.hadoop.mapred.SequenceFileAsBinaryOutputFormat.WritableValueBytes.getSize()
7709    2   1    1 org.apache.hadoop.mapred.SequenceFileAsBinaryOutputFormat.setSequenceFileOutputKeyClass(JobConf,Class)
7710    2   1    1 org.apache.hadoop.mapred.SequenceFileAsBinaryOutputFormat.setSequenceFileOutputValueClass(JobConf,Class)
7711    2   1    1 org.apache.hadoop.mapred.SequenceFileAsBinaryOutputFormat.getSequenceFileOutputKeyClass(JobConf)
7712    2   1    1 org.apache.hadoop.mapred.SequenceFileAsBinaryOutputFormat.getSequenceFileOutputValueClass(JobConf)
7713    4   1    0 org.apache.hadoop.mapred.SequenceFileAsBinaryOutputFormat.RecordWriter$1.write(BytesWritable,BytesWritable)
7714    2   1    0 org.apache.hadoop.mapred.SequenceFileAsBinaryOutputFormat.RecordWriter$1.close(Reporter)
7715   18   2    0 org.apache.hadoop.mapred.SequenceFileAsBinaryOutputFormat.getRecordWriter(FileSystem,JobConf,String,Progressable)
7716    4   4    0 org.apache.hadoop.mapred.SequenceFileAsBinaryOutputFormat.checkOutputSpecs(FileSystem,JobConf)
7717    2   1    0 org.apache.hadoop.mapred.SequenceFileAsTextInputFormat.SequenceFileAsTextInputFormat()
7718    3   1    0 org.apache.hadoop.mapred.SequenceFileAsTextInputFormat.getRecordReader(InputSplit,JobConf,Reporter)
7719    1   1    0 org.apache.hadoop.mapred.SequenceFileInputFilter.SequenceFileInputFilter()
7720    3   1    1 org.apache.hadoop.mapred.SequenceFileInputFilter.getRecordReader(InputSplit,JobConf,Reporter)
7721    2   1    1 org.apache.hadoop.mapred.SequenceFileInputFilter.setFilterClass(Configuration,Class)
7722    1   1    1 org.apache.hadoop.mapred.SequenceFileInputFilter.Filter.accept(Object)
7723    2   1    0 org.apache.hadoop.mapred.SequenceFileInputFilter.FilterBase.getConf()
7724    5   3    1 org.apache.hadoop.mapred.SequenceFileInputFilter.RegexFilter.setPattern(Configuration,String)
7725    1   1    0 org.apache.hadoop.mapred.SequenceFileInputFilter.RegexFilter.RegexFilter()
7726    6   3    1 org.apache.hadoop.mapred.SequenceFileInputFilter.RegexFilter.setConf(Configuration)
7727    2   1    1 org.apache.hadoop.mapred.SequenceFileInputFilter.RegexFilter.accept(Object)
7728    4   3    1 org.apache.hadoop.mapred.SequenceFileInputFilter.PercentFilter.setFrequency(Configuration,int)
7729    1   1    0 org.apache.hadoop.mapred.SequenceFileInputFilter.PercentFilter.PercentFilter()
7730    5   3    1 org.apache.hadoop.mapred.SequenceFileInputFilter.PercentFilter.setConf(Configuration)
7731    7   3    1 org.apache.hadoop.mapred.SequenceFileInputFilter.PercentFilter.accept(Object)
7732    4   3    1 org.apache.hadoop.mapred.SequenceFileInputFilter.MD5Filter.setFrequency(Configuration,int)
7733    1   1    0 org.apache.hadoop.mapred.SequenceFileInputFilter.MD5Filter.MD5Filter()
7734    5   3    1 org.apache.hadoop.mapred.SequenceFileInputFilter.MD5Filter.setConf(Configuration)
7735   17   7    1 org.apache.hadoop.mapred.SequenceFileInputFilter.MD5Filter.accept(Object)
7736    2   1    0 org.apache.hadoop.mapred.SequenceFileInputFilter.MD5Filter.MD5Hashcode(Text)
7737    2   1    0 org.apache.hadoop.mapred.SequenceFileInputFilter.MD5Filter.MD5Hashcode(BytesWritable)
7738    7   2    0 org.apache.hadoop.mapred.SequenceFileInputFilter.MD5Filter.MD5Hashcode(byte[],int,int)
7739    3   1    0 org.apache.hadoop.mapred.SequenceFileInputFilter.FilterRecordReader.FilterRecordReader(Configuration,FileSplit)
7740    6   4    0 org.apache.hadoop.mapred.SequenceFileInputFilter.FilterRecordReader.next(K,V)
7741    2   1    0 org.apache.hadoop.mapred.SequenceFileInputFormat.SequenceFileInputFormat()
7742    9   3    0 org.apache.hadoop.mapred.SequenceFileInputFormat.listStatus(JobConf)
7743    3   1    0 org.apache.hadoop.mapred.SequenceFileInputFormat.getRecordReader(InputSplit,JobConf,Reporter)
7744    2   1    0 org.apache.hadoop.mapred.SequenceFileOutputFormat.RecordWriter$1.write(K,V)
7745    2   1    0 org.apache.hadoop.mapred.SequenceFileOutputFormat.RecordWriter$1.close(Reporter)
7746   15   2    0 org.apache.hadoop.mapred.SequenceFileOutputFormat.getRecordWriter(FileSystem,JobConf,String,Progressable)
7747    8   2    1 org.apache.hadoop.mapred.SequenceFileOutputFormat.getReaders(Configuration,Path)
7748    3   1    1 org.apache.hadoop.mapred.SequenceFileOutputFormat.getOutputCompressionType(JobConf)
7749    3   1    1 org.apache.hadoop.mapred.SequenceFileOutputFormat.setOutputCompressionType(JobConf,CompressionType)
7750   10   2    0 org.apache.hadoop.mapred.SequenceFileRecordReader.SequenceFileRecordReader(Configuration,FileSplit)
7751    2   1    1 org.apache.hadoop.mapred.SequenceFileRecordReader.getKeyClass()
7752    2   1    1 org.apache.hadoop.mapred.SequenceFileRecordReader.getValueClass()
7753    2   1    0 org.apache.hadoop.mapred.SequenceFileRecordReader.createKey()
7754    2   1    0 org.apache.hadoop.mapred.SequenceFileRecordReader.createValue()
7755   12   6    0 org.apache.hadoop.mapred.SequenceFileRecordReader.next(K,V)
7756   10   5    0 org.apache.hadoop.mapred.SequenceFileRecordReader.next(K)
7757    2   1    0 org.apache.hadoop.mapred.SequenceFileRecordReader.getCurrentValue(V)
7758    5   3    1 org.apache.hadoop.mapred.SequenceFileRecordReader.getProgress()
7759    2   1    0 org.apache.hadoop.mapred.SequenceFileRecordReader.getPos()
7760    2   1    0 org.apache.hadoop.mapred.SequenceFileRecordReader.seek(long)
7761    2   1    0 org.apache.hadoop.mapred.SequenceFileRecordReader.close()
7762    2   1    1 org.apache.hadoop.mapred.SkipBadRecords.getAttemptsToStartSkipping(Configuration)
7763    2   1    1 org.apache.hadoop.mapred.SkipBadRecords.setAttemptsToStartSkipping(Configuration,int)
7764    2   1    1 org.apache.hadoop.mapred.SkipBadRecords.getAutoIncrMapperProcCount(Configuration)
7765    2   1    1 org.apache.hadoop.mapred.SkipBadRecords.setAutoIncrMapperProcCount(Configuration,boolean)
7766    2   1    1 org.apache.hadoop.mapred.SkipBadRecords.getAutoIncrReducerProcCount(Configuration)
7767    2   1    1 org.apache.hadoop.mapred.SkipBadRecords.setAutoIncrReducerProcCount(Configuration,boolean)
7768    8   6    1 org.apache.hadoop.mapred.SkipBadRecords.getSkipOutputPath(Configuration)
7769    7   2    1 org.apache.hadoop.mapred.SkipBadRecords.setSkipOutputPath(JobConf,Path)
7770    2   1    1 org.apache.hadoop.mapred.SkipBadRecords.getMapperMaxSkipRecords(Configuration)
7771    2   1    1 org.apache.hadoop.mapred.SkipBadRecords.setMapperMaxSkipRecords(Configuration,long)
7772    2   1    1 org.apache.hadoop.mapred.SkipBadRecords.getReducerMaxSkipGroups(Configuration)
7773    2   1    1 org.apache.hadoop.mapred.SkipBadRecords.setReducerMaxSkipGroups(Configuration,long)
7774    2   1    1 org.apache.hadoop.mapred.SortedRanges.skipRangeIterator()
7775    2   1    1 org.apache.hadoop.mapred.SortedRanges.getIndicesCount()
7776    2   1    1 org.apache.hadoop.mapred.SortedRanges.getRanges()
7777   27  10    1 org.apache.hadoop.mapred.SortedRanges.add(Range)
7778   28  10    1 org.apache.hadoop.mapred.SortedRanges.remove(Range)
7779    6   2    0 org.apache.hadoop.mapred.SortedRanges.add(long,long)
7780    8   2    0 org.apache.hadoop.mapred.SortedRanges.readFields(DataInput)
7781    7   2    0 org.apache.hadoop.mapred.SortedRanges.write(DataOutput)
7782    7   2    0 org.apache.hadoop.mapred.SortedRanges.toString()
7783    5   3    0 org.apache.hadoop.mapred.SortedRanges.Range.Range(long,long)
7784    2   1    0 org.apache.hadoop.mapred.SortedRanges.Range.Range()
7785    2   1    1 org.apache.hadoop.mapred.SortedRanges.Range.getStartIndex()
7786    2   1    1 org.apache.hadoop.mapred.SortedRanges.Range.getEndIndex()
7787    2   1    1 org.apache.hadoop.mapred.SortedRanges.Range.getLength()
7788    2   1    1 org.apache.hadoop.mapred.SortedRanges.Range.isEmpty()
7789    5   5    0 org.apache.hadoop.mapred.SortedRanges.Range.equals(Object)
7790    2   1    0 org.apache.hadoop.mapred.SortedRanges.Range.hashCode()
7791    4   4    0 org.apache.hadoop.mapred.SortedRanges.Range.compareTo(Range)
7792    3   1    0 org.apache.hadoop.mapred.SortedRanges.Range.readFields(DataInput)
7793    3   1    0 org.apache.hadoop.mapred.SortedRanges.Range.write(DataOutput)
7794    2   1    0 org.apache.hadoop.mapred.SortedRanges.Range.toString()
7795    3   1    1 org.apache.hadoop.mapred.SortedRanges.SkipRangeIterator.SkipRangeIterator(Iterator)
7796    2   1    1 org.apache.hadoop.mapred.SortedRanges.SkipRangeIterator.hasNext()
7797    4   1    1 org.apache.hadoop.mapred.SortedRanges.SkipRangeIterator.next()
7798    7   3    0 org.apache.hadoop.mapred.SortedRanges.SkipRangeIterator.doNext()
7799    4   3    0 org.apache.hadoop.mapred.SortedRanges.SkipRangeIterator.skipIfInRange()
7800    2   2    1 org.apache.hadoop.mapred.SortedRanges.SkipRangeIterator.skippedAllRanges()
7801    2   2    1 org.apache.hadoop.mapred.SortedRanges.SkipRangeIterator.remove()
7802    3   1    1 org.apache.hadoop.mapred.StatusHttpServer.StatusHttpServer(String,String,int,boolean,Configuration)
7803   73  17    0 org.apache.hadoop.mapred.StatusHttpServer.TaskGraphServlet.doGet(HttpServletRequest,HttpServletResponse)
7804    7   3    1 org.apache.hadoop.mapred.StatusHttpServer.TaskGraphServlet.getMapAvarageProgress(int,int,TaskReport[])
7805   13   6    1 org.apache.hadoop.mapred.StatusHttpServer.TaskGraphServlet.getReduceAvarageProgresses(int,int,TaskReport[])
7806   13   2    0 org.apache.hadoop.mapred.StatusHttpServer.TaskGraphServlet.printRect(PrintWriter,int,int,int,int,String)
7807   12   1    0 org.apache.hadoop.mapred.StatusHttpServer.TaskGraphServlet.printLine(PrintWriter,int,int,int,int,String)
7808   10   1    0 org.apache.hadoop.mapred.StatusHttpServer.TaskGraphServlet.printText(PrintWriter,int,int,String,String)
7809    2   1    0 org.apache.hadoop.mapred.Task.getOutputName(int)
7810    2   1    0 org.apache.hadoop.mapred.Task.Task()
7811    6   2    0 org.apache.hadoop.mapred.Task.Task(String,TaskAttemptID,int)
7812    2   1    0 org.apache.hadoop.mapred.Task.setJobFile(String)
7813    2   1    0 org.apache.hadoop.mapred.Task.getJobFile()
7814    2   1    0 org.apache.hadoop.mapred.Task.getTaskID()
7815    2   1    0 org.apache.hadoop.mapred.Task.getCounters()
7816    2   1    1 org.apache.hadoop.mapred.Task.getJobID()
7817    2   1    1 org.apache.hadoop.mapred.Task.getPartition()
7818    2   1    1 org.apache.hadoop.mapred.Task.getPhase()
7819    2   1    1 org.apache.hadoop.mapred.Task.setPhase(TaskStatus.Phase)
7820    2   1    1 org.apache.hadoop.mapred.Task.toWriteSkipRecs()
7821    2   1    1 org.apache.hadoop.mapred.Task.setWriteSkipRecs(boolean)
7822    2   1    1 org.apache.hadoop.mapred.Task.getSkipRanges()
7823    2   1    1 org.apache.hadoop.mapred.Task.setSkipRanges(SortedRanges)
7824    2   1    1 org.apache.hadoop.mapred.Task.isSkipping()
7825    2   1    1 org.apache.hadoop.mapred.Task.setSkipping(boolean)
7826    2   1    1 org.apache.hadoop.mapred.Task.setCleanupTask()
7827    2   1    0 org.apache.hadoop.mapred.Task.setSetupTask()
7828   10   1    0 org.apache.hadoop.mapred.Task.write(DataOutput)
7829   13   1    0 org.apache.hadoop.mapred.Task.readFields(DataInput)
7830    2   1    0 org.apache.hadoop.mapred.Task.toString()
7831   14   3    1 org.apache.hadoop.mapred.Task.localizeConfiguration(JobConf)
7832    1   1    1 org.apache.hadoop.mapred.Task.run(JobConf,TaskUmbilicalProtocol)
7833    1   1    1 org.apache.hadoop.mapred.Task.createRunner(TaskTracker,TaskTracker.TaskInProgress)
7834    2   1    0 org.apache.hadoop.mapred.Task.setProgressFlag()
7835    2   1    0 org.apache.hadoop.mapred.Task.resetProgressFlag()
7836    1   1    0 org.apache.hadoop.mapred.Task.isMapTask()
7837    2   1    0 org.apache.hadoop.mapred.Task.getProgress()
7838    2   2    0 org.apache.hadoop.mapred.Task.getInputSplit()
7839   32   8    0 org.apache.hadoop.mapred.Task.Runnable$1.run()
7840   37   1    1 org.apache.hadoop.mapred.Task.startCommunicationThread(TaskUmbilicalProtocol)
7841    5   1    0 org.apache.hadoop.mapred.Task.initialize(JobConf,Reporter)
7842    3   1    0 org.apache.hadoop.mapred.Task.Reporter$2.setStatus(String)
7843    2   1    0 org.apache.hadoop.mapred.Task.Reporter$2.progress()
7844    5   2    0 org.apache.hadoop.mapred.Task.Reporter$2.getCounter(String,String)
7845    4   2    0 org.apache.hadoop.mapred.Task.Reporter$2.incrCounter(Enum,long)
7846    7   7    0 org.apache.hadoop.mapred.Task.Reporter$2.incrCounter(String,String,long)
7847    2   1    0 org.apache.hadoop.mapred.Task.Reporter$2.getInputSplit()
7848   25   1    0 org.apache.hadoop.mapred.Task.getReporter(TaskUmbilicalProtocol)
7849    6   1    1 org.apache.hadoop.mapred.Task.reportNextRecordRange(TaskUmbilicalProtocol,long)
7850    3   1    0 org.apache.hadoop.mapred.Task.setProgress(float)
7851    4   1    0 org.apache.hadoop.mapred.Task.FileSystemStatisticUpdater.FileSystemStatisticUpdater(FileSystemCounter,FileSystemCounter,Class)
7852   13   5    0 org.apache.hadoop.mapred.Task.FileSystemStatisticUpdater.updateCounters()
7853    3   2    0 org.apache.hadoop.mapred.Task.updateCounters()
7854   24   7    0 org.apache.hadoop.mapred.Task.done(TaskUmbilicalProtocol)
7855   18   9    0 org.apache.hadoop.mapred.Task.sendLastUpdate(TaskUmbilicalProtocol)
7856   10   6    0 org.apache.hadoop.mapred.Task.sendDone(TaskUmbilicalProtocol)
7857   19   9    0 org.apache.hadoop.mapred.Task.commit(TaskUmbilicalProtocol,OutputCommitter)
7858    4   2    0 org.apache.hadoop.mapred.Task.discardOutput(TaskAttemptContext,OutputCommitter)
7859    5   1    0 org.apache.hadoop.mapred.Task.runCleanup(TaskUmbilicalProtocol)
7860    4   1    0 org.apache.hadoop.mapred.Task.runSetupJob(TaskUmbilicalProtocol)
7861   13   4    0 org.apache.hadoop.mapred.Task.setConf(Configuration)
7862    2   1    0 org.apache.hadoop.mapred.Task.getConf()
7863    2   1    0 org.apache.hadoop.mapred.Task.CombineOutputCollector.CombineOutputCollector(Counters.Counter)
7864    2   1    0 org.apache.hadoop.mapred.Task.CombineOutputCollector.setWriter(Writer)
7865    3   1    0 org.apache.hadoop.mapred.Task.CombineOutputCollector.collect(K,V)
7866   13   1    0 org.apache.hadoop.mapred.Task.ValuesIterator.ValuesIterator(RawKeyValueIterator,RawComparator,Class,Class,Configuration,Progressable)
7867    2   1    0 org.apache.hadoop.mapred.Task.ValuesIterator.getRawIterator()
7868    2   1    0 org.apache.hadoop.mapred.Task.ValuesIterator.hasNext()
7869    9   5    0 org.apache.hadoop.mapred.Task.ValuesIterator.next()
7870    2   2    0 org.apache.hadoop.mapred.Task.ValuesIterator.remove()
7871    8   2    1 org.apache.hadoop.mapred.Task.ValuesIterator.nextKey()
7872    2   1    1 org.apache.hadoop.mapred.Task.ValuesIterator.more()
7873    2   1    1 org.apache.hadoop.mapred.Task.ValuesIterator.getKey()
7874    9   3    1 org.apache.hadoop.mapred.Task.ValuesIterator.readNextKey()
7875    4   1    1 org.apache.hadoop.mapred.Task.ValuesIterator.readNextValue()
7876    3   1    0 org.apache.hadoop.mapred.Task.CombineValuesIterator.CombineValuesIterator(RawKeyValueIterator,RawComparator,Class,Class,Configuration,Reporter,Counters.Counter)
7877    3   1    0 org.apache.hadoop.mapred.Task.CombineValuesIterator.next()
7878    2   1    0 org.apache.hadoop.mapred.TaskAttemptContext.TaskAttemptContext(JobConf,TaskAttemptID)
7879    4   1    0 org.apache.hadoop.mapred.TaskAttemptContext.TaskAttemptContext(JobConf,TaskAttemptID,Progressable)
7880    2   1    1 org.apache.hadoop.mapred.TaskAttemptContext.getTaskAttemptID()
7881    2   1    1 org.apache.hadoop.mapred.TaskAttemptContext.getJobConf()
7882    5   3    1 org.apache.hadoop.mapred.TaskAttemptID.TaskAttemptID(TaskID,int)
7883    2   1    1 org.apache.hadoop.mapred.TaskAttemptID.TaskAttemptID(String,int,boolean,int,int)
7884    1   1    0 org.apache.hadoop.mapred.TaskAttemptID.TaskAttemptID()
7885    2   1    1 org.apache.hadoop.mapred.TaskAttemptID.getJobID()
7886    2   1    1 org.apache.hadoop.mapred.TaskAttemptID.getTaskID()
7887    2   1    1 org.apache.hadoop.mapred.TaskAttemptID.isMap()
7888    8   6    0 org.apache.hadoop.mapred.TaskAttemptID.equals(Object)
7889    7   3    0 org.apache.hadoop.mapred.TaskAttemptID.compareTo(ID)
7890    3   1    0 org.apache.hadoop.mapred.TaskAttemptID.toString()
7891    3   1    0 org.apache.hadoop.mapred.TaskAttemptID.toStringWOPrefix()
7892    2   1    0 org.apache.hadoop.mapred.TaskAttemptID.hashCode()
7893    3   1    0 org.apache.hadoop.mapred.TaskAttemptID.readFields(DataInput)
7894    3   1    0 org.apache.hadoop.mapred.TaskAttemptID.write(DataOutput)
7895    4   1    0 org.apache.hadoop.mapred.TaskAttemptID.read(DataInput)
7896   17  11    1 org.apache.hadoop.mapred.TaskAttemptID.forName(String)
7897    4   1    1 org.apache.hadoop.mapred.TaskAttemptID.getTaskAttemptIDsPattern(String,Integer,Boolean,Integer,Integer)
7898    4   2    0 org.apache.hadoop.mapred.TaskAttemptID.getTaskAttemptIDsPatternWOPrefix(String,Integer,Boolean,Integer,Integer)
7899    1   1    1 org.apache.hadoop.mapred.TaskCompletionEvent.TaskCompletionEvent()
7900    7   1    1 org.apache.hadoop.mapred.TaskCompletionEvent.TaskCompletionEvent(int,TaskAttemptID,int,boolean,Status,String)
7901    2   1    1 org.apache.hadoop.mapred.TaskCompletionEvent.getEventId()
7902    2   1    0 org.apache.hadoop.mapred.TaskCompletionEvent.getTaskId()
7903    2   1    1 org.apache.hadoop.mapred.TaskCompletionEvent.getTaskAttemptId()
7904    2   1    1 org.apache.hadoop.mapred.TaskCompletionEvent.getTaskStatus()
7905    2   1    1 org.apache.hadoop.mapred.TaskCompletionEvent.getTaskTrackerHttp()
7906    2   1    1 org.apache.hadoop.mapred.TaskCompletionEvent.getTaskRunTime()
7907    2   1    1 org.apache.hadoop.mapred.TaskCompletionEvent.setTaskRunTime(int)
7908    2   1    1 org.apache.hadoop.mapred.TaskCompletionEvent.setEventId(int)
7909    2   1    0 org.apache.hadoop.mapred.TaskCompletionEvent.setTaskId(String)
7910    2   1    1 org.apache.hadoop.mapred.TaskCompletionEvent.setTaskID(TaskAttemptID)
7911    2   1    1 org.apache.hadoop.mapred.TaskCompletionEvent.setTaskStatus(Status)
7912    2   1    1 org.apache.hadoop.mapred.TaskCompletionEvent.setTaskTrackerHttp(String)
7913    7   1    0 org.apache.hadoop.mapred.TaskCompletionEvent.toString()
7914    7  11    0 org.apache.hadoop.mapred.TaskCompletionEvent.equals(Object)
7915    2   1    0 org.apache.hadoop.mapred.TaskCompletionEvent.hashCode()
7916    2   1    0 org.apache.hadoop.mapred.TaskCompletionEvent.isMapTask()
7917    2   1    0 org.apache.hadoop.mapred.TaskCompletionEvent.idWithinJob()
7918    8   1    0 org.apache.hadoop.mapred.TaskCompletionEvent.write(DataOutput)
7919    8   1    0 org.apache.hadoop.mapred.TaskCompletionEvent.readFields(DataInput)
7920    6   3    1 org.apache.hadoop.mapred.TaskID.TaskID(JobID,boolean,int)
7921    2   1    1 org.apache.hadoop.mapred.TaskID.TaskID(String,int,boolean,int)
7922    1   1    0 org.apache.hadoop.mapred.TaskID.TaskID()
7923    2   1    1 org.apache.hadoop.mapred.TaskID.getJobID()
7924    2   1    1 org.apache.hadoop.mapred.TaskID.isMap()
7925    8   7    0 org.apache.hadoop.mapred.TaskID.equals(Object)
7926   10   6    0 org.apache.hadoop.mapred.TaskID.compareTo(ID)
7927    3   1    0 org.apache.hadoop.mapred.TaskID.toString()
7928    4   2    0 org.apache.hadoop.mapred.TaskID.toStringWOPrefix()
7929    2   1    0 org.apache.hadoop.mapred.TaskID.hashCode()
7930    4   1    0 org.apache.hadoop.mapred.TaskID.readFields(DataInput)
7931    4   1    0 org.apache.hadoop.mapred.TaskID.write(DataOutput)
7932    4   1    0 org.apache.hadoop.mapred.TaskID.read(DataInput)
7933   17  11    1 org.apache.hadoop.mapred.TaskID.forName(String)
7934    3   1    1 org.apache.hadoop.mapred.TaskID.getTaskIDsPattern(String,Integer,Boolean,Integer)
7935    4   4    0 org.apache.hadoop.mapred.TaskID.getTaskIDsPatternWOPrefix(String,Integer,Boolean,Integer)
7936   10   1    1 org.apache.hadoop.mapred.TaskInProgress.TaskInProgress(JobID,String,RawSplit,JobTracker,JobConf,JobInProgress,int)
7937   10   1    1 org.apache.hadoop.mapred.TaskInProgress.TaskInProgress(JobID,String,int,int,JobTracker,JobConf,JobInProgress)
7938    5   2    1 org.apache.hadoop.mapred.TaskInProgress.setMaxTaskAttempts()
7939    2   1    1 org.apache.hadoop.mapred.TaskInProgress.idWithinJob()
7940    2   1    0 org.apache.hadoop.mapred.TaskInProgress.isCleanupTask()
7941    2   1    0 org.apache.hadoop.mapred.TaskInProgress.setCleanupTask()
7942    2   1    0 org.apache.hadoop.mapred.TaskInProgress.isSetupTask()
7943    2   1    0 org.apache.hadoop.mapred.TaskInProgress.setSetupTask()
7944    5   4    0 org.apache.hadoop.mapred.TaskInProgress.isOnlyCommitPending()
7945    5   3    0 org.apache.hadoop.mapred.TaskInProgress.isCommitPending(TaskAttemptID)
7946    4   1    1 org.apache.hadoop.mapred.TaskInProgress.init(JobID)
7947    2   1    1 org.apache.hadoop.mapred.TaskInProgress.getStartTime()
7948    2   1    1 org.apache.hadoop.mapred.TaskInProgress.getExecStartTime()
7949    2   1    1 org.apache.hadoop.mapred.TaskInProgress.setExecStartTime(long)
7950    2   1    1 org.apache.hadoop.mapred.TaskInProgress.getExecFinishTime()
7951    2   1    1 org.apache.hadoop.mapred.TaskInProgress.setExecFinishTime(long)
7952    2   1    1 org.apache.hadoop.mapred.TaskInProgress.getJob()
7953    2   1    1 org.apache.hadoop.mapred.TaskInProgress.getTIPId()
7954    2   1    1 org.apache.hadoop.mapred.TaskInProgress.isMapTask()
7955    2   1    1 org.apache.hadoop.mapred.TaskInProgress.getTask(TaskAttemptID)
7956    2   1    1 org.apache.hadoop.mapred.TaskInProgress.isFirstAttempt(TaskAttemptID)
7957    2   1    1 org.apache.hadoop.mapred.TaskInProgress.isRunning()
7958    2   1    0 org.apache.hadoop.mapred.TaskInProgress.getSuccessfulTaskid()
7959    2   1    0 org.apache.hadoop.mapred.TaskInProgress.setSuccessfulTaskid(TaskAttemptID)
7960    2   1    0 org.apache.hadoop.mapred.TaskInProgress.resetSuccessfulTaskid()
7961    2   1    1 org.apache.hadoop.mapred.TaskInProgress.isComplete()
7962    2   2    1 org.apache.hadoop.mapred.TaskInProgress.isComplete(TaskAttemptID)
7963    2   1    1 org.apache.hadoop.mapred.TaskInProgress.isFailed()
7964    2   1    1 org.apache.hadoop.mapred.TaskInProgress.numTaskFailures()
7965    2   1    1 org.apache.hadoop.mapred.TaskInProgress.numKilledTasks()
7966    2   1    1 org.apache.hadoop.mapred.TaskInProgress.getProgress()
7967    2   1    1 org.apache.hadoop.mapred.TaskInProgress.getCounters()
7968   17  15    1 org.apache.hadoop.mapred.TaskInProgress.shouldClose(TaskAttemptID)
7969    2   1    1 org.apache.hadoop.mapred.TaskInProgress.doCommit(TaskAttemptID)
7970    2   2    1 org.apache.hadoop.mapred.TaskInProgress.shouldCommit(TaskAttemptID)
7971    6   2    1 org.apache.hadoop.mapred.TaskInProgress.generateSingleReport()
7972    2   1    1 org.apache.hadoop.mapred.TaskInProgress.getDiagnosticInfo(TaskAttemptID)
7973    6   2    1 org.apache.hadoop.mapred.TaskInProgress.addDiagnosticInfo(TaskAttemptID,String)
7974   22  17    1 org.apache.hadoop.mapred.TaskInProgress.updateStatus(TaskStatus)
7975   35  16    1 org.apache.hadoop.mapred.TaskInProgress.incompleteSubTask(TaskAttemptID,TaskTrackerStatus,JobStatus)
7976    4   4    1 org.apache.hadoop.mapred.TaskInProgress.startSkipping()
7977    4   1    1 org.apache.hadoop.mapred.TaskInProgress.completedTask(TaskAttemptID,TaskStatus.State)
7978    4   1    1 org.apache.hadoop.mapred.TaskInProgress.alreadyCompletedTask(TaskAttemptID)
7979    6   1    1 org.apache.hadoop.mapred.TaskInProgress.completed(TaskAttemptID)
7980    2   1    1 org.apache.hadoop.mapred.TaskInProgress.getSplitLocations()
7981    2   1    1 org.apache.hadoop.mapred.TaskInProgress.getTaskStatuses()
7982    2   1    1 org.apache.hadoop.mapred.TaskInProgress.getTaskStatus(TaskAttemptID)
7983    7   4    1 org.apache.hadoop.mapred.TaskInProgress.kill()
7984    2   1    1 org.apache.hadoop.mapred.TaskInProgress.wasKilled()
7985    8   8    1 org.apache.hadoop.mapred.TaskInProgress.killTask(TaskAttemptID,boolean)
7986   37   8    1 org.apache.hadoop.mapred.TaskInProgress.recomputeProgress()
7987    2   2    1 org.apache.hadoop.mapred.TaskInProgress.isRunnable()
7988    4   8    1 org.apache.hadoop.mapred.TaskInProgress.hasSpeculativeTask(long,double)
7989   12   4    1 org.apache.hadoop.mapred.TaskInProgress.getTaskToRun(String)
7990   21   5    1 org.apache.hadoop.mapred.TaskInProgress.addRunningTask(TaskAttemptID,String)
7991    2   1    1 org.apache.hadoop.mapred.TaskInProgress.hasFailedOnMachine(String)
7992    2   2    1 org.apache.hadoop.mapred.TaskInProgress.hasRunOnMachine(String,String)
7993    2   1    1 org.apache.hadoop.mapred.TaskInProgress.getNumberOfFailedMachines()
7994    2   1    1 org.apache.hadoop.mapred.TaskInProgress.getIdWithinJob()
7995    2   1    1 org.apache.hadoop.mapred.TaskInProgress.setSuccessEventNumber(int)
7996    2   1    1 org.apache.hadoop.mapred.TaskInProgress.getSuccessEventNumber()
7997    4   1    0 org.apache.hadoop.mapred.TaskInProgress.Comparator$1.compare(Node,Node)
7998   13   4    1 org.apache.hadoop.mapred.TaskInProgress.getSplitNodes()
7999    8   5    0 org.apache.hadoop.mapred.TaskInProgress.nodeToString(Node[])
8000    5   5    0 org.apache.hadoop.mapred.TaskInProgress.getMapInputSize()
8001    2   1    0 org.apache.hadoop.mapred.TaskInProgress.clearSplit()
8002    4   3    0 org.apache.hadoop.mapred.TaskInProgress.FailedRanges.getSkipRanges()
8003    2   1    0 org.apache.hadoop.mapred.TaskInProgress.FailedRanges.isTestAttempt()
8004    4   3    0 org.apache.hadoop.mapred.TaskInProgress.FailedRanges.getIndicesCount()
8005    4   3    0 org.apache.hadoop.mapred.TaskInProgress.FailedRanges.updateState(TaskStatus)
8006   13   5    0 org.apache.hadoop.mapred.TaskInProgress.FailedRanges.add(Range)
8007    9   2    0 org.apache.hadoop.mapred.TaskInProgress.FailedRanges.Divide.Divide(Range)
8008    2   1    0 org.apache.hadoop.mapred.TaskInProgress.getActiveTasks()
8009    2   1    0 org.apache.hadoop.mapred.TaskLog.getTaskLogFile(TaskAttemptID,LogName)
8010    7   3    0 org.apache.hadoop.mapred.TaskLog.getRealTaskLogFileLocation(TaskAttemptID,LogName)
8011   24   8    0 org.apache.hadoop.mapred.TaskLog.getTaskLogFileDetail(TaskAttemptID,LogName)
8012    2   1    0 org.apache.hadoop.mapred.TaskLog.getIndexFile(String)
8013    2   1    0 org.apache.hadoop.mapred.TaskLog.getBaseDir(String)
8014   12   1    0 org.apache.hadoop.mapred.TaskLog.writeToIndexFile(TaskAttemptID)
8015    4   1    0 org.apache.hadoop.mapred.TaskLog.resetPrevLengths(TaskAttemptID)
8016   15   5    0 org.apache.hadoop.mapred.TaskLog.syncLogs(TaskAttemptID,TaskAttemptID)
8017    2   1    0 org.apache.hadoop.mapred.TaskLog.LogName.LogName(String)
8018    2   1    0 org.apache.hadoop.mapred.TaskLog.LogName.toString()
8019    2   1    0 org.apache.hadoop.mapred.TaskLog.TaskLogsPurgeFilter.TaskLogsPurgeFilter(long)
8020    3   1    0 org.apache.hadoop.mapred.TaskLog.TaskLogsPurgeFilter.accept(File)
8021    6   3    1 org.apache.hadoop.mapred.TaskLog.cleanup(int)
8022   20   5    1 org.apache.hadoop.mapred.TaskLog.Reader.Reader(TaskAttemptID,LogName,long,long)
8023    6   2    0 org.apache.hadoop.mapred.TaskLog.Reader.read()
8024    6   2    0 org.apache.hadoop.mapred.TaskLog.Reader.read(byte[],int,int)
8025    2   1    0 org.apache.hadoop.mapred.TaskLog.Reader.available()
8026    2   1    0 org.apache.hadoop.mapred.TaskLog.Reader.close()
8027    2   1    1 org.apache.hadoop.mapred.TaskLog.getTaskLogLength(JobConf)
8028    2   1    1 org.apache.hadoop.mapred.TaskLog.captureOutAndError(List,File,File,long)
8029    2   1    1 org.apache.hadoop.mapred.TaskLog.captureOutAndError(List,List,File,File,long)
8030   41   6    1 org.apache.hadoop.mapred.TaskLog.captureOutAndError(List,List,File,File,long,String)
8031   12   3    1 org.apache.hadoop.mapred.TaskLog.addCommand(List,boolean)
8032   21   3    1 org.apache.hadoop.mapred.TaskLog.captureDebugOut(List,File)
8033    7   2    0 org.apache.hadoop.mapred.TaskLogAppender.activateOptions()
8034    8   3    0 org.apache.hadoop.mapred.TaskLogAppender.append(LoggingEvent)
8035    2   1    0 org.apache.hadoop.mapred.TaskLogAppender.flush()
8036    5   3    0 org.apache.hadoop.mapred.TaskLogAppender.close()
8037    2   1    1 org.apache.hadoop.mapred.TaskLogAppender.getTaskId()
8038    2   1    0 org.apache.hadoop.mapred.TaskLogAppender.setTaskId(String)
8039    2   1    0 org.apache.hadoop.mapred.TaskLogAppender.getTotalLogFileSize()
8040    2   1    0 org.apache.hadoop.mapred.TaskLogAppender.setTotalLogFileSize(long)
8041    3   1    0 org.apache.hadoop.mapred.TaskLogServlet.haveTaskLog(TaskAttemptID,TaskLog.LogName)
8042    2   1    1 org.apache.hadoop.mapred.TaskLogServlet.getTaskLogUrl(String,String,String)
8043   10   6    1 org.apache.hadoop.mapred.TaskLogServlet.findFirstQuotable(byte[],int,int)
8044   21   6    0 org.apache.hadoop.mapred.TaskLogServlet.quotedWrite(OutputStream,byte[],int,int)
8045   25   9    0 org.apache.hadoop.mapred.TaskLogServlet.printTaskLog(HttpServletResponse,OutputStream,TaskAttemptID,long,long,boolean,TaskLog.LogName)
8046   45  14    0 org.apache.hadoop.mapred.TaskLogServlet.doGet(HttpServletRequest,HttpServletResponse)
8047    8   1    0 org.apache.hadoop.mapred.TaskMemoryManagerThread.TaskMemoryManagerThread(TaskTracker)
8048    6   2    0 org.apache.hadoop.mapred.TaskMemoryManagerThread.addTask(TaskAttemptID,long)
8049    3   1    0 org.apache.hadoop.mapred.TaskMemoryManagerThread.removeTask(TaskAttemptID)
8050    7   2    0 org.apache.hadoop.mapred.TaskMemoryManagerThread.ProcessTreeInfo.ProcessTreeInfo(TaskAttemptID,String,ProcfsBasedProcessTree,long,long)
8051    2   1    0 org.apache.hadoop.mapred.TaskMemoryManagerThread.ProcessTreeInfo.getTID()
8052    2   1    0 org.apache.hadoop.mapred.TaskMemoryManagerThread.ProcessTreeInfo.getPID()
8053    2   1    0 org.apache.hadoop.mapred.TaskMemoryManagerThread.ProcessTreeInfo.setPid(String)
8054    2   1    0 org.apache.hadoop.mapred.TaskMemoryManagerThread.ProcessTreeInfo.getProcessTree()
8055    2   1    0 org.apache.hadoop.mapred.TaskMemoryManagerThread.ProcessTreeInfo.setProcessTree(ProcfsBasedProcessTree)
8056    2   1    0 org.apache.hadoop.mapred.TaskMemoryManagerThread.ProcessTreeInfo.getMemLimit()
8057   50  12    0 org.apache.hadoop.mapred.TaskMemoryManagerThread.run()
8058    5   3    1 org.apache.hadoop.mapred.TaskMemoryManagerThread.getPid(TaskAttemptID)
8059    6   2    1 org.apache.hadoop.mapred.TaskMemoryManagerThread.getPidFilePath(TaskAttemptID,JobConf)
8060    6   4    0 org.apache.hadoop.mapred.TaskMemoryManagerThread.removePidFile(TaskAttemptID)
8061    1   1    0 org.apache.hadoop.mapred.TaskReport.TaskReport()
8062    8   1    0 org.apache.hadoop.mapred.TaskReport.TaskReport(TaskID,float,String,String[],long,long,Counters)
8063    2   1    0 org.apache.hadoop.mapred.TaskReport.getTaskId()
8064    2   1    1 org.apache.hadoop.mapred.TaskReport.getTaskID()
8065    2   1    1 org.apache.hadoop.mapred.TaskReport.getProgress()
8066    2   1    1 org.apache.hadoop.mapred.TaskReport.getState()
8067    2   1    1 org.apache.hadoop.mapred.TaskReport.getDiagnostics()
8068    2   1    1 org.apache.hadoop.mapred.TaskReport.getCounters()
8069    2   1    1 org.apache.hadoop.mapred.TaskReport.getFinishTime()
8070    2   1    1 org.apache.hadoop.mapred.TaskReport.setFinishTime(long)
8071    2   1    1 org.apache.hadoop.mapred.TaskReport.getStartTime()
8072    2   1    1 org.apache.hadoop.mapred.TaskReport.setStartTime(long)
8073    7  11    0 org.apache.hadoop.mapred.TaskReport.equals(Object)
8074    2   1    0 org.apache.hadoop.mapred.TaskReport.hashCode()
8075    8   1    0 org.apache.hadoop.mapred.TaskReport.write(DataOutput)
8076    9   1    0 org.apache.hadoop.mapred.TaskReport.readFields(DataInput)
8077    8   1    0 org.apache.hadoop.mapred.TaskRunner.TaskRunner(TaskTracker.TaskInProgress,TaskTracker,JobConf)
8078    2   1    0 org.apache.hadoop.mapred.TaskRunner.getTask()
8079    2   1    0 org.apache.hadoop.mapred.TaskRunner.getTaskInProgress()
8080    2   1    0 org.apache.hadoop.mapred.TaskRunner.getTracker()
8081    2   1    1 org.apache.hadoop.mapred.TaskRunner.prepare()
8082    1   1    1 org.apache.hadoop.mapred.TaskRunner.close()
8083    8   4    0 org.apache.hadoop.mapred.TaskRunner.stringifyPathArray(Path[])
8084  192  59    0 org.apache.hadoop.mapred.TaskRunner.run()
8085   37  16    0 org.apache.hadoop.mapred.TaskRunner.setupWorkDir(JobConf)
8086    4   1    1 org.apache.hadoop.mapred.TaskRunner.kill()
8087    4   1    0 org.apache.hadoop.mapred.TaskRunner.signalDone()
8088    3   1    0 org.apache.hadoop.mapred.TaskRunner.setExitCode(int)
8089    2   1    0 org.apache.hadoop.mapred.TaskScheduler.getConf()
8090    2   1    0 org.apache.hadoop.mapred.TaskScheduler.setConf(Configuration)
8091    2   1    0 org.apache.hadoop.mapred.TaskScheduler.setTaskTrackerManager(TaskTrackerManager)
8092    1   1    1 org.apache.hadoop.mapred.TaskScheduler.start()
8093    1   1    1 org.apache.hadoop.mapred.TaskScheduler.terminate()
8094    1   1    1 org.apache.hadoop.mapred.TaskScheduler.assignTasks(TaskTrackerStatus)
8095    1   1    1 org.apache.hadoop.mapred.TaskScheduler.getJobs(String)
8096    1   1    0 org.apache.hadoop.mapred.TaskStatus.TaskStatus()
8097   10   1    0 org.apache.hadoop.mapred.TaskStatus.TaskStatus(TaskAttemptID,float,State,String,String,String,Phase,Counters)
8098    2   1    0 org.apache.hadoop.mapred.TaskStatus.getTaskID()
8099    1   1    0 org.apache.hadoop.mapred.TaskStatus.getIsMap()
8100    2   1    0 org.apache.hadoop.mapred.TaskStatus.getProgress()
8101    2   1    0 org.apache.hadoop.mapred.TaskStatus.setProgress(float)
8102    2   1    0 org.apache.hadoop.mapred.TaskStatus.getRunState()
8103    2   1    0 org.apache.hadoop.mapred.TaskStatus.getTaskTracker()
8104    2   1    0 org.apache.hadoop.mapred.TaskStatus.setTaskTracker(String)
8105    2   1    0 org.apache.hadoop.mapred.TaskStatus.setRunState(State)
8106    2   1    0 org.apache.hadoop.mapred.TaskStatus.getDiagnosticInfo()
8107    2   2    0 org.apache.hadoop.mapred.TaskStatus.setDiagnosticInfo(String)
8108    2   1    0 org.apache.hadoop.mapred.TaskStatus.getStateString()
8109    2   1    0 org.apache.hadoop.mapred.TaskStatus.setStateString(String)
8110    2   1    1 org.apache.hadoop.mapred.TaskStatus.getNextRecordRange()
8111    2   1    1 org.apache.hadoop.mapred.TaskStatus.setNextRecordRange(SortedRanges.Range)
8112    2   1    1 org.apache.hadoop.mapred.TaskStatus.getFinishTime()
8113    2   1    1 org.apache.hadoop.mapred.TaskStatus.setFinishTime(long)
8114    2   1    1 org.apache.hadoop.mapred.TaskStatus.getShuffleFinishTime()
8115    1   1    1 org.apache.hadoop.mapred.TaskStatus.setShuffleFinishTime(long)
8116    2   1    1 org.apache.hadoop.mapred.TaskStatus.getSortFinishTime()
8117    1   1    1 org.apache.hadoop.mapred.TaskStatus.setSortFinishTime(long)
8118    2   1    1 org.apache.hadoop.mapred.TaskStatus.getStartTime()
8119    2   1    1 org.apache.hadoop.mapred.TaskStatus.setStartTime(long)
8120    2   1    1 org.apache.hadoop.mapred.TaskStatus.getPhase()
8121    9   4    1 org.apache.hadoop.mapred.TaskStatus.setPhase(Phase)
8122    2   1    0 org.apache.hadoop.mapred.TaskStatus.getIncludeCounters()
8123    2   1    0 org.apache.hadoop.mapred.TaskStatus.setIncludeCounters(boolean)
8124    2   1    1 org.apache.hadoop.mapred.TaskStatus.getCounters()
8125    2   1    1 org.apache.hadoop.mapred.TaskStatus.setCounters(Counters)
8126    2   1    1 org.apache.hadoop.mapred.TaskStatus.getOutputSize()
8127    2   1    1 org.apache.hadoop.mapred.TaskStatus.setOutputSize(long)
8128    2   1    1 org.apache.hadoop.mapred.TaskStatus.getFetchFailedMaps()
8129    1   1    1 org.apache.hadoop.mapred.TaskStatus.addFetchFailedMap(TaskAttemptID)
8130    5   1    1 org.apache.hadoop.mapred.TaskStatus.statusUpdate(State,float,String,Counters)
8131   13   3    1 org.apache.hadoop.mapred.TaskStatus.statusUpdate(TaskStatus)
8132    2   1    1 org.apache.hadoop.mapred.TaskStatus.clearStatus()
8133    4   4    0 org.apache.hadoop.mapred.TaskStatus.clone()
8134   14   2    0 org.apache.hadoop.mapred.TaskStatus.write(DataOutput)
8135   15   2    0 org.apache.hadoop.mapred.TaskStatus.readFields(DataInput)
8136    3   1    0 org.apache.hadoop.mapred.TaskStatus.createTaskStatus(DataInput,TaskAttemptID,float,State,String,String,String,Phase,Counters)
8137    2   2    0 org.apache.hadoop.mapred.TaskStatus.createTaskStatus(boolean,TaskAttemptID,float,State,String,String,String,Phase,Counters)
8138    2   2    0 org.apache.hadoop.mapred.TaskStatus.createTaskStatus(boolean)
8139    5   1    0 org.apache.hadoop.mapred.TaskStatus.readTaskStatus(DataInput)
8140    3   1    0 org.apache.hadoop.mapred.TaskStatus.writeTaskStatus(DataOutput,TaskStatus)
8141    5   1    0 org.apache.hadoop.mapred.TaskTracker.ShuffleServerMetrics.ShuffleServerMetrics(JobConf)
8142    2   1    0 org.apache.hadoop.mapred.TaskTracker.ShuffleServerMetrics.serverHandlerBusy()
8143    2   1    0 org.apache.hadoop.mapred.TaskTracker.ShuffleServerMetrics.serverHandlerFree()
8144    2   1    0 org.apache.hadoop.mapred.TaskTracker.ShuffleServerMetrics.outputBytes(long)
8145    2   1    0 org.apache.hadoop.mapred.TaskTracker.ShuffleServerMetrics.failedOutput()
8146    2   1    0 org.apache.hadoop.mapred.TaskTracker.ShuffleServerMetrics.successOutput()
8147   13   2    0 org.apache.hadoop.mapred.TaskTracker.ShuffleServerMetrics.doUpdates(MetricsContext)
8148    2   1    0 org.apache.hadoop.mapred.TaskTracker.getTaskTrackerInstrumentation()
8149   17   5    0 org.apache.hadoop.mapred.TaskTracker.Runnable$1.run()
8150   15   2    0 org.apache.hadoop.mapred.TaskTracker.addTaskToJob(JobID,Path,TaskInProgress)
8151    8   2    0 org.apache.hadoop.mapred.TaskTracker.removeTaskFromJob(JobID,TaskInProgress)
8152    2   1    0 org.apache.hadoop.mapred.TaskTracker.getCacheSubdir()
8153    2   1    0 org.apache.hadoop.mapred.TaskTracker.getJobCacheSubdir()
8154    2   1    0 org.apache.hadoop.mapred.TaskTracker.getPidFilesSubdir()
8155    5   4    0 org.apache.hadoop.mapred.TaskTracker.getProtocolVersion(String,long)
8156   57   6    1 org.apache.hadoop.mapred.TaskTracker.initialize()
8157    2   1    0 org.apache.hadoop.mapred.TaskTracker.getInstrumentationClass(Configuration)
8158    2   1    0 org.apache.hadoop.mapred.TaskTracker.setInstrumentationClass(Configuration,Class)
8159    2   1    1 org.apache.hadoop.mapred.TaskTracker.cleanupStorage()
8160   19   6    0 org.apache.hadoop.mapred.TaskTracker.MapEventsFetcherThread.reducesInShuffle()
8161   28  13    0 org.apache.hadoop.mapred.TaskTracker.MapEventsFetcherThread.run()
8162    4   1    0 org.apache.hadoop.mapred.TaskTracker.FetchStatus.FetchStatus(JobID,int)
8163    7   2    1 org.apache.hadoop.mapred.TaskTracker.FetchStatus.purgeMapEvents(int)
8164   14   3    0 org.apache.hadoop.mapred.TaskTracker.FetchStatus.getMapEvents(int,int)
8165   15   6    0 org.apache.hadoop.mapred.TaskTracker.FetchStatus.fetchMapCompletionEvents(long)
8166   50  13    0 org.apache.hadoop.mapred.TaskTracker.localizeJob(TaskInProgress)
8167    4   1    0 org.apache.hadoop.mapred.TaskTracker.launchTaskForJob(TaskInProgress,JobConf)
8168    8   3    0 org.apache.hadoop.mapred.TaskTracker.shutdown()
8169    4   2    0 org.apache.hadoop.mapred.TaskTracker.Thread$2.run()
8170   21   4    1 org.apache.hadoop.mapred.TaskTracker.close()
8171   26   1    1 org.apache.hadoop.mapred.TaskTracker.TaskTracker(JobConf)
8172    6   1    0 org.apache.hadoop.mapred.TaskTracker.startCleanupThreads()
8173    2   1    1 org.apache.hadoop.mapred.TaskTracker.getJobClient()
8174    2   1    1 org.apache.hadoop.mapred.TaskTracker.getTaskTrackerReportAddress()
8175    8   3    1 org.apache.hadoop.mapred.TaskTracker.queryJobTracker(IntWritable,JobID,InterTrackerProtocol)
8176   82  36    1 org.apache.hadoop.mapred.TaskTracker.offerService()
8177   42  13    1 org.apache.hadoop.mapred.TaskTracker.transmitHeartBeat(long)
8178    2   1    1 org.apache.hadoop.mapred.TaskTracker.getMaxVirtualMemoryForTasks()
8179    8   6    1 org.apache.hadoop.mapred.TaskTracker.findFreeVirtualMemory()
8180    5   2    1 org.apache.hadoop.mapred.TaskTracker.getMemoryForTask(JobConf)
8181    7   5    1 org.apache.hadoop.mapred.TaskTracker.reinitTaskTracker(TaskTrackerAction[])
8182   15   7    1 org.apache.hadoop.mapred.TaskTracker.markUnresponsiveTasks()
8183   20   5    1 org.apache.hadoop.mapred.TaskTracker.purgeJob(KillJobAction)
8184    7   3    1 org.apache.hadoop.mapred.TaskTracker.purgeTask(TaskInProgress,boolean)
8185   13   3    1 org.apache.hadoop.mapred.TaskTracker.killOverflowingTasks()
8186   15  11    1 org.apache.hadoop.mapred.TaskTracker.findTaskToKill()
8187    4   3    1 org.apache.hadoop.mapred.TaskTracker.enoughFreeSpace(long)
8188   14   4    0 org.apache.hadoop.mapred.TaskTracker.getFreeSpace()
8189   21   9    1 org.apache.hadoop.mapred.TaskTracker.tryToGetOutputSize(TaskAttemptID,JobConf)
8190    2   1    0 org.apache.hadoop.mapred.TaskTracker.getJvmManagerInstance()
8191    5   2    0 org.apache.hadoop.mapred.TaskTracker.addToTaskQueue(LaunchTaskAction)
8192    6   1    0 org.apache.hadoop.mapred.TaskTracker.TaskLauncher.TaskLauncher(int)
8193    5   1    0 org.apache.hadoop.mapred.TaskTracker.TaskLauncher.addToTaskQueue(LaunchTaskAction)
8194    2   1    0 org.apache.hadoop.mapred.TaskTracker.TaskLauncher.cleanTaskQueue()
8195    6   1    0 org.apache.hadoop.mapred.TaskTracker.TaskLauncher.addFreeSlot()
8196   24   8    0 org.apache.hadoop.mapred.TaskTracker.TaskLauncher.run()
8197   13   2    0 org.apache.hadoop.mapred.TaskTracker.registerTask(LaunchTaskAction,TaskLauncher)
8198   14   6    1 org.apache.hadoop.mapred.TaskTracker.startNewTask(TaskInProgress)
8199   28  17    1 org.apache.hadoop.mapred.TaskTracker.run()
8200    2   1    1 org.apache.hadoop.mapred.TaskTracker.TaskInProgress.TaskInProgress(Task,JobConf)
8201    8   2    0 org.apache.hadoop.mapred.TaskTracker.TaskInProgress.TaskInProgress(Task,JobConf,TaskLauncher)
8202   46  17    0 org.apache.hadoop.mapred.TaskTracker.TaskInProgress.localizeTask(Task)
8203    2   1    1 org.apache.hadoop.mapred.TaskTracker.TaskInProgress.getTask()
8204    2   1    0 org.apache.hadoop.mapred.TaskTracker.TaskInProgress.getTaskRunner()
8205    4   1    0 org.apache.hadoop.mapred.TaskTracker.TaskInProgress.setJobConf(JobConf)
8206    2   1    0 org.apache.hadoop.mapred.TaskTracker.TaskInProgress.getJobConf()
8207    5   2    1 org.apache.hadoop.mapred.TaskTracker.TaskInProgress.getStatus()
8208    9   2    1 org.apache.hadoop.mapred.TaskTracker.TaskInProgress.launchTask()
8209    7   6    1 org.apache.hadoop.mapred.TaskTracker.TaskInProgress.reportProgress(TaskStatus)
8210    2   1    1 org.apache.hadoop.mapred.TaskTracker.TaskInProgress.getLastProgressReport()
8211    2   1    1 org.apache.hadoop.mapred.TaskTracker.TaskInProgress.getRunState()
8212    2   1    1 org.apache.hadoop.mapred.TaskTracker.TaskInProgress.getTaskTimeout()
8213    2   1    1 org.apache.hadoop.mapred.TaskTracker.TaskInProgress.reportDiagnosticInfo(String)
8214    2   1    0 org.apache.hadoop.mapred.TaskTracker.TaskInProgress.reportNextRecordRange(SortedRanges.Range)
8215    9   1    1 org.apache.hadoop.mapred.TaskTracker.TaskInProgress.reportDone()
8216   60  18    1 org.apache.hadoop.mapred.TaskTracker.TaskInProgress.taskFinished()
8217    6   3    1 org.apache.hadoop.mapred.TaskTracker.TaskInProgress.runScript(List,File)
8218   39  15    1 org.apache.hadoop.mapred.TaskTracker.TaskInProgress.addDiagnostics(String,int,String)
8219    5   4    1 org.apache.hadoop.mapred.TaskTracker.TaskInProgress.jobHasFinished(boolean)
8220   15   7    1 org.apache.hadoop.mapred.TaskTracker.TaskInProgress.kill(boolean)
8221    5   3    0 org.apache.hadoop.mapred.TaskTracker.TaskInProgress.releaseSlot()
8222   10   3    1 org.apache.hadoop.mapred.TaskTracker.TaskInProgress.mapOutputLost(String)
8223   23  11    1 org.apache.hadoop.mapred.TaskTracker.TaskInProgress.cleanup(boolean)
8224    2   2    0 org.apache.hadoop.mapred.TaskTracker.TaskInProgress.equals(Object)
8225    2   1    0 org.apache.hadoop.mapred.TaskTracker.TaskInProgress.hashCode()
8226   19   9    1 org.apache.hadoop.mapred.TaskTracker.getTask(JVMId)
8227    8   3    1 org.apache.hadoop.mapred.TaskTracker.statusUpdate(TaskAttemptID,TaskStatus)
8228    6   2    1 org.apache.hadoop.mapred.TaskTracker.reportDiagnosticInfo(TaskAttemptID,String)
8229    6   2    0 org.apache.hadoop.mapred.TaskTracker.reportNextRecordRange(TaskAttemptID,SortedRanges.Range)
8230    2   1    1 org.apache.hadoop.mapred.TaskTracker.ping(TaskAttemptID)
8231    4   1    1 org.apache.hadoop.mapred.TaskTracker.commitPending(TaskAttemptID,TaskStatus)
8232    2   1    1 org.apache.hadoop.mapred.TaskTracker.canCommit(TaskAttemptID)
8233    7   2    1 org.apache.hadoop.mapred.TaskTracker.done(TaskAttemptID)
8234    5   1    1 org.apache.hadoop.mapred.TaskTracker.shuffleError(TaskAttemptID,String)
8235    5   1    1 org.apache.hadoop.mapred.TaskTracker.fsError(TaskAttemptID,String)
8236   14   5    0 org.apache.hadoop.mapred.TaskTracker.getMapCompletionEvents(JobID,int,int,TaskAttemptID)
8237   15   4    1 org.apache.hadoop.mapred.TaskTracker.reportTaskFinished(TaskAttemptID,boolean)
8238    6   2    1 org.apache.hadoop.mapred.TaskTracker.mapOutputLost(TaskAttemptID,String)
8239    6   1    0 org.apache.hadoop.mapred.TaskTracker.RunningJob.RunningJob(JobID,Path)
8240    2   1    0 org.apache.hadoop.mapred.TaskTracker.RunningJob.getJobFile()
8241    2   1    0 org.apache.hadoop.mapred.TaskTracker.RunningJob.getJobID()
8242    2   1    0 org.apache.hadoop.mapred.TaskTracker.RunningJob.setFetchStatus(FetchStatus)
8243    2   1    0 org.apache.hadoop.mapred.TaskTracker.RunningJob.getFetchStatus()
8244    2   1    1 org.apache.hadoop.mapred.TaskTracker.getName()
8245   11   3    0 org.apache.hadoop.mapred.TaskTracker.cloneAndResetRunningTaskStatuses(boolean)
8246    5   2    1 org.apache.hadoop.mapred.TaskTracker.getRunningTaskStatuses()
8247    6   3    1 org.apache.hadoop.mapred.TaskTracker.getNonRunningTasks()
8248    8   3    1 org.apache.hadoop.mapred.TaskTracker.getTasksFromRunningJobs()
8249    2   1    1 org.apache.hadoop.mapred.TaskTracker.getJobConf()
8250   10   6    1 org.apache.hadoop.mapred.TaskTracker.checkLocalDirs(String[])
8251    2   2    1 org.apache.hadoop.mapred.TaskTracker.isIdle()
8252   11   3    1 org.apache.hadoop.mapred.TaskTracker.main(String[])
8253   72  18    0 org.apache.hadoop.mapred.TaskTracker.MapOutputServlet.doGet(HttpServletRequest,HttpServletResponse)
8254    8   2    0 org.apache.hadoop.mapred.TaskTracker.getLocalFiles(JobConf,String)
8255    4   1    0 org.apache.hadoop.mapred.TaskTracker.CleanupQueue.CleanupQueue(JobConf)
8256    5   3    0 org.apache.hadoop.mapred.TaskTracker.CleanupQueue.addToQueue(Path)
8257   10   4    0 org.apache.hadoop.mapred.TaskTracker.CleanupQueue.run()
8258    2   1    0 org.apache.hadoop.mapred.TaskTracker.getMaxCurrentMapTasks()
8259    2   1    0 org.apache.hadoop.mapred.TaskTracker.getMaxCurrentReduceTasks()
8260    2   1    1 org.apache.hadoop.mapred.TaskTracker.isTaskMemoryManagerEnabled()
8261    2   1    0 org.apache.hadoop.mapred.TaskTracker.getTaskMemoryManager()
8262   11   5    0 org.apache.hadoop.mapred.TaskTracker.setTaskMemoryManagerEnabledFlag()
8263    7   3    1 org.apache.hadoop.mapred.TaskTracker.cleanUpOverMemoryTask(TaskAttemptID,String)
8264   19   6    1 org.apache.hadoop.mapred.TaskTrackerAction.createAction(ActionType)
8265    2   1    0 org.apache.hadoop.mapred.TaskTrackerAction.TaskTrackerAction(ActionType)
8266    2   1    1 org.apache.hadoop.mapred.TaskTrackerAction.getActionId()
8267    2   1    0 org.apache.hadoop.mapred.TaskTrackerAction.write(DataOutput)
8268    2   1    0 org.apache.hadoop.mapred.TaskTrackerAction.readFields(DataInput)
8269    2   1    0 org.apache.hadoop.mapred.TaskTrackerInstrumentation.TaskTrackerInstrumentation(TaskTracker)
8270    1   1    1 org.apache.hadoop.mapred.TaskTrackerInstrumentation.completeTask(TaskAttemptID)
8271    1   1    0 org.apache.hadoop.mapred.TaskTrackerInstrumentation.timedoutTask(TaskAttemptID)
8272    1   1    0 org.apache.hadoop.mapred.TaskTrackerInstrumentation.taskFailedPing(TaskAttemptID)
8273    1   1    1 org.apache.hadoop.mapred.TaskTrackerInstrumentation.reportTaskLaunch(TaskAttemptID,File,File)
8274    1   1    1 org.apache.hadoop.mapred.TaskTrackerInstrumentation.reportTaskEnd(TaskAttemptID)
8275    1   1    1 org.apache.hadoop.mapred.TaskTrackerManager.taskTrackers()
8276    1   1    1 org.apache.hadoop.mapred.TaskTrackerManager.getNumberOfUniqueHosts()
8277    1   1    1 org.apache.hadoop.mapred.TaskTrackerManager.getClusterStatus()
8278    1   1    1 org.apache.hadoop.mapred.TaskTrackerManager.addJobInProgressListener(JobInProgressListener)
8279    1   1    1 org.apache.hadoop.mapred.TaskTrackerManager.removeJobInProgressListener(JobInProgressListener)
8280    1   1    1 org.apache.hadoop.mapred.TaskTrackerManager.getQueueManager()
8281    1   1    1 org.apache.hadoop.mapred.TaskTrackerManager.getNextHeartbeatInterval()
8282    9   1    0 org.apache.hadoop.mapred.TaskTrackerMetricsInst.TaskTrackerMetricsInst(TaskTracker)
8283    2   1    0 org.apache.hadoop.mapred.TaskTrackerMetricsInst.completeTask()
8284    2   1    0 org.apache.hadoop.mapred.TaskTrackerMetricsInst.timedoutTask()
8285    2   1    0 org.apache.hadoop.mapred.TaskTrackerMetricsInst.taskFailedPing()
8286   13   1    1 org.apache.hadoop.mapred.TaskTrackerMetricsInst.doUpdates(MetricsContext)
8287    2   1    0 org.apache.hadoop.mapred.TaskTrackerStatus.WritableFactory$1.newInstance()
8288    4   1    0 org.apache.hadoop.mapred.TaskTrackerStatus.ResourceStatus.ResourceStatus()
8289    2   1    1 org.apache.hadoop.mapred.TaskTrackerStatus.ResourceStatus.setFreeVirtualMemory(long)
8290    2   1    1 org.apache.hadoop.mapred.TaskTrackerStatus.ResourceStatus.getFreeVirtualMemory()
8291    2   1    1 org.apache.hadoop.mapred.TaskTrackerStatus.ResourceStatus.setTotalMemory(long)
8292    2   1    1 org.apache.hadoop.mapred.TaskTrackerStatus.ResourceStatus.getTotalMemory()
8293    2   1    0 org.apache.hadoop.mapred.TaskTrackerStatus.ResourceStatus.setAvailableSpace(long)
8294    2   1    1 org.apache.hadoop.mapred.TaskTrackerStatus.ResourceStatus.getAvailableSpace()
8295    4   1    0 org.apache.hadoop.mapred.TaskTrackerStatus.ResourceStatus.write(DataOutput)
8296    4   1    0 org.apache.hadoop.mapred.TaskTrackerStatus.ResourceStatus.readFields(DataInput)
8297    3   1    1 org.apache.hadoop.mapred.TaskTrackerStatus.TaskTrackerStatus()
8298    9   1    1 org.apache.hadoop.mapred.TaskTrackerStatus.TaskTrackerStatus(String,String,int,List,int,int,int)
8299    2   1    1 org.apache.hadoop.mapred.TaskTrackerStatus.getTrackerName()
8300    2   1    1 org.apache.hadoop.mapred.TaskTrackerStatus.getHost()
8301    2   1    1 org.apache.hadoop.mapred.TaskTrackerStatus.getHttpPort()
8302    2   1    1 org.apache.hadoop.mapred.TaskTrackerStatus.getFailures()
8303    2   1    1 org.apache.hadoop.mapred.TaskTrackerStatus.getTaskReports()
8304    8   5    1 org.apache.hadoop.mapred.TaskTrackerStatus.countMapTasks()
8305    8   5    1 org.apache.hadoop.mapred.TaskTrackerStatus.countReduceTasks()
8306    2   1    1 org.apache.hadoop.mapred.TaskTrackerStatus.getLastSeen()
8307    2   1    1 org.apache.hadoop.mapred.TaskTrackerStatus.setLastSeen(long)
8308    2   1    1 org.apache.hadoop.mapred.TaskTrackerStatus.getMaxMapTasks()
8309    2   1    0 org.apache.hadoop.mapred.TaskTrackerStatus.getMaxReduceTasks()
8310    2   1    1 org.apache.hadoop.mapred.TaskTrackerStatus.getResourceStatus()
8311   11   2    0 org.apache.hadoop.mapred.TaskTrackerStatus.write(DataOutput)
8312   12   2    0 org.apache.hadoop.mapred.TaskTrackerStatus.readFields(DataInput)
8313    1   1    1 org.apache.hadoop.mapred.TaskUmbilicalProtocol.getTask(JVMId)
8314    1   1    1 org.apache.hadoop.mapred.TaskUmbilicalProtocol.statusUpdate(TaskAttemptID,TaskStatus)
8315    1   1    1 org.apache.hadoop.mapred.TaskUmbilicalProtocol.reportDiagnosticInfo(TaskAttemptID,String)
8316    1   1    1 org.apache.hadoop.mapred.TaskUmbilicalProtocol.reportNextRecordRange(TaskAttemptID,SortedRanges.Range)
8317    1   1    1 org.apache.hadoop.mapred.TaskUmbilicalProtocol.ping(TaskAttemptID)
8318    1   1    1 org.apache.hadoop.mapred.TaskUmbilicalProtocol.done(TaskAttemptID)
8319    1   1    1 org.apache.hadoop.mapred.TaskUmbilicalProtocol.commitPending(TaskAttemptID,TaskStatus)
8320    1   1    1 org.apache.hadoop.mapred.TaskUmbilicalProtocol.canCommit(TaskAttemptID)
8321    1   1    1 org.apache.hadoop.mapred.TaskUmbilicalProtocol.shuffleError(TaskAttemptID,String)
8322    1   1    1 org.apache.hadoop.mapred.TaskUmbilicalProtocol.fsError(TaskAttemptID,String)
8323    1   1    1 org.apache.hadoop.mapred.TaskUmbilicalProtocol.getMapCompletionEvents(JobID,int,int,TaskAttemptID)
8324    2   1    0 org.apache.hadoop.mapred.TextInputFormat.configure(JobConf)
8325    2   1    0 org.apache.hadoop.mapred.TextInputFormat.isSplitable(FileSystem,Path)
8326    3   1    0 org.apache.hadoop.mapred.TextInputFormat.getRecordReader(InputSplit,JobConf,Reporter)
8327    5   3    0 org.apache.hadoop.mapred.TextOutputFormat.LineRecordWriter.LineRecordWriter(DataOutputStream,String)
8328    2   1    0 org.apache.hadoop.mapred.TextOutputFormat.LineRecordWriter.LineRecordWriter(DataOutputStream)
8329    6   2    1 org.apache.hadoop.mapred.TextOutputFormat.LineRecordWriter.writeObject(Object)
8330   12  10    0 org.apache.hadoop.mapred.TextOutputFormat.LineRecordWriter.write(K,V)
8331    2   1    0 org.apache.hadoop.mapred.TextOutputFormat.LineRecordWriter.close(Reporter)
8332   15   3    0 org.apache.hadoop.mapred.TextOutputFormat.getRecordWriter(FileSystem,JobConf,String,Progressable)
Average Function NCSS:       6.22
Average Function CCN:        2.64
Average Function JVDC:       0.47
Program NCSS:           60,986.00
