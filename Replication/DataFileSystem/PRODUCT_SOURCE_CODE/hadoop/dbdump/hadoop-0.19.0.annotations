./src/core/org/apache/hadoop/conf/Configurable.java:21:/** Something that may be configured with a {@link Configuration}. */
./src/core/org/apache/hadoop/conf/Configuration.java:77: * <code>String</code> or by a {@link Path}. If named by a <code>String</code>, 
./src/core/org/apache/hadoop/conf/Configuration.java:84: * <li><tt><a href="{@docRoot}/../hadoop-default.html">hadoop-default.xml</a>
./src/core/org/apache/hadoop/conf/Configuration.java:114: * <li>Properties in {@link System#getProperties()}.</li>
./src/core/org/apache/hadoop/conf/Configuration.java:170:   * If the parameter {@code loadDefaults} is false, the new instance
./src/core/org/apache/hadoop/conf/Configuration.java:172:   * @param loadDefaults specifies whether to load from the default files
./src/core/org/apache/hadoop/conf/Configuration.java:187:   * @param other the configuration from which to clone settings.
./src/core/org/apache/hadoop/conf/Configuration.java:189:  @SuppressWarnings("unchecked")
./src/core/org/apache/hadoop/conf/Configuration.java:209:   * @param name resource to be added, the classpath is examined for a file 
./src/core/org/apache/hadoop/conf/Configuration.java:222:   * @param url url of the resource to be added, the local filesystem is 
./src/core/org/apache/hadoop/conf/Configuration.java:236:   * @param file file-path of resource to be added, the local filesystem is
./src/core/org/apache/hadoop/conf/Configuration.java:250:   * @param in InputStream to deserialize the object from. 
./src/core/org/apache/hadoop/conf/Configuration.java:317:   * @param name the property name.
./src/core/org/apache/hadoop/conf/Configuration.java:318:   * @return the value of the <code>name</code> property, 
./src/core/org/apache/hadoop/conf/Configuration.java:329:   * @param name the property name.
./src/core/org/apache/hadoop/conf/Configuration.java:330:   * @return the value of the <code>name</code> property, 
./src/core/org/apache/hadoop/conf/Configuration.java:340:   * @param name property name.
./src/core/org/apache/hadoop/conf/Configuration.java:341:   * @param value property value.
./src/core/org/apache/hadoop/conf/Configuration.java:359:   * @param name property name.
./src/core/org/apache/hadoop/conf/Configuration.java:360:   * @param defaultValue default value.
./src/core/org/apache/hadoop/conf/Configuration.java:361:   * @return property value, or <code>defaultValue</code> if the property 
./src/core/org/apache/hadoop/conf/Configuration.java:374:   * @param name property name.
./src/core/org/apache/hadoop/conf/Configuration.java:375:   * @param defaultValue default value.
./src/core/org/apache/hadoop/conf/Configuration.java:376:   * @return property value as an <code>int</code>, 
./src/core/org/apache/hadoop/conf/Configuration.java:397:   * @param name property name.
./src/core/org/apache/hadoop/conf/Configuration.java:398:   * @param value <code>int</code> value of the property.
./src/core/org/apache/hadoop/conf/Configuration.java:410:   * @param name property name.
./src/core/org/apache/hadoop/conf/Configuration.java:411:   * @param defaultValue default value.
./src/core/org/apache/hadoop/conf/Configuration.java:412:   * @return property value as a <code>long</code>, 
./src/core/org/apache/hadoop/conf/Configuration.java:451:   * @param name property name.
./src/core/org/apache/hadoop/conf/Configuration.java:452:   * @param value <code>long</code> value of the property.
./src/core/org/apache/hadoop/conf/Configuration.java:463:   * @param name property name.
./src/core/org/apache/hadoop/conf/Configuration.java:464:   * @param defaultValue default value.
./src/core/org/apache/hadoop/conf/Configuration.java:465:   * @return property value as a <code>float</code>, 
./src/core/org/apache/hadoop/conf/Configuration.java:484:   * @param name property name.
./src/core/org/apache/hadoop/conf/Configuration.java:485:   * @param defaultValue default value.
./src/core/org/apache/hadoop/conf/Configuration.java:486:   * @return property value as a <code>boolean</code>, 
./src/core/org/apache/hadoop/conf/Configuration.java:501:   * @param name property name.
./src/core/org/apache/hadoop/conf/Configuration.java:502:   * @param value <code>boolean</code> value of the property.
./src/core/org/apache/hadoop/conf/Configuration.java:552:     * @param value the string value
./src/core/org/apache/hadoop/conf/Configuration.java:553:     * @param defaultValue the value for if the string is empty
./src/core/org/apache/hadoop/conf/Configuration.java:554:     * @return the desired integer
./src/core/org/apache/hadoop/conf/Configuration.java:566:     * @param value the value to check
./src/core/org/apache/hadoop/conf/Configuration.java:567:     * @return is the value in the ranges?
./src/core/org/apache/hadoop/conf/Configuration.java:578:    @Override
./src/core/org/apache/hadoop/conf/Configuration.java:598:   * @param name the attribute name
./src/core/org/apache/hadoop/conf/Configuration.java:599:   * @param defaultValue the default value if it is not set
./src/core/org/apache/hadoop/conf/Configuration.java:600:   * @return a new set of ranges from the configured value
./src/core/org/apache/hadoop/conf/Configuration.java:611:   * This is an optimized version of {@link #getStrings(String)}
./src/core/org/apache/hadoop/conf/Configuration.java:613:   * @param name property name.
./src/core/org/apache/hadoop/conf/Configuration.java:614:   * @return property value as a collection of <code>String</code>s. 
./src/core/org/apache/hadoop/conf/Configuration.java:626:   * @param name property name.
./src/core/org/apache/hadoop/conf/Configuration.java:627:   * @return property value as an array of <code>String</code>s, 
./src/core/org/apache/hadoop/conf/Configuration.java:640:   * @param name property name.
./src/core/org/apache/hadoop/conf/Configuration.java:641:   * @param defaultValue The default value
./src/core/org/apache/hadoop/conf/Configuration.java:642:   * @return property value as an array of <code>String</code>s, 
./src/core/org/apache/hadoop/conf/Configuration.java:658:   * @param name property name.
./src/core/org/apache/hadoop/conf/Configuration.java:659:   * @param values The values
./src/core/org/apache/hadoop/conf/Configuration.java:668:   * @param name the class name.
./src/core/org/apache/hadoop/conf/Configuration.java:669:   * @return the class object.
./src/core/org/apache/hadoop/conf/Configuration.java:670:   * @throws ClassNotFoundException if the class is not found.
./src/core/org/apache/hadoop/conf/Configuration.java:683:   * @param name the property name.
./src/core/org/apache/hadoop/conf/Configuration.java:684:   * @param defaultValue default value.
./src/core/org/apache/hadoop/conf/Configuration.java:685:   * @return property value as a <code>Class[]</code>, 
./src/core/org/apache/hadoop/conf/Configuration.java:708:   * @param name the class name.
./src/core/org/apache/hadoop/conf/Configuration.java:709:   * @param defaultValue default value.
./src/core/org/apache/hadoop/conf/Configuration.java:710:   * @return property value as a <code>Class</code>, 
./src/core/org/apache/hadoop/conf/Configuration.java:734:   * @param name the class name.
./src/core/org/apache/hadoop/conf/Configuration.java:735:   * @param defaultValue default value.
./src/core/org/apache/hadoop/conf/Configuration.java:736:   * @param xface the interface implemented by the named class.
./src/core/org/apache/hadoop/conf/Configuration.java:737:   * @return property value as a <code>Class</code>, 
./src/core/org/apache/hadoop/conf/Configuration.java:763:   * @param name property name.
./src/core/org/apache/hadoop/conf/Configuration.java:764:   * @param theClass property value.
./src/core/org/apache/hadoop/conf/Configuration.java:765:   * @param xface the interface implemented by the named class.
./src/core/org/apache/hadoop/conf/Configuration.java:779:   * @param dirsProp directory in which to locate the file.
./src/core/org/apache/hadoop/conf/Configuration.java:780:   * @param path file-path.
./src/core/org/apache/hadoop/conf/Configuration.java:781:   * @return local file under the directory with the given path.
./src/core/org/apache/hadoop/conf/Configuration.java:811:   * @param dirsProp directory in which to locate the file.
./src/core/org/apache/hadoop/conf/Configuration.java:812:   * @param path file-path.
./src/core/org/apache/hadoop/conf/Configuration.java:813:   * @return local file under the directory with the given path.
./src/core/org/apache/hadoop/conf/Configuration.java:831:   * Get the {@link URL} for the named resource.
./src/core/org/apache/hadoop/conf/Configuration.java:833:   * @param name resource name.
./src/core/org/apache/hadoop/conf/Configuration.java:834:   * @return the url for the named resource.
./src/core/org/apache/hadoop/conf/Configuration.java:844:   * @param name configuration resource name.
./src/core/org/apache/hadoop/conf/Configuration.java:845:   * @return an input stream attached to the resource.
./src/core/org/apache/hadoop/conf/Configuration.java:865:   * Get a {@link Reader} attached to the configuration resource with the
./src/core/org/apache/hadoop/conf/Configuration.java:868:   * @param name configuration resource name.
./src/core/org/apache/hadoop/conf/Configuration.java:869:   * @return a reader attached to the resource.
./src/core/org/apache/hadoop/conf/Configuration.java:901:   * @return number of keys in the configuration.
./src/core/org/apache/hadoop/conf/Configuration.java:916:   * Get an {@link Iterator} to go through the list of <code>String</code> 
./src/core/org/apache/hadoop/conf/Configuration.java:919:   * @return an iterator over the entries.
./src/core/org/apache/hadoop/conf/Configuration.java:1058:   * {@link OutputStream}.
./src/core/org/apache/hadoop/conf/Configuration.java:1060:   * @param out the output stream to write to.
./src/core/org/apache/hadoop/conf/Configuration.java:1104:   * Get the {@link ClassLoader} for this job.
./src/core/org/apache/hadoop/conf/Configuration.java:1106:   * @return the correct class loader.
./src/core/org/apache/hadoop/conf/Configuration.java:1115:   * @param classLoader the new class loader.
./src/core/org/apache/hadoop/conf/Configuration.java:1121:  @Override
./src/core/org/apache/hadoop/conf/Configuration.java:1144:   * @param quietmode <code>true</code> to set quiet-mode on, <code>false</code>
./src/core/org/apache/hadoop/conf/Configuration.java:1156:  @Override
./src/core/org/apache/hadoop/conf/Configuration.java:1166:  //@Override
./src/core/org/apache/hadoop/conf/Configured.java:21:/** Base class for things that may be configured with a {@link Configuration}. */
./src/core/org/apache/hadoop/filecache/DistributedCache.java:38: * via the {@link org.apache.hadoop.mapred.JobConf}.
./src/core/org/apache/hadoop/filecache/DistributedCache.java:41: * {@link FileSystem} at the path specified by the url.</p>
./src/core/org/apache/hadoop/filecache/DistributedCache.java:85: *     3. Use the cached files in the {@link org.apache.hadoop.mapred.Mapper}
./src/core/org/apache/hadoop/filecache/DistributedCache.java:86: *     or {@link org.apache.hadoop.mapred.Reducer}:
./src/core/org/apache/hadoop/filecache/DistributedCache.java:112: * @see org.apache.hadoop.mapred.JobConf
./src/core/org/apache/hadoop/filecache/DistributedCache.java:113: * @see org.apache.hadoop.mapred.JobClient
./src/core/org/apache/hadoop/filecache/DistributedCache.java:127:   * previously cached (and valid) or copy it from the {@link FileSystem} now.
./src/core/org/apache/hadoop/filecache/DistributedCache.java:129:   * @param cache the cache to be localized, this should be specified as 
./src/core/org/apache/hadoop/filecache/DistributedCache.java:133:   * @param conf The Confguration file which contains the filesystem
./src/core/org/apache/hadoop/filecache/DistributedCache.java:134:   * @param baseDir The base cache Dir where you wnat to localize the files/archives
./src/core/org/apache/hadoop/filecache/DistributedCache.java:135:   * @param fileStatus The file status on the dfs.
./src/core/org/apache/hadoop/filecache/DistributedCache.java:136:   * @param isArchive if the cache is an archive or a file. In case it is an
./src/core/org/apache/hadoop/filecache/DistributedCache.java:142:   * @param confFileStamp this is the hdfs file modification timestamp to verify that the 
./src/core/org/apache/hadoop/filecache/DistributedCache.java:144:   * @param currentWorkDir this is the directory where you would want to create symlinks 
./src/core/org/apache/hadoop/filecache/DistributedCache.java:146:   * @return the path to directory where the archives are unjarred in case of archives,
./src/core/org/apache/hadoop/filecache/DistributedCache.java:148:   * @throws IOException
./src/core/org/apache/hadoop/filecache/DistributedCache.java:160:   * previously cached (and valid) or copy it from the {@link FileSystem} now.
./src/core/org/apache/hadoop/filecache/DistributedCache.java:162:   * @param cache the cache to be localized, this should be specified as 
./src/core/org/apache/hadoop/filecache/DistributedCache.java:166:   * @param conf The Confguration file which contains the filesystem
./src/core/org/apache/hadoop/filecache/DistributedCache.java:167:   * @param baseDir The base cache Dir where you wnat to localize the files/archives
./src/core/org/apache/hadoop/filecache/DistributedCache.java:168:   * @param fileStatus The file status on the dfs.
./src/core/org/apache/hadoop/filecache/DistributedCache.java:169:   * @param isArchive if the cache is an archive or a file. In case it is an
./src/core/org/apache/hadoop/filecache/DistributedCache.java:175:   * @param confFileStamp this is the hdfs file modification timestamp to verify that the 
./src/core/org/apache/hadoop/filecache/DistributedCache.java:177:   * @param currentWorkDir this is the directory where you would want to create symlinks 
./src/core/org/apache/hadoop/filecache/DistributedCache.java:179:   * @param honorSymLinkConf if this is false, then the symlinks are not
./src/core/org/apache/hadoop/filecache/DistributedCache.java:182:   * @return the path to directory where the archives are unjarred in case of archives,
./src/core/org/apache/hadoop/filecache/DistributedCache.java:184:   * @throws IOException
./src/core/org/apache/hadoop/filecache/DistributedCache.java:223:   * previously cached (and valid) or copy it from the {@link FileSystem} now.
./src/core/org/apache/hadoop/filecache/DistributedCache.java:225:   * @param cache the cache to be localized, this should be specified as 
./src/core/org/apache/hadoop/filecache/DistributedCache.java:229:   * @param conf The Confguration file which contains the filesystem
./src/core/org/apache/hadoop/filecache/DistributedCache.java:230:   * @param baseDir The base cache Dir where you wnat to localize the files/archives
./src/core/org/apache/hadoop/filecache/DistributedCache.java:231:   * @param isArchive if the cache is an archive or a file. In case it is an 
./src/core/org/apache/hadoop/filecache/DistributedCache.java:237:   * @param confFileStamp this is the hdfs file modification timestamp to verify that the 
./src/core/org/apache/hadoop/filecache/DistributedCache.java:239:   * @param currentWorkDir this is the directory where you would want to create symlinks 
./src/core/org/apache/hadoop/filecache/DistributedCache.java:241:   * @return the path to directory where the archives are unjarred in case of archives,
./src/core/org/apache/hadoop/filecache/DistributedCache.java:243:   * @throws IOException
./src/core/org/apache/hadoop/filecache/DistributedCache.java:258:   * @param cache The cache URI to be released
./src/core/org/apache/hadoop/filecache/DistributedCache.java:259:   * @param conf configuration which contains the filesystem the cache 
./src/core/org/apache/hadoop/filecache/DistributedCache.java:261:   * @throws IOException
./src/core/org/apache/hadoop/filecache/DistributedCache.java:465:   * @param conf configuration
./src/core/org/apache/hadoop/filecache/DistributedCache.java:466:   * @param cache cache file 
./src/core/org/apache/hadoop/filecache/DistributedCache.java:467:   * @return mtime of a given cache file on hdfs
./src/core/org/apache/hadoop/filecache/DistributedCache.java:468:   * @throws IOException
./src/core/org/apache/hadoop/filecache/DistributedCache.java:480:   * @param conf the configuration
./src/core/org/apache/hadoop/filecache/DistributedCache.java:481:   * @param jobCacheDir the target directory for creating symlinks
./src/core/org/apache/hadoop/filecache/DistributedCache.java:482:   * @param workDir the directory in which the symlinks are created
./src/core/org/apache/hadoop/filecache/DistributedCache.java:483:   * @throws IOException
./src/core/org/apache/hadoop/filecache/DistributedCache.java:523:   * @param archives The list of archives that need to be localized
./src/core/org/apache/hadoop/filecache/DistributedCache.java:524:   * @param conf Configuration which will be changed
./src/core/org/apache/hadoop/filecache/DistributedCache.java:533:   * @param files The list of files that need to be localized
./src/core/org/apache/hadoop/filecache/DistributedCache.java:534:   * @param conf Configuration which will be changed
./src/core/org/apache/hadoop/filecache/DistributedCache.java:543:   * @param conf The configuration which contains the archives
./src/core/org/apache/hadoop/filecache/DistributedCache.java:544:   * @return A URI array of the caches set in the Configuration
./src/core/org/apache/hadoop/filecache/DistributedCache.java:545:   * @throws IOException
./src/core/org/apache/hadoop/filecache/DistributedCache.java:553:   * @param conf The configuration which contains the files
./src/core/org/apache/hadoop/filecache/DistributedCache.java:554:   * @return A URI array of the files set in the Configuration
./src/core/org/apache/hadoop/filecache/DistributedCache.java:555:   * @throws IOException
./src/core/org/apache/hadoop/filecache/DistributedCache.java:564:   * @param conf Configuration that contains the localized archives
./src/core/org/apache/hadoop/filecache/DistributedCache.java:565:   * @return A path array of localized caches
./src/core/org/apache/hadoop/filecache/DistributedCache.java:566:   * @throws IOException
./src/core/org/apache/hadoop/filecache/DistributedCache.java:576:   * @param conf Configuration that contains the localized files
./src/core/org/apache/hadoop/filecache/DistributedCache.java:577:   * @return A path array of localized files
./src/core/org/apache/hadoop/filecache/DistributedCache.java:578:   * @throws IOException
./src/core/org/apache/hadoop/filecache/DistributedCache.java:587:   * @param conf The configuration which stored the timestamps
./src/core/org/apache/hadoop/filecache/DistributedCache.java:588:   * @return a string array of timestamps 
./src/core/org/apache/hadoop/filecache/DistributedCache.java:589:   * @throws IOException
./src/core/org/apache/hadoop/filecache/DistributedCache.java:598:   * @param conf The configuration which stored the timestamps
./src/core/org/apache/hadoop/filecache/DistributedCache.java:599:   * @return a string array of timestamps 
./src/core/org/apache/hadoop/filecache/DistributedCache.java:600:   * @throws IOException
./src/core/org/apache/hadoop/filecache/DistributedCache.java:608:   * @param conf Configuration which stores the timestamp's
./src/core/org/apache/hadoop/filecache/DistributedCache.java:609:   * @param timestamps comma separated list of timestamps of archives.
./src/core/org/apache/hadoop/filecache/DistributedCache.java:618:   * @param conf Configuration which stores the timestamp's
./src/core/org/apache/hadoop/filecache/DistributedCache.java:619:   * @param timestamps comma separated list of timestamps of files.
./src/core/org/apache/hadoop/filecache/DistributedCache.java:628:   * @param conf The conf to modify to contain the localized caches
./src/core/org/apache/hadoop/filecache/DistributedCache.java:629:   * @param str a comma separated list of local archives
./src/core/org/apache/hadoop/filecache/DistributedCache.java:637:   * @param conf The conf to modify to contain the localized caches
./src/core/org/apache/hadoop/filecache/DistributedCache.java:638:   * @param str a comma separated list of local files
./src/core/org/apache/hadoop/filecache/DistributedCache.java:646:   * @param uri The uri of the cache to be localized
./src/core/org/apache/hadoop/filecache/DistributedCache.java:647:   * @param conf Configuration to add the cache to
./src/core/org/apache/hadoop/filecache/DistributedCache.java:657:   * @param uri The uri of the cache to be localized
./src/core/org/apache/hadoop/filecache/DistributedCache.java:658:   * @param conf Configuration to add the cache to
./src/core/org/apache/hadoop/filecache/DistributedCache.java:670:   * @param file Path of the file to be added
./src/core/org/apache/hadoop/filecache/DistributedCache.java:671:   * @param conf Configuration that contains the classpath setting
./src/core/org/apache/hadoop/filecache/DistributedCache.java:687:   * @param conf Configuration that contains the classpath setting
./src/core/org/apache/hadoop/filecache/DistributedCache.java:706:   * @param archive Path of the archive to be added
./src/core/org/apache/hadoop/filecache/DistributedCache.java:707:   * @param conf Configuration that contains the classpath setting
./src/core/org/apache/hadoop/filecache/DistributedCache.java:724:   * @param conf Configuration that contains the classpath setting
./src/core/org/apache/hadoop/filecache/DistributedCache.java:742:   * @param conf the jobconf 
./src/core/org/apache/hadoop/filecache/DistributedCache.java:751:   * @param conf the jobconf
./src/core/org/apache/hadoop/filecache/DistributedCache.java:752:   * @return true if symlinks are to be created- else return false
./src/core/org/apache/hadoop/filecache/DistributedCache.java:767:   * @param uriFiles The uri array of urifiles
./src/core/org/apache/hadoop/filecache/DistributedCache.java:768:   * @param uriArchives the uri array of uri archives
./src/core/org/apache/hadoop/fs/BufferedFSInputStream.java:39:   * @param   in     the underlying input stream.
./src/core/org/apache/hadoop/fs/BufferedFSInputStream.java:40:   * @param   size   the buffer size.
./src/core/org/apache/hadoop/fs/BufferedFSInputStream.java:41:   * @exception IllegalArgumentException if size <= 0.
./src/core/org/apache/hadoop/fs/ChecksumFileSystem.java:182:    @Override
./src/core/org/apache/hadoop/fs/ChecksumFileSystem.java:190:    @Override
./src/core/org/apache/hadoop/fs/ChecksumFileSystem.java:233:     * @param      n   the number of bytes to be skipped.
./src/core/org/apache/hadoop/fs/ChecksumFileSystem.java:234:     * @return     the actual number of bytes skipped.
./src/core/org/apache/hadoop/fs/ChecksumFileSystem.java:235:     * @exception  IOException  if an I/O error occurs.
./src/core/org/apache/hadoop/fs/ChecksumFileSystem.java:254:     * @param      pos   the postion to seek to.
./src/core/org/apache/hadoop/fs/ChecksumFileSystem.java:255:     * @exception  IOException  if an I/O error occurs or seeks after EOF
./src/core/org/apache/hadoop/fs/ChecksumFileSystem.java:270:   * @param f the file name to open
./src/core/org/apache/hadoop/fs/ChecksumFileSystem.java:271:   * @param bufferSize the size of the buffer to be used.
./src/core/org/apache/hadoop/fs/ChecksumFileSystem.java:273:  @Override
./src/core/org/apache/hadoop/fs/ChecksumFileSystem.java:279:  /** {@inheritDoc} */
./src/core/org/apache/hadoop/fs/ChecksumFileSystem.java:287:   * @param size the length of the data file in bytes
./src/core/org/apache/hadoop/fs/ChecksumFileSystem.java:288:   * @param bytesPerSum the number of bytes in a checksum block
./src/core/org/apache/hadoop/fs/ChecksumFileSystem.java:289:   * @return the number of bytes in the checksum file
./src/core/org/apache/hadoop/fs/ChecksumFileSystem.java:343:    @Override
./src/core/org/apache/hadoop/fs/ChecksumFileSystem.java:351:  /** {@inheritDoc} */
./src/core/org/apache/hadoop/fs/ChecksumFileSystem.java:352:  @Override
./src/core/org/apache/hadoop/fs/ChecksumFileSystem.java:372:   * @param src file name
./src/core/org/apache/hadoop/fs/ChecksumFileSystem.java:373:   * @param replication new replication
./src/core/org/apache/hadoop/fs/ChecksumFileSystem.java:374:   * @throws IOException
./src/core/org/apache/hadoop/fs/ChecksumFileSystem.java:375:   * @return true if successful;
./src/core/org/apache/hadoop/fs/ChecksumFileSystem.java:450:   * @param f
./src/core/org/apache/hadoop/fs/ChecksumFileSystem.java:452:   * @return the statuses of the files/directories in the given patch
./src/core/org/apache/hadoop/fs/ChecksumFileSystem.java:453:   * @throws IOException
./src/core/org/apache/hadoop/fs/ChecksumFileSystem.java:455:  @Override
./src/core/org/apache/hadoop/fs/ChecksumFileSystem.java:460:  @Override
./src/core/org/apache/hadoop/fs/ChecksumFileSystem.java:465:  @Override
./src/core/org/apache/hadoop/fs/ChecksumFileSystem.java:476:  @Override
./src/core/org/apache/hadoop/fs/ChecksumFileSystem.java:517:  @Override
./src/core/org/apache/hadoop/fs/ChecksumFileSystem.java:523:  @Override
./src/core/org/apache/hadoop/fs/ChecksumFileSystem.java:531:   * @param f the file name containing the error
./src/core/org/apache/hadoop/fs/ChecksumFileSystem.java:532:   * @param in the stream open on the file
./src/core/org/apache/hadoop/fs/ChecksumFileSystem.java:533:   * @param inPos the position of the beginning of the bad data in the file
./src/core/org/apache/hadoop/fs/ChecksumFileSystem.java:534:   * @param sums the stream open on the checksum file
./src/core/org/apache/hadoop/fs/ChecksumFileSystem.java:535:   * @param sumsPos the position of the beginning of the bad data in the checksum file
./src/core/org/apache/hadoop/fs/ChecksumFileSystem.java:536:   * @return if retry is neccessary
./src/core/org/apache/hadoop/fs/ContentSummary.java:56:  /** @return the length */
./src/core/org/apache/hadoop/fs/ContentSummary.java:59:  /** @return the directory count */
./src/core/org/apache/hadoop/fs/ContentSummary.java:62:  /** @return the file count */
./src/core/org/apache/hadoop/fs/ContentSummary.java:74:  /** {@inheritDoc} */
./src/core/org/apache/hadoop/fs/ContentSummary.java:84:  /** {@inheritDoc} */
./src/core/org/apache/hadoop/fs/ContentSummary.java:121:   * @param qOption a flag indicating if quota needs to be printed or not
./src/core/org/apache/hadoop/fs/ContentSummary.java:122:   * @return the header of the output
./src/core/org/apache/hadoop/fs/ContentSummary.java:128:  /** {@inheritDoc} */
./src/core/org/apache/hadoop/fs/ContentSummary.java:137:   * @param qOption a flag indicating if quota needs to be printed or not
./src/core/org/apache/hadoop/fs/ContentSummary.java:138:   * @return the string representation of the object
./src/core/org/apache/hadoop/fs/DU.java:40:   * @param path the path to check disk usage in
./src/core/org/apache/hadoop/fs/DU.java:41:   * @param interval refresh the disk usage at this interval
./src/core/org/apache/hadoop/fs/DU.java:42:   * @throws IOException if we fail to refresh the disk usage
./src/core/org/apache/hadoop/fs/DU.java:58:   * @param path the path to check disk usage in
./src/core/org/apache/hadoop/fs/DU.java:59:   * @param conf configuration object
./src/core/org/apache/hadoop/fs/DU.java:60:   * @throws IOException if we fail to refresh the disk usage
./src/core/org/apache/hadoop/fs/DU.java:101:   * @param value decrease by this value
./src/core/org/apache/hadoop/fs/DU.java:109:   * @param value increase by this value
./src/core/org/apache/hadoop/fs/DU.java:116:   * @return disk space used 
./src/core/org/apache/hadoop/fs/DU.java:117:   * @throws IOException if the shell command fails
./src/core/org/apache/hadoop/fs/DU.java:138:   * @return the path of which we're keeping track of disk usage
./src/core/org/apache/hadoop/fs/FileChecksum.java:49:  /** {@inheritDoc} */
./src/core/org/apache/hadoop/fs/FileStatus.java:71:   * @return the length of this file, in blocks
./src/core/org/apache/hadoop/fs/FileStatus.java:79:   * @return true if this is a directory
./src/core/org/apache/hadoop/fs/FileStatus.java:87:   * @return the number of bytes
./src/core/org/apache/hadoop/fs/FileStatus.java:95:   * @return the replication factor of a file.
./src/core/org/apache/hadoop/fs/FileStatus.java:103:   * @return the modification time of file in milliseconds since January 1, 1970 UTC.
./src/core/org/apache/hadoop/fs/FileStatus.java:111:   * @return the access time of file in milliseconds since January 1, 1970 UTC.
./src/core/org/apache/hadoop/fs/FileStatus.java:119:   * @return permssion. If a filesystem does not have a notion of permissions
./src/core/org/apache/hadoop/fs/FileStatus.java:129:   * @return owner of the file. The string could be empty if there is no
./src/core/org/apache/hadoop/fs/FileStatus.java:139:   * @return group for the file. The string could be empty if there is no
./src/core/org/apache/hadoop/fs/FileStatus.java:157:   * @param permission if permission is null, default value is set
./src/core/org/apache/hadoop/fs/FileStatus.java:166:   * @param owner if it is null, default value is set
./src/core/org/apache/hadoop/fs/FileStatus.java:174:   * @param group if it is null, default value is set
./src/core/org/apache/hadoop/fs/FileStatus.java:213:   * @param   o the object to be compared.
./src/core/org/apache/hadoop/fs/FileStatus.java:214:   * @return  a negative integer, zero, or a positive integer as this object
./src/core/org/apache/hadoop/fs/FileStatus.java:217:   * @throws ClassCastException if the specified object's is not of 
./src/core/org/apache/hadoop/fs/FileStatus.java:226:   * @param   o the object to be compared.
./src/core/org/apache/hadoop/fs/FileStatus.java:227:   * @return  true if two file status has the same path name; false if not.
./src/core/org/apache/hadoop/fs/FileStatus.java:247:   * @return  a hash code value for the path name.
./src/core/org/apache/hadoop/fs/FileSystem.java:53: * The local implementation is {@link LocalFileSystem} and distributed
./src/core/org/apache/hadoop/fs/FileSystem.java:87:   * @deprecated Consider using {@link GenericOptionsParser} instead.
./src/core/org/apache/hadoop/fs/FileSystem.java:89:  @Deprecated
./src/core/org/apache/hadoop/fs/FileSystem.java:124:   * @param conf the configuration to access
./src/core/org/apache/hadoop/fs/FileSystem.java:125:   * @return the uri of the default filesystem
./src/core/org/apache/hadoop/fs/FileSystem.java:132:   * @param conf the configuration to alter
./src/core/org/apache/hadoop/fs/FileSystem.java:133:   * @param uri the new default filesystem uri
./src/core/org/apache/hadoop/fs/FileSystem.java:140:   * @param conf the configuration to alter
./src/core/org/apache/hadoop/fs/FileSystem.java:141:   * @param uri the new default filesystem uri
./src/core/org/apache/hadoop/fs/FileSystem.java:148:   * @param name a uri whose authority section names the host, port, etc.
./src/core/org/apache/hadoop/fs/FileSystem.java:150:   * @param conf the configuration
./src/core/org/apache/hadoop/fs/FileSystem.java:158:  /** @deprecated call #getUri() instead.*/
./src/core/org/apache/hadoop/fs/FileSystem.java:161:  /** @deprecated call #get(URI,Configuration) instead. */
./src/core/org/apache/hadoop/fs/FileSystem.java:186:   * @param conf the configuration to configure the file system with
./src/core/org/apache/hadoop/fs/FileSystem.java:187:   * @return a LocalFileSystem
./src/core/org/apache/hadoop/fs/FileSystem.java:233:   * @throws IOException
./src/core/org/apache/hadoop/fs/FileSystem.java:253:   * @param fs file system handle
./src/core/org/apache/hadoop/fs/FileSystem.java:254:   * @param file the name of the file to be created
./src/core/org/apache/hadoop/fs/FileSystem.java:255:   * @param permission the permission of the file
./src/core/org/apache/hadoop/fs/FileSystem.java:256:   * @return an output stream
./src/core/org/apache/hadoop/fs/FileSystem.java:257:   * @throws IOException
./src/core/org/apache/hadoop/fs/FileSystem.java:272:   * @see #create(FileSystem, Path, FsPermission)
./src/core/org/apache/hadoop/fs/FileSystem.java:274:   * @param fs file system handle
./src/core/org/apache/hadoop/fs/FileSystem.java:275:   * @param dir the name of the directory to be created
./src/core/org/apache/hadoop/fs/FileSystem.java:276:   * @param permission the permission of the directory
./src/core/org/apache/hadoop/fs/FileSystem.java:277:   * @return true if the directory creation succeeds; false otherwise
./src/core/org/apache/hadoop/fs/FileSystem.java:278:   * @throws IOException
./src/core/org/apache/hadoop/fs/FileSystem.java:348:   * @param f the file name to open
./src/core/org/apache/hadoop/fs/FileSystem.java:349:   * @param bufferSize the size of the buffer to be used.
./src/core/org/apache/hadoop/fs/FileSystem.java:356:   * @param f the file to open
./src/core/org/apache/hadoop/fs/FileSystem.java:421:   * @param f the file name to open
./src/core/org/apache/hadoop/fs/FileSystem.java:422:   * @param overwrite if a file with this name already exists, then if true,
./src/core/org/apache/hadoop/fs/FileSystem.java:424:   * @param bufferSize the size of the buffer to be used.
./src/core/org/apache/hadoop/fs/FileSystem.java:438:   * @param f the file name to open
./src/core/org/apache/hadoop/fs/FileSystem.java:439:   * @param overwrite if a file with this name already exists, then if true,
./src/core/org/apache/hadoop/fs/FileSystem.java:441:   * @param bufferSize the size of the buffer to be used.
./src/core/org/apache/hadoop/fs/FileSystem.java:456:   * @param f the file name to open
./src/core/org/apache/hadoop/fs/FileSystem.java:457:   * @param overwrite if a file with this name already exists, then if true,
./src/core/org/apache/hadoop/fs/FileSystem.java:459:   * @param bufferSize the size of the buffer to be used.
./src/core/org/apache/hadoop/fs/FileSystem.java:460:   * @param replication required block replication for the file. 
./src/core/org/apache/hadoop/fs/FileSystem.java:474:   * @param f the file name to open
./src/core/org/apache/hadoop/fs/FileSystem.java:475:   * @param overwrite if a file with this name already exists, then if true,
./src/core/org/apache/hadoop/fs/FileSystem.java:477:   * @param bufferSize the size of the buffer to be used.
./src/core/org/apache/hadoop/fs/FileSystem.java:478:   * @param replication required block replication for the file. 
./src/core/org/apache/hadoop/fs/FileSystem.java:494:   * @param f the file name to open
./src/core/org/apache/hadoop/fs/FileSystem.java:495:   * @param permission
./src/core/org/apache/hadoop/fs/FileSystem.java:496:   * @param overwrite if a file with this name already exists, then if true,
./src/core/org/apache/hadoop/fs/FileSystem.java:498:   * @param bufferSize the size of the buffer to be used.
./src/core/org/apache/hadoop/fs/FileSystem.java:499:   * @param replication required block replication for the file.
./src/core/org/apache/hadoop/fs/FileSystem.java:500:   * @param blockSize
./src/core/org/apache/hadoop/fs/FileSystem.java:501:   * @param progress
./src/core/org/apache/hadoop/fs/FileSystem.java:502:   * @throws IOException
./src/core/org/apache/hadoop/fs/FileSystem.java:503:   * @see #setPermission(Path, FsPermission)
./src/core/org/apache/hadoop/fs/FileSystem.java:529:   * @param f the existing file to be appended.
./src/core/org/apache/hadoop/fs/FileSystem.java:530:   * @throws IOException
./src/core/org/apache/hadoop/fs/FileSystem.java:538:   * @param f the existing file to be appended.
./src/core/org/apache/hadoop/fs/FileSystem.java:539:   * @param bufferSize the size of the buffer to be used.
./src/core/org/apache/hadoop/fs/FileSystem.java:540:   * @throws IOException
./src/core/org/apache/hadoop/fs/FileSystem.java:548:   * @param f the existing file to be appended.
./src/core/org/apache/hadoop/fs/FileSystem.java:549:   * @param bufferSize the size of the buffer to be used.
./src/core/org/apache/hadoop/fs/FileSystem.java:550:   * @param progress for reporting progress if it is not null.
./src/core/org/apache/hadoop/fs/FileSystem.java:551:   * @throws IOException
./src/core/org/apache/hadoop/fs/FileSystem.java:559:   * @deprecated Use getFileStatus() instead
./src/core/org/apache/hadoop/fs/FileSystem.java:560:   * @param src file name
./src/core/org/apache/hadoop/fs/FileSystem.java:561:   * @return file replication
./src/core/org/apache/hadoop/fs/FileSystem.java:562:   * @throws IOException
./src/core/org/apache/hadoop/fs/FileSystem.java:564:  @Deprecated
./src/core/org/apache/hadoop/fs/FileSystem.java:572:   * @param src file name
./src/core/org/apache/hadoop/fs/FileSystem.java:573:   * @param replication new replication
./src/core/org/apache/hadoop/fs/FileSystem.java:574:   * @throws IOException
./src/core/org/apache/hadoop/fs/FileSystem.java:575:   * @return true if successful;
./src/core/org/apache/hadoop/fs/FileSystem.java:590:  /** @deprecated Use delete(Path, boolean) instead */ @Deprecated 
./src/core/org/apache/hadoop/fs/FileSystem.java:595:   * @param f the path to delete.
./src/core/org/apache/hadoop/fs/FileSystem.java:596:   * @param recursive if path is a directory and set to 
./src/core/org/apache/hadoop/fs/FileSystem.java:599:   * @return  true if delete is successful else false. 
./src/core/org/apache/hadoop/fs/FileSystem.java:600:   * @throws IOException
./src/core/org/apache/hadoop/fs/FileSystem.java:613:   * @param f the path to delete.
./src/core/org/apache/hadoop/fs/FileSystem.java:614:   * @return  true if deleteOnExit is successful, otherwise false.
./src/core/org/apache/hadoop/fs/FileSystem.java:615:   * @throws IOException
./src/core/org/apache/hadoop/fs/FileSystem.java:647:   * @param f source file
./src/core/org/apache/hadoop/fs/FileSystem.java:658:  /** @deprecated Use getFileStatus() instead */ @Deprecated
./src/core/org/apache/hadoop/fs/FileSystem.java:677:  /** @deprecated Use getFileStatus() instead */ @Deprecated
./src/core/org/apache/hadoop/fs/FileSystem.java:682:  /** Return the {@link ContentSummary} of a given {@link Path}. */
./src/core/org/apache/hadoop/fs/FileSystem.java:710:   * @param f
./src/core/org/apache/hadoop/fs/FileSystem.java:712:   * @return the statuses of the files/directories in the given patch
./src/core/org/apache/hadoop/fs/FileSystem.java:713:   * @throws IOException
./src/core/org/apache/hadoop/fs/FileSystem.java:737:   * @param f
./src/core/org/apache/hadoop/fs/FileSystem.java:739:   * @param filter
./src/core/org/apache/hadoop/fs/FileSystem.java:741:   * @return an array of FileStatus objects for the files under the given path
./src/core/org/apache/hadoop/fs/FileSystem.java:743:   * @throws IOException
./src/core/org/apache/hadoop/fs/FileSystem.java:756:   * @param files
./src/core/org/apache/hadoop/fs/FileSystem.java:758:   * @return a list of statuses for the files under the given paths after
./src/core/org/apache/hadoop/fs/FileSystem.java:760:   * @exception IOException
./src/core/org/apache/hadoop/fs/FileSystem.java:771:   * @param files
./src/core/org/apache/hadoop/fs/FileSystem.java:773:   * @param filter
./src/core/org/apache/hadoop/fs/FileSystem.java:775:   * @return a list of statuses for the files under the given paths after
./src/core/org/apache/hadoop/fs/FileSystem.java:777:   * @exception IOException
./src/core/org/apache/hadoop/fs/FileSystem.java:840:   * @param pathPattern a regular expression specifying a pth pattern
./src/core/org/apache/hadoop/fs/FileSystem.java:842:   * @return an array of paths that match the path pattern
./src/core/org/apache/hadoop/fs/FileSystem.java:843:   * @throws IOException
./src/core/org/apache/hadoop/fs/FileSystem.java:856:   * @param pathPattern
./src/core/org/apache/hadoop/fs/FileSystem.java:858:   * @param filter
./src/core/org/apache/hadoop/fs/FileSystem.java:860:   * @return an array of FileStatus objects
./src/core/org/apache/hadoop/fs/FileSystem.java:861:   * @throws IOException if any I/O error occurs when fetching file status
./src/core/org/apache/hadoop/fs/FileSystem.java:1104:   * @param new_dir
./src/core/org/apache/hadoop/fs/FileSystem.java:1110:   * @return the directory pathname
./src/core/org/apache/hadoop/fs/FileSystem.java:1115:   * Call {@link #mkdirs(Path, FsPermission)} with default permission.
./src/core/org/apache/hadoop/fs/FileSystem.java:1261:   * @param f the filename
./src/core/org/apache/hadoop/fs/FileSystem.java:1262:   * @return the number of bytes in a block
./src/core/org/apache/hadoop/fs/FileSystem.java:1264:  /** @deprecated Use getFileStatus() instead */ @Deprecated
./src/core/org/apache/hadoop/fs/FileSystem.java:1283:   * @param f The path we want information from
./src/core/org/apache/hadoop/fs/FileSystem.java:1284:   * @return a FileStatus object
./src/core/org/apache/hadoop/fs/FileSystem.java:1285:   * @throws FileNotFoundException when the path does not exist;
./src/core/org/apache/hadoop/fs/FileSystem.java:1293:   * @param f The file path
./src/core/org/apache/hadoop/fs/FileSystem.java:1294:   * @return The file checksum.  The default return value is null,
./src/core/org/apache/hadoop/fs/FileSystem.java:1306:   * @param paths
./src/core/org/apache/hadoop/fs/FileSystem.java:1308:   * @return a list of FileStatus objects
./src/core/org/apache/hadoop/fs/FileSystem.java:1309:   * @throws IOException
./src/core/org/apache/hadoop/fs/FileSystem.java:1328:   * @param p
./src/core/org/apache/hadoop/fs/FileSystem.java:1329:   * @param permission
./src/core/org/apache/hadoop/fs/FileSystem.java:1338:   * @param p The path
./src/core/org/apache/hadoop/fs/FileSystem.java:1339:   * @param username If it is null, the original username remains unchanged.
./src/core/org/apache/hadoop/fs/FileSystem.java:1340:   * @param groupname If it is null, the original groupname remains unchanged.
./src/core/org/apache/hadoop/fs/FileSystem.java:1348:   * @param p The path
./src/core/org/apache/hadoop/fs/FileSystem.java:1349:   * @param mtime Set the modification time of this file.
./src/core/org/apache/hadoop/fs/FileSystem.java:1352:   * @param atime Set the access time of this file.
./src/core/org/apache/hadoop/fs/FileSystem.java:1446:      /** {@inheritDoc} */
./src/core/org/apache/hadoop/fs/FileSystem.java:1455:      /** {@inheritDoc} */
./src/core/org/apache/hadoop/fs/FileSystem.java:1469:      /** {@inheritDoc} */
./src/core/org/apache/hadoop/fs/FileSystem.java:1471:        return username + "@" + scheme + "://" + authority;        
./src/core/org/apache/hadoop/fs/FileSystem.java:1482:     * @param newBytes the additional bytes read
./src/core/org/apache/hadoop/fs/FileSystem.java:1490:     * @param newBytes the additional bytes written
./src/core/org/apache/hadoop/fs/FileSystem.java:1498:     * @return the number of bytes
./src/core/org/apache/hadoop/fs/FileSystem.java:1506:     * @return the number of bytes
./src/core/org/apache/hadoop/fs/FileSystem.java:1520:   * @param cls the class to lookup
./src/core/org/apache/hadoop/fs/FileSystem.java:1521:   * @return a statistics object
./src/core/org/apache/hadoop/fs/FileUtil.java:38:   * @param stats
./src/core/org/apache/hadoop/fs/FileUtil.java:40:   * @return an array of paths corresponding to the input
./src/core/org/apache/hadoop/fs/FileUtil.java:55:   * @param stats
./src/core/org/apache/hadoop/fs/FileUtil.java:57:   * @param path
./src/core/org/apache/hadoop/fs/FileUtil.java:59:   * @return an array of paths corresponding to the input
./src/core/org/apache/hadoop/fs/FileUtil.java:103:   * @param fs {@link FileSystem} on which the path is present
./src/core/org/apache/hadoop/fs/FileUtil.java:104:   * @param dir directory to recursively delete 
./src/core/org/apache/hadoop/fs/FileUtil.java:105:   * @throws IOException
./src/core/org/apache/hadoop/fs/FileUtil.java:106:   * @deprecated Use {@link FileSystem#delete(Path, boolean)}
./src/core/org/apache/hadoop/fs/FileUtil.java:108:  @Deprecated
./src/core/org/apache/hadoop/fs/FileUtil.java:376:   * @param filename The filename to convert
./src/core/org/apache/hadoop/fs/FileUtil.java:377:   * @return The unix pathname
./src/core/org/apache/hadoop/fs/FileUtil.java:378:   * @throws IOException on windows, there can be problems with the subprocess
./src/core/org/apache/hadoop/fs/FileUtil.java:390:   * @param file The filename to convert
./src/core/org/apache/hadoop/fs/FileUtil.java:391:   * @return The unix pathname
./src/core/org/apache/hadoop/fs/FileUtil.java:392:   * @throws IOException on windows, there can be problems with the subprocess
./src/core/org/apache/hadoop/fs/FileUtil.java:400:   * @param file The filename to convert
./src/core/org/apache/hadoop/fs/FileUtil.java:401:   * @param makeCanonicalPath 
./src/core/org/apache/hadoop/fs/FileUtil.java:403:   * @return The unix pathname
./src/core/org/apache/hadoop/fs/FileUtil.java:404:   * @throws IOException on windows, there can be problems with the subprocess
./src/core/org/apache/hadoop/fs/FileUtil.java:419:   * @param dir
./src/core/org/apache/hadoop/fs/FileUtil.java:421:   * @return The total disk space of the input local directory
./src/core/org/apache/hadoop/fs/FileUtil.java:442:   * @param inFile The zip file as input
./src/core/org/apache/hadoop/fs/FileUtil.java:443:   * @param unzipDir The unzip directory where to unzip the zip file.
./src/core/org/apache/hadoop/fs/FileUtil.java:444:   * @throws IOException
./src/core/org/apache/hadoop/fs/FileUtil.java:490:   * @param inFile The tar file as input. 
./src/core/org/apache/hadoop/fs/FileUtil.java:491:   * @param untarDir The untar directory where to untar the tar file.
./src/core/org/apache/hadoop/fs/FileUtil.java:492:   * @throws IOException
./src/core/org/apache/hadoop/fs/FileUtil.java:669:   * @param target the target for symlink 
./src/core/org/apache/hadoop/fs/FileUtil.java:670:   * @param linkname the symlink
./src/core/org/apache/hadoop/fs/FileUtil.java:671:   * @return value returned by the command
./src/core/org/apache/hadoop/fs/FileUtil.java:687:   * @param filename the name of the file to change
./src/core/org/apache/hadoop/fs/FileUtil.java:688:   * @param perm the permission string
./src/core/org/apache/hadoop/fs/FileUtil.java:689:   * @return the exit code from the command
./src/core/org/apache/hadoop/fs/FileUtil.java:690:   * @throws IOException
./src/core/org/apache/hadoop/fs/FileUtil.java:691:   * @throws InterruptedException
./src/core/org/apache/hadoop/fs/FileUtil.java:702:   * @param basefile the base file of the tmp
./src/core/org/apache/hadoop/fs/FileUtil.java:703:   * @param prefix file name prefix of tmp
./src/core/org/apache/hadoop/fs/FileUtil.java:704:   * @param isDeleteOnExit if true, the tmp will be deleted when the VM exits
./src/core/org/apache/hadoop/fs/FileUtil.java:705:   * @return a newly created tmp file
./src/core/org/apache/hadoop/fs/FileUtil.java:706:   * @exception IOException If a tmp file cannot created
./src/core/org/apache/hadoop/fs/FileUtil.java:707:   * @see java.io.File#createTempFile(String, String, File)
./src/core/org/apache/hadoop/fs/FileUtil.java:708:   * @see java.io.File#deleteOnExit()
./src/core/org/apache/hadoop/fs/FileUtil.java:724:   * @param src the source file
./src/core/org/apache/hadoop/fs/FileUtil.java:725:   * @param target the target file
./src/core/org/apache/hadoop/fs/FileUtil.java:726:   * @exception IOException If this operation fails
./src/core/org/apache/hadoop/fs/FilterFileSystem.java:58:   * @param name a uri whose authority section names the host, port, etc.
./src/core/org/apache/hadoop/fs/FilterFileSystem.java:60:   * @param conf the configuration
./src/core/org/apache/hadoop/fs/FilterFileSystem.java:71:  /** @deprecated call #getUri() instead.*/
./src/core/org/apache/hadoop/fs/FilterFileSystem.java:97:   * @param f the file name to open
./src/core/org/apache/hadoop/fs/FilterFileSystem.java:98:   * @param bufferSize the size of the buffer to be used.
./src/core/org/apache/hadoop/fs/FilterFileSystem.java:104:  /** {@inheritDoc} */
./src/core/org/apache/hadoop/fs/FilterFileSystem.java:110:  /** {@inheritDoc} */
./src/core/org/apache/hadoop/fs/FilterFileSystem.java:111:  @Override
./src/core/org/apache/hadoop/fs/FilterFileSystem.java:122:   * @param src file name
./src/core/org/apache/hadoop/fs/FilterFileSystem.java:123:   * @param replication new replication
./src/core/org/apache/hadoop/fs/FilterFileSystem.java:124:   * @throws IOException
./src/core/org/apache/hadoop/fs/FilterFileSystem.java:125:   * @return true if successful;
./src/core/org/apache/hadoop/fs/FilterFileSystem.java:140:  /** Delete a file */@Deprecated
./src/core/org/apache/hadoop/fs/FilterFileSystem.java:164:   * @param newDir
./src/core/org/apache/hadoop/fs/FilterFileSystem.java:173:   * @return the directory pathname
./src/core/org/apache/hadoop/fs/FilterFileSystem.java:179:  /** {@inheritDoc} */
./src/core/org/apache/hadoop/fs/FilterFileSystem.java:180:  @Override
./src/core/org/apache/hadoop/fs/FilterFileSystem.java:247:  /** {@inheritDoc} */
./src/core/org/apache/hadoop/fs/FilterFileSystem.java:252:  @Override
./src/core/org/apache/hadoop/fs/FilterFileSystem.java:257:  @Override
./src/core/org/apache/hadoop/fs/FilterFileSystem.java:263:  /** {@inheritDoc} */
./src/core/org/apache/hadoop/fs/FilterFileSystem.java:264:  @Override
./src/core/org/apache/hadoop/fs/FilterFileSystem.java:270:  /** {@inheritDoc} */
./src/core/org/apache/hadoop/fs/FilterFileSystem.java:271:  @Override
./src/core/org/apache/hadoop/fs/FSDataInputStream.java:22:/** Utility that wraps a {@link FSInputStream} in a {@link DataInputStream}
./src/core/org/apache/hadoop/fs/FSDataInputStream.java:23: * and buffers input through a {@link BufferedInputStream}. */
./src/core/org/apache/hadoop/fs/FSDataOutputStream.java:22:/** Utility that wraps a {@link OutputStream} in a {@link DataOutputStream},
./src/core/org/apache/hadoop/fs/FSDataOutputStream.java:23: * buffers output through a {@link BufferedOutputStream} and creates a checksum
./src/core/org/apache/hadoop/fs/FSDataOutputStream.java:63:  @Deprecated
./src/core/org/apache/hadoop/fs/FSDataOutputStream.java:87:  /** {@inheritDoc} */
./src/core/org/apache/hadoop/fs/FSInputChecker.java:53:   * @param file The name of the file to be read
./src/core/org/apache/hadoop/fs/FSInputChecker.java:54:   * @param numOfRetries Number of read retries when ChecksumError occurs
./src/core/org/apache/hadoop/fs/FSInputChecker.java:63:   * @param file The name of the file to be read
./src/core/org/apache/hadoop/fs/FSInputChecker.java:64:   * @param numOfRetries Number of read retries when ChecksumError occurs
./src/core/org/apache/hadoop/fs/FSInputChecker.java:65:   * @param sum the type of Checksum engine
./src/core/org/apache/hadoop/fs/FSInputChecker.java:66:   * @param chunkSize maximun chunk size
./src/core/org/apache/hadoop/fs/FSInputChecker.java:67:   * @param checksumSize the number byte of each checksum
./src/core/org/apache/hadoop/fs/FSInputChecker.java:80:   * @param pos chunkPos
./src/core/org/apache/hadoop/fs/FSInputChecker.java:81:   * @param buf desitination buffer
./src/core/org/apache/hadoop/fs/FSInputChecker.java:82:   * @param offset offset in buf at which to store data
./src/core/org/apache/hadoop/fs/FSInputChecker.java:83:   * @param len maximun number of bytes to read
./src/core/org/apache/hadoop/fs/FSInputChecker.java:84:   * @return number of bytes read
./src/core/org/apache/hadoop/fs/FSInputChecker.java:91:   * @param pos a postion in the file
./src/core/org/apache/hadoop/fs/FSInputChecker.java:92:   * @return the starting position of the chunk which contains the byte
./src/core/org/apache/hadoop/fs/FSInputChecker.java:104:   * @return     the next byte of data, or <code>-1</code> if the end of the
./src/core/org/apache/hadoop/fs/FSInputChecker.java:106:   * @exception  IOException  if an I/O error occurs.
./src/core/org/apache/hadoop/fs/FSInputChecker.java:124:   * <code>{@link InputStream#read(byte[], int, int) read}</code> method of
./src/core/org/apache/hadoop/fs/FSInputChecker.java:125:   * the <code>{@link InputStream}</code> class.  As an additional
./src/core/org/apache/hadoop/fs/FSInputChecker.java:141:   * @param      b     destination buffer.
./src/core/org/apache/hadoop/fs/FSInputChecker.java:142:   * @param      off   offset at which to start storing bytes.
./src/core/org/apache/hadoop/fs/FSInputChecker.java:143:   * @param      len   maximum number of bytes to read.
./src/core/org/apache/hadoop/fs/FSInputChecker.java:144:   * @return     the number of bytes read, or <code>-1</code> if the end of
./src/core/org/apache/hadoop/fs/FSInputChecker.java:146:   * @exception  IOException  if an I/O error occurs.
./src/core/org/apache/hadoop/fs/FSInputChecker.java:217:   * @param b   the buffer into which the data is read.
./src/core/org/apache/hadoop/fs/FSInputChecker.java:218:   * @param off the start offset in array <code>b</code>
./src/core/org/apache/hadoop/fs/FSInputChecker.java:220:   * @param len the maximum number of bytes to read.
./src/core/org/apache/hadoop/fs/FSInputChecker.java:221:   * @return    the total number of bytes read into the buffer, or
./src/core/org/apache/hadoop/fs/FSInputChecker.java:224:   * @throws IOException if an I/O error occurs.
./src/core/org/apache/hadoop/fs/FSInputChecker.java:271:   * @throws ChecksumException if there is a mismatch
./src/core/org/apache/hadoop/fs/FSInputChecker.java:297:  @Override
./src/core/org/apache/hadoop/fs/FSInputChecker.java:302:  @Override
./src/core/org/apache/hadoop/fs/FSInputChecker.java:319:   * @param      n   the number of bytes to be skipped.
./src/core/org/apache/hadoop/fs/FSInputChecker.java:320:   * @return     the actual number of bytes skipped.
./src/core/org/apache/hadoop/fs/FSInputChecker.java:321:   * @exception  IOException  if an I/O error occurs.
./src/core/org/apache/hadoop/fs/FSInputChecker.java:341:   * @param      pos   the postion to seek to.
./src/core/org/apache/hadoop/fs/FSInputChecker.java:342:   * @exception  IOException  if an I/O error occurs.
./src/core/org/apache/hadoop/fs/FSInputChecker.java:374:   * @param stm    an input stream
./src/core/org/apache/hadoop/fs/FSInputChecker.java:375:   * @param buf    destiniation buffer
./src/core/org/apache/hadoop/fs/FSInputChecker.java:376:   * @param offset offset at which to store data
./src/core/org/apache/hadoop/fs/FSInputChecker.java:377:   * @param len    number of bytes to read
./src/core/org/apache/hadoop/fs/FSInputChecker.java:378:   * @return actual number of bytes read
./src/core/org/apache/hadoop/fs/FSInputChecker.java:379:   * @throws IOException if there is any IO error
./src/core/org/apache/hadoop/fs/FSInputChecker.java:396:   * @param sum which type of checksum to use
./src/core/org/apache/hadoop/fs/FSInputChecker.java:397:   * @param maxChunkSize maximun chunk size
./src/core/org/apache/hadoop/fs/FSInputChecker.java:398:   * @param checksumSize checksum size
./src/core/org/apache/hadoop/fs/FSOutputSummer.java:75:   * @param      b     the data.
./src/core/org/apache/hadoop/fs/FSOutputSummer.java:76:   * @param      off   the start offset in the data.
./src/core/org/apache/hadoop/fs/FSOutputSummer.java:77:   * @param      len   the number of bytes to write.
./src/core/org/apache/hadoop/fs/FSOutputSummer.java:78:   * @exception  IOException  if an I/O error occurs.
./src/core/org/apache/hadoop/fs/FsShell.java:160:   * @param argv: arguments
./src/core/org/apache/hadoop/fs/FsShell.java:161:   * @param pos: Ignore everything before argv[pos]  
./src/core/org/apache/hadoop/fs/FsShell.java:162:   * @exception: IOException  
./src/core/org/apache/hadoop/fs/FsShell.java:163:   * @see org.apache.hadoop.fs.FileSystem.globStatus 
./src/core/org/apache/hadoop/fs/FsShell.java:206:   * Return the {@link FileSystem} specified by src and the conf.
./src/core/org/apache/hadoop/fs/FsShell.java:207:   * It the {@link FileSystem} supports checksum, set verifyChecksum.
./src/core/org/apache/hadoop/fs/FsShell.java:221:   * {@link java.io.File#createTempFile(String, String, File)}.
./src/core/org/apache/hadoop/fs/FsShell.java:227:   * @param srcFS source file system
./src/core/org/apache/hadoop/fs/FsShell.java:228:   * @param src source path
./src/core/org/apache/hadoop/fs/FsShell.java:229:   * @param dst destination
./src/core/org/apache/hadoop/fs/FsShell.java:230:   * @param copyCrc copy CRC files?
./src/core/org/apache/hadoop/fs/FsShell.java:231:   * @exception IOException If some IO failed
./src/core/org/apache/hadoop/fs/FsShell.java:281:   * @param srcf: a file pattern specifying source files
./src/core/org/apache/hadoop/fs/FsShell.java:282:   * @param dstf: a destination local file/directory 
./src/core/org/apache/hadoop/fs/FsShell.java:283:   * @exception: IOException  
./src/core/org/apache/hadoop/fs/FsShell.java:284:   * @see org.apache.hadoop.fs.FileSystem.globStatus 
./src/core/org/apache/hadoop/fs/FsShell.java:298:   * @param srcf: a file pattern specifying source files
./src/core/org/apache/hadoop/fs/FsShell.java:299:   * @param dstf: a destination local file/directory
./src/core/org/apache/hadoop/fs/FsShell.java:300:   * @param endline: if an end of line character is added to a text file 
./src/core/org/apache/hadoop/fs/FsShell.java:301:   * @exception: IOException  
./src/core/org/apache/hadoop/fs/FsShell.java:302:   * @see org.apache.hadoop.fs.FileSystem.globStatus 
./src/core/org/apache/hadoop/fs/FsShell.java:331:   * @param srcf: a file pattern specifying source files
./src/core/org/apache/hadoop/fs/FsShell.java:332:   * @exception: IOException
./src/core/org/apache/hadoop/fs/FsShell.java:333:   * @see org.apache.hadoop.fs.FileSystem.globStatus 
./src/core/org/apache/hadoop/fs/FsShell.java:346:      @Override
./src/core/org/apache/hadoop/fs/FsShell.java:414:      @Override
./src/core/org/apache/hadoop/fs/FsShell.java:426:   * @param cmd
./src/core/org/apache/hadoop/fs/FsShell.java:427:   * @param pos ignore anything before this pos in cmd
./src/core/org/apache/hadoop/fs/FsShell.java:428:   * @throws IOException 
./src/core/org/apache/hadoop/fs/FsShell.java:463:   * @param waitList The files are waited for.
./src/core/org/apache/hadoop/fs/FsShell.java:464:   * @param rep The new replication number.
./src/core/org/apache/hadoop/fs/FsShell.java:465:   * @throws IOException IOException
./src/core/org/apache/hadoop/fs/FsShell.java:503:   * @param newRep new replication factor
./src/core/org/apache/hadoop/fs/FsShell.java:504:   * @param srcf a file pattern specifying source files
./src/core/org/apache/hadoop/fs/FsShell.java:505:   * @param recursive if need to set replication factor for files in subdirs
./src/core/org/apache/hadoop/fs/FsShell.java:506:   * @throws IOException  
./src/core/org/apache/hadoop/fs/FsShell.java:507:   * @see org.apache.hadoop.fs.FileSystem#globStatus(Path)
./src/core/org/apache/hadoop/fs/FsShell.java:548:   * @param file: a file/directory
./src/core/org/apache/hadoop/fs/FsShell.java:549:   * @param newRep: new replication factor
./src/core/org/apache/hadoop/fs/FsShell.java:550:   * @throws IOException
./src/core/org/apache/hadoop/fs/FsShell.java:567:   * @param srcf a file pattern specifying source files
./src/core/org/apache/hadoop/fs/FsShell.java:568:   * @param recursive if need to list files in subdirs
./src/core/org/apache/hadoop/fs/FsShell.java:569:   * @throws IOException  
./src/core/org/apache/hadoop/fs/FsShell.java:570:   * @see org.apache.hadoop.fs.FileSystem#globStatus(Path)
./src/core/org/apache/hadoop/fs/FsShell.java:647:   * @param src a file pattern specifying source files
./src/core/org/apache/hadoop/fs/FsShell.java:648:   * @throws IOException  
./src/core/org/apache/hadoop/fs/FsShell.java:649:   * @see org.apache.hadoop.fs.FileSystem#globStatus(Path)
./src/core/org/apache/hadoop/fs/FsShell.java:681:   * @param src a file pattern specifying source files
./src/core/org/apache/hadoop/fs/FsShell.java:682:   * @throws IOException  
./src/core/org/apache/hadoop/fs/FsShell.java:683:   * @see org.apache.hadoop.fs.FileSystem#globStatus(Path)
./src/core/org/apache/hadoop/fs/FsShell.java:826:   * @param srcf a file pattern specifying source files
./src/core/org/apache/hadoop/fs/FsShell.java:827:   * @param dstf a destination local file/directory 
./src/core/org/apache/hadoop/fs/FsShell.java:828:   * @throws IOException  
./src/core/org/apache/hadoop/fs/FsShell.java:829:   * @see org.apache.hadoop.fs.FileSystem#globStatus(Path)
./src/core/org/apache/hadoop/fs/FsShell.java:878:   * @exception: IOException  
./src/core/org/apache/hadoop/fs/FsShell.java:937:   * @param srcf a file pattern specifying source files
./src/core/org/apache/hadoop/fs/FsShell.java:938:   * @param dstf a destination local file/directory 
./src/core/org/apache/hadoop/fs/FsShell.java:939:   * @throws IOException  
./src/core/org/apache/hadoop/fs/FsShell.java:940:   * @see org.apache.hadoop.fs.FileSystem#globStatus(Path)
./src/core/org/apache/hadoop/fs/FsShell.java:963:   * @exception: IOException  
./src/core/org/apache/hadoop/fs/FsShell.java:1019:   * @param srcf a file pattern specifying source files
./src/core/org/apache/hadoop/fs/FsShell.java:1020:   * @param recursive if need to delete subdirs
./src/core/org/apache/hadoop/fs/FsShell.java:1021:   * @throws IOException  
./src/core/org/apache/hadoop/fs/FsShell.java:1022:   * @see org.apache.hadoop.fs.FileSystem#globStatus(Path)
./src/core/org/apache/hadoop/fs/FsShell.java:1033:      @Override
./src/core/org/apache/hadoop/fs/FsShell.java:1076:   * @param cmd
./src/core/org/apache/hadoop/fs/FsShell.java:1077:   * @param pos ignore anything before this pos in cmd
./src/core/org/apache/hadoop/fs/FsShell.java:1078:   * @throws IOException 
./src/core/org/apache/hadoop/fs/FsShell.java:1397:      "\t\tand any of '-_.@/' i.e. [-_.@/a-zA-Z0-9]. The names are case\n" +
./src/core/org/apache/hadoop/fs/FsShellPermissions.java:168:    @Override
./src/core/org/apache/hadoop/fs/FsShellPermissions.java:193:  static private String allowedChars = "[-_./@a-zA-Z0-9]";
./src/core/org/apache/hadoop/fs/FsShellPermissions.java:230:    @Override
./src/core/org/apache/hadoop/fs/FsUrlConnection.java:42:  @Override
./src/core/org/apache/hadoop/fs/FsUrlConnection.java:52:  /* @inheritDoc */
./src/core/org/apache/hadoop/fs/FsUrlConnection.java:53:  @Override
./src/core/org/apache/hadoop/fs/FsUrlStreamHandler.java:42:  @Override
./src/core/org/apache/hadoop/fs/ftp/FTPException.java:21: * A class to wrap a {@link Throwable} into a Runtime Exception.
./src/core/org/apache/hadoop/fs/ftp/FTPFileSystem.java:42: * A {@link FileSystem} backed by an FTP client provided by <a
./src/core/org/apache/hadoop/fs/ftp/FTPFileSystem.java:57:  @Override
./src/core/org/apache/hadoop/fs/ftp/FTPFileSystem.java:95:   * @return An FTPClient instance
./src/core/org/apache/hadoop/fs/ftp/FTPFileSystem.java:96:   * @throws IOException
./src/core/org/apache/hadoop/fs/ftp/FTPFileSystem.java:126:   * @param client
./src/core/org/apache/hadoop/fs/ftp/FTPFileSystem.java:127:   * @throws IOException
./src/core/org/apache/hadoop/fs/ftp/FTPFileSystem.java:146:   * @param workDir
./src/core/org/apache/hadoop/fs/ftp/FTPFileSystem.java:147:   * @param path
./src/core/org/apache/hadoop/fs/ftp/FTPFileSystem.java:148:   * @return
./src/core/org/apache/hadoop/fs/ftp/FTPFileSystem.java:157:  @Override
./src/core/org/apache/hadoop/fs/ftp/FTPFileSystem.java:193:  @Override
./src/core/org/apache/hadoop/fs/ftp/FTPFileSystem.java:223:      @Override
./src/core/org/apache/hadoop/fs/ftp/FTPFileSystem.java:267:  /** @deprecated Use delete(Path, boolean) instead */
./src/core/org/apache/hadoop/fs/ftp/FTPFileSystem.java:268:  @Override
./src/core/org/apache/hadoop/fs/ftp/FTPFileSystem.java:269:  @Deprecated
./src/core/org/apache/hadoop/fs/ftp/FTPFileSystem.java:274:  @Override
./src/core/org/apache/hadoop/fs/ftp/FTPFileSystem.java:285:  /** @deprecated Use delete(Path, boolean) instead */
./src/core/org/apache/hadoop/fs/ftp/FTPFileSystem.java:286:  @Deprecated
./src/core/org/apache/hadoop/fs/ftp/FTPFileSystem.java:339:  @Override
./src/core/org/apache/hadoop/fs/ftp/FTPFileSystem.java:344:  @Override
./src/core/org/apache/hadoop/fs/ftp/FTPFileSystem.java:376:  @Override
./src/core/org/apache/hadoop/fs/ftp/FTPFileSystem.java:427:   * Convert the file information in FTPFile to a {@link FileStatus} object. *
./src/core/org/apache/hadoop/fs/ftp/FTPFileSystem.java:429:   * @param ftpFile
./src/core/org/apache/hadoop/fs/ftp/FTPFileSystem.java:430:   * @param parentPath
./src/core/org/apache/hadoop/fs/ftp/FTPFileSystem.java:431:   * @return FileStatus
./src/core/org/apache/hadoop/fs/ftp/FTPFileSystem.java:450:  @Override
./src/core/org/apache/hadoop/fs/ftp/FTPFileSystem.java:507:  @Override
./src/core/org/apache/hadoop/fs/ftp/FTPFileSystem.java:523:   * @param client
./src/core/org/apache/hadoop/fs/ftp/FTPFileSystem.java:524:   * @param src
./src/core/org/apache/hadoop/fs/ftp/FTPFileSystem.java:525:   * @param dst
./src/core/org/apache/hadoop/fs/ftp/FTPFileSystem.java:526:   * @return
./src/core/org/apache/hadoop/fs/ftp/FTPFileSystem.java:527:   * @throws IOException
./src/core/org/apache/hadoop/fs/ftp/FTPFileSystem.java:554:  @Override
./src/core/org/apache/hadoop/fs/ftp/FTPFileSystem.java:560:  @Override
./src/core/org/apache/hadoop/fs/ftp/FTPFileSystem.java:578:  @Override
./src/core/org/apache/hadoop/fs/GlobExpander.java:40:   * @param filePattern
./src/core/org/apache/hadoop/fs/GlobExpander.java:41:   * @return expanded file patterns
./src/core/org/apache/hadoop/fs/GlobExpander.java:42:   * @throws IOException 
./src/core/org/apache/hadoop/fs/GlobExpander.java:63:   * @param filePattern
./src/core/org/apache/hadoop/fs/GlobExpander.java:64:   * @return expanded file patterns
./src/core/org/apache/hadoop/fs/GlobExpander.java:65:   * @throws IOException 
./src/core/org/apache/hadoop/fs/GlobExpander.java:132:   * @param filePattern
./src/core/org/apache/hadoop/fs/GlobExpander.java:133:   * @return the index of the leftmost opening curly bracket containing a
./src/core/org/apache/hadoop/fs/GlobExpander.java:135:   * @throws IOException 
./src/core/org/apache/hadoop/fs/HarFileSystem.java:74:   * @param fs
./src/core/org/apache/hadoop/fs/HarFileSystem.java:170:   * @param rawURI raw Har URI
./src/core/org/apache/hadoop/fs/HarFileSystem.java:171:   * @return filtered URI of the underlying fileSystem
./src/core/org/apache/hadoop/fs/HarFileSystem.java:217:   * @param underLyingURI the uri of underlying
./src/core/org/apache/hadoop/fs/HarFileSystem.java:219:   * @return har specific auth
./src/core/org/apache/hadoop/fs/HarFileSystem.java:240:  @Override
./src/core/org/apache/hadoop/fs/HarFileSystem.java:250:   * @param path the fully qualified path in the har filesystem.
./src/core/org/apache/hadoop/fs/HarFileSystem.java:251:   * @return relative path in the filesystem.
./src/core/org/apache/hadoop/fs/HarFileSystem.java:291:   * @see org.apache.hadoop.fs.FilterFileSystem#makeQualified(
./src/core/org/apache/hadoop/fs/HarFileSystem.java:294:  @Override
./src/core/org/apache/hadoop/fs/HarFileSystem.java:321:   * @param file the input filestatus to get block locations
./src/core/org/apache/hadoop/fs/HarFileSystem.java:322:   * @param start the start in the file
./src/core/org/apache/hadoop/fs/HarFileSystem.java:323:   * @param len the length in the file
./src/core/org/apache/hadoop/fs/HarFileSystem.java:324:   * @return block locations for this segment of file
./src/core/org/apache/hadoop/fs/HarFileSystem.java:325:   * @throws IOException
./src/core/org/apache/hadoop/fs/HarFileSystem.java:327:  @Override
./src/core/org/apache/hadoop/fs/HarFileSystem.java:354:   * @param rawBlocks the raw blocks returned by the filesystem
./src/core/org/apache/hadoop/fs/HarFileSystem.java:355:   * @return faked blocks with changed offsets.
./src/core/org/apache/hadoop/fs/HarFileSystem.java:369:   * @param p the path in the harfilesystem
./src/core/org/apache/hadoop/fs/HarFileSystem.java:370:   * @return the hash code of the path.
./src/core/org/apache/hadoop/fs/HarFileSystem.java:514:   * @param f the path in har filesystem
./src/core/org/apache/hadoop/fs/HarFileSystem.java:515:   * @return filestatus.
./src/core/org/apache/hadoop/fs/HarFileSystem.java:516:   * @throws IOException
./src/core/org/apache/hadoop/fs/HarFileSystem.java:518:  @Override
./src/core/org/apache/hadoop/fs/HarFileSystem.java:548:  @Override
./src/core/org/apache/hadoop/fs/HarFileSystem.java:591:  @Override
./src/core/org/apache/hadoop/fs/HarFileSystem.java:606:  @Override
./src/core/org/apache/hadoop/fs/HarFileSystem.java:614:  @Override
./src/core/org/apache/hadoop/fs/HarFileSystem.java:623:  @Override
./src/core/org/apache/hadoop/fs/HarFileSystem.java:761:      @Override
./src/core/org/apache/hadoop/fs/HarFileSystem.java:867:     * @param fs the underlying filesystem
./src/core/org/apache/hadoop/fs/HarFileSystem.java:868:     * @param p The path in the underlying filesystem
./src/core/org/apache/hadoop/fs/HarFileSystem.java:869:     * @param start the start position in the part file
./src/core/org/apache/hadoop/fs/HarFileSystem.java:870:     * @param length the length of valid data in the part file
./src/core/org/apache/hadoop/fs/HarFileSystem.java:871:     * @param bufsize the buffer size
./src/core/org/apache/hadoop/fs/HarFileSystem.java:872:     * @throws IOException
./src/core/org/apache/hadoop/fs/HarFileSystem.java:881:     * @param fs the underlying filesystem
./src/core/org/apache/hadoop/fs/HarFileSystem.java:882:     * @param p the path in the underlying file system
./src/core/org/apache/hadoop/fs/HarFileSystem.java:883:     * @param start the start position in the part file
./src/core/org/apache/hadoop/fs/HarFileSystem.java:884:     * @param length the length of valid data in the part file.
./src/core/org/apache/hadoop/fs/HarFileSystem.java:885:     * @throws IOException
./src/core/org/apache/hadoop/fs/InMemoryFileSystem.java:38:@Deprecated
./src/core/org/apache/hadoop/fs/InMemoryFileSystem.java:186:     * @param permission Currently ignored.
./src/core/org/apache/hadoop/fs/InMemoryFileSystem.java:244:    @Deprecated
./src/core/org/apache/hadoop/fs/InMemoryFileSystem.java:277:     * @param permission Currently ignored.
./src/core/org/apache/hadoop/fs/InMemoryFileSystem.java:366:     * @TODO: Fix for Java6?
./src/core/org/apache/hadoop/fs/kfs/IFSImpl.java:15: * @author: Sriram Rao (Kosmix Corp.)
./src/core/org/apache/hadoop/fs/kfs/KFSImpl.java:15: * @author: Sriram Rao (Kosmix Corp.)
./src/core/org/apache/hadoop/fs/kfs/KFSImpl.java:37:    @Deprecated
./src/core/org/apache/hadoop/fs/kfs/KFSInputStream.java:15: * @author: Sriram Rao (Kosmix Corp.)
./src/core/org/apache/hadoop/fs/kfs/KFSInputStream.java:38:    @Deprecated
./src/core/org/apache/hadoop/fs/kfs/KFSOutputStream.java:15: * @author: Sriram Rao (Kosmix Corp.)
./src/core/org/apache/hadoop/fs/kfs/KosmosFileSystem.java:15: * @author: Sriram Rao (Kosmix Corp.)
./src/core/org/apache/hadoop/fs/kfs/KosmosFileSystem.java:79:    @Deprecated
./src/core/org/apache/hadoop/fs/kfs/KosmosFileSystem.java:113:    @Deprecated
./src/core/org/apache/hadoop/fs/kfs/KosmosFileSystem.java:123:    @Deprecated
./src/core/org/apache/hadoop/fs/kfs/KosmosFileSystem.java:232:    @Deprecated
./src/core/org/apache/hadoop/fs/kfs/KosmosFileSystem.java:237:    @Deprecated
./src/core/org/apache/hadoop/fs/kfs/KosmosFileSystem.java:244:    @Deprecated
./src/core/org/apache/hadoop/fs/kfs/KosmosFileSystem.java:271:    @Deprecated            
./src/core/org/apache/hadoop/fs/kfs/KosmosFileSystem.java:276:    @Deprecated            
./src/core/org/apache/hadoop/fs/kfs/KosmosFileSystem.java:285:    @Override
./src/core/org/apache/hadoop/fs/LocalDirAllocator.java:73:   * @param contextCfgItemName
./src/core/org/apache/hadoop/fs/LocalDirAllocator.java:100:   *  @param pathStr the requested path (this will be created on the first 
./src/core/org/apache/hadoop/fs/LocalDirAllocator.java:102:   *  @param conf the Configuration object
./src/core/org/apache/hadoop/fs/LocalDirAllocator.java:103:   *  @return the complete path to the file on a local disk
./src/core/org/apache/hadoop/fs/LocalDirAllocator.java:104:   *  @throws IOException
./src/core/org/apache/hadoop/fs/LocalDirAllocator.java:114:   *  @param pathStr the requested path (this will be created on the first 
./src/core/org/apache/hadoop/fs/LocalDirAllocator.java:116:   *  @param size the size of the file that is going to be written
./src/core/org/apache/hadoop/fs/LocalDirAllocator.java:117:   *  @param conf the Configuration object
./src/core/org/apache/hadoop/fs/LocalDirAllocator.java:118:   *  @return the complete path to the file on a local disk
./src/core/org/apache/hadoop/fs/LocalDirAllocator.java:119:   *  @throws IOException
./src/core/org/apache/hadoop/fs/LocalDirAllocator.java:130:   *  @param pathStr the requested file (this will be searched)
./src/core/org/apache/hadoop/fs/LocalDirAllocator.java:131:   *  @param conf the Configuration object
./src/core/org/apache/hadoop/fs/LocalDirAllocator.java:132:   *  @return the complete path to the file on a local disk
./src/core/org/apache/hadoop/fs/LocalDirAllocator.java:133:   *  @throws IOException
./src/core/org/apache/hadoop/fs/LocalDirAllocator.java:146:   *  @param pathStr prefix for the temporary file
./src/core/org/apache/hadoop/fs/LocalDirAllocator.java:147:   *  @param size the size of the file that is going to be written
./src/core/org/apache/hadoop/fs/LocalDirAllocator.java:148:   *  @param conf the Configuration object
./src/core/org/apache/hadoop/fs/LocalDirAllocator.java:149:   *  @return a unique temporary file
./src/core/org/apache/hadoop/fs/LocalDirAllocator.java:150:   *  @throws IOException
./src/core/org/apache/hadoop/fs/LocalDirAllocator.java:159:   * @param contextCfgItemName
./src/core/org/apache/hadoop/fs/LocalDirAllocator.java:160:   * @return true/false
./src/core/org/apache/hadoop/fs/LocalDirAllocator.java:170:   *  @param pathStr the requested file (this will be searched)
./src/core/org/apache/hadoop/fs/LocalDirAllocator.java:171:   *  @param conf the Configuration object
./src/core/org/apache/hadoop/fs/LocalDirAllocator.java:172:   *  @return true if files exist. false otherwise
./src/core/org/apache/hadoop/fs/LocalDirAllocator.java:173:   *  @throws IOException
./src/core/org/apache/hadoop/fs/LocalDirAllocator.java:182:   * @return the current directory index for the given configuration item.
./src/core/org/apache/hadoop/fs/LocalDirAllocator.java:263:     * @return the current directory index.
./src/core/org/apache/hadoop/fs/LocalFileSystem.java:52:  @Override
./src/core/org/apache/hadoop/fs/LocalFileSystem.java:58:  @Override
./src/core/org/apache/hadoop/fs/MD5MD5CRC32FileChecksum.java:51:  /** {@inheritDoc} */ 
./src/core/org/apache/hadoop/fs/MD5MD5CRC32FileChecksum.java:56:  /** {@inheritDoc} */ 
./src/core/org/apache/hadoop/fs/MD5MD5CRC32FileChecksum.java:59:  /** {@inheritDoc} */ 
./src/core/org/apache/hadoop/fs/MD5MD5CRC32FileChecksum.java:64:  /** {@inheritDoc} */ 
./src/core/org/apache/hadoop/fs/MD5MD5CRC32FileChecksum.java:71:  /** {@inheritDoc} */ 
./src/core/org/apache/hadoop/fs/MD5MD5CRC32FileChecksum.java:109:  /** {@inheritDoc} */ 
./src/core/org/apache/hadoop/fs/Path.java:26:/** Names a file or directory in a {@link FileSystem}.
./src/core/org/apache/hadoop/fs/PathFilter.java:25:   * @param  path  The abstract pathname to be tested
./src/core/org/apache/hadoop/fs/PathFilter.java:26:   * @return  <code>true</code> if and only if <code>pathname</code>
./src/core/org/apache/hadoop/fs/permission/AccessControlException.java:22:@Deprecated
./src/core/org/apache/hadoop/fs/permission/AccessControlException.java:24:  //Required by {@link java.io.Serializable}.
./src/core/org/apache/hadoop/fs/permission/AccessControlException.java:29:   * {@link org.apache.hadoop.ipc.RemoteException}.
./src/core/org/apache/hadoop/fs/permission/AccessControlException.java:36:   * Constructs an {@link AccessControlException}
./src/core/org/apache/hadoop/fs/permission/AccessControlException.java:38:   * @param s the detail message.
./src/core/org/apache/hadoop/fs/permission/FsAction.java:47:   * @param that
./src/core/org/apache/hadoop/fs/permission/FsPermission.java:38:  /** Create an immutable {@link FsPermission} object. */
./src/core/org/apache/hadoop/fs/permission/FsPermission.java:58:   * Construct by the given {@link FsAction}.
./src/core/org/apache/hadoop/fs/permission/FsPermission.java:59:   * @param u user action
./src/core/org/apache/hadoop/fs/permission/FsPermission.java:60:   * @param g group action
./src/core/org/apache/hadoop/fs/permission/FsPermission.java:61:   * @param o other action
./src/core/org/apache/hadoop/fs/permission/FsPermission.java:67:   * @param mode
./src/core/org/apache/hadoop/fs/permission/FsPermission.java:68:   * @see #toShort()
./src/core/org/apache/hadoop/fs/permission/FsPermission.java:75:   * @param other other permission
./src/core/org/apache/hadoop/fs/permission/FsPermission.java:83:  /** Return user {@link FsAction}. */
./src/core/org/apache/hadoop/fs/permission/FsPermission.java:86:  /** Return group {@link FsAction}. */
./src/core/org/apache/hadoop/fs/permission/FsPermission.java:89:  /** Return other {@link FsAction}. */
./src/core/org/apache/hadoop/fs/permission/FsPermission.java:102:  /** {@inheritDoc} */
./src/core/org/apache/hadoop/fs/permission/FsPermission.java:107:  /** {@inheritDoc} */
./src/core/org/apache/hadoop/fs/permission/FsPermission.java:113:   * Create and initialize a {@link FsPermission} from {@link DataInput}.
./src/core/org/apache/hadoop/fs/permission/FsPermission.java:129:  /** {@inheritDoc} */
./src/core/org/apache/hadoop/fs/permission/FsPermission.java:140:  /** {@inheritDoc} */
./src/core/org/apache/hadoop/fs/permission/FsPermission.java:143:  /** {@inheritDoc} */
./src/core/org/apache/hadoop/fs/permission/FsPermission.java:179:   * @param unixSymbolicPermission e.g. "-rw-rw-rw-"
./src/core/org/apache/hadoop/fs/permission/PermissionStatus.java:37:  /** Create an immutable {@link PermissionStatus} object. */
./src/core/org/apache/hadoop/fs/permission/PermissionStatus.java:74:   * @see FsPermission#applyUMask(FsPermission)
./src/core/org/apache/hadoop/fs/permission/PermissionStatus.java:81:  /** {@inheritDoc} */
./src/core/org/apache/hadoop/fs/permission/PermissionStatus.java:88:  /** {@inheritDoc} */
./src/core/org/apache/hadoop/fs/permission/PermissionStatus.java:94:   * Create and initialize a {@link PermissionStatus} from {@link DataInput}.
./src/core/org/apache/hadoop/fs/permission/PermissionStatus.java:103:   * Serialize a {@link PermissionStatus} from its base components.
./src/core/org/apache/hadoop/fs/permission/PermissionStatus.java:114:  /** {@inheritDoc} */
./src/core/org/apache/hadoop/fs/RawLocalFileSystem.java:59:  /** @deprecated */
./src/core/org/apache/hadoop/fs/RawLocalFileSystem.java:211:    /** {@inheritDoc} */
./src/core/org/apache/hadoop/fs/RawLocalFileSystem.java:217:  /** {@inheritDoc} */
./src/core/org/apache/hadoop/fs/RawLocalFileSystem.java:230:  /** {@inheritDoc} */
./src/core/org/apache/hadoop/fs/RawLocalFileSystem.java:245:  /** {@inheritDoc} */
./src/core/org/apache/hadoop/fs/RawLocalFileSystem.java:246:  @Override
./src/core/org/apache/hadoop/fs/RawLocalFileSystem.java:263:  @Deprecated
./src/core/org/apache/hadoop/fs/RawLocalFileSystem.java:313:  /** {@inheritDoc} */
./src/core/org/apache/hadoop/fs/RawLocalFileSystem.java:314:  @Override
./src/core/org/apache/hadoop/fs/RawLocalFileSystem.java:321:  @Override
./src/core/org/apache/hadoop/fs/RawLocalFileSystem.java:329:  @Override
./src/core/org/apache/hadoop/fs/RawLocalFileSystem.java:334:  @Override
./src/core/org/apache/hadoop/fs/RawLocalFileSystem.java:339:  /** @deprecated */ @Deprecated
./src/core/org/apache/hadoop/fs/RawLocalFileSystem.java:362:  /** @deprecated */ @Deprecated
./src/core/org/apache/hadoop/fs/RawLocalFileSystem.java:438:    @Override
./src/core/org/apache/hadoop/fs/RawLocalFileSystem.java:446:    @Override
./src/core/org/apache/hadoop/fs/RawLocalFileSystem.java:454:    @Override
./src/core/org/apache/hadoop/fs/RawLocalFileSystem.java:498:    @Override
./src/core/org/apache/hadoop/fs/RawLocalFileSystem.java:510:  @Override
./src/core/org/apache/hadoop/fs/RawLocalFileSystem.java:529:  @Override
./src/core/org/apache/hadoop/fs/s3/Block.java:22: * Holds metadata about a block of data being stored in a {@link FileSystemStore}.
./src/core/org/apache/hadoop/fs/s3/Block.java:42:  @Override
./src/core/org/apache/hadoop/fs/s3/FileSystemStore.java:30: * A facility for storing and retrieving {@link INode}s and {@link Block}s.
./src/core/org/apache/hadoop/fs/s3/FileSystemStore.java:54:   * @throws IOException
./src/core/org/apache/hadoop/fs/s3/FileSystemStore.java:60:   * @throws IOException
./src/core/org/apache/hadoop/fs/s3/S3Credentials.java:36:   * @throws IllegalArgumentException if credentials for S3 cannot be
./src/core/org/apache/hadoop/fs/s3/S3FileSystem.java:45: * A block-based {@link FileSystem} backed by
./src/core/org/apache/hadoop/fs/s3/S3FileSystem.java:48: * @see NativeS3FileSystem
./src/core/org/apache/hadoop/fs/s3/S3FileSystem.java:66:  @Override
./src/core/org/apache/hadoop/fs/s3/S3FileSystem.java:71:  @Override
./src/core/org/apache/hadoop/fs/s3/S3FileSystem.java:104:  @Override
./src/core/org/apache/hadoop/fs/s3/S3FileSystem.java:109:  @Override
./src/core/org/apache/hadoop/fs/s3/S3FileSystem.java:114:  @Override
./src/core/org/apache/hadoop/fs/s3/S3FileSystem.java:127:   * @param permission Currently ignored.
./src/core/org/apache/hadoop/fs/s3/S3FileSystem.java:129:  @Override
./src/core/org/apache/hadoop/fs/s3/S3FileSystem.java:158:  @Override
./src/core/org/apache/hadoop/fs/s3/S3FileSystem.java:178:  @Override
./src/core/org/apache/hadoop/fs/s3/S3FileSystem.java:204:   * @param permission Currently ignored.
./src/core/org/apache/hadoop/fs/s3/S3FileSystem.java:206:  @Override
./src/core/org/apache/hadoop/fs/s3/S3FileSystem.java:233:  @Override
./src/core/org/apache/hadoop/fs/s3/S3FileSystem.java:240:  @Override
./src/core/org/apache/hadoop/fs/s3/S3FileSystem.java:320:  @Override
./src/core/org/apache/hadoop/fs/s3/S3FileSystem.java:321:  @Deprecated
./src/core/org/apache/hadoop/fs/s3/S3FileSystem.java:329:  @Override
./src/core/org/apache/hadoop/fs/s3/S3FileSystemException.java:23: * Thrown when there is a fatal exception while using {@link S3FileSystem}.
./src/core/org/apache/hadoop/fs/s3/S3InputStream.java:50:  @Deprecated
./src/core/org/apache/hadoop/fs/s3/S3InputStream.java:67:  @Override
./src/core/org/apache/hadoop/fs/s3/S3InputStream.java:72:  @Override
./src/core/org/apache/hadoop/fs/s3/S3InputStream.java:77:  @Override
./src/core/org/apache/hadoop/fs/s3/S3InputStream.java:86:  @Override
./src/core/org/apache/hadoop/fs/s3/S3InputStream.java:91:  @Override
./src/core/org/apache/hadoop/fs/s3/S3InputStream.java:112:  @Override
./src/core/org/apache/hadoop/fs/s3/S3InputStream.java:168:  @Override
./src/core/org/apache/hadoop/fs/s3/S3InputStream.java:187:  @Override
./src/core/org/apache/hadoop/fs/s3/S3InputStream.java:192:  @Override
./src/core/org/apache/hadoop/fs/s3/S3InputStream.java:197:  @Override
./src/core/org/apache/hadoop/fs/s3/S3OutputStream.java:95:  @Override
./src/core/org/apache/hadoop/fs/s3/S3OutputStream.java:108:  @Override
./src/core/org/apache/hadoop/fs/s3/S3OutputStream.java:128:  @Override
./src/core/org/apache/hadoop/fs/s3/S3OutputStream.java:200:  @Override
./src/core/org/apache/hadoop/fs/s3/VersionMismatchException.java:22: * in {@link S3FileSystem}.
./src/core/org/apache/hadoop/fs/s3native/FileMetadata.java:23: * Holds basic metadata for a file stored in a {@link NativeFileSystemStore}.
./src/core/org/apache/hadoop/fs/s3native/FileMetadata.java:49:  @Override
./src/core/org/apache/hadoop/fs/s3native/NativeFileSystemStore.java:30: * An abstraction for a key-based {@link File} store.
./src/core/org/apache/hadoop/fs/s3native/NativeFileSystemStore.java:56:   * @throws IOException
./src/core/org/apache/hadoop/fs/s3native/NativeFileSystemStore.java:62:   * @throws IOException
./src/core/org/apache/hadoop/fs/s3native/NativeS3FileSystem.java:59: * A {@link FileSystem} for reading and writing files stored on
./src/core/org/apache/hadoop/fs/s3native/NativeS3FileSystem.java:61: * Unlike {@link org.apache.hadoop.fs.s3.S3FileSystem} this implementation
./src/core/org/apache/hadoop/fs/s3native/NativeS3FileSystem.java:65: * @see org.apache.hadoop.fs.s3.S3FileSystem
./src/core/org/apache/hadoop/fs/s3native/NativeS3FileSystem.java:159:    @Override
./src/core/org/apache/hadoop/fs/s3native/NativeS3FileSystem.java:164:    @Override
./src/core/org/apache/hadoop/fs/s3native/NativeS3FileSystem.java:185:    @Override
./src/core/org/apache/hadoop/fs/s3native/NativeS3FileSystem.java:190:    @Override
./src/core/org/apache/hadoop/fs/s3native/NativeS3FileSystem.java:210:  @Override
./src/core/org/apache/hadoop/fs/s3native/NativeS3FileSystem.java:268:  @Override
./src/core/org/apache/hadoop/fs/s3native/NativeS3FileSystem.java:282:  @Override
./src/core/org/apache/hadoop/fs/s3native/NativeS3FileSystem.java:283:  @Deprecated
./src/core/org/apache/hadoop/fs/s3native/NativeS3FileSystem.java:288:  @Override
./src/core/org/apache/hadoop/fs/s3native/NativeS3FileSystem.java:315:  @Override
./src/core/org/apache/hadoop/fs/s3native/NativeS3FileSystem.java:344:  @Override
./src/core/org/apache/hadoop/fs/s3native/NativeS3FileSystem.java:357:  @Override
./src/core/org/apache/hadoop/fs/s3native/NativeS3FileSystem.java:413:  @Override
./src/core/org/apache/hadoop/fs/s3native/NativeS3FileSystem.java:444:  @Override
./src/core/org/apache/hadoop/fs/s3native/NativeS3FileSystem.java:499:  @Override
./src/core/org/apache/hadoop/fs/s3native/NativeS3FileSystem.java:567:  @Override
./src/core/org/apache/hadoop/fs/s3native/NativeS3FileSystem.java:572:  @Override
./src/core/org/apache/hadoop/fs/s3native/PartialListing.java:24: * {@link NativeFileSystemStore}.
./src/core/org/apache/hadoop/fs/s3native/PartialListing.java:25: * This includes the {@link FileMetadata files} and directories
./src/core/org/apache/hadoop/fs/s3native/PartialListing.java:32: * @see NativeFileSystemStore#list(String, int, String)
./src/core/org/apache/hadoop/fs/shell/Command.java:46:   * @param path the input path
./src/core/org/apache/hadoop/fs/shell/Command.java:47:   * @throws IOException if any error occurs
./src/core/org/apache/hadoop/fs/shell/Command.java:54:   * @return 0 if it runs successfully; -1 if it fails
./src/core/org/apache/hadoop/fs/shell/CommandFormat.java:44:   * @param args an array of input arguments
./src/core/org/apache/hadoop/fs/shell/CommandFormat.java:45:   * @param pos the position at which starts to parse
./src/core/org/apache/hadoop/fs/shell/CommandFormat.java:46:   * @return a list of parameters
./src/core/org/apache/hadoop/fs/shell/CommandFormat.java:69:   * @param option String representation of an option
./src/core/org/apache/hadoop/fs/shell/CommandFormat.java:70:   * @return true is the option is set; false otherwise
./src/core/org/apache/hadoop/fs/shell/Count.java:44:   * @param cmd the count command
./src/core/org/apache/hadoop/fs/shell/Count.java:45:   * @param pos the starting index of the arguments 
./src/core/org/apache/hadoop/fs/shell/Count.java:60:   * @param cmd A string representation of a command starting with "-"
./src/core/org/apache/hadoop/fs/shell/Count.java:61:   * @return true if this is a count command; false otherwise
./src/core/org/apache/hadoop/fs/shell/Count.java:67:  @Override
./src/core/org/apache/hadoop/fs/shell/Count.java:72:  @Override
./src/core/org/apache/hadoop/fs/Syncable.java:27:   * @throws IOException
./src/core/org/apache/hadoop/fs/Trash.java:59:   * @param conf a Configuration
./src/core/org/apache/hadoop/fs/Trash.java:75:   * @return false if the item is already in the trash or trash is disabled
./src/core/org/apache/hadoop/fs/Trash.java:190:  /** Return a {@link Runnable} that periodically empties the trash of all
./src/core/org/apache/hadoop/http/FilterInitializer.java:26:   * @param container The filter container
./src/core/org/apache/hadoop/http/HttpServer.java:78:   * @param name The name of the server
./src/core/org/apache/hadoop/http/HttpServer.java:79:   * @param port The port to use on the server
./src/core/org/apache/hadoop/http/HttpServer.java:80:   * @param findPort whether the server should start at the given port and 
./src/core/org/apache/hadoop/http/HttpServer.java:82:   * @param conf Configuration 
./src/core/org/apache/hadoop/http/HttpServer.java:127:   * @param appDir The application directory
./src/core/org/apache/hadoop/http/HttpServer.java:128:   * @throws IOException
./src/core/org/apache/hadoop/http/HttpServer.java:152:   * @param pathSpec The path spec for the context
./src/core/org/apache/hadoop/http/HttpServer.java:153:   * @param dir The directory containing the context
./src/core/org/apache/hadoop/http/HttpServer.java:154:   * @param isFiltered if true, the servlet is added to the filter path mapping 
./src/core/org/apache/hadoop/http/HttpServer.java:155:   * @throws IOException 
./src/core/org/apache/hadoop/http/HttpServer.java:165:   * @param name The name of the attribute
./src/core/org/apache/hadoop/http/HttpServer.java:166:   * @param value The value of the attribute
./src/core/org/apache/hadoop/http/HttpServer.java:174:   * @param name The name of the servlet (can be passed as null)
./src/core/org/apache/hadoop/http/HttpServer.java:175:   * @param pathSpec The path spec for the servlet
./src/core/org/apache/hadoop/http/HttpServer.java:176:   * @param clazz The servlet class
./src/core/org/apache/hadoop/http/HttpServer.java:186:   * @param name The name of the servlet (can be passed as null)
./src/core/org/apache/hadoop/http/HttpServer.java:187:   * @param pathSpec The path spec for the servlet
./src/core/org/apache/hadoop/http/HttpServer.java:188:   * @param clazz The servlet class
./src/core/org/apache/hadoop/http/HttpServer.java:189:   * @deprecated this is a temporary method
./src/core/org/apache/hadoop/http/HttpServer.java:191:  @Deprecated
./src/core/org/apache/hadoop/http/HttpServer.java:209:  /** {@inheritDoc} */
./src/core/org/apache/hadoop/http/HttpServer.java:250:   * @param pathSpec The path spec
./src/core/org/apache/hadoop/http/HttpServer.java:261:   * @param name The name of the attribute
./src/core/org/apache/hadoop/http/HttpServer.java:262:   * @return The value of the attribute
./src/core/org/apache/hadoop/http/HttpServer.java:270:   * @return the pathname as a URL
./src/core/org/apache/hadoop/http/HttpServer.java:271:   * @throws IOException if 'webapps' directory cannot be found on CLASSPATH.
./src/core/org/apache/hadoop/http/HttpServer.java:282:   * @return the port
./src/core/org/apache/hadoop/http/HttpServer.java:295:   * @param addr address to listen on
./src/core/org/apache/hadoop/http/HttpServer.java:296:   * @param keystore location of the keystore
./src/core/org/apache/hadoop/http/HttpServer.java:297:   * @param storPass password for the keystore
./src/core/org/apache/hadoop/http/HttpServer.java:298:   * @param keyPass password for the key
./src/core/org/apache/hadoop/http/HttpServer.java:365:    @Override
./src/core/org/apache/hadoop/io/IOUtils.java:35:   * @param in InputStrem to read from
./src/core/org/apache/hadoop/io/IOUtils.java:36:   * @param out OutputStream to write to
./src/core/org/apache/hadoop/io/IOUtils.java:37:   * @param buffSize the size of the buffer 
./src/core/org/apache/hadoop/io/IOUtils.java:38:   * @param close whether or not close the InputStream and 
./src/core/org/apache/hadoop/io/IOUtils.java:66:   * @param in InputStrem to read from
./src/core/org/apache/hadoop/io/IOUtils.java:67:   * @param out OutputStream to write to
./src/core/org/apache/hadoop/io/IOUtils.java:68:   * @param conf the Configuration object 
./src/core/org/apache/hadoop/io/IOUtils.java:77:   * @param in InputStrem to read from
./src/core/org/apache/hadoop/io/IOUtils.java:78:   * @param out OutputStream to write to
./src/core/org/apache/hadoop/io/IOUtils.java:79:   * @param conf the Configuration object
./src/core/org/apache/hadoop/io/IOUtils.java:80:   * @param close whether or not close the InputStream and 
./src/core/org/apache/hadoop/io/IOUtils.java:89:   * @param in The InputStream to read from
./src/core/org/apache/hadoop/io/IOUtils.java:90:   * @param buf The buffer to fill
./src/core/org/apache/hadoop/io/IOUtils.java:91:   * @param off offset from the buffer
./src/core/org/apache/hadoop/io/IOUtils.java:92:   * @param len the length of bytes to read
./src/core/org/apache/hadoop/io/IOUtils.java:93:   * @throws IOException if it could not read requested number of bytes 
./src/core/org/apache/hadoop/io/IOUtils.java:110:   * @param in The InputStream to skip bytes from
./src/core/org/apache/hadoop/io/IOUtils.java:111:   * @param len number of bytes to skip.
./src/core/org/apache/hadoop/io/IOUtils.java:112:   * @throws IOException if it could not skip requested number of bytes 
./src/core/org/apache/hadoop/io/IOUtils.java:126:   * Close the Closeable objects and <b>ignore</b> any {@link IOException} or 
./src/core/org/apache/hadoop/io/IOUtils.java:128:   * @param log the log to record problems to at debug level. Can be null.
./src/core/org/apache/hadoop/io/IOUtils.java:129:   * @param closeables the objects to close
./src/core/org/apache/hadoop/io/IOUtils.java:146:   * Closes the stream ignoring {@link IOException}.
./src/core/org/apache/hadoop/io/IOUtils.java:148:   * @param stream the Stream to close
./src/core/org/apache/hadoop/io/IOUtils.java:155:   * Closes the socket ignoring {@link IOException} 
./src/core/org/apache/hadoop/io/IOUtils.java:156:   * @param sock the Socket to close
./src/core/org/apache/hadoop/io/AbstractMapWritable.java:54:  /** @return the number of known classes */
./src/core/org/apache/hadoop/io/AbstractMapWritable.java:94:  /** @return the Class class for the specified id */
./src/core/org/apache/hadoop/io/AbstractMapWritable.java:99:  /** @return the id for the specified Class */
./src/core/org/apache/hadoop/io/AbstractMapWritable.java:164:  /** @return the conf */
./src/core/org/apache/hadoop/io/AbstractMapWritable.java:169:  /** @param conf the conf to set */
./src/core/org/apache/hadoop/io/AbstractMapWritable.java:174:  /** {@inheritDoc} */
./src/core/org/apache/hadoop/io/AbstractMapWritable.java:188:  /** {@inheritDoc} */
./src/core/org/apache/hadoop/io/ArrayFile.java:79:    /** Returns the key associated with the most recent call to {@link
./src/core/org/apache/hadoop/io/ArrayFile.java:80:     * #seek(long)}, {@link #next(Writable)}, or {@link
./src/core/org/apache/hadoop/io/BinaryComparable.java:22: * Interface supported by {@link org.apache.hadoop.io.WritableComparable}
./src/core/org/apache/hadoop/io/BinaryComparable.java:39:   * @see org.apache.hadoop.io.WritableComparator#compareBytes(byte[],int,int,byte[],int,int)
./src/core/org/apache/hadoop/io/BinaryComparable.java:70:   * @see org.apache.hadoop.io.WritableComparator#hashBytes(byte[],int)
./src/core/org/apache/hadoop/io/BytesWritable.java:46:   * @param bytes This array becomes the backing storage for the object.
./src/core/org/apache/hadoop/io/BytesWritable.java:55:   * @return The data is only valid between 0 and getLength() - 1.
./src/core/org/apache/hadoop/io/BytesWritable.java:63:   * @deprecated Use {@link #getBytes()} instead.
./src/core/org/apache/hadoop/io/BytesWritable.java:65:  @Deprecated
./src/core/org/apache/hadoop/io/BytesWritable.java:79:   * @deprecated Use {@link #getLength()} instead.
./src/core/org/apache/hadoop/io/BytesWritable.java:81:  @Deprecated
./src/core/org/apache/hadoop/io/BytesWritable.java:90:   * @param size The new number of bytes
./src/core/org/apache/hadoop/io/BytesWritable.java:102:   * @return The number of bytes
./src/core/org/apache/hadoop/io/BytesWritable.java:111:   * @param new_cap The new capacity in bytes.
./src/core/org/apache/hadoop/io/BytesWritable.java:128:   * @param newData the value to set this BytesWritable to.
./src/core/org/apache/hadoop/io/BytesWritable.java:136:   * @param newData the new values to copy in
./src/core/org/apache/hadoop/io/BytesWritable.java:137:   * @param offset the offset in newData to start at
./src/core/org/apache/hadoop/io/BytesWritable.java:138:   * @param length the number of bytes to copy
./src/core/org/apache/hadoop/io/compress/BlockCompressorStream.java:25: * A {@link org.apache.hadoop.io.compress.CompressorStream} which works
./src/core/org/apache/hadoop/io/compress/BlockCompressorStream.java:31: * {@link org.apache.hadoop.io.compress.Compressor} requires buffering to
./src/core/org/apache/hadoop/io/compress/BlockCompressorStream.java:41:   * Create a {@link BlockCompressorStream}.
./src/core/org/apache/hadoop/io/compress/BlockCompressorStream.java:43:   * @param out stream
./src/core/org/apache/hadoop/io/compress/BlockCompressorStream.java:44:   * @param compressor compressor to be used
./src/core/org/apache/hadoop/io/compress/BlockCompressorStream.java:45:   * @param bufferSize size of buffer
./src/core/org/apache/hadoop/io/compress/BlockCompressorStream.java:46:   * @param compressionOverhead maximum 'overhead' of the compression 
./src/core/org/apache/hadoop/io/compress/BlockCompressorStream.java:56:   * Create a {@link BlockCompressorStream} with given output-stream and 
./src/core/org/apache/hadoop/io/compress/BlockCompressorStream.java:61:   * @param out stream
./src/core/org/apache/hadoop/io/compress/BlockCompressorStream.java:62:   * @param compressor compressor to be used
./src/core/org/apache/hadoop/io/compress/BlockDecompressorStream.java:26: * A {@link org.apache.hadoop.io.compress.DecompressorStream} which works
./src/core/org/apache/hadoop/io/compress/BlockDecompressorStream.java:36:   * Create a {@link BlockDecompressorStream}.
./src/core/org/apache/hadoop/io/compress/BlockDecompressorStream.java:38:   * @param in input stream
./src/core/org/apache/hadoop/io/compress/BlockDecompressorStream.java:39:   * @param decompressor decompressor to use
./src/core/org/apache/hadoop/io/compress/BlockDecompressorStream.java:40:   * @param bufferSize size of buffer
./src/core/org/apache/hadoop/io/compress/BlockDecompressorStream.java:48:   * Create a {@link BlockDecompressorStream}.
./src/core/org/apache/hadoop/io/compress/BlockDecompressorStream.java:50:   * @param in input stream
./src/core/org/apache/hadoop/io/compress/BlockDecompressorStream.java:51:   * @param decompressor decompressor to use
./src/core/org/apache/hadoop/io/compress/bzip2/BZip2Constants.java:21: * <keiron@aftexsw.com> to whom the Ant project is very grateful for his
./src/core/org/apache/hadoop/io/compress/bzip2/CBZip2InputStream.java:21: * <keiron@aftexsw.com> to whom the Ant project is very grateful for his
./src/core/org/apache/hadoop/io/compress/bzip2/CBZip2InputStream.java:35: * {@link #close() close()} method as soon as possible, to force
./src/core/org/apache/hadoop/io/compress/bzip2/CBZip2InputStream.java:37: * {@link CBZip2OutputStream CBZip2OutputStream} for information about memory
./src/core/org/apache/hadoop/io/compress/bzip2/CBZip2InputStream.java:43: * the single byte {@link java.io.InputStream#read() read()} method exclusively.
./src/core/org/apache/hadoop/io/compress/bzip2/CBZip2InputStream.java:145:  * @throws IOException
./src/core/org/apache/hadoop/io/compress/bzip2/CBZip2InputStream.java:147:  * @throws NullPointerException
./src/core/org/apache/hadoop/io/compress/bzip2/CBZip2InputStream.java:950:    * Initializes the {@link #tt} array.
./src/core/org/apache/hadoop/io/compress/bzip2/CBZip2OutputStream.java:21: * <keiron@aftexsw.com> to whom the Ant project is very grateful for his
./src/core/org/apache/hadoop/io/compress/bzip2/CBZip2OutputStream.java:36: * {@link #close() close()} method as soon as possible, to force
./src/core/org/apache/hadoop/io/compress/bzip2/CBZip2OutputStream.java:56: * To get the memory required for decompression by {@link CBZip2InputStream
./src/core/org/apache/hadoop/io/compress/bzip2/CBZip2OutputStream.java:562:  * @return The blocksize, between {@link #MIN_BLOCKSIZE} and
./src/core/org/apache/hadoop/io/compress/bzip2/CBZip2OutputStream.java:563:  *         {@link #MAX_BLOCKSIZE} both inclusive. For a negative
./src/core/org/apache/hadoop/io/compress/bzip2/CBZip2OutputStream.java:567:  * @param inputLength
./src/core/org/apache/hadoop/io/compress/bzip2/CBZip2OutputStream.java:585:  * @param out *
./src/core/org/apache/hadoop/io/compress/bzip2/CBZip2OutputStream.java:588:  * @throws IOException
./src/core/org/apache/hadoop/io/compress/bzip2/CBZip2OutputStream.java:590:  * @throws NullPointerException
./src/core/org/apache/hadoop/io/compress/bzip2/CBZip2OutputStream.java:607:  * @param out
./src/core/org/apache/hadoop/io/compress/bzip2/CBZip2OutputStream.java:609:  * @param blockSize
./src/core/org/apache/hadoop/io/compress/bzip2/CBZip2OutputStream.java:612:  * @throws IOException
./src/core/org/apache/hadoop/io/compress/bzip2/CBZip2OutputStream.java:614:  * @throws IllegalArgumentException
./src/core/org/apache/hadoop/io/compress/bzip2/CBZip2OutputStream.java:616:  * @throws NullPointerException
./src/core/org/apache/hadoop/io/compress/bzip2/CBZip2OutputStream.java:619:  * @see #MIN_BLOCKSIZE
./src/core/org/apache/hadoop/io/compress/bzip2/CBZip2OutputStream.java:620:  * @see #MAX_BLOCKSIZE
./src/core/org/apache/hadoop/io/compress/bzip2/CRC.java:21: * <keiron@aftexsw.com> to whom the Ant project is very grateful for his
./src/core/org/apache/hadoop/io/compress/BZip2Codec.java:51:  * @param out
./src/core/org/apache/hadoop/io/compress/BZip2Codec.java:53:  * @return The BZip2 CompressionOutputStream
./src/core/org/apache/hadoop/io/compress/BZip2Codec.java:54:  * @throws java.io.IOException
./src/core/org/apache/hadoop/io/compress/BZip2Codec.java:65:   * @throws java.lang.UnsupportedOperationException
./src/core/org/apache/hadoop/io/compress/BZip2Codec.java:76:  * @throws java.lang.UnsupportedOperationException
./src/core/org/apache/hadoop/io/compress/BZip2Codec.java:86:  * @throws java.lang.UnsupportedOperationException
./src/core/org/apache/hadoop/io/compress/BZip2Codec.java:96:  * @param in
./src/core/org/apache/hadoop/io/compress/BZip2Codec.java:98:  * @return Returns CompressionInputStream for BZip2
./src/core/org/apache/hadoop/io/compress/BZip2Codec.java:99:  * @throws java.io.IOException
./src/core/org/apache/hadoop/io/compress/BZip2Codec.java:110:  * @throws java.lang.UnsupportedOperationException
./src/core/org/apache/hadoop/io/compress/BZip2Codec.java:122:  * @throws java.lang.UnsupportedOperationException
./src/core/org/apache/hadoop/io/compress/BZip2Codec.java:132:  * @throws java.lang.UnsupportedOperationException
./src/core/org/apache/hadoop/io/compress/BZip2Codec.java:142:  * @return A String telling the default bzip2 file extension
./src/core/org/apache/hadoop/io/compress/CodecPool.java:89:   * Get a {@link Compressor} for the given {@link CompressionCodec} from the 
./src/core/org/apache/hadoop/io/compress/CodecPool.java:92:   * @param codec the <code>CompressionCodec</code> for which to get the 
./src/core/org/apache/hadoop/io/compress/CodecPool.java:94:   * @return <code>Compressor</code> for the given 
./src/core/org/apache/hadoop/io/compress/CodecPool.java:109:   * Get a {@link Decompressor} for the given {@link CompressionCodec} from the
./src/core/org/apache/hadoop/io/compress/CodecPool.java:112:   * @param codec the <code>CompressionCodec</code> for which to get the 
./src/core/org/apache/hadoop/io/compress/CodecPool.java:114:   * @return <code>Decompressor</code> for the given 
./src/core/org/apache/hadoop/io/compress/CodecPool.java:129:   * Return the {@link Compressor} to the pool.
./src/core/org/apache/hadoop/io/compress/CodecPool.java:131:   * @param compressor the <code>Compressor</code> to be returned to the pool
./src/core/org/apache/hadoop/io/compress/CodecPool.java:142:   * Return the {@link Decompressor} to the pool.
./src/core/org/apache/hadoop/io/compress/CodecPool.java:144:   * @param decompressor the <code>Decompressor</code> to be returned to the 
./src/core/org/apache/hadoop/io/compress/CompressionCodec.java:31:   * Create a {@link CompressionOutputStream} that will write to the given 
./src/core/org/apache/hadoop/io/compress/CompressionCodec.java:32:   * {@link OutputStream}.
./src/core/org/apache/hadoop/io/compress/CompressionCodec.java:34:   * @param out the location for the final output stream
./src/core/org/apache/hadoop/io/compress/CompressionCodec.java:35:   * @return a stream the user can write uncompressed data to have it compressed
./src/core/org/apache/hadoop/io/compress/CompressionCodec.java:36:   * @throws IOException
./src/core/org/apache/hadoop/io/compress/CompressionCodec.java:42:   * Create a {@link CompressionOutputStream} that will write to the given 
./src/core/org/apache/hadoop/io/compress/CompressionCodec.java:43:   * {@link OutputStream} with the given {@link Compressor}.
./src/core/org/apache/hadoop/io/compress/CompressionCodec.java:45:   * @param out the location for the final output stream
./src/core/org/apache/hadoop/io/compress/CompressionCodec.java:46:   * @param compressor compressor to use
./src/core/org/apache/hadoop/io/compress/CompressionCodec.java:47:   * @return a stream the user can write uncompressed data to have it compressed
./src/core/org/apache/hadoop/io/compress/CompressionCodec.java:48:   * @throws IOException
./src/core/org/apache/hadoop/io/compress/CompressionCodec.java:55:   * Get the type of {@link Compressor} needed by this {@link CompressionCodec}.
./src/core/org/apache/hadoop/io/compress/CompressionCodec.java:57:   * @return the type of compressor needed by this codec.
./src/core/org/apache/hadoop/io/compress/CompressionCodec.java:62:   * Create a new {@link Compressor} for use by this {@link CompressionCodec}.
./src/core/org/apache/hadoop/io/compress/CompressionCodec.java:64:   * @return a new compressor for use by this codec
./src/core/org/apache/hadoop/io/compress/CompressionCodec.java:71:   * @param in the stream to read compressed bytes from
./src/core/org/apache/hadoop/io/compress/CompressionCodec.java:72:   * @return a stream to read uncompressed bytes from
./src/core/org/apache/hadoop/io/compress/CompressionCodec.java:73:   * @throws IOException
./src/core/org/apache/hadoop/io/compress/CompressionCodec.java:78:   * Create a {@link CompressionInputStream} that will read from the given 
./src/core/org/apache/hadoop/io/compress/CompressionCodec.java:79:   * {@link InputStream} with the given {@link Decompressor}.
./src/core/org/apache/hadoop/io/compress/CompressionCodec.java:81:   * @param in the stream to read compressed bytes from
./src/core/org/apache/hadoop/io/compress/CompressionCodec.java:82:   * @param decompressor decompressor to use
./src/core/org/apache/hadoop/io/compress/CompressionCodec.java:83:   * @return a stream to read uncompressed bytes from
./src/core/org/apache/hadoop/io/compress/CompressionCodec.java:84:   * @throws IOException
./src/core/org/apache/hadoop/io/compress/CompressionCodec.java:92:   * Get the type of {@link Decompressor} needed by this {@link CompressionCodec}.
./src/core/org/apache/hadoop/io/compress/CompressionCodec.java:94:   * @return the type of decompressor needed by this codec.
./src/core/org/apache/hadoop/io/compress/CompressionCodec.java:99:   * Create a new {@link Decompressor} for use by this {@link CompressionCodec}.
./src/core/org/apache/hadoop/io/compress/CompressionCodec.java:101:   * @return a new decompressor for use by this codec
./src/core/org/apache/hadoop/io/compress/CompressionCodec.java:107:   * @return the extension including the '.'
./src/core/org/apache/hadoop/io/compress/CompressionCodecFactory.java:75:   * @param conf the configuration to look in
./src/core/org/apache/hadoop/io/compress/CompressionCodecFactory.java:76:   * @return a list of the Configuration classes or null if the attribute
./src/core/org/apache/hadoop/io/compress/CompressionCodecFactory.java:110:   * @param conf the configuration to modify
./src/core/org/apache/hadoop/io/compress/CompressionCodecFactory.java:111:   * @param classes the list of classes to set
./src/core/org/apache/hadoop/io/compress/CompressionCodecFactory.java:150:   * @param file the filename to check
./src/core/org/apache/hadoop/io/compress/CompressionCodecFactory.java:151:   * @return the codec object
./src/core/org/apache/hadoop/io/compress/CompressionCodecFactory.java:172:   * @param filename the filename to strip
./src/core/org/apache/hadoop/io/compress/CompressionCodecFactory.java:173:   * @param suffix the suffix to remove
./src/core/org/apache/hadoop/io/compress/CompressionCodecFactory.java:174:   * @return the shortened filename
./src/core/org/apache/hadoop/io/compress/CompressionCodecFactory.java:185:   * @param args
./src/core/org/apache/hadoop/io/compress/CompressionInputStream.java:28: * reposition the underlying input stream then call {@link #resetState()},
./src/core/org/apache/hadoop/io/compress/CompressionInputStream.java:41:   * @param in The input stream to be compressed.
./src/core/org/apache/hadoop/io/compress/CompressionOutputStream.java:36:   * @param out
./src/core/org/apache/hadoop/io/compress/Compressor.java:25: * plugged into a {@link CompressionOutputStream} to compress data.
./src/core/org/apache/hadoop/io/compress/Compressor.java:26: * This is modelled after {@link java.util.zip.Deflater}
./src/core/org/apache/hadoop/io/compress/Compressor.java:35:   * @param b Input data
./src/core/org/apache/hadoop/io/compress/Compressor.java:36:   * @param off Start offset
./src/core/org/apache/hadoop/io/compress/Compressor.java:37:   * @param len Length
./src/core/org/apache/hadoop/io/compress/Compressor.java:45:   * @return <code>true</code> if the input data buffer is empty and 
./src/core/org/apache/hadoop/io/compress/Compressor.java:54:   * @param b Dictionary data bytes
./src/core/org/apache/hadoop/io/compress/Compressor.java:55:   * @param off Start offset
./src/core/org/apache/hadoop/io/compress/Compressor.java:56:   * @param len Length
./src/core/org/apache/hadoop/io/compress/Compressor.java:79:   * @return <code>true</code> if the end of the compressed
./src/core/org/apache/hadoop/io/compress/Compressor.java:90:   * @param b Buffer for the compressed data
./src/core/org/apache/hadoop/io/compress/Compressor.java:91:   * @param off Start offset of the data
./src/core/org/apache/hadoop/io/compress/Compressor.java:92:   * @param len Size of the buffer
./src/core/org/apache/hadoop/io/compress/Compressor.java:93:   * @return The actual number of bytes of compressed data.
./src/core/org/apache/hadoop/io/compress/CompressorStream.java:52:   * @param out Underlying output stream.
./src/core/org/apache/hadoop/io/compress/Decompressor.java:25: * plugged into a {@link CompressionInputStream} to compress data.
./src/core/org/apache/hadoop/io/compress/Decompressor.java:26: * This is modelled after {@link java.util.zip.Inflater}
./src/core/org/apache/hadoop/io/compress/Decompressor.java:35:   * @param b Input data
./src/core/org/apache/hadoop/io/compress/Decompressor.java:36:   * @param off Start offset
./src/core/org/apache/hadoop/io/compress/Decompressor.java:37:   * @param len Length
./src/core/org/apache/hadoop/io/compress/Decompressor.java:45:   * @return <code>true</code> if the input data buffer is empty and 
./src/core/org/apache/hadoop/io/compress/Decompressor.java:54:   * @param b Dictionary data bytes
./src/core/org/apache/hadoop/io/compress/Decompressor.java:55:   * @param off Start offset
./src/core/org/apache/hadoop/io/compress/Decompressor.java:56:   * @param len Length
./src/core/org/apache/hadoop/io/compress/Decompressor.java:62:   * @return <code>true</code> if a preset dictionary is needed for decompression
./src/core/org/apache/hadoop/io/compress/Decompressor.java:69:   * @return <code>true</code> if the end of the compressed
./src/core/org/apache/hadoop/io/compress/Decompressor.java:80:   * @param b Buffer for the compressed data
./src/core/org/apache/hadoop/io/compress/Decompressor.java:81:   * @param off Start offset of the data
./src/core/org/apache/hadoop/io/compress/Decompressor.java:82:   * @param len Size of the buffer
./src/core/org/apache/hadoop/io/compress/Decompressor.java:83:   * @return The actual number of bytes of compressed data.
./src/core/org/apache/hadoop/io/compress/Decompressor.java:84:   * @throws IOException
./src/core/org/apache/hadoop/io/compress/DecompressorStream.java:53:   * @param in Underlying input stream.
./src/core/org/apache/hadoop/io/compress/GzipCodec.java:55:     * @param out the Deflater stream to use
./src/core/org/apache/hadoop/io/compress/lzo/LzoCompressor.java:31: * A {@link Compressor} based on the lzo algorithm.
./src/core/org/apache/hadoop/io/compress/lzo/LzoCompressor.java:178:   * @return <code>true</code> if lzo compressors are loaded & initialized,
./src/core/org/apache/hadoop/io/compress/lzo/LzoCompressor.java:186:   * Creates a new compressor using the specified {@link CompressionStrategy}.
./src/core/org/apache/hadoop/io/compress/lzo/LzoCompressor.java:188:   * @param strategy lzo compression algorithm to use
./src/core/org/apache/hadoop/io/compress/lzo/LzoCompressor.java:189:   * @param directBufferSize size of the direct buffer to be used.
./src/core/org/apache/hadoop/io/compress/lzo/LzoCompressor.java:199:     * Initialize {@link #lzoCompress} and {@link #workingMemoryBufLen}
./src/core/org/apache/hadoop/io/compress/lzo/LzoCompressor.java:257:  /** {@inheritDoc} */
./src/core/org/apache/hadoop/io/compress/lzo/LzoDecompressor.java:31: * A {@link Decompressor} based on the lzo algorithm.
./src/core/org/apache/hadoop/io/compress/lzo/LzoDecompressor.java:155:   * @return <code>true</code> if lzo decompressors are loaded & initialized,
./src/core/org/apache/hadoop/io/compress/lzo/LzoDecompressor.java:165:   * @param strategy lzo decompression algorithm
./src/core/org/apache/hadoop/io/compress/lzo/LzoDecompressor.java:166:   * @param directBufferSize size of the direct-buffer
./src/core/org/apache/hadoop/io/compress/lzo/LzoDecompressor.java:177:     * Initialize {@link #lzoDecompress}
./src/core/org/apache/hadoop/io/compress/LzoCodec.java:33: * A {@link org.apache.hadoop.io.compress.CompressionCodec} for a streaming
./src/core/org/apache/hadoop/io/compress/LzoCodec.java:72:   * @param conf configuration
./src/core/org/apache/hadoop/io/compress/LzoCodec.java:73:   * @return <code>true</code> if native-lzo library is loaded & initialized;
./src/core/org/apache/hadoop/io/compress/LzoCodec.java:193:   * @return the extension including the '.'
./src/core/org/apache/hadoop/io/compress/LzopCodec.java:44: * A {@link org.apache.hadoop.io.compress.CompressionCodec} for a streaming
./src/core/org/apache/hadoop/io/compress/zlib/ZlibCompressor.java:29: * A {@link Compressor} based on the popular 
./src/core/org/apache/hadoop/io/compress/zlib/ZlibCompressor.java:185:   * @param level Compression level #CompressionLevel
./src/core/org/apache/hadoop/io/compress/zlib/ZlibCompressor.java:186:   * @param strategy Compression strategy #CompressionStrategy
./src/core/org/apache/hadoop/io/compress/zlib/ZlibCompressor.java:187:   * @param header Compression header #CompressionHeader
./src/core/org/apache/hadoop/io/compress/zlib/ZlibCompressor.java:188:   * @param directBufferSize Size of the direct buffer to be used.
./src/core/org/apache/hadoop/io/compress/zlib/ZlibCompressor.java:328:   * @return the total (non-negative) number of compressed bytes output so far
./src/core/org/apache/hadoop/io/compress/zlib/ZlibCompressor.java:338:   * @return the total (non-negative) number of uncompressed bytes input so far
./src/core/org/apache/hadoop/io/compress/zlib/ZlibDecompressor.java:29: * A {@link Decompressor} based on the popular 
./src/core/org/apache/hadoop/io/compress/zlib/ZlibDecompressor.java:234:   * @return the total (non-negative) number of compressed bytes output so far
./src/core/org/apache/hadoop/io/compress/zlib/ZlibDecompressor.java:244:   * @return the total (non-negative) number of uncompressed bytes input so far
./src/core/org/apache/hadoop/io/compress/zlib/ZlibFactory.java:56:   * @param conf configuration
./src/core/org/apache/hadoop/io/compress/zlib/ZlibFactory.java:57:   * @return <code>true</code> if native-zlib is loaded & initialized 
./src/core/org/apache/hadoop/io/compress/zlib/ZlibFactory.java:67:   * @param conf configuration
./src/core/org/apache/hadoop/io/compress/zlib/ZlibFactory.java:68:   * @return the appropriate type of the zlib compressor.
./src/core/org/apache/hadoop/io/compress/zlib/ZlibFactory.java:79:   * @param conf configuration
./src/core/org/apache/hadoop/io/compress/zlib/ZlibFactory.java:80:   * @return the appropriate implementation of the zlib compressor.
./src/core/org/apache/hadoop/io/compress/zlib/ZlibFactory.java:90:   * @param conf configuration
./src/core/org/apache/hadoop/io/compress/zlib/ZlibFactory.java:91:   * @return the appropriate type of the zlib decompressor.
./src/core/org/apache/hadoop/io/compress/zlib/ZlibFactory.java:102:   * @param conf configuration
./src/core/org/apache/hadoop/io/compress/zlib/ZlibFactory.java:103:   * @return the appropriate implementation of the zlib decompressor.
./src/core/org/apache/hadoop/io/CompressedWritable.java:64:  /** Subclasses implement this instead of {@link #readFields(DataInput)}. */
./src/core/org/apache/hadoop/io/CompressedWritable.java:83:  /** Subclasses implement this instead of {@link #write(DataOutput)}. */
./src/core/org/apache/hadoop/io/DataInputBuffer.java:23:/** A reusable {@link DataInput} implementation that reads from an in-memory
./src/core/org/apache/hadoop/io/DataOutputBuffer.java:23:/** A reusable {@link DataOutput} implementation that writes to an in-memory
./src/core/org/apache/hadoop/io/DataOutputBuffer.java:85:   *  Data is only valid to {@link #getLength()}.
./src/core/org/apache/hadoop/io/DefaultStringifier.java:34: * DefaultStringifier is the default implementation of the {@link Stringifier}
./src/core/org/apache/hadoop/io/DefaultStringifier.java:36: * serialized version of the objects. The {@link Serializer} and
./src/core/org/apache/hadoop/io/DefaultStringifier.java:37: * {@link Deserializer} are obtained from the {@link SerializationFactory}.
./src/core/org/apache/hadoop/io/DefaultStringifier.java:42: * @param <T> the class of the objects to stringify
./src/core/org/apache/hadoop/io/DefaultStringifier.java:100:   * @param <K>  the class of the item
./src/core/org/apache/hadoop/io/DefaultStringifier.java:101:   * @param conf the configuration to store
./src/core/org/apache/hadoop/io/DefaultStringifier.java:102:   * @param item the object to be stored
./src/core/org/apache/hadoop/io/DefaultStringifier.java:103:   * @param keyName the name of the key to use
./src/core/org/apache/hadoop/io/DefaultStringifier.java:104:   * @throws IOException : forwards Exceptions from the underlying 
./src/core/org/apache/hadoop/io/DefaultStringifier.java:105:   * {@link Serialization} classes. 
./src/core/org/apache/hadoop/io/DefaultStringifier.java:119:   * @param <K> the class of the item
./src/core/org/apache/hadoop/io/DefaultStringifier.java:120:   * @param conf the configuration to use
./src/core/org/apache/hadoop/io/DefaultStringifier.java:121:   * @param keyName the name of the key to use
./src/core/org/apache/hadoop/io/DefaultStringifier.java:122:   * @param itemClass the class of the item
./src/core/org/apache/hadoop/io/DefaultStringifier.java:123:   * @return restored object
./src/core/org/apache/hadoop/io/DefaultStringifier.java:124:   * @throws IOException : forwards Exceptions from the underlying 
./src/core/org/apache/hadoop/io/DefaultStringifier.java:125:   * {@link Serialization} classes.
./src/core/org/apache/hadoop/io/DefaultStringifier.java:142:   * @param <K> the class of the item
./src/core/org/apache/hadoop/io/DefaultStringifier.java:143:   * @param conf the configuration to use 
./src/core/org/apache/hadoop/io/DefaultStringifier.java:144:   * @param items the objects to be stored
./src/core/org/apache/hadoop/io/DefaultStringifier.java:145:   * @param keyName the name of the key to use
./src/core/org/apache/hadoop/io/DefaultStringifier.java:146:   * @throws IndexOutOfBoundsException if the items array is empty
./src/core/org/apache/hadoop/io/DefaultStringifier.java:147:   * @throws IOException : forwards Exceptions from the underlying 
./src/core/org/apache/hadoop/io/DefaultStringifier.java:148:   * {@link Serialization} classes.         
./src/core/org/apache/hadoop/io/DefaultStringifier.java:170:   * @param <K> the class of the item
./src/core/org/apache/hadoop/io/DefaultStringifier.java:171:   * @param conf the configuration to use
./src/core/org/apache/hadoop/io/DefaultStringifier.java:172:   * @param keyName the name of the key to use
./src/core/org/apache/hadoop/io/DefaultStringifier.java:173:   * @param itemClass the class of the item
./src/core/org/apache/hadoop/io/DefaultStringifier.java:174:   * @return restored object
./src/core/org/apache/hadoop/io/DefaultStringifier.java:175:   * @throws IOException : forwards Exceptions from the underlying 
./src/core/org/apache/hadoop/io/DefaultStringifier.java:176:   * {@link Serialization} classes.
./src/core/org/apache/hadoop/io/GenericWritable.java:44: * Generic Writable implements {@link Configurable} interface, so that it will be 
./src/core/org/apache/hadoop/io/GenericWritable.java:46: * implementing {@link Configurable} interface <i>before deserialization</i>. 
./src/core/org/apache/hadoop/io/GenericWritable.java:74: * @since Nov 8, 2006
./src/core/org/apache/hadoop/io/GenericWritable.java:89:   * @param obj
./src/core/org/apache/hadoop/io/InputBuffer.java:24:/** A reusable {@link InputStream} implementation that reads from an in-memory
./src/core/org/apache/hadoop/io/InputBuffer.java:40: * @see DataInputBuffer
./src/core/org/apache/hadoop/io/InputBuffer.java:41: * @see DataOutput
./src/core/org/apache/hadoop/io/MapFile.java:38: * {@link Writer#getIndexInterval()}.
./src/core/org/apache/hadoop/io/MapFile.java:46: * a new file.  Sorting large change lists can be done with {@link
./src/core/org/apache/hadoop/io/MapFile.java:170:     * @see #getIndexInterval()
./src/core/org/apache/hadoop/io/MapFile.java:175:     * @see #getIndexInterval()
./src/core/org/apache/hadoop/io/MapFile.java:266:     * @see #createDataFileReader(FileSystem, Path, Configuration)
./src/core/org/apache/hadoop/io/MapFile.java:298:     * {@link SequenceFile.Reader} returned.
./src/core/org/apache/hadoop/io/MapFile.java:364:     * @throws IOException
./src/core/org/apache/hadoop/io/MapFile.java:379:     * @param key key to read into
./src/core/org/apache/hadoop/io/MapFile.java:411:     * @return  0   - exact match found
./src/core/org/apache/hadoop/io/MapFile.java:425:     * @param before - IF true, and <code>key</code> does not exist, position
./src/core/org/apache/hadoop/io/MapFile.java:428:     * @return  0   - exact match found
./src/core/org/apache/hadoop/io/MapFile.java:535:-     * @param key       - key that we're trying to find
./src/core/org/apache/hadoop/io/MapFile.java:536:-     * @param val       - data value if key is found
./src/core/org/apache/hadoop/io/MapFile.java:537:-     * @return          - the key that was the closest match or null if eof.
./src/core/org/apache/hadoop/io/MapFile.java:548:     * @param key       - key that we're trying to find
./src/core/org/apache/hadoop/io/MapFile.java:549:     * @param val       - data value if key is found
./src/core/org/apache/hadoop/io/MapFile.java:550:     * @param before    - IF true, and <code>key</code> does not exist, return
./src/core/org/apache/hadoop/io/MapFile.java:553:     * @return          - the key that was the closest match or null if eof.
./src/core/org/apache/hadoop/io/MapFile.java:606:   * @param fs filesystem
./src/core/org/apache/hadoop/io/MapFile.java:607:   * @param dir directory containing the MapFile data and index
./src/core/org/apache/hadoop/io/MapFile.java:608:   * @param keyClass key class (has to be a subclass of Writable)
./src/core/org/apache/hadoop/io/MapFile.java:609:   * @param valueClass value class (has to be a subclass of Writable)
./src/core/org/apache/hadoop/io/MapFile.java:610:   * @param dryrun do not perform any changes, just report what needs to be done
./src/core/org/apache/hadoop/io/MapFile.java:611:   * @return number of valid entries in this MapFile, or -1 if no fixing was needed
./src/core/org/apache/hadoop/io/MapFile.java:612:   * @throws Exception
./src/core/org/apache/hadoop/io/MapWritable.java:49:   * @param other the map to copy from
./src/core/org/apache/hadoop/io/MapWritable.java:56:  /** {@inheritDoc} */
./src/core/org/apache/hadoop/io/MapWritable.java:61:  /** {@inheritDoc} */
./src/core/org/apache/hadoop/io/MapWritable.java:66:  /** {@inheritDoc} */
./src/core/org/apache/hadoop/io/MapWritable.java:71:  /** {@inheritDoc} */
./src/core/org/apache/hadoop/io/MapWritable.java:76:  /** {@inheritDoc} */
./src/core/org/apache/hadoop/io/MapWritable.java:81:  /** {@inheritDoc} */
./src/core/org/apache/hadoop/io/MapWritable.java:86:  /** {@inheritDoc} */
./src/core/org/apache/hadoop/io/MapWritable.java:91:  /** {@inheritDoc} */
./src/core/org/apache/hadoop/io/MapWritable.java:92:  @SuppressWarnings("unchecked")
./src/core/org/apache/hadoop/io/MapWritable.java:99:  /** {@inheritDoc} */
./src/core/org/apache/hadoop/io/MapWritable.java:106:  /** {@inheritDoc} */
./src/core/org/apache/hadoop/io/MapWritable.java:111:  /** {@inheritDoc} */
./src/core/org/apache/hadoop/io/MapWritable.java:116:  /** {@inheritDoc} */
./src/core/org/apache/hadoop/io/MapWritable.java:123:  /** {@inheritDoc} */
./src/core/org/apache/hadoop/io/MapWritable.java:124:  @Override
./src/core/org/apache/hadoop/io/MapWritable.java:142:  /** {@inheritDoc} */
./src/core/org/apache/hadoop/io/MapWritable.java:143:  @SuppressWarnings("unchecked")
./src/core/org/apache/hadoop/io/MapWritable.java:144:  @Override
./src/core/org/apache/hadoop/io/MD5Hash.java:133:   * @return the first 4 bytes of the md5
./src/core/org/apache/hadoop/io/MultipleIOException.java:23:/** Encapsulate a list of {@link IOException} into an {@link IOException} */
./src/core/org/apache/hadoop/io/MultipleIOException.java:25:  /** Require by {@link java.io.Serializable} */
./src/core/org/apache/hadoop/io/MultipleIOException.java:30:  /** Constructor is private, use {@link #createIOException(List)}. */
./src/core/org/apache/hadoop/io/MultipleIOException.java:36:  /** @return the underlying exceptions */
./src/core/org/apache/hadoop/io/MultipleIOException.java:39:  /** A convenient method to create an {@link IOException}. */
./src/core/org/apache/hadoop/io/ObjectWritable.java:109:  /** Write a {@link Writable}, {@link String}, primitive type, or an array of
./src/core/org/apache/hadoop/io/ObjectWritable.java:167:  /** Read a {@link Writable}, {@link String}, primitive type, or an array of
./src/core/org/apache/hadoop/io/ObjectWritable.java:174:  /** Read a {@link Writable}, {@link String}, primitive type, or an array of
./src/core/org/apache/hadoop/io/ObjectWritable.java:176:  @SuppressWarnings("unchecked")
./src/core/org/apache/hadoop/io/OutputBuffer.java:23:/** A reusable {@link OutputStream} implementation that writes to an in-memory
./src/core/org/apache/hadoop/io/OutputBuffer.java:40: * @see DataOutputBuffer
./src/core/org/apache/hadoop/io/OutputBuffer.java:41: * @see InputBuffer
./src/core/org/apache/hadoop/io/OutputBuffer.java:75:   *  Data is only valid to {@link #getLength()}.
./src/core/org/apache/hadoop/io/RawComparator.java:27: * A {@link Comparator} that operates directly on byte representations of
./src/core/org/apache/hadoop/io/RawComparator.java:30: * @param <T>
./src/core/org/apache/hadoop/io/RawComparator.java:31: * @see DeserializerComparator
./src/core/org/apache/hadoop/io/retry/RetryPolicies.java:31: * A collection of useful implementations of {@link RetryPolicy}.
./src/core/org/apache/hadoop/io/retry/RetryPolicies.java:173:    @Override
./src/core/org/apache/hadoop/io/retry/RetryPolicies.java:190:    @Override
./src/core/org/apache/hadoop/io/retry/RetryPolicies.java:253:    @Override
./src/core/org/apache/hadoop/io/retry/RetryPolicy.java:34:   * @param e The exception that caused the method to fail.
./src/core/org/apache/hadoop/io/retry/RetryPolicy.java:35:   * @param retries The number of times the method has been retried.
./src/core/org/apache/hadoop/io/retry/RetryPolicy.java:36:   * @return <code>true</code> if the method should be retried,
./src/core/org/apache/hadoop/io/retry/RetryPolicy.java:39:   * @throws Exception The re-thrown exception <code>e</code> indicating
./src/core/org/apache/hadoop/io/retry/RetryProxy.java:34:   * @param iface the interface that the retry will implement
./src/core/org/apache/hadoop/io/retry/RetryProxy.java:35:   * @param implementation the instance whose methods should be retried
./src/core/org/apache/hadoop/io/retry/RetryProxy.java:36:   * @param retryPolicy the policy for retirying method call failures
./src/core/org/apache/hadoop/io/retry/RetryProxy.java:37:   * @return the retry proxy
./src/core/org/apache/hadoop/io/retry/RetryProxy.java:53:   * {@link RetryPolicies#TRY_ONCE_THEN_FAIL} is used.
./src/core/org/apache/hadoop/io/retry/RetryProxy.java:55:   * @param iface the interface that the retry will implement
./src/core/org/apache/hadoop/io/retry/RetryProxy.java:56:   * @param implementation the instance whose methods should be retried
./src/core/org/apache/hadoop/io/retry/RetryProxy.java:57:   * @param methodNameToPolicyMap a map of method names to retry policies
./src/core/org/apache/hadoop/io/retry/RetryProxy.java:58:   * @return the retry proxy
./src/core/org/apache/hadoop/io/SequenceFile.java:51: * <p><code>SequenceFile</code> provides {@link Writer}, {@link Reader} and
./src/core/org/apache/hadoop/io/SequenceFile.java:52: * {@link Sorter} classes for writing, reading and sorting respectively.</p>
./src/core/org/apache/hadoop/io/SequenceFile.java:55: * {@link CompressionType} used to compress key/value pairs:
./src/core/org/apache/hadoop/io/SequenceFile.java:72: * specified by using the appropriate {@link CompressionCodec}.</p>
./src/core/org/apache/hadoop/io/SequenceFile.java:77: * <p>The {@link Reader} acts as the bridge and can read any of the above 
./src/core/org/apache/hadoop/io/SequenceFile.java:112: *   metadata - {@link Metadata} for this file.
./src/core/org/apache/hadoop/io/SequenceFile.java:184: * @see CompressionCodec
./src/core/org/apache/hadoop/io/SequenceFile.java:207:   * {@link SequenceFile}.
./src/core/org/apache/hadoop/io/SequenceFile.java:209:   * @see SequenceFile.Writer
./src/core/org/apache/hadoop/io/SequenceFile.java:222:   * @param job the job config to look in
./src/core/org/apache/hadoop/io/SequenceFile.java:223:   * @return the kind of compression to use
./src/core/org/apache/hadoop/io/SequenceFile.java:224:   * @deprecated Use 
./src/core/org/apache/hadoop/io/SequenceFile.java:225:   *             {@link org.apache.hadoop.mapred.SequenceFileOutputFormat#getOutputCompressionType(org.apache.hadoop.mapred.JobConf)} 
./src/core/org/apache/hadoop/io/SequenceFile.java:226:   *             to get {@link CompressionType} for job-outputs.
./src/core/org/apache/hadoop/io/SequenceFile.java:228:  @Deprecated
./src/core/org/apache/hadoop/io/SequenceFile.java:237:   * @param job the configuration to modify
./src/core/org/apache/hadoop/io/SequenceFile.java:238:   * @param val the new compression type (none, block, record)
./src/core/org/apache/hadoop/io/SequenceFile.java:239:   * @deprecated Use the one of the many SequenceFile.createWriter methods to specify
./src/core/org/apache/hadoop/io/SequenceFile.java:240:   *             the {@link CompressionType} while creating the {@link SequenceFile} or
./src/core/org/apache/hadoop/io/SequenceFile.java:241:   *             {@link org.apache.hadoop.mapred.SequenceFileOutputFormat#setOutputCompressionType(org.apache.hadoop.mapred.JobConf, org.apache.hadoop.io.SequenceFile.CompressionType)}
./src/core/org/apache/hadoop/io/SequenceFile.java:242:   *             to specify the {@link CompressionType} for job-outputs. 
./src/core/org/apache/hadoop/io/SequenceFile.java:245:  @Deprecated
./src/core/org/apache/hadoop/io/SequenceFile.java:253:   * @param fs The configured filesystem. 
./src/core/org/apache/hadoop/io/SequenceFile.java:254:   * @param conf The configuration.
./src/core/org/apache/hadoop/io/SequenceFile.java:255:   * @param name The name of the file. 
./src/core/org/apache/hadoop/io/SequenceFile.java:256:   * @param keyClass The 'key' type.
./src/core/org/apache/hadoop/io/SequenceFile.java:257:   * @param valClass The 'value' type.
./src/core/org/apache/hadoop/io/SequenceFile.java:258:   * @return Returns the handle to the constructed SequenceFile Writer.
./src/core/org/apache/hadoop/io/SequenceFile.java:259:   * @throws IOException
./src/core/org/apache/hadoop/io/SequenceFile.java:271:   * @param fs The configured filesystem. 
./src/core/org/apache/hadoop/io/SequenceFile.java:272:   * @param conf The configuration.
./src/core/org/apache/hadoop/io/SequenceFile.java:273:   * @param name The name of the file. 
./src/core/org/apache/hadoop/io/SequenceFile.java:274:   * @param keyClass The 'key' type.
./src/core/org/apache/hadoop/io/SequenceFile.java:275:   * @param valClass The 'value' type.
./src/core/org/apache/hadoop/io/SequenceFile.java:276:   * @param compressionType The compression type.
./src/core/org/apache/hadoop/io/SequenceFile.java:277:   * @return Returns the handle to the constructed SequenceFile Writer.
./src/core/org/apache/hadoop/io/SequenceFile.java:278:   * @throws IOException
./src/core/org/apache/hadoop/io/SequenceFile.java:292:   * @param fs The configured filesystem. 
./src/core/org/apache/hadoop/io/SequenceFile.java:293:   * @param conf The configuration.
./src/core/org/apache/hadoop/io/SequenceFile.java:294:   * @param name The name of the file. 
./src/core/org/apache/hadoop/io/SequenceFile.java:295:   * @param keyClass The 'key' type.
./src/core/org/apache/hadoop/io/SequenceFile.java:296:   * @param valClass The 'value' type.
./src/core/org/apache/hadoop/io/SequenceFile.java:297:   * @param compressionType The compression type.
./src/core/org/apache/hadoop/io/SequenceFile.java:298:   * @param progress The Progressable object to track progress.
./src/core/org/apache/hadoop/io/SequenceFile.java:299:   * @return Returns the handle to the constructed SequenceFile Writer.
./src/core/org/apache/hadoop/io/SequenceFile.java:300:   * @throws IOException
./src/core/org/apache/hadoop/io/SequenceFile.java:314:   * @param fs The configured filesystem. 
./src/core/org/apache/hadoop/io/SequenceFile.java:315:   * @param conf The configuration.
./src/core/org/apache/hadoop/io/SequenceFile.java:316:   * @param name The name of the file. 
./src/core/org/apache/hadoop/io/SequenceFile.java:317:   * @param keyClass The 'key' type.
./src/core/org/apache/hadoop/io/SequenceFile.java:318:   * @param valClass The 'value' type.
./src/core/org/apache/hadoop/io/SequenceFile.java:319:   * @param compressionType The compression type.
./src/core/org/apache/hadoop/io/SequenceFile.java:320:   * @param codec The compression codec.
./src/core/org/apache/hadoop/io/SequenceFile.java:321:   * @return Returns the handle to the constructed SequenceFile Writer.
./src/core/org/apache/hadoop/io/SequenceFile.java:322:   * @throws IOException
./src/core/org/apache/hadoop/io/SequenceFile.java:337:   * @param fs The configured filesystem. 
./src/core/org/apache/hadoop/io/SequenceFile.java:338:   * @param conf The configuration.
./src/core/org/apache/hadoop/io/SequenceFile.java:339:   * @param name The name of the file. 
./src/core/org/apache/hadoop/io/SequenceFile.java:340:   * @param keyClass The 'key' type.
./src/core/org/apache/hadoop/io/SequenceFile.java:341:   * @param valClass The 'value' type.
./src/core/org/apache/hadoop/io/SequenceFile.java:342:   * @param compressionType The compression type.
./src/core/org/apache/hadoop/io/SequenceFile.java:343:   * @param codec The compression codec.
./src/core/org/apache/hadoop/io/SequenceFile.java:344:   * @param progress The Progressable object to track progress.
./src/core/org/apache/hadoop/io/SequenceFile.java:345:   * @param metadata The metadata of the file.
./src/core/org/apache/hadoop/io/SequenceFile.java:346:   * @return Returns the handle to the constructed SequenceFile Writer.
./src/core/org/apache/hadoop/io/SequenceFile.java:347:   * @throws IOException
./src/core/org/apache/hadoop/io/SequenceFile.java:362:   * @param fs The configured filesystem.
./src/core/org/apache/hadoop/io/SequenceFile.java:363:   * @param conf The configuration.
./src/core/org/apache/hadoop/io/SequenceFile.java:364:   * @param name The name of the file.
./src/core/org/apache/hadoop/io/SequenceFile.java:365:   * @param keyClass The 'key' type.
./src/core/org/apache/hadoop/io/SequenceFile.java:366:   * @param valClass The 'value' type.
./src/core/org/apache/hadoop/io/SequenceFile.java:367:   * @param bufferSize buffer size for the underlaying outputstream.
./src/core/org/apache/hadoop/io/SequenceFile.java:368:   * @param replication replication factor for the file.
./src/core/org/apache/hadoop/io/SequenceFile.java:369:   * @param blockSize block size for the file.
./src/core/org/apache/hadoop/io/SequenceFile.java:370:   * @param compressionType The compression type.
./src/core/org/apache/hadoop/io/SequenceFile.java:371:   * @param codec The compression codec.
./src/core/org/apache/hadoop/io/SequenceFile.java:372:   * @param progress The Progressable object to track progress.
./src/core/org/apache/hadoop/io/SequenceFile.java:373:   * @param metadata The metadata of the file.
./src/core/org/apache/hadoop/io/SequenceFile.java:374:   * @return Returns the handle to the constructed SequenceFile Writer.
./src/core/org/apache/hadoop/io/SequenceFile.java:375:   * @throws IOException
./src/core/org/apache/hadoop/io/SequenceFile.java:411:   * @param fs The configured filesystem. 
./src/core/org/apache/hadoop/io/SequenceFile.java:412:   * @param conf The configuration.
./src/core/org/apache/hadoop/io/SequenceFile.java:413:   * @param name The name of the file. 
./src/core/org/apache/hadoop/io/SequenceFile.java:414:   * @param keyClass The 'key' type.
./src/core/org/apache/hadoop/io/SequenceFile.java:415:   * @param valClass The 'value' type.
./src/core/org/apache/hadoop/io/SequenceFile.java:416:   * @param compressionType The compression type.
./src/core/org/apache/hadoop/io/SequenceFile.java:417:   * @param codec The compression codec.
./src/core/org/apache/hadoop/io/SequenceFile.java:418:   * @param progress The Progressable object to track progress.
./src/core/org/apache/hadoop/io/SequenceFile.java:419:   * @return Returns the handle to the constructed SequenceFile Writer.
./src/core/org/apache/hadoop/io/SequenceFile.java:420:   * @throws IOException
./src/core/org/apache/hadoop/io/SequenceFile.java:434:   * @param out The stream on top which the writer is to be constructed.
./src/core/org/apache/hadoop/io/SequenceFile.java:435:   * @param keyClass The 'key' type.
./src/core/org/apache/hadoop/io/SequenceFile.java:436:   * @param valClass The 'value' type.
./src/core/org/apache/hadoop/io/SequenceFile.java:437:   * @param compress Compress data?
./src/core/org/apache/hadoop/io/SequenceFile.java:438:   * @param blockCompress Compress blocks?
./src/core/org/apache/hadoop/io/SequenceFile.java:439:   * @param metadata The metadata of the file.
./src/core/org/apache/hadoop/io/SequenceFile.java:440:   * @return Returns the handle to the constructed SequenceFile Writer.
./src/core/org/apache/hadoop/io/SequenceFile.java:441:   * @throws IOException
./src/core/org/apache/hadoop/io/SequenceFile.java:470:   * @param fs The configured filesystem. 
./src/core/org/apache/hadoop/io/SequenceFile.java:471:   * @param conf The configuration.
./src/core/org/apache/hadoop/io/SequenceFile.java:472:   * @param file The name of the file. 
./src/core/org/apache/hadoop/io/SequenceFile.java:473:   * @param keyClass The 'key' type.
./src/core/org/apache/hadoop/io/SequenceFile.java:474:   * @param valClass The 'value' type.
./src/core/org/apache/hadoop/io/SequenceFile.java:475:   * @param compress Compress data?
./src/core/org/apache/hadoop/io/SequenceFile.java:476:   * @param blockCompress Compress blocks?
./src/core/org/apache/hadoop/io/SequenceFile.java:477:   * @param codec The compression codec.
./src/core/org/apache/hadoop/io/SequenceFile.java:478:   * @param progress
./src/core/org/apache/hadoop/io/SequenceFile.java:479:   * @param metadata The metadata of the file.
./src/core/org/apache/hadoop/io/SequenceFile.java:480:   * @return Returns the handle to the constructed SequenceFile Writer.
./src/core/org/apache/hadoop/io/SequenceFile.java:481:   * @throws IOException
./src/core/org/apache/hadoop/io/SequenceFile.java:513:   * @param conf The configuration.
./src/core/org/apache/hadoop/io/SequenceFile.java:514:   * @param out The stream on top which the writer is to be constructed.
./src/core/org/apache/hadoop/io/SequenceFile.java:515:   * @param keyClass The 'key' type.
./src/core/org/apache/hadoop/io/SequenceFile.java:516:   * @param valClass The 'value' type.
./src/core/org/apache/hadoop/io/SequenceFile.java:517:   * @param compressionType The compression type.
./src/core/org/apache/hadoop/io/SequenceFile.java:518:   * @param codec The compression codec.
./src/core/org/apache/hadoop/io/SequenceFile.java:519:   * @param metadata The metadata of the file.
./src/core/org/apache/hadoop/io/SequenceFile.java:520:   * @return Returns the handle to the constructed SequenceFile Writer.
./src/core/org/apache/hadoop/io/SequenceFile.java:521:   * @throws IOException
./src/core/org/apache/hadoop/io/SequenceFile.java:550:   * @param conf The configuration.
./src/core/org/apache/hadoop/io/SequenceFile.java:551:   * @param out The stream on top which the writer is to be constructed.
./src/core/org/apache/hadoop/io/SequenceFile.java:552:   * @param keyClass The 'key' type.
./src/core/org/apache/hadoop/io/SequenceFile.java:553:   * @param valClass The 'value' type.
./src/core/org/apache/hadoop/io/SequenceFile.java:554:   * @param compressionType The compression type.
./src/core/org/apache/hadoop/io/SequenceFile.java:555:   * @param codec The compression codec.
./src/core/org/apache/hadoop/io/SequenceFile.java:556:   * @return Returns the handle to the constructed SequenceFile Writer.
./src/core/org/apache/hadoop/io/SequenceFile.java:557:   * @throws IOException
./src/core/org/apache/hadoop/io/SequenceFile.java:574:     * @param outStream : Stream to write uncompressed bytes into.
./src/core/org/apache/hadoop/io/SequenceFile.java:575:     * @throws IOException
./src/core/org/apache/hadoop/io/SequenceFile.java:582:     * @param outStream : Stream to write compressed bytes into.
./src/core/org/apache/hadoop/io/SequenceFile.java:808:        digester.update((new UID()+"@"+time).getBytes());
./src/core/org/apache/hadoop/io/SequenceFile.java:895:    @SuppressWarnings("unchecked")
./src/core/org/apache/hadoop/io/SequenceFile.java:981:    @SuppressWarnings("unchecked")
./src/core/org/apache/hadoop/io/SequenceFile.java:1034:     * immediately after calling {@link SequenceFile.Reader#seek(long)} with a position
./src/core/org/apache/hadoop/io/SequenceFile.java:1035:     * returned by this method, {@link SequenceFile.Reader#next(Writable)} may be called.  However
./src/core/org/apache/hadoop/io/SequenceFile.java:1108:    @SuppressWarnings("unchecked")
./src/core/org/apache/hadoop/io/SequenceFile.java:1294:    @SuppressWarnings("unchecked")
./src/core/org/apache/hadoop/io/SequenceFile.java:1433:     * {@link FSDataInputStream} returned.
./src/core/org/apache/hadoop/io/SequenceFile.java:1441:     * Initialize the {@link Reader}
./src/core/org/apache/hadoop/io/SequenceFile.java:1442:     * @param tmpReader <code>true</code> if we are constructing a temporary
./src/core/org/apache/hadoop/io/SequenceFile.java:1443:     *                  reader {@link SequenceFile.Sorter.cloneFileAttributes}, 
./src/core/org/apache/hadoop/io/SequenceFile.java:1446:     * @throws IOException
./src/core/org/apache/hadoop/io/SequenceFile.java:1561:    @SuppressWarnings("unchecked")
./src/core/org/apache/hadoop/io/SequenceFile.java:1738:     * @param val : The 'value' to be read.
./src/core/org/apache/hadoop/io/SequenceFile.java:1739:     * @throws IOException
./src/core/org/apache/hadoop/io/SequenceFile.java:1777:     * @param val : The 'value' to be read.
./src/core/org/apache/hadoop/io/SequenceFile.java:1778:     * @throws IOException
./src/core/org/apache/hadoop/io/SequenceFile.java:1815:    @SuppressWarnings("unchecked")
./src/core/org/apache/hadoop/io/SequenceFile.java:1888:     * @return the length of the next record or -1 if there is no next record
./src/core/org/apache/hadoop/io/SequenceFile.java:1889:     * @throws IOException
./src/core/org/apache/hadoop/io/SequenceFile.java:1917:    /** @deprecated Call {@link #nextRaw(DataOutputBuffer,SequenceFile.ValueBytes)}. */
./src/core/org/apache/hadoop/io/SequenceFile.java:1950:     * @param key - The buffer into which the key is read
./src/core/org/apache/hadoop/io/SequenceFile.java:1951:     * @param val - The 'raw' value
./src/core/org/apache/hadoop/io/SequenceFile.java:1952:     * @return Returns the total record length or -1 for end of file
./src/core/org/apache/hadoop/io/SequenceFile.java:1953:     * @throws IOException
./src/core/org/apache/hadoop/io/SequenceFile.java:2010:     * @param key - The buffer into which the key is read
./src/core/org/apache/hadoop/io/SequenceFile.java:2011:     * @return Returns the key length or -1 for end of file
./src/core/org/apache/hadoop/io/SequenceFile.java:2012:     * @throws IOException
./src/core/org/apache/hadoop/io/SequenceFile.java:2100:    @SuppressWarnings("unchecked")
./src/core/org/apache/hadoop/io/SequenceFile.java:2107:     * @param val - The 'raw' value
./src/core/org/apache/hadoop/io/SequenceFile.java:2108:     * @return Returns the value length
./src/core/org/apache/hadoop/io/SequenceFile.java:2109:     * @throws IOException
./src/core/org/apache/hadoop/io/SequenceFile.java:2150:     * <p>The position passed must be a position returned by {@link
./src/core/org/apache/hadoop/io/SequenceFile.java:2152:     * position, use {@link SequenceFile.Reader#sync(long)}.
./src/core/org/apache/hadoop/io/SequenceFile.java:2207:   * <p>For best performance, applications should make sure that the {@link
./src/core/org/apache/hadoop/io/SequenceFile.java:2239:    /** Sort and merge using an arbitrary {@link RawComparator}. */
./src/core/org/apache/hadoop/io/SequenceFile.java:2270:     * @param inFiles the files to be sorted
./src/core/org/apache/hadoop/io/SequenceFile.java:2271:     * @param outFile the sorted output file
./src/core/org/apache/hadoop/io/SequenceFile.java:2272:     * @param deleteInput should the input files be deleted as they are read?
./src/core/org/apache/hadoop/io/SequenceFile.java:2291:     * @param inFiles the files to be sorted
./src/core/org/apache/hadoop/io/SequenceFile.java:2292:     * @param tempDir the directory where temp files are created during sort
./src/core/org/apache/hadoop/io/SequenceFile.java:2293:     * @param deleteInput should the input files be deleted as they are read?
./src/core/org/apache/hadoop/io/SequenceFile.java:2294:     * @return iterator the RawKeyValueIterator
./src/core/org/apache/hadoop/io/SequenceFile.java:2320:     * @param inFile the input file to sort
./src/core/org/apache/hadoop/io/SequenceFile.java:2321:     * @param outFile the sorted output file
./src/core/org/apache/hadoop/io/SequenceFile.java:2535:       * @return DataOutputBuffer
./src/core/org/apache/hadoop/io/SequenceFile.java:2536:       * @throws IOException
./src/core/org/apache/hadoop/io/SequenceFile.java:2540:       * @return ValueBytes 
./src/core/org/apache/hadoop/io/SequenceFile.java:2541:       * @throws IOException
./src/core/org/apache/hadoop/io/SequenceFile.java:2545:       * @return true if there exists a key/value, false otherwise 
./src/core/org/apache/hadoop/io/SequenceFile.java:2546:       * @throws IOException
./src/core/org/apache/hadoop/io/SequenceFile.java:2550:       * @throws IOException
./src/core/org/apache/hadoop/io/SequenceFile.java:2561:     * @param segments the list of SegmentDescriptors
./src/core/org/apache/hadoop/io/SequenceFile.java:2562:     * @param tmpDir the directory to write temporary files into
./src/core/org/apache/hadoop/io/SequenceFile.java:2563:     * @return RawKeyValueIterator
./src/core/org/apache/hadoop/io/SequenceFile.java:2564:     * @throws IOException
./src/core/org/apache/hadoop/io/SequenceFile.java:2577:     * @param inNames the array of path names
./src/core/org/apache/hadoop/io/SequenceFile.java:2578:     * @param deleteInputs true if the input files should be deleted when 
./src/core/org/apache/hadoop/io/SequenceFile.java:2580:     * @param tmpDir the directory to write temporary files into
./src/core/org/apache/hadoop/io/SequenceFile.java:2581:     * @return RawKeyValueIteratorMergeQueue
./src/core/org/apache/hadoop/io/SequenceFile.java:2582:     * @throws IOException
./src/core/org/apache/hadoop/io/SequenceFile.java:2594:     * @param inNames the array of path names
./src/core/org/apache/hadoop/io/SequenceFile.java:2595:     * @param deleteInputs true if the input files should be deleted when 
./src/core/org/apache/hadoop/io/SequenceFile.java:2597:     * @param factor the factor that will be used as the maximum merge fan-in
./src/core/org/apache/hadoop/io/SequenceFile.java:2598:     * @param tmpDir the directory to write temporary files into
./src/core/org/apache/hadoop/io/SequenceFile.java:2599:     * @return RawKeyValueIteratorMergeQueue
./src/core/org/apache/hadoop/io/SequenceFile.java:2600:     * @throws IOException
./src/core/org/apache/hadoop/io/SequenceFile.java:2621:     * @param inNames the array of path names
./src/core/org/apache/hadoop/io/SequenceFile.java:2622:     * @param tempDir the directory for creating temp files during merge
./src/core/org/apache/hadoop/io/SequenceFile.java:2623:     * @param deleteInputs true if the input files should be deleted when 
./src/core/org/apache/hadoop/io/SequenceFile.java:2625:     * @return RawKeyValueIteratorMergeQueue
./src/core/org/apache/hadoop/io/SequenceFile.java:2626:     * @throws IOException
./src/core/org/apache/hadoop/io/SequenceFile.java:2652:     * @param inputFile the path of the input file whose attributes should be 
./src/core/org/apache/hadoop/io/SequenceFile.java:2654:     * @param outputFile the path of the output file 
./src/core/org/apache/hadoop/io/SequenceFile.java:2655:     * @param prog the Progressable to report status during the file write
./src/core/org/apache/hadoop/io/SequenceFile.java:2656:     * @return Writer
./src/core/org/apache/hadoop/io/SequenceFile.java:2657:     * @throws IOException
./src/core/org/apache/hadoop/io/SequenceFile.java:2679:     * @param records the RawKeyValueIterator
./src/core/org/apache/hadoop/io/SequenceFile.java:2680:     * @param writer the Writer created earlier 
./src/core/org/apache/hadoop/io/SequenceFile.java:2681:     * @throws IOException
./src/core/org/apache/hadoop/io/SequenceFile.java:2693:     * @param inFiles the array of input path names
./src/core/org/apache/hadoop/io/SequenceFile.java:2694:     * @param outFile the final output file
./src/core/org/apache/hadoop/io/SequenceFile.java:2695:     * @throws IOException
./src/core/org/apache/hadoop/io/SequenceFile.java:2723:     * @param inName the name of the input file containing sorted segments
./src/core/org/apache/hadoop/io/SequenceFile.java:2724:     * @param indexIn the offsets of the sorted segments
./src/core/org/apache/hadoop/io/SequenceFile.java:2725:     * @param tmpDir the relative directory to store intermediate results in
./src/core/org/apache/hadoop/io/SequenceFile.java:2726:     * @return RawKeyValueIterator
./src/core/org/apache/hadoop/io/SequenceFile.java:2727:     * @throws IOException
./src/core/org/apache/hadoop/io/SequenceFile.java:2760:      @SuppressWarnings("unchecked")
./src/core/org/apache/hadoop/io/SequenceFile.java:2774:       * @param segments the file segments to merge
./src/core/org/apache/hadoop/io/SequenceFile.java:2775:       * @param tmpDir a relative local directory to save intermediate files in
./src/core/org/apache/hadoop/io/SequenceFile.java:2776:       * @param progress the reference to the Progressable object
./src/core/org/apache/hadoop/io/SequenceFile.java:2864:       * @return RawKeyValueIterator
./src/core/org/apache/hadoop/io/SequenceFile.java:2865:       * @throws IOException
./src/core/org/apache/hadoop/io/SequenceFile.java:3009:       * @param segmentOffset the offset of the segment in the file
./src/core/org/apache/hadoop/io/SequenceFile.java:3010:       * @param segmentLength the length of the segment
./src/core/org/apache/hadoop/io/SequenceFile.java:3011:       * @param segmentPathName the path name of the file containing the segment
./src/core/org/apache/hadoop/io/SequenceFile.java:3063:       * @return true if there is a key returned; false, otherwise
./src/core/org/apache/hadoop/io/SequenceFile.java:3064:       * @throws IOException
./src/core/org/apache/hadoop/io/SequenceFile.java:3096:       * @param rawValue
./src/core/org/apache/hadoop/io/SequenceFile.java:3097:       * @return the length of the value
./src/core/org/apache/hadoop/io/SequenceFile.java:3098:       * @throws IOException
./src/core/org/apache/hadoop/io/SequenceFile.java:3135:       * @param segmentOffset the offset of the segment in the file
./src/core/org/apache/hadoop/io/SequenceFile.java:3136:       * @param segmentLength the length of the segment
./src/core/org/apache/hadoop/io/SequenceFile.java:3137:       * @param segmentPathName the path name of the file containing the segment
./src/core/org/apache/hadoop/io/SequenceFile.java:3138:       * @param parent the parent SegmentContainer that holds the segment
./src/core/org/apache/hadoop/io/serializer/Deserializer.java:27: * {@link InputStream}.
./src/core/org/apache/hadoop/io/serializer/Deserializer.java:33: * {@link #deserialize(Object)}.
./src/core/org/apache/hadoop/io/serializer/Deserializer.java:35: * @param <T>
./src/core/org/apache/hadoop/io/serializer/Deserializer.java:51:   * @return the deserialized object
./src/core/org/apache/hadoop/io/serializer/DeserializerComparator.java:29: * A {@link RawComparator} that uses a {@link Deserializer} to deserialize
./src/core/org/apache/hadoop/io/serializer/DeserializerComparator.java:30: * the objects to be compared so that the standard {@link Comparator} can
./src/core/org/apache/hadoop/io/serializer/DeserializerComparator.java:35: * implementation of {@link RawComparator} that operates directly
./src/core/org/apache/hadoop/io/serializer/DeserializerComparator.java:38: * @param <T>
./src/core/org/apache/hadoop/io/serializer/JavaSerialization.java:30: * An experimental {@link Serialization} for Java {@link Serializable} classes.
./src/core/org/apache/hadoop/io/serializer/JavaSerialization.java:32: * @see JavaSerializationComparator
./src/core/org/apache/hadoop/io/serializer/JavaSerialization.java:43:        @Override protected void readStreamHeader() {
./src/core/org/apache/hadoop/io/serializer/JavaSerialization.java:49:    @SuppressWarnings("unchecked")
./src/core/org/apache/hadoop/io/serializer/JavaSerialization.java:72:        @Override protected void writeStreamHeader() {
./src/core/org/apache/hadoop/io/serializer/JavaSerializationComparator.java:28: * A {@link RawComparator} that uses a {@link JavaSerialization}
./src/core/org/apache/hadoop/io/serializer/JavaSerializationComparator.java:29: * {@link Deserializer} to deserialize objects that are then compared via
./src/core/org/apache/hadoop/io/serializer/JavaSerializationComparator.java:30: * their {@link Comparable} interfaces.
./src/core/org/apache/hadoop/io/serializer/JavaSerializationComparator.java:32: * @param <T>
./src/core/org/apache/hadoop/io/serializer/JavaSerializationComparator.java:33: * @see JavaSerialization
./src/core/org/apache/hadoop/io/serializer/Serialization.java:23: * Encapsulates a {@link Serializer}/{@link Deserializer} pair.
./src/core/org/apache/hadoop/io/serializer/Serialization.java:25: * @param <T>
./src/core/org/apache/hadoop/io/serializer/Serialization.java:30:   * Allows clients to test whether this {@link Serialization}
./src/core/org/apache/hadoop/io/serializer/Serialization.java:36:   * @return a {@link Serializer} for the given class.
./src/core/org/apache/hadoop/io/serializer/Serialization.java:41:   * @return a {@link Deserializer} for the given class.
./src/core/org/apache/hadoop/io/serializer/SerializationFactory.java:33: * A factory for {@link Serialization}s.
./src/core/org/apache/hadoop/io/serializer/SerializationFactory.java:58:  @SuppressWarnings("unchecked")
./src/core/org/apache/hadoop/io/serializer/SerializationFactory.java:80:  @SuppressWarnings("unchecked")
./src/core/org/apache/hadoop/io/serializer/Serializer.java:27: * {@link OutputStream}.
./src/core/org/apache/hadoop/io/serializer/Serializer.java:33: * {@link #serialize(Object)}.
./src/core/org/apache/hadoop/io/serializer/Serializer.java:35: * @param <T>
./src/core/org/apache/hadoop/io/serializer/WritableSerialization.java:33: * A {@link Serialization} for {@link Writable}s that delegates to
./src/core/org/apache/hadoop/io/serializer/WritableSerialization.java:34: * {@link Writable#write(java.io.DataOutput)} and
./src/core/org/apache/hadoop/io/serializer/WritableSerialization.java:35: * {@link Writable#readFields(java.io.DataInput)}.
./src/core/org/apache/hadoop/io/SetFile.java:37:     *  @deprecated pass a Configuration too
./src/core/org/apache/hadoop/io/SortedMapWritable.java:51:   * @param other the map to copy from
./src/core/org/apache/hadoop/io/SortedMapWritable.java:58:  /** {@inheritDoc} */
./src/core/org/apache/hadoop/io/SortedMapWritable.java:64:  /** {@inheritDoc} */
./src/core/org/apache/hadoop/io/SortedMapWritable.java:69:  /** {@inheritDoc} */
./src/core/org/apache/hadoop/io/SortedMapWritable.java:76:  /** {@inheritDoc} */
./src/core/org/apache/hadoop/io/SortedMapWritable.java:81:  /** {@inheritDoc} */
./src/core/org/apache/hadoop/io/SortedMapWritable.java:88:  /** {@inheritDoc} */
./src/core/org/apache/hadoop/io/SortedMapWritable.java:95:  /** {@inheritDoc} */
./src/core/org/apache/hadoop/io/SortedMapWritable.java:100:  /** {@inheritDoc} */
./src/core/org/apache/hadoop/io/SortedMapWritable.java:105:  /** {@inheritDoc} */
./src/core/org/apache/hadoop/io/SortedMapWritable.java:110:  /** {@inheritDoc} */
./src/core/org/apache/hadoop/io/SortedMapWritable.java:115:  /** {@inheritDoc} */
./src/core/org/apache/hadoop/io/SortedMapWritable.java:120:  /** {@inheritDoc} */
./src/core/org/apache/hadoop/io/SortedMapWritable.java:125:  /** {@inheritDoc} */
./src/core/org/apache/hadoop/io/SortedMapWritable.java:130:  /** {@inheritDoc} */
./src/core/org/apache/hadoop/io/SortedMapWritable.java:137:  /** {@inheritDoc} */
./src/core/org/apache/hadoop/io/SortedMapWritable.java:146:  /** {@inheritDoc} */
./src/core/org/apache/hadoop/io/SortedMapWritable.java:151:  /** {@inheritDoc} */
./src/core/org/apache/hadoop/io/SortedMapWritable.java:156:  /** {@inheritDoc} */
./src/core/org/apache/hadoop/io/SortedMapWritable.java:161:  /** {@inheritDoc} */
./src/core/org/apache/hadoop/io/SortedMapWritable.java:162:  @SuppressWarnings("unchecked")
./src/core/org/apache/hadoop/io/SortedMapWritable.java:163:  @Override
./src/core/org/apache/hadoop/io/SortedMapWritable.java:188:  /** {@inheritDoc} */
./src/core/org/apache/hadoop/io/SortedMapWritable.java:189:  @Override
./src/core/org/apache/hadoop/io/Stringifier.java:27: * @param <T> the class of the objects to stringify
./src/core/org/apache/hadoop/io/Stringifier.java:33:   * @param obj the object to convert
./src/core/org/apache/hadoop/io/Stringifier.java:34:   * @return the string representation of the object
./src/core/org/apache/hadoop/io/Stringifier.java:35:   * @throws IOException if the object cannot be converted
./src/core/org/apache/hadoop/io/Stringifier.java:41:   * @param str the string representation of the object
./src/core/org/apache/hadoop/io/Stringifier.java:42:   * @return restored object
./src/core/org/apache/hadoop/io/Stringifier.java:43:   * @throws IOException if the object cannot be restored
./src/core/org/apache/hadoop/io/Stringifier.java:50:   * @throws IOException if an I/O error occurs 
./src/core/org/apache/hadoop/io/Text.java:96:   * Returns the raw bytes; however, only data up to {@link #getLength()} is
./src/core/org/apache/hadoop/io/Text.java:112:   * @return the Unicode scalar value at position or -1
./src/core/org/apache/hadoop/io/Text.java:134:   * @return byte position of the first occurence of the search
./src/core/org/apache/hadoop/io/Text.java:199:   * @param utf8 the data to copy from
./src/core/org/apache/hadoop/io/Text.java:200:   * @param start the first position of the new string
./src/core/org/apache/hadoop/io/Text.java:201:   * @param len the number of bytes of the new string
./src/core/org/apache/hadoop/io/Text.java:211:   * @param utf8 the data to copy from
./src/core/org/apache/hadoop/io/Text.java:212:   * @param start the first position to append from utf8
./src/core/org/apache/hadoop/io/Text.java:213:   * @param len the number of bytes to append
./src/core/org/apache/hadoop/io/Text.java:235:   * @param len the number of bytes we need
./src/core/org/apache/hadoop/io/Text.java:236:   * @param keepData should the old data be kept
./src/core/org/apache/hadoop/io/Text.java:250:   * @see java.lang.Object#toString()
./src/core/org/apache/hadoop/io/Text.java:278:   * @see Writable#write(DataOutput)
./src/core/org/apache/hadoop/io/Text.java:363:   * @return ByteBuffer: bytes stores at ByteBuffer.array() 
./src/core/org/apache/hadoop/io/Text.java:378:   * @return ByteBuffer: bytes stores at ByteBuffer.array() 
./src/core/org/apache/hadoop/io/Text.java:426:   * @param utf8 byte array
./src/core/org/apache/hadoop/io/Text.java:427:   * @throws MalformedInputException if the byte array contains invalid utf-8
./src/core/org/apache/hadoop/io/Text.java:435:   * @param utf8 the array of bytes
./src/core/org/apache/hadoop/io/Text.java:436:   * @param start the offset of the first byte in the array
./src/core/org/apache/hadoop/io/Text.java:437:   * @param len the length of the byte sequence
./src/core/org/apache/hadoop/io/Text.java:438:   * @throws MalformedInputException if the byte array contains invalid bytes
./src/core/org/apache/hadoop/io/Text.java:563:   * @param string text to encode
./src/core/org/apache/hadoop/io/Text.java:564:   * @return number of UTF-8 bytes required to encode
./src/core/org/apache/hadoop/io/UTF8.java:32: * @deprecated replaced by Text
./src/core/org/apache/hadoop/io/UTF8.java:180:   * @see String#getBytes(String)
./src/core/org/apache/hadoop/io/UTF8.java:198:   * @see DataInput#readUTF()
./src/core/org/apache/hadoop/io/UTF8.java:232:   * @see DataOutput#writeUTF(String)
./src/core/org/apache/hadoop/io/VersionedWritable.java:29: * handle this situation, {@link #readFields(DataInput)}
./src/core/org/apache/hadoop/io/VersionedWritable.java:30: * implementations should catch {@link VersionMismatchException}.
./src/core/org/apache/hadoop/io/VersionMismatchException.java:23:/** Thrown by {@link VersionedWritable#readFields(DataInput)} when the
./src/core/org/apache/hadoop/io/VersionMismatchException.java:25: * version as returned by {@link VersionedWritable#getVersion()}. */
./src/core/org/apache/hadoop/io/VIntWritable.java:26: * @see org.apache.hadoop.io.WritableUtils#readVInt(DataInput)
./src/core/org/apache/hadoop/io/VLongWritable.java:26: *  @see org.apache.hadoop.io.WritableUtils#readVLong(DataInput)
./src/core/org/apache/hadoop/io/Writable.java:27: * protocol, based on {@link DataInput} and {@link DataOutput}.
./src/core/org/apache/hadoop/io/Writable.java:33: * method which constructs a new instance, calls {@link #readFields(DataInput)} 
./src/core/org/apache/hadoop/io/Writable.java:65:   * @param out <code>DataOuput</code> to serialize this object into.
./src/core/org/apache/hadoop/io/Writable.java:66:   * @throws IOException
./src/core/org/apache/hadoop/io/Writable.java:76:   * @param in <code>DataInput</code> to deseriablize this object from.
./src/core/org/apache/hadoop/io/Writable.java:77:   * @throws IOException
./src/core/org/apache/hadoop/io/WritableComparable.java:22: * A {@link Writable} which is also {@link Comparable}. 
./src/core/org/apache/hadoop/io/WritableComparator.java:26:/** A Comparator for {@link WritableComparable}s.
./src/core/org/apache/hadoop/io/WritableComparator.java:29: * orderings, override {@link #compare(WritableComparable,WritableComparable)}.
./src/core/org/apache/hadoop/io/WritableComparator.java:32: * {@link #compare(byte[],int,int,byte[],int,int)}.  Static utility methods are
./src/core/org/apache/hadoop/io/WritableComparator.java:40:  /** Get a comparator for a {@link WritableComparable} implementation. */
./src/core/org/apache/hadoop/io/WritableComparator.java:48:  /** Register an optimized comparator for a {@link WritableComparable}
./src/core/org/apache/hadoop/io/WritableComparator.java:61:  /** Construct for a {@link WritableComparable} implementation. */
./src/core/org/apache/hadoop/io/WritableComparator.java:82:  /** Construct a new {@link WritableComparable} instance. */
./src/core/org/apache/hadoop/io/WritableComparator.java:89:   * <p>The default implementation reads the data into two {@link
./src/core/org/apache/hadoop/io/WritableComparator.java:90:   * WritableComparable}s (using {@link
./src/core/org/apache/hadoop/io/WritableComparator.java:91:   * Writable#readFields(DataInput)}, then calls {@link
./src/core/org/apache/hadoop/io/WritableComparator.java:111:   * <p> The default implementation uses the natural ordering, calling {@link
./src/core/org/apache/hadoop/io/WritableComparator.java:113:  @SuppressWarnings("unchecked")
./src/core/org/apache/hadoop/io/WritableComparator.java:178:   * @param bytes byte array with decode long
./src/core/org/apache/hadoop/io/WritableComparator.java:179:   * @param start starting index
./src/core/org/apache/hadoop/io/WritableComparator.java:180:   * @throws java.io.IOException 
./src/core/org/apache/hadoop/io/WritableComparator.java:181:   * @return deserialized long
./src/core/org/apache/hadoop/io/WritableComparator.java:203:   * @param bytes byte array with the encoded integer
./src/core/org/apache/hadoop/io/WritableComparator.java:204:   * @param start start index
./src/core/org/apache/hadoop/io/WritableComparator.java:205:   * @throws java.io.IOException 
./src/core/org/apache/hadoop/io/WritableComparator.java:206:   * @return deserialized integer
./src/core/org/apache/hadoop/io/WritableFactories.java:25:/** Factories for non-public writables.  Defining a factory permits {@link
./src/core/org/apache/hadoop/io/WritableFactory.java:22: * @see WritableFactories
./src/core/org/apache/hadoop/io/WritableName.java:56:  /** Return the name for a class.  Default is {@link Class#getName()}. */
./src/core/org/apache/hadoop/io/WritableName.java:64:  /** Return the class for a name.  Default is {@link Class#forName(String)}.*/
./src/core/org/apache/hadoop/io/WritableUtils.java:225:   * @param orig The object to copy
./src/core/org/apache/hadoop/io/WritableUtils.java:226:   * @return The copied object
./src/core/org/apache/hadoop/io/WritableUtils.java:230:      @SuppressWarnings("unchecked") // Unchecked cast from Class to Class<T>
./src/core/org/apache/hadoop/io/WritableUtils.java:241:   * @param dst the object to copy from
./src/core/org/apache/hadoop/io/WritableUtils.java:242:   * @param src the object to copy into, which is destroyed
./src/core/org/apache/hadoop/io/WritableUtils.java:243:   * @throws IOException
./src/core/org/apache/hadoop/io/WritableUtils.java:265:   * @param stream Binary output stream
./src/core/org/apache/hadoop/io/WritableUtils.java:266:   * @param i Integer to be serialized
./src/core/org/apache/hadoop/io/WritableUtils.java:267:   * @throws java.io.IOException 
./src/core/org/apache/hadoop/io/WritableUtils.java:284:   * @param stream Binary output stream
./src/core/org/apache/hadoop/io/WritableUtils.java:285:   * @param i Long to be serialized
./src/core/org/apache/hadoop/io/WritableUtils.java:286:   * @throws java.io.IOException 
./src/core/org/apache/hadoop/io/WritableUtils.java:320:   * @param stream Binary input stream
./src/core/org/apache/hadoop/io/WritableUtils.java:321:   * @throws java.io.IOException 
./src/core/org/apache/hadoop/io/WritableUtils.java:322:   * @return deserialized long from stream.
./src/core/org/apache/hadoop/io/WritableUtils.java:341:   * @param stream Binary input stream
./src/core/org/apache/hadoop/io/WritableUtils.java:342:   * @throws java.io.IOException 
./src/core/org/apache/hadoop/io/WritableUtils.java:343:   * @return deserialized integer from stream.
./src/core/org/apache/hadoop/io/WritableUtils.java:351:   * @param value the first byte
./src/core/org/apache/hadoop/io/WritableUtils.java:352:   * @return is the value negative
./src/core/org/apache/hadoop/io/WritableUtils.java:360:   * @param value the first byte of the vint/vlong
./src/core/org/apache/hadoop/io/WritableUtils.java:361:   * @return the total number of bytes (1 to 9)
./src/core/org/apache/hadoop/io/WritableUtils.java:374:   * @return the encoded length 
./src/core/org/apache/hadoop/io/WritableUtils.java:392:   * @param <T> Enum type
./src/core/org/apache/hadoop/io/WritableUtils.java:393:   * @param in DataInput to read from 
./src/core/org/apache/hadoop/io/WritableUtils.java:394:   * @param enumType Class type of Enum
./src/core/org/apache/hadoop/io/WritableUtils.java:395:   * @return Enum represented by String read from DataInput
./src/core/org/apache/hadoop/io/WritableUtils.java:396:   * @throws IOException
./src/core/org/apache/hadoop/io/WritableUtils.java:404:   * @param out Dataoutput stream
./src/core/org/apache/hadoop/io/WritableUtils.java:405:   * @param enumVal enum value
./src/core/org/apache/hadoop/io/WritableUtils.java:406:   * @throws IOException
./src/core/org/apache/hadoop/io/WritableUtils.java:414:   * @param in input stream
./src/core/org/apache/hadoop/io/WritableUtils.java:415:   * @param len number of bytes to skip
./src/core/org/apache/hadoop/io/WritableUtils.java:416:   * @throws IOException when skipped less number of bytes
./src/core/org/apache/hadoop/ipc/Client.java:54:/** A client for an IPC service.  IPC calls take a single {@link Writable} as a
./src/core/org/apache/hadoop/ipc/Client.java:55: * parameter, and return a {@link Writable} as their value.  A service runs on
./src/core/org/apache/hadoop/ipc/Client.java:58: * @see Server
./src/core/org/apache/hadoop/ipc/Client.java:87:   * @param conf Configuration
./src/core/org/apache/hadoop/ipc/Client.java:88:   * @param pingInterval the ping interval
./src/core/org/apache/hadoop/ipc/Client.java:98:   * @param conf Configuration
./src/core/org/apache/hadoop/ipc/Client.java:99:   * @return the ping interval
./src/core/org/apache/hadoop/ipc/Client.java:124:   * @return true if this client has no reference; false otherwise
./src/core/org/apache/hadoop/ipc/Client.java:155:     * @param error exception thrown by the call; either local or remote
./src/core/org/apache/hadoop/ipc/Client.java:165:     * @param value return value of the call.
./src/core/org/apache/hadoop/ipc/Client.java:214:     * @param call to add
./src/core/org/apache/hadoop/ipc/Client.java:215:     * @return true if the call was added.
./src/core/org/apache/hadoop/ipc/Client.java:250:       * @throws IOException for any IO problem other than socket timeout
./src/core/org/apache/hadoop/ipc/Client.java:266:       * @return the total number of bytes read; -1 if the connection is closed.
./src/core/org/apache/hadoop/ipc/Client.java:337:     * @param curRetries current number of retries
./src/core/org/apache/hadoop/ipc/Client.java:338:     * @param maxRetries max number of retries allowed
./src/core/org/apache/hadoop/ipc/Client.java:339:     * @param ioe failure reason
./src/core/org/apache/hadoop/ipc/Client.java:340:     * @throws IOException if max number of retries is reached
./src/core/org/apache/hadoop/ipc/Client.java:612:  /** Construct an IPC client whose values are of the given {@link Writable}
./src/core/org/apache/hadoop/ipc/Client.java:631:   * @param valueClass
./src/core/org/apache/hadoop/ipc/Client.java:632:   * @param conf
./src/core/org/apache/hadoop/ipc/Client.java:640:   * @return this client's socket factory
./src/core/org/apache/hadoop/ipc/Client.java:796:    @Override
./src/core/org/apache/hadoop/ipc/Client.java:806:    @Override
./src/core/org/apache/hadoop/ipc/metrics/RpcMetrics.java:42: *  <p> {@link #rpcQueueTime}.inc(time)
./src/core/org/apache/hadoop/ipc/metrics/RpcMgt.java:50:   * @inheritDoc
./src/core/org/apache/hadoop/ipc/metrics/RpcMgt.java:57:   * @inheritDoc
./src/core/org/apache/hadoop/ipc/metrics/RpcMgt.java:64:   * @inheritDoc
./src/core/org/apache/hadoop/ipc/metrics/RpcMgt.java:71:   * @inheritDoc
./src/core/org/apache/hadoop/ipc/metrics/RpcMgt.java:78:   * @inheritDoc
./src/core/org/apache/hadoop/ipc/metrics/RpcMgt.java:85:   * @inheritDoc
./src/core/org/apache/hadoop/ipc/metrics/RpcMgt.java:92:   * @inheritDoc
./src/core/org/apache/hadoop/ipc/metrics/RpcMgt.java:99:   * @inheritDoc
./src/core/org/apache/hadoop/ipc/metrics/RpcMgt.java:106:   * @inheritDoc
./src/core/org/apache/hadoop/ipc/metrics/RpcMgt.java:113:   * @inheritDoc
./src/core/org/apache/hadoop/ipc/RemoteException.java:48:   * @param lookupTypes the desired exception class.
./src/core/org/apache/hadoop/ipc/RemoteException.java:49:   * @return IOException, which is either the lookupClass exception or this.
./src/core/org/apache/hadoop/ipc/RemoteException.java:75:   * @return <code>Throwable
./src/core/org/apache/hadoop/ipc/RPC.java:54: * <li>a {@link String}; or</li>
./src/core/org/apache/hadoop/ipc/RPC.java:56: * <li>a {@link Writable}; or</li>
./src/core/org/apache/hadoop/ipc/RPC.java:146:     * @param conf Configuration
./src/core/org/apache/hadoop/ipc/RPC.java:147:     * @return an IPC client
./src/core/org/apache/hadoop/ipc/RPC.java:170:     * @param conf Configuration
./src/core/org/apache/hadoop/ipc/RPC.java:171:     * @return an IPC client
./src/core/org/apache/hadoop/ipc/RPC.java:244:     * @param interfaceName the name of the protocol mismatch
./src/core/org/apache/hadoop/ipc/RPC.java:245:     * @param clientVersion the client's version of the protocol
./src/core/org/apache/hadoop/ipc/RPC.java:246:     * @param serverVersion the server's version of the protocol
./src/core/org/apache/hadoop/ipc/RPC.java:259:     * @return the java class name 
./src/core/org/apache/hadoop/ipc/RPC.java:332:   * @param protocol
./src/core/org/apache/hadoop/ipc/RPC.java:333:   * @param clientVersion
./src/core/org/apache/hadoop/ipc/RPC.java:334:   * @param addr
./src/core/org/apache/hadoop/ipc/RPC.java:335:   * @param conf
./src/core/org/apache/hadoop/ipc/RPC.java:336:   * @return a proxy instance
./src/core/org/apache/hadoop/ipc/RPC.java:337:   * @throws IOException
./src/core/org/apache/hadoop/ipc/RPC.java:349:   * @param proxy the proxy to be stopped
./src/core/org/apache/hadoop/ipc/RPC.java:408:     * @param instance the instance whose methods will be called
./src/core/org/apache/hadoop/ipc/RPC.java:409:     * @param conf the configuration to use
./src/core/org/apache/hadoop/ipc/RPC.java:410:     * @param bindAddress the address to bind on to listen for connection
./src/core/org/apache/hadoop/ipc/RPC.java:411:     * @param port the port to listen for connections on
./src/core/org/apache/hadoop/ipc/RPC.java:427:     * @param instance the instance whose methods will be called
./src/core/org/apache/hadoop/ipc/RPC.java:428:     * @param conf the configuration to use
./src/core/org/apache/hadoop/ipc/RPC.java:429:     * @param bindAddress the address to bind on to listen for connection
./src/core/org/apache/hadoop/ipc/RPC.java:430:     * @param port the port to listen for connections on
./src/core/org/apache/hadoop/ipc/RPC.java:431:     * @param numHandlers the number of method handler threads to run
./src/core/org/apache/hadoop/ipc/RPC.java:432:     * @param verbose whether each call should be logged
./src/core/org/apache/hadoop/ipc/Server.java:63:/** An abstract IPC service.  IPC calls take a single {@link Writable} as a
./src/core/org/apache/hadoop/ipc/Server.java:64: * parameter, and return a {@link Writable} as their value.  A service runs on
./src/core/org/apache/hadoop/ipc/Server.java:67: * @see Client
./src/core/org/apache/hadoop/ipc/Server.java:90:   * {@link #call(Writable, long)} implementations, and under {@link Writable}
./src/core/org/apache/hadoop/ipc/Server.java:157:   * @param socket the socket to bind
./src/core/org/apache/hadoop/ipc/Server.java:158:   * @param address the address to bind to
./src/core/org/apache/hadoop/ipc/Server.java:159:   * @param backlog the number of connections allowed in the queue
./src/core/org/apache/hadoop/ipc/Server.java:160:   * @throws BindException if the address can't be bound
./src/core/org/apache/hadoop/ipc/Server.java:161:   * @throws UnknownHostException if the address isn't a valid host name
./src/core/org/apache/hadoop/ipc/Server.java:162:   * @throws IOException other random errors from bind
./src/core/org/apache/hadoop/ipc/Server.java:202:    @Override
./src/core/org/apache/hadoop/ipc/Server.java:290:    @Override
./src/core/org/apache/hadoop/ipc/Server.java:447:    @Override
./src/core/org/apache/hadoop/ipc/Server.java:721:    @Override
./src/core/org/apache/hadoop/ipc/Server.java:871:    @Override
./src/core/org/apache/hadoop/ipc/Server.java:1017:   *  See {@link #stop()}.
./src/core/org/apache/hadoop/ipc/Server.java:1027:   * @return the socket (ip+port) on which the RPC server is listening to.
./src/core/org/apache/hadoop/ipc/Server.java:1040:   * @return the number of open rpc connections
./src/core/org/apache/hadoop/ipc/Server.java:1048:   * @return The number of rpc calls in the queue.
./src/core/org/apache/hadoop/ipc/VersionedProtocol.java:32:   * @param protocol The classname of the protocol interface
./src/core/org/apache/hadoop/ipc/VersionedProtocol.java:33:   * @param clientVersion The version of the protocol that the client speaks
./src/core/org/apache/hadoop/ipc/VersionedProtocol.java:34:   * @return the version that the server will speak
./src/core/org/apache/hadoop/metrics/ContextFactory.java:64:   * @param attributeName the attribute name
./src/core/org/apache/hadoop/metrics/ContextFactory.java:65:   * @return the attribute value
./src/core/org/apache/hadoop/metrics/ContextFactory.java:74:   * @return the attribute names
./src/core/org/apache/hadoop/metrics/ContextFactory.java:92:   * @param attributeName the attribute name
./src/core/org/apache/hadoop/metrics/ContextFactory.java:93:   * @param value the new attribute value
./src/core/org/apache/hadoop/metrics/ContextFactory.java:102:   * @param attributeName the attribute name
./src/core/org/apache/hadoop/metrics/ContextFactory.java:119:   * @param contextName the name of the context
./src/core/org/apache/hadoop/metrics/ContextFactory.java:120:   * @return the named MetricsContext
./src/core/org/apache/hadoop/metrics/ContextFactory.java:162:   * @return the singleton ContextFactory instance
./src/core/org/apache/hadoop/metrics/file/FileContext.java:109:   * @see #close()
./src/core/org/apache/hadoop/metrics/MetricsContext.java:38:   * @return the context name
./src/core/org/apache/hadoop/metrics/MetricsContext.java:54:   * @see #close()
./src/core/org/apache/hadoop/metrics/MetricsContext.java:74:   * @param recordName the name of the record
./src/core/org/apache/hadoop/metrics/MetricsContext.java:75:   * @throws MetricsException if recordName conflicts with configuration data
./src/core/org/apache/hadoop/metrics/MetricsContext.java:83:   * @param updater object to be run periodically; it should updated
./src/core/org/apache/hadoop/metrics/MetricsContext.java:91:   * @param updater object to be removed from the callback list
./src/core/org/apache/hadoop/metrics/MetricsException.java:36:   * @param message an error message
./src/core/org/apache/hadoop/metrics/MetricsRecord.java:73:   * @return the record name
./src/core/org/apache/hadoop/metrics/MetricsRecord.java:81:   * @param tagName name of the tag
./src/core/org/apache/hadoop/metrics/MetricsRecord.java:82:   * @param tagValue new value of the tag
./src/core/org/apache/hadoop/metrics/MetricsRecord.java:83:   * @throws MetricsException if the tagName conflicts with the configuration
./src/core/org/apache/hadoop/metrics/MetricsRecord.java:90:   * @param tagName name of the tag
./src/core/org/apache/hadoop/metrics/MetricsRecord.java:91:   * @param tagValue new value of the tag
./src/core/org/apache/hadoop/metrics/MetricsRecord.java:92:   * @throws MetricsException if the tagName conflicts with the configuration
./src/core/org/apache/hadoop/metrics/MetricsRecord.java:99:   * @param tagName name of the tag
./src/core/org/apache/hadoop/metrics/MetricsRecord.java:100:   * @param tagValue new value of the tag
./src/core/org/apache/hadoop/metrics/MetricsRecord.java:101:   * @throws MetricsException if the tagName conflicts with the configuration
./src/core/org/apache/hadoop/metrics/MetricsRecord.java:108:   * @param tagName name of the tag
./src/core/org/apache/hadoop/metrics/MetricsRecord.java:109:   * @param tagValue new value of the tag
./src/core/org/apache/hadoop/metrics/MetricsRecord.java:110:   * @throws MetricsException if the tagName conflicts with the configuration
./src/core/org/apache/hadoop/metrics/MetricsRecord.java:117:   * @param tagName name of the tag
./src/core/org/apache/hadoop/metrics/MetricsRecord.java:118:   * @param tagValue new value of the tag
./src/core/org/apache/hadoop/metrics/MetricsRecord.java:119:   * @throws MetricsException if the tagName conflicts with the configuration
./src/core/org/apache/hadoop/metrics/MetricsRecord.java:126:   * @param tagName name of a tag
./src/core/org/apache/hadoop/metrics/MetricsRecord.java:133:   * @param metricName name of the metric
./src/core/org/apache/hadoop/metrics/MetricsRecord.java:134:   * @param metricValue new value of the metric
./src/core/org/apache/hadoop/metrics/MetricsRecord.java:135:   * @throws MetricsException if the metricName or the type of the metricValue 
./src/core/org/apache/hadoop/metrics/MetricsRecord.java:143:   * @param metricName name of the metric
./src/core/org/apache/hadoop/metrics/MetricsRecord.java:144:   * @param metricValue new value of the metric
./src/core/org/apache/hadoop/metrics/MetricsRecord.java:145:   * @throws MetricsException if the metricName or the type of the metricValue 
./src/core/org/apache/hadoop/metrics/MetricsRecord.java:153:   * @param metricName name of the metric
./src/core/org/apache/hadoop/metrics/MetricsRecord.java:154:   * @param metricValue new value of the metric
./src/core/org/apache/hadoop/metrics/MetricsRecord.java:155:   * @throws MetricsException if the metricName or the type of the metricValue 
./src/core/org/apache/hadoop/metrics/MetricsRecord.java:163:   * @param metricName name of the metric
./src/core/org/apache/hadoop/metrics/MetricsRecord.java:164:   * @param metricValue new value of the metric
./src/core/org/apache/hadoop/metrics/MetricsRecord.java:165:   * @throws MetricsException if the metricName or the type of the metricValue 
./src/core/org/apache/hadoop/metrics/MetricsRecord.java:173:   * @param metricName name of the metric
./src/core/org/apache/hadoop/metrics/MetricsRecord.java:174:   * @param metricValue new value of the metric
./src/core/org/apache/hadoop/metrics/MetricsRecord.java:175:   * @throws MetricsException if the metricName or the type of the metricValue 
./src/core/org/apache/hadoop/metrics/MetricsRecord.java:183:   * @param metricName name of the metric
./src/core/org/apache/hadoop/metrics/MetricsRecord.java:184:   * @param metricValue incremental value
./src/core/org/apache/hadoop/metrics/MetricsRecord.java:185:   * @throws MetricsException if the metricName or the type of the metricValue 
./src/core/org/apache/hadoop/metrics/MetricsRecord.java:193:   * @param metricName name of the metric
./src/core/org/apache/hadoop/metrics/MetricsRecord.java:194:   * @param metricValue incremental value
./src/core/org/apache/hadoop/metrics/MetricsRecord.java:195:   * @throws MetricsException if the metricName or the type of the metricValue 
./src/core/org/apache/hadoop/metrics/MetricsRecord.java:203:   * @param metricName name of the metric
./src/core/org/apache/hadoop/metrics/MetricsRecord.java:204:   * @param metricValue incremental value
./src/core/org/apache/hadoop/metrics/MetricsRecord.java:205:   * @throws MetricsException if the metricName or the type of the metricValue 
./src/core/org/apache/hadoop/metrics/MetricsRecord.java:213:   * @param metricName name of the metric
./src/core/org/apache/hadoop/metrics/MetricsRecord.java:214:   * @param metricValue incremental value
./src/core/org/apache/hadoop/metrics/MetricsRecord.java:215:   * @throws MetricsException if the metricName or the type of the metricValue 
./src/core/org/apache/hadoop/metrics/MetricsRecord.java:223:   * @param metricName name of the metric
./src/core/org/apache/hadoop/metrics/MetricsRecord.java:224:   * @param metricValue incremental value
./src/core/org/apache/hadoop/metrics/MetricsRecord.java:225:   * @throws MetricsException if the metricName or the type of the metricValue 
./src/core/org/apache/hadoop/metrics/MetricsUtil.java:30: * @see org.apache.hadoop.metrics.MetricsRecord
./src/core/org/apache/hadoop/metrics/MetricsUtil.java:31: * @see org.apache.hadoop.metrics.MetricsContext
./src/core/org/apache/hadoop/metrics/MetricsUtil.java:32: * @see org.apache.hadoop.metrics.ContextFactory
./src/core/org/apache/hadoop/metrics/MetricsUtil.java:67:   * @param context the context
./src/core/org/apache/hadoop/metrics/MetricsUtil.java:68:   * @param recordName name of the record
./src/core/org/apache/hadoop/metrics/MetricsUtil.java:69:   * @return newly created metrics record
./src/core/org/apache/hadoop/metrics/spi/AbstractMetricsContext.java:164:   * @see #close()
./src/core/org/apache/hadoop/metrics/spi/AbstractMetricsContext.java:194:   * @param recordName the name of the record
./src/core/org/apache/hadoop/metrics/spi/AbstractMetricsContext.java:195:   * @throws MetricsException if recordName conflicts with configuration data
./src/core/org/apache/hadoop/metrics/spi/AbstractMetricsContext.java:206:   * @param recordName the name of the record
./src/core/org/apache/hadoop/metrics/spi/AbstractMetricsContext.java:207:   * @return newly created instance of MetricsRecordImpl or subclass
./src/core/org/apache/hadoop/metrics/spi/AbstractMetricsContext.java:217:   * @param updater object to be run periodically; it should update
./src/core/org/apache/hadoop/metrics/spi/AbstractMetricsContext.java:229:   * @param updater object to be removed from the callback list
./src/core/org/apache/hadoop/metrics/spi/MetricsRecordImpl.java:52:   * @return the record name
./src/core/org/apache/hadoop/metrics/spi/MetricsRecordImpl.java:61:   * @param tagName name of the tag
./src/core/org/apache/hadoop/metrics/spi/MetricsRecordImpl.java:62:   * @param tagValue new value of the tag
./src/core/org/apache/hadoop/metrics/spi/MetricsRecordImpl.java:63:   * @throws MetricsException if the tagName conflicts with the configuration
./src/core/org/apache/hadoop/metrics/spi/MetricsRecordImpl.java:75:   * @param tagName name of the tag
./src/core/org/apache/hadoop/metrics/spi/MetricsRecordImpl.java:76:   * @param tagValue new value of the tag
./src/core/org/apache/hadoop/metrics/spi/MetricsRecordImpl.java:77:   * @throws MetricsException if the tagName conflicts with the configuration
./src/core/org/apache/hadoop/metrics/spi/MetricsRecordImpl.java:86:   * @param tagName name of the tag
./src/core/org/apache/hadoop/metrics/spi/MetricsRecordImpl.java:87:   * @param tagValue new value of the tag
./src/core/org/apache/hadoop/metrics/spi/MetricsRecordImpl.java:88:   * @throws MetricsException if the tagName conflicts with the configuration
./src/core/org/apache/hadoop/metrics/spi/MetricsRecordImpl.java:97:   * @param tagName name of the tag
./src/core/org/apache/hadoop/metrics/spi/MetricsRecordImpl.java:98:   * @param tagValue new value of the tag
./src/core/org/apache/hadoop/metrics/spi/MetricsRecordImpl.java:99:   * @throws MetricsException if the tagName conflicts with the configuration
./src/core/org/apache/hadoop/metrics/spi/MetricsRecordImpl.java:108:   * @param tagName name of the tag
./src/core/org/apache/hadoop/metrics/spi/MetricsRecordImpl.java:109:   * @param tagValue new value of the tag
./src/core/org/apache/hadoop/metrics/spi/MetricsRecordImpl.java:110:   * @throws MetricsException if the tagName conflicts with the configuration
./src/core/org/apache/hadoop/metrics/spi/MetricsRecordImpl.java:126:   * @param metricName name of the metric
./src/core/org/apache/hadoop/metrics/spi/MetricsRecordImpl.java:127:   * @param metricValue new value of the metric
./src/core/org/apache/hadoop/metrics/spi/MetricsRecordImpl.java:128:   * @throws MetricsException if the metricName or the type of the metricValue 
./src/core/org/apache/hadoop/metrics/spi/MetricsRecordImpl.java:138:   * @param metricName name of the metric
./src/core/org/apache/hadoop/metrics/spi/MetricsRecordImpl.java:139:   * @param metricValue new value of the metric
./src/core/org/apache/hadoop/metrics/spi/MetricsRecordImpl.java:140:   * @throws MetricsException if the metricName or the type of the metricValue 
./src/core/org/apache/hadoop/metrics/spi/MetricsRecordImpl.java:150:   * @param metricName name of the metric
./src/core/org/apache/hadoop/metrics/spi/MetricsRecordImpl.java:151:   * @param metricValue new value of the metric
./src/core/org/apache/hadoop/metrics/spi/MetricsRecordImpl.java:152:   * @throws MetricsException if the metricName or the type of the metricValue 
./src/core/org/apache/hadoop/metrics/spi/MetricsRecordImpl.java:162:   * @param metricName name of the metric
./src/core/org/apache/hadoop/metrics/spi/MetricsRecordImpl.java:163:   * @param metricValue new value of the metric
./src/core/org/apache/hadoop/metrics/spi/MetricsRecordImpl.java:164:   * @throws MetricsException if the metricName or the type of the metricValue 
./src/core/org/apache/hadoop/metrics/spi/MetricsRecordImpl.java:174:   * @param metricName name of the metric
./src/core/org/apache/hadoop/metrics/spi/MetricsRecordImpl.java:175:   * @param metricValue new value of the metric
./src/core/org/apache/hadoop/metrics/spi/MetricsRecordImpl.java:176:   * @throws MetricsException if the metricName or the type of the metricValue 
./src/core/org/apache/hadoop/metrics/spi/MetricsRecordImpl.java:186:   * @param metricName name of the metric
./src/core/org/apache/hadoop/metrics/spi/MetricsRecordImpl.java:187:   * @param metricValue incremental value
./src/core/org/apache/hadoop/metrics/spi/MetricsRecordImpl.java:188:   * @throws MetricsException if the metricName or the type of the metricValue 
./src/core/org/apache/hadoop/metrics/spi/MetricsRecordImpl.java:198:   * @param metricName name of the metric
./src/core/org/apache/hadoop/metrics/spi/MetricsRecordImpl.java:199:   * @param metricValue incremental value
./src/core/org/apache/hadoop/metrics/spi/MetricsRecordImpl.java:200:   * @throws MetricsException if the metricName or the type of the metricValue 
./src/core/org/apache/hadoop/metrics/spi/MetricsRecordImpl.java:210:   * @param metricName name of the metric
./src/core/org/apache/hadoop/metrics/spi/MetricsRecordImpl.java:211:   * @param metricValue incremental value
./src/core/org/apache/hadoop/metrics/spi/MetricsRecordImpl.java:212:   * @throws MetricsException if the metricName or the type of the metricValue 
./src/core/org/apache/hadoop/metrics/spi/MetricsRecordImpl.java:222:   * @param metricName name of the metric
./src/core/org/apache/hadoop/metrics/spi/MetricsRecordImpl.java:223:   * @param metricValue incremental value
./src/core/org/apache/hadoop/metrics/spi/MetricsRecordImpl.java:224:   * @throws MetricsException if the metricName or the type of the metricValue 
./src/core/org/apache/hadoop/metrics/spi/MetricsRecordImpl.java:234:   * @param metricName name of the metric
./src/core/org/apache/hadoop/metrics/spi/MetricsRecordImpl.java:235:   * @param metricValue incremental value
./src/core/org/apache/hadoop/metrics/spi/MetricsRecordImpl.java:236:   * @throws MetricsException if the metricName or the type of the metricValue 
./src/core/org/apache/hadoop/metrics/spi/OutputRecord.java:52:   * @return the tag value, or null if there is no such tag
./src/core/org/apache/hadoop/metrics/spi/Util.java:44:   * @return a list of InetSocketAddress objects.
./src/core/org/apache/hadoop/metrics/util/MBeanUtil.java:32: *  for {link {@link #registerMBean(String, String, Object)}
./src/core/org/apache/hadoop/metrics/util/MBeanUtil.java:42:   * @param serviceName
./src/core/org/apache/hadoop/metrics/util/MBeanUtil.java:43:   * @param nameName
./src/core/org/apache/hadoop/metrics/util/MBeanUtil.java:44:   * @param theMbean - the MBean to register
./src/core/org/apache/hadoop/metrics/util/MBeanUtil.java:45:   * @return the named used to register the MBean
./src/core/org/apache/hadoop/metrics/util/MetricsIntValue.java:44:   * @param nam the name of the metrics to be used to publish the metric
./src/core/org/apache/hadoop/metrics/util/MetricsIntValue.java:54:   * @param newValue
./src/core/org/apache/hadoop/metrics/util/MetricsIntValue.java:63:   * @return the value last set
./src/core/org/apache/hadoop/metrics/util/MetricsIntValue.java:71:   * @param incr - value to be added
./src/core/org/apache/hadoop/metrics/util/MetricsIntValue.java:88:   * @param decr - value to subtract
./src/core/org/apache/hadoop/metrics/util/MetricsIntValue.java:112:   * (JMX gets the info via {@link #get()}
./src/core/org/apache/hadoop/metrics/util/MetricsIntValue.java:114:   * @param mr
./src/core/org/apache/hadoop/metrics/util/MetricsLongValue.java:37:   * @param nam the name of the metrics to be used to publish the metric
./src/core/org/apache/hadoop/metrics/util/MetricsLongValue.java:47:   * @param newValue
./src/core/org/apache/hadoop/metrics/util/MetricsLongValue.java:56:   * @return the value last set
./src/core/org/apache/hadoop/metrics/util/MetricsLongValue.java:64:   * @param incr - value to be added
./src/core/org/apache/hadoop/metrics/util/MetricsLongValue.java:81:   * @param decr - value to subtract
./src/core/org/apache/hadoop/metrics/util/MetricsLongValue.java:105:   * (JMX gets the info via {@link #get()}
./src/core/org/apache/hadoop/metrics/util/MetricsLongValue.java:107:   * @param mr
./src/core/org/apache/hadoop/metrics/util/MetricsTimeVaryingInt.java:32: * @see org.apache.hadoop.metrics.util.MetricsTimeVaryingRate
./src/core/org/apache/hadoop/metrics/util/MetricsTimeVaryingInt.java:46:   * @param nam the name of the metrics to be used to publish the metric
./src/core/org/apache/hadoop/metrics/util/MetricsTimeVaryingInt.java:56:   * @param incr - number of operations
./src/core/org/apache/hadoop/metrics/util/MetricsTimeVaryingInt.java:79:   * (JMX gets the info via {@link #previousIntervalValue}
./src/core/org/apache/hadoop/metrics/util/MetricsTimeVaryingInt.java:81:   * @param mr
./src/core/org/apache/hadoop/metrics/util/MetricsTimeVaryingInt.java:96:   * @return prev interval value
./src/core/org/apache/hadoop/metrics/util/MetricsTimeVaryingRate.java:82:   * @param n the name of the metrics to be used to publish the metric
./src/core/org/apache/hadoop/metrics/util/MetricsTimeVaryingRate.java:94:   * @param numOps - number of operations
./src/core/org/apache/hadoop/metrics/util/MetricsTimeVaryingRate.java:95:   * @param time - time for numOps operations
./src/core/org/apache/hadoop/metrics/util/MetricsTimeVaryingRate.java:106:   * @param time for one operation
./src/core/org/apache/hadoop/metrics/util/MetricsTimeVaryingRate.java:128:   * (JMX gets the info via {@link #getPreviousIntervalAverageTime()} and
./src/core/org/apache/hadoop/metrics/util/MetricsTimeVaryingRate.java:129:   * {@link #getPreviousIntervalNumOps()}
./src/core/org/apache/hadoop/metrics/util/MetricsTimeVaryingRate.java:131:   * @param mr
./src/core/org/apache/hadoop/metrics/util/MetricsTimeVaryingRate.java:146:   * @return - ops in prev interval
./src/core/org/apache/hadoop/metrics/util/MetricsTimeVaryingRate.java:154:   * @return - the average rate.
./src/core/org/apache/hadoop/metrics/util/MetricsTimeVaryingRate.java:162:   *  {@link #resetMinMax()}
./src/core/org/apache/hadoop/metrics/util/MetricsTimeVaryingRate.java:163:   * @return min time for an operation
./src/core/org/apache/hadoop/metrics/util/MetricsTimeVaryingRate.java:171:   *  {@link #resetMinMax()}
./src/core/org/apache/hadoop/metrics/util/MetricsTimeVaryingRate.java:172:   * @return max time for an operation
./src/core/org/apache/hadoop/net/DNS.java:46:   * @param hostIp
./src/core/org/apache/hadoop/net/DNS.java:48:   * @param ns
./src/core/org/apache/hadoop/net/DNS.java:50:   * @return The host name associated with the provided IP
./src/core/org/apache/hadoop/net/DNS.java:51:   * @throws NamingException
./src/core/org/apache/hadoop/net/DNS.java:79:   * @param strInterface
./src/core/org/apache/hadoop/net/DNS.java:81:   * @return A string vector of all the IPs associated with the provided
./src/core/org/apache/hadoop/net/DNS.java:83:   * @throws UnknownHostException
./src/core/org/apache/hadoop/net/DNS.java:111:   * @param strInterface
./src/core/org/apache/hadoop/net/DNS.java:113:   * @return The IP address in text form
./src/core/org/apache/hadoop/net/DNS.java:114:   * @throws UnknownHostException
./src/core/org/apache/hadoop/net/DNS.java:127:   * @param strInterface
./src/core/org/apache/hadoop/net/DNS.java:129:   * @param nameserver
./src/core/org/apache/hadoop/net/DNS.java:131:   * @return A string vector of all host names associated with the IPs tied to
./src/core/org/apache/hadoop/net/DNS.java:133:   * @throws UnknownHostException
./src/core/org/apache/hadoop/net/DNS.java:156:   * @param strInterface
./src/core/org/apache/hadoop/net/DNS.java:158:   * @return The list of host names associated with IPs bound to the network
./src/core/org/apache/hadoop/net/DNS.java:160:   * @throws UnknownHostException
./src/core/org/apache/hadoop/net/DNS.java:173:   * @param strInterface
./src/core/org/apache/hadoop/net/DNS.java:175:   * @param nameserver
./src/core/org/apache/hadoop/net/DNS.java:177:   * @return The default host names associated with IPs bound to the network
./src/core/org/apache/hadoop/net/DNS.java:179:   * @throws UnknownHostException
./src/core/org/apache/hadoop/net/DNS.java:198:   * @param strInterface
./src/core/org/apache/hadoop/net/DNS.java:200:   * @return The default host name associated with IPs bound to the network
./src/core/org/apache/hadoop/net/DNS.java:202:   * @throws UnknownHostException
./src/core/org/apache/hadoop/net/DNSToSwitchMapping.java:38:   * @param names
./src/core/org/apache/hadoop/net/DNSToSwitchMapping.java:39:   * @return list of resolved network paths
./src/core/org/apache/hadoop/net/NetUtils.java:56:   * @param conf the configuration
./src/core/org/apache/hadoop/net/NetUtils.java:57:   * @param clazz the class (usually a {@link VersionedProtocol})
./src/core/org/apache/hadoop/net/NetUtils.java:58:   * @return a socket factory
./src/core/org/apache/hadoop/net/NetUtils.java:80:   * @param conf the configuration
./src/core/org/apache/hadoop/net/NetUtils.java:81:   * @return the default socket factory as specified in the configuration or
./src/core/org/apache/hadoop/net/NetUtils.java:99:   * @param propValue the property which is the class name of the
./src/core/org/apache/hadoop/net/NetUtils.java:101:   * @return a socket factory as defined in the property value.
./src/core/org/apache/hadoop/net/NetUtils.java:166:   * @param conf the configuration to check
./src/core/org/apache/hadoop/net/NetUtils.java:167:   * @param oldBindAddressName the old address attribute name
./src/core/org/apache/hadoop/net/NetUtils.java:168:   * @param oldPortName the old port attribute name
./src/core/org/apache/hadoop/net/NetUtils.java:169:   * @param newBindAddressName the new combined name
./src/core/org/apache/hadoop/net/NetUtils.java:170:   * @return the complete address from the configuration
./src/core/org/apache/hadoop/net/NetUtils.java:172:  @Deprecated
./src/core/org/apache/hadoop/net/NetUtils.java:209:   * {@link NetUtils#getStaticResolution(String)} can be used to query for
./src/core/org/apache/hadoop/net/NetUtils.java:211:   * @param host
./src/core/org/apache/hadoop/net/NetUtils.java:212:   * @param resolvedName
./src/core/org/apache/hadoop/net/NetUtils.java:223:   * {@link NetUtils#addStaticResolution(String, String)}
./src/core/org/apache/hadoop/net/NetUtils.java:224:   * @param host
./src/core/org/apache/hadoop/net/NetUtils.java:225:   * @return the resolution
./src/core/org/apache/hadoop/net/NetUtils.java:235:   * {@link NetUtils#addStaticResolution(String, String)}. The return
./src/core/org/apache/hadoop/net/NetUtils.java:238:   * @return the list of resolutions
./src/core/org/apache/hadoop/net/NetUtils.java:260:   * @param server
./src/core/org/apache/hadoop/net/NetUtils.java:261:   * @return socket address that a client can use to connect to the server.
./src/core/org/apache/hadoop/net/NetUtils.java:274:   * From documentation for {@link #getInputStream(Socket, long)}:<br>
./src/core/org/apache/hadoop/net/NetUtils.java:277:   * {@link SocketInputStream} with the given timeout. If the socket does not
./src/core/org/apache/hadoop/net/NetUtils.java:278:   * have a channel, {@link Socket#getInputStream()} is returned. In the later
./src/core/org/apache/hadoop/net/NetUtils.java:280:   * {@link Socket#setSoTimeout(int)} applies for reads.<br><br>
./src/core/org/apache/hadoop/net/NetUtils.java:282:   * Any socket created using socket factories returned by {@link #NetUtils},
./src/core/org/apache/hadoop/net/NetUtils.java:283:   * must use this interface instead of {@link Socket#getInputStream()}.
./src/core/org/apache/hadoop/net/NetUtils.java:285:   * @see #getInputStream(Socket, long)
./src/core/org/apache/hadoop/net/NetUtils.java:287:   * @param socket
./src/core/org/apache/hadoop/net/NetUtils.java:288:   * @return InputStream for reading from the socket.
./src/core/org/apache/hadoop/net/NetUtils.java:289:   * @throws IOException
./src/core/org/apache/hadoop/net/NetUtils.java:299:   * {@link SocketInputStream} with the given timeout. If the socket does not
./src/core/org/apache/hadoop/net/NetUtils.java:300:   * have a channel, {@link Socket#getInputStream()} is returned. In the later
./src/core/org/apache/hadoop/net/NetUtils.java:302:   * {@link Socket#setSoTimeout(int)} applies for reads.<br><br>
./src/core/org/apache/hadoop/net/NetUtils.java:304:   * Any socket created using socket factories returned by {@link #NetUtils},
./src/core/org/apache/hadoop/net/NetUtils.java:305:   * must use this interface instead of {@link Socket#getInputStream()}.
./src/core/org/apache/hadoop/net/NetUtils.java:307:   * @see Socket#getChannel()
./src/core/org/apache/hadoop/net/NetUtils.java:309:   * @param socket
./src/core/org/apache/hadoop/net/NetUtils.java:310:   * @param timeout timeout in milliseconds. This may not always apply. zero
./src/core/org/apache/hadoop/net/NetUtils.java:312:   * @return InputStream for reading from the socket.
./src/core/org/apache/hadoop/net/NetUtils.java:313:   * @throws IOException
./src/core/org/apache/hadoop/net/NetUtils.java:325:   * From documentation for {@link #getOutputStream(Socket, long)} : <br>
./src/core/org/apache/hadoop/net/NetUtils.java:328:   * {@link SocketOutputStream} with the given timeout. If the socket does not
./src/core/org/apache/hadoop/net/NetUtils.java:329:   * have a channel, {@link Socket#getOutputStream()} is returned. In the later
./src/core/org/apache/hadoop/net/NetUtils.java:333:   * Any socket created using socket factories returned by {@link #NetUtils},
./src/core/org/apache/hadoop/net/NetUtils.java:334:   * must use this interface instead of {@link Socket#getOutputStream()}.
./src/core/org/apache/hadoop/net/NetUtils.java:336:   * @see #getOutputStream(Socket, long)
./src/core/org/apache/hadoop/net/NetUtils.java:338:   * @param socket
./src/core/org/apache/hadoop/net/NetUtils.java:339:   * @return OutputStream for writing to the socket.
./src/core/org/apache/hadoop/net/NetUtils.java:340:   * @throws IOException
./src/core/org/apache/hadoop/net/NetUtils.java:350:   * {@link SocketOutputStream} with the given timeout. If the socket does not
./src/core/org/apache/hadoop/net/NetUtils.java:351:   * have a channel, {@link Socket#getOutputStream()} is returned. In the later
./src/core/org/apache/hadoop/net/NetUtils.java:355:   * Any socket created using socket factories returned by {@link #NetUtils},
./src/core/org/apache/hadoop/net/NetUtils.java:356:   * must use this interface instead of {@link Socket#getOutputStream()}.
./src/core/org/apache/hadoop/net/NetUtils.java:358:   * @see Socket#getChannel()
./src/core/org/apache/hadoop/net/NetUtils.java:360:   * @param socket
./src/core/org/apache/hadoop/net/NetUtils.java:361:   * @param timeout timeout in milliseconds. This may not always apply. zero
./src/core/org/apache/hadoop/net/NetUtils.java:363:   * @return OutputStream for writing to the socket.
./src/core/org/apache/hadoop/net/NetUtils.java:364:   * @throws IOException   
./src/core/org/apache/hadoop/net/NetUtils.java:376:   * @param name a string representation of a host:
./src/core/org/apache/hadoop/net/NetUtils.java:378:   * @return its IP address in the string format
./src/core/org/apache/hadoop/net/NetUtils.java:397:   * @param names a collection of string representations of hosts
./src/core/org/apache/hadoop/net/NetUtils.java:398:   * @return a list of corresponding IP addresses in the string format
./src/core/org/apache/hadoop/net/NetUtils.java:399:   * @see #normalizeHostName(String)
./src/core/org/apache/hadoop/net/NetworkTopology.java:94:     * @param n a node
./src/core/org/apache/hadoop/net/NetworkTopology.java:95:     * @return true if this node is an ancestor of <i>n</i>
./src/core/org/apache/hadoop/net/NetworkTopology.java:105:     * @param n a node
./src/core/org/apache/hadoop/net/NetworkTopology.java:106:     * @return true if this node is the parent of <i>n</i>
./src/core/org/apache/hadoop/net/NetworkTopology.java:129:     * @param n node to be added
./src/core/org/apache/hadoop/net/NetworkTopology.java:130:     * @return true if the node is added; false otherwise
./src/core/org/apache/hadoop/net/NetworkTopology.java:177:     * @param n node to be deleted 
./src/core/org/apache/hadoop/net/NetworkTopology.java:178:     * @return true if the node is deleted; false otherwise
./src/core/org/apache/hadoop/net/NetworkTopology.java:308:   * @param node
./src/core/org/apache/hadoop/net/NetworkTopology.java:310:   * @exception IllegalArgumentException if add a node to a leave 
./src/core/org/apache/hadoop/net/NetworkTopology.java:341:   * @param node
./src/core/org/apache/hadoop/net/NetworkTopology.java:367:   * @param node
./src/core/org/apache/hadoop/net/NetworkTopology.java:369:   * @return true if <i>node</i> is already in the tree; false otherwise
./src/core/org/apache/hadoop/net/NetworkTopology.java:389:   * @param loc
./src/core/org/apache/hadoop/net/NetworkTopology.java:391:   * @return a reference to the node; null if the node is not in the tree
./src/core/org/apache/hadoop/net/NetworkTopology.java:429:   * @param node1 one node
./src/core/org/apache/hadoop/net/NetworkTopology.java:430:   * @param node2 another node
./src/core/org/apache/hadoop/net/NetworkTopology.java:431:   * @return the distance between node1 and node2
./src/core/org/apache/hadoop/net/NetworkTopology.java:473:   * @param node1 one node
./src/core/org/apache/hadoop/net/NetworkTopology.java:474:   * @param node2 another node
./src/core/org/apache/hadoop/net/NetworkTopology.java:475:   * @return true if node1 and node2 are pm the same rack; false otherwise
./src/core/org/apache/hadoop/net/NetworkTopology.java:476:   * @exception IllegalArgumentException when either node1 or node2 is null, or
./src/core/org/apache/hadoop/net/NetworkTopology.java:496:   * @param scope range of nodes from which a node will be choosen
./src/core/org/apache/hadoop/net/NetworkTopology.java:497:   * @return the choosen node
./src/core/org/apache/hadoop/net/NetworkTopology.java:544:   * @param scope a path string that may start with ~
./src/core/org/apache/hadoop/net/NetworkTopology.java:545:   * @param excludedNodes a list of nodes
./src/core/org/apache/hadoop/net/NetworkTopology.java:546:   * @return number of available nodes
./src/core/org/apache/hadoop/net/NodeBase.java:39:   * @param path 
./src/core/org/apache/hadoop/net/NodeBase.java:53:   * @param name this node's name 
./src/core/org/apache/hadoop/net/NodeBase.java:54:   * @param location this node's location 
./src/core/org/apache/hadoop/net/NodeBase.java:61:   * @param name this node's name 
./src/core/org/apache/hadoop/net/NodeBase.java:62:   * @param location this node's location 
./src/core/org/apache/hadoop/net/NodeBase.java:63:   * @param parent this node's parent node
./src/core/org/apache/hadoop/net/NodeBase.java:64:   * @param level this node's level in the tree
./src/core/org/apache/hadoop/net/ScriptBasedMapping.java:31: * This class implements the {@link DNSToSwitchMapping} interface using a 
./src/core/org/apache/hadoop/net/SocketInputStream.java:36: * {@link Socket#getInputStream()} and write() on 
./src/core/org/apache/hadoop/net/SocketInputStream.java:37: * {@link Socket#getOutputStream()} for the associated socket will throw 
./src/core/org/apache/hadoop/net/SocketInputStream.java:39: * Please use {@link SocketOutputStream} for writing.
./src/core/org/apache/hadoop/net/SocketInputStream.java:64:   * @param channel 
./src/core/org/apache/hadoop/net/SocketInputStream.java:65:   *        Channel for reading, should also be a {@link SelectableChannel}.
./src/core/org/apache/hadoop/net/SocketInputStream.java:67:   * @param timeout timeout in milliseconds. must not be negative.
./src/core/org/apache/hadoop/net/SocketInputStream.java:68:   * @throws IOException
./src/core/org/apache/hadoop/net/SocketInputStream.java:83:   * @see SocketInputStream#SocketInputStream(ReadableByteChannel, long)
./src/core/org/apache/hadoop/net/SocketInputStream.java:85:   * @param socket should have a channel associated with it.
./src/core/org/apache/hadoop/net/SocketInputStream.java:86:   * @param timeout timeout timeout in milliseconds. must not be negative.
./src/core/org/apache/hadoop/net/SocketInputStream.java:87:   * @throws IOException
./src/core/org/apache/hadoop/net/SocketInputStream.java:101:   * @see SocketInputStream#SocketInputStream(ReadableByteChannel, long)
./src/core/org/apache/hadoop/net/SocketInputStream.java:103:   * @param socket should have a channel associated with it.
./src/core/org/apache/hadoop/net/SocketInputStream.java:104:   * @throws IOException
./src/core/org/apache/hadoop/net/SocketInputStream.java:110:  @Override
./src/core/org/apache/hadoop/net/SocketInputStream.java:137:   * {@link FileChannel#transferFrom(ReadableByteChannel, long, long)}.
./src/core/org/apache/hadoop/net/SocketInputStream.java:157:   * @throws SocketTimeoutException 
./src/core/org/apache/hadoop/net/SocketInputStream.java:159:   * @throws IOException
./src/core/org/apache/hadoop/net/SocketIOWithTimeout.java:102:   * @param buf
./src/core/org/apache/hadoop/net/SocketIOWithTimeout.java:103:   * @return number of bytes (or some equivalent). 0 implies underlying
./src/core/org/apache/hadoop/net/SocketIOWithTimeout.java:106:   * @throws IOException
./src/core/org/apache/hadoop/net/SocketIOWithTimeout.java:115:   * @param buf buffer for IO
./src/core/org/apache/hadoop/net/SocketIOWithTimeout.java:116:   * @param ops Selection Ops used for waiting. Suggested values: 
./src/core/org/apache/hadoop/net/SocketIOWithTimeout.java:120:   * @return number of bytes read or written. negative implies end of stream.
./src/core/org/apache/hadoop/net/SocketIOWithTimeout.java:121:   * @throws IOException
./src/core/org/apache/hadoop/net/SocketIOWithTimeout.java:171:   * This is similar to {@link #doIO(ByteBuffer, int)} except that it
./src/core/org/apache/hadoop/net/SocketIOWithTimeout.java:175:   * @param ops Selection Ops used for waiting
./src/core/org/apache/hadoop/net/SocketIOWithTimeout.java:177:   * @throws SocketTimeoutException 
./src/core/org/apache/hadoop/net/SocketIOWithTimeout.java:179:   * @throws IOException
./src/core/org/apache/hadoop/net/SocketIOWithTimeout.java:241:     * @param channel
./src/core/org/apache/hadoop/net/SocketIOWithTimeout.java:242:     * @param ops
./src/core/org/apache/hadoop/net/SocketIOWithTimeout.java:243:     * @param timeout
./src/core/org/apache/hadoop/net/SocketIOWithTimeout.java:244:     * @return
./src/core/org/apache/hadoop/net/SocketIOWithTimeout.java:245:     * @throws IOException
./src/core/org/apache/hadoop/net/SocketIOWithTimeout.java:308:     * @param channel
./src/core/org/apache/hadoop/net/SocketIOWithTimeout.java:309:     * @return 
./src/core/org/apache/hadoop/net/SocketIOWithTimeout.java:310:     * @throws IOException
./src/core/org/apache/hadoop/net/SocketIOWithTimeout.java:351:     * @param info
./src/core/org/apache/hadoop/net/SocketOutputStream.java:36: * {@link Socket#getInputStream()} and write() on 
./src/core/org/apache/hadoop/net/SocketOutputStream.java:37: * {@link Socket#getOutputStream()} on the associated socket will throw 
./src/core/org/apache/hadoop/net/SocketOutputStream.java:39: * Please use {@link SocketInputStream} for reading.
./src/core/org/apache/hadoop/net/SocketOutputStream.java:64:   * @param channel 
./src/core/org/apache/hadoop/net/SocketOutputStream.java:65:   *        Channel for writing, should also be a {@link SelectableChannel}.  
./src/core/org/apache/hadoop/net/SocketOutputStream.java:67:   * @param timeout timeout in milliseconds. must not be negative.
./src/core/org/apache/hadoop/net/SocketOutputStream.java:68:   * @throws IOException
./src/core/org/apache/hadoop/net/SocketOutputStream.java:83:   * @see SocketOutputStream#SocketOutputStream(WritableByteChannel, long)
./src/core/org/apache/hadoop/net/SocketOutputStream.java:85:   * @param socket should have a channel associated with it.
./src/core/org/apache/hadoop/net/SocketOutputStream.java:86:   * @param timeout timeout timeout in milliseconds. must not be negative.
./src/core/org/apache/hadoop/net/SocketOutputStream.java:87:   * @throws IOException
./src/core/org/apache/hadoop/net/SocketOutputStream.java:133:   * {@link FileChannel#transferTo(long, long, WritableByteChannel)}
./src/core/org/apache/hadoop/net/SocketOutputStream.java:153:   * @throws SocketTimeoutException 
./src/core/org/apache/hadoop/net/SocketOutputStream.java:155:   * @throws IOException
./src/core/org/apache/hadoop/net/SocketOutputStream.java:164:   * {@link FileChannel#transferTo(long, long, WritableByteChannel)}. 
./src/core/org/apache/hadoop/net/SocketOutputStream.java:169:   * @param fileCh FileChannel to transfer data from.
./src/core/org/apache/hadoop/net/SocketOutputStream.java:170:   * @param position position within the channel where the transfer begins
./src/core/org/apache/hadoop/net/SocketOutputStream.java:171:   * @param count number of bytes to transfer.
./src/core/org/apache/hadoop/net/SocketOutputStream.java:173:   * @throws EOFException 
./src/core/org/apache/hadoop/net/SocketOutputStream.java:177:   * @throws SocketTimeoutException 
./src/core/org/apache/hadoop/net/SocketOutputStream.java:181:   * @throws IOException Includes any exception thrown by 
./src/core/org/apache/hadoop/net/SocketOutputStream.java:182:   *         {@link FileChannel#transferTo(long, long, WritableByteChannel)}. 
./src/core/org/apache/hadoop/net/SocksSocketFactory.java:52:   * @param proxy the proxy to use to create sockets
./src/core/org/apache/hadoop/net/SocksSocketFactory.java:58:  /* @inheritDoc */
./src/core/org/apache/hadoop/net/SocksSocketFactory.java:59:  @Override
./src/core/org/apache/hadoop/net/SocksSocketFactory.java:65:  /* @inheritDoc */
./src/core/org/apache/hadoop/net/SocksSocketFactory.java:66:  @Override
./src/core/org/apache/hadoop/net/SocksSocketFactory.java:74:  /* @inheritDoc */
./src/core/org/apache/hadoop/net/SocksSocketFactory.java:75:  @Override
./src/core/org/apache/hadoop/net/SocksSocketFactory.java:85:  /* @inheritDoc */
./src/core/org/apache/hadoop/net/SocksSocketFactory.java:86:  @Override
./src/core/org/apache/hadoop/net/SocksSocketFactory.java:95:  /* @inheritDoc */
./src/core/org/apache/hadoop/net/SocksSocketFactory.java:96:  @Override
./src/core/org/apache/hadoop/net/SocksSocketFactory.java:107:  /* @inheritDoc */
./src/core/org/apache/hadoop/net/SocksSocketFactory.java:108:  @Override
./src/core/org/apache/hadoop/net/SocksSocketFactory.java:113:  /* @inheritDoc */
./src/core/org/apache/hadoop/net/SocksSocketFactory.java:114:  @Override
./src/core/org/apache/hadoop/net/SocksSocketFactory.java:131:  /* @inheritDoc */
./src/core/org/apache/hadoop/net/SocksSocketFactory.java:136:  /* @inheritDoc */
./src/core/org/apache/hadoop/net/SocksSocketFactory.java:149:   * @param proxyStr the proxy address using the format "host:port"
./src/core/org/apache/hadoop/net/StandardSocketFactory.java:40:  /* @inheritDoc */
./src/core/org/apache/hadoop/net/StandardSocketFactory.java:41:  @Override
./src/core/org/apache/hadoop/net/StandardSocketFactory.java:61:  /* @inheritDoc */
./src/core/org/apache/hadoop/net/StandardSocketFactory.java:62:  @Override
./src/core/org/apache/hadoop/net/StandardSocketFactory.java:70:  /* @inheritDoc */
./src/core/org/apache/hadoop/net/StandardSocketFactory.java:71:  @Override
./src/core/org/apache/hadoop/net/StandardSocketFactory.java:81:  /* @inheritDoc */
./src/core/org/apache/hadoop/net/StandardSocketFactory.java:82:  @Override
./src/core/org/apache/hadoop/net/StandardSocketFactory.java:91:  /* @inheritDoc */
./src/core/org/apache/hadoop/net/StandardSocketFactory.java:92:  @Override
./src/core/org/apache/hadoop/net/StandardSocketFactory.java:103:  /* @inheritDoc */
./src/core/org/apache/hadoop/net/StandardSocketFactory.java:104:  @Override
./src/core/org/apache/hadoop/net/StandardSocketFactory.java:115:  /* @inheritDoc */
./src/core/org/apache/hadoop/net/StandardSocketFactory.java:116:  @Override
./src/core/org/apache/hadoop/record/BinaryRecordInput.java:59:   * @param inp data input stream
./src/core/org/apache/hadoop/record/BinaryRecordInput.java:60:   * @return binary record input corresponding to the supplied DataInput.
./src/core/org/apache/hadoop/record/BinaryRecordOutput.java:48:   * @param out data output stream
./src/core/org/apache/hadoop/record/BinaryRecordOutput.java:49:   * @return binary record output corresponding to the supplied DataOutput.
./src/core/org/apache/hadoop/record/Buffer.java:45:   * @param bytes This array becomes the backing storage for the object.
./src/core/org/apache/hadoop/record/Buffer.java:55:   * @param bytes Copy of this array becomes the backing storage for the object.
./src/core/org/apache/hadoop/record/Buffer.java:56:   * @param offset offset into byte array
./src/core/org/apache/hadoop/record/Buffer.java:57:   * @param length length of data
./src/core/org/apache/hadoop/record/Buffer.java:67:   * @param bytes byte sequence
./src/core/org/apache/hadoop/record/Buffer.java:77:   * @param bytes byte array to be assigned
./src/core/org/apache/hadoop/record/Buffer.java:78:   * @param offset offset into byte array
./src/core/org/apache/hadoop/record/Buffer.java:79:   * @param length length of data
./src/core/org/apache/hadoop/record/Buffer.java:92:   * @return The data is only valid between 0 and getCount() - 1.
./src/core/org/apache/hadoop/record/Buffer.java:112:   * @return The number of bytes
./src/core/org/apache/hadoop/record/Buffer.java:121:   * @param newCapacity The new capacity in bytes.
./src/core/org/apache/hadoop/record/Buffer.java:162:   * @param bytes byte array to be appended
./src/core/org/apache/hadoop/record/Buffer.java:163:   * @param offset offset into byte array
./src/core/org/apache/hadoop/record/Buffer.java:164:   * @param length length of data
./src/core/org/apache/hadoop/record/Buffer.java:176:   * @param bytes byte array to be appended
./src/core/org/apache/hadoop/record/Buffer.java:194:   * @param other The other buffer
./src/core/org/apache/hadoop/record/Buffer.java:195:   * @return Positive if this is bigger than other, 0 if they are equal, and
./src/core/org/apache/hadoop/record/Buffer.java:233:   * @param charsetName Valid Java Character Set Name
./src/core/org/apache/hadoop/record/compiler/ant/RccTask.java:64:   * @param language "java"/"c++"
./src/core/org/apache/hadoop/record/compiler/ant/RccTask.java:72:   * @param file record definition file
./src/core/org/apache/hadoop/record/compiler/ant/RccTask.java:80:   * @param flag true will throw build exception in case of failure (default)
./src/core/org/apache/hadoop/record/compiler/ant/RccTask.java:88:   * @param dir output directory
./src/core/org/apache/hadoop/record/compiler/ant/RccTask.java:96:   * @param set Set of record definition files
./src/core/org/apache/hadoop/record/compiler/JavaGenerator.java:37:   * @param name possibly full pathname to the file
./src/core/org/apache/hadoop/record/compiler/JavaGenerator.java:38:   * @param ilist included files (as JFile)
./src/core/org/apache/hadoop/record/compiler/JavaGenerator.java:39:   * @param rlist List of records defined within this file
./src/core/org/apache/hadoop/record/compiler/JavaGenerator.java:40:   * @param destDir output directory
./src/core/org/apache/hadoop/record/compiler/JFile.java:39:   * @param name possibly full pathname to the file
./src/core/org/apache/hadoop/record/compiler/JFile.java:40:   * @param inclFiles included files (as JFile)
./src/core/org/apache/hadoop/record/compiler/JFile.java:41:   * @param recList List of records defined within this file
./src/core/org/apache/hadoop/record/meta/RecordTypeInfo.java:55:   * @param name Name of the record
./src/core/org/apache/hadoop/record/meta/RecordTypeInfo.java:86:   * @param fieldName Name of the field
./src/core/org/apache/hadoop/record/meta/RecordTypeInfo.java:87:   * @param tid Type ID of the field
./src/core/org/apache/hadoop/record/meta/RecordTypeInfo.java:107:   * @param name Name of the nested record
./src/core/org/apache/hadoop/record/Record.java:35:   * @param rout Record output destination
./src/core/org/apache/hadoop/record/Record.java:36:   * @param tag record tag (Used only in tagged serialization e.g. XML)
./src/core/org/apache/hadoop/record/Record.java:43:   * @param rin Record input source
./src/core/org/apache/hadoop/record/Record.java:44:   * @param tag Record tag (Used only in tagged serialization e.g. XML)
./src/core/org/apache/hadoop/record/Record.java:54:   * @param rout Record output destination
./src/core/org/apache/hadoop/record/Record.java:62:   * @param rin Record input source
./src/core/org/apache/hadoop/record/RecordComparator.java:30:   * Construct a raw {@link Record} comparison implementation. */
./src/core/org/apache/hadoop/record/RecordComparator.java:39:   * Register an optimized comparator for a {@link Record} implementation.
./src/core/org/apache/hadoop/record/RecordComparator.java:41:   * @param c record classs for which a raw comparator is provided
./src/core/org/apache/hadoop/record/RecordComparator.java:42:   * @param comparator Raw comparator instance for class c 
./src/core/org/apache/hadoop/record/RecordInput.java:29:   * @param tag Used by tagged serialization formats (such as XML)
./src/core/org/apache/hadoop/record/RecordInput.java:30:   * @return value read from serialized record.
./src/core/org/apache/hadoop/record/RecordInput.java:36:   * @param tag Used by tagged serialization formats (such as XML)
./src/core/org/apache/hadoop/record/RecordInput.java:37:   * @return value read from serialized record.
./src/core/org/apache/hadoop/record/RecordInput.java:43:   * @param tag Used by tagged serialization formats (such as XML)
./src/core/org/apache/hadoop/record/RecordInput.java:44:   * @return value read from serialized record.
./src/core/org/apache/hadoop/record/RecordInput.java:50:   * @param tag Used by tagged serialization formats (such as XML)
./src/core/org/apache/hadoop/record/RecordInput.java:51:   * @return value read from serialized record.
./src/core/org/apache/hadoop/record/RecordInput.java:57:   * @param tag Used by tagged serialization formats (such as XML)
./src/core/org/apache/hadoop/record/RecordInput.java:58:   * @return value read from serialized record.
./src/core/org/apache/hadoop/record/RecordInput.java:64:   * @param tag Used by tagged serialization formats (such as XML)
./src/core/org/apache/hadoop/record/RecordInput.java:65:   * @return value read from serialized record.
./src/core/org/apache/hadoop/record/RecordInput.java:71:   * @param tag Used by tagged serialization formats (such as XML)
./src/core/org/apache/hadoop/record/RecordInput.java:72:   * @return value read from serialized record.
./src/core/org/apache/hadoop/record/RecordInput.java:78:   * @param tag Used by tagged serialization formats (such as XML)
./src/core/org/apache/hadoop/record/RecordInput.java:79:   * @return value read from serialized record.
./src/core/org/apache/hadoop/record/RecordInput.java:85:   * @param tag Used by tagged serialization formats (such as XML)
./src/core/org/apache/hadoop/record/RecordInput.java:91:   * @param tag Used by tagged serialization formats (such as XML)
./src/core/org/apache/hadoop/record/RecordInput.java:97:   * @param tag Used by tagged serialization formats (such as XML)
./src/core/org/apache/hadoop/record/RecordInput.java:98:   * @return Index that is used to count the number of elements.
./src/core/org/apache/hadoop/record/RecordInput.java:104:   * @param tag Used by tagged serialization formats (such as XML)
./src/core/org/apache/hadoop/record/RecordInput.java:110:   * @param tag Used by tagged serialization formats (such as XML)
./src/core/org/apache/hadoop/record/RecordInput.java:111:   * @return Index that is used to count the number of map entries.
./src/core/org/apache/hadoop/record/RecordInput.java:117:   * @param tag Used by tagged serialization formats (such as XML)
./src/core/org/apache/hadoop/record/RecordOutput.java:31:   * @param b Byte to be serialized
./src/core/org/apache/hadoop/record/RecordOutput.java:32:   * @param tag Used by tagged serialization formats (such as XML)
./src/core/org/apache/hadoop/record/RecordOutput.java:33:   * @throws IOException Indicates error in serialization
./src/core/org/apache/hadoop/record/RecordOutput.java:39:   * @param b Boolean to be serialized
./src/core/org/apache/hadoop/record/RecordOutput.java:40:   * @param tag Used by tagged serialization formats (such as XML)
./src/core/org/apache/hadoop/record/RecordOutput.java:41:   * @throws IOException Indicates error in serialization
./src/core/org/apache/hadoop/record/RecordOutput.java:47:   * @param i Integer to be serialized
./src/core/org/apache/hadoop/record/RecordOutput.java:48:   * @param tag Used by tagged serialization formats (such as XML)
./src/core/org/apache/hadoop/record/RecordOutput.java:49:   * @throws IOException Indicates error in serialization
./src/core/org/apache/hadoop/record/RecordOutput.java:55:   * @param l Long to be serialized
./src/core/org/apache/hadoop/record/RecordOutput.java:56:   * @param tag Used by tagged serialization formats (such as XML)
./src/core/org/apache/hadoop/record/RecordOutput.java:57:   * @throws IOException Indicates error in serialization
./src/core/org/apache/hadoop/record/RecordOutput.java:63:   * @param f Float to be serialized
./src/core/org/apache/hadoop/record/RecordOutput.java:64:   * @param tag Used by tagged serialization formats (such as XML)
./src/core/org/apache/hadoop/record/RecordOutput.java:65:   * @throws IOException Indicates error in serialization
./src/core/org/apache/hadoop/record/RecordOutput.java:71:   * @param d Double to be serialized
./src/core/org/apache/hadoop/record/RecordOutput.java:72:   * @param tag Used by tagged serialization formats (such as XML)
./src/core/org/apache/hadoop/record/RecordOutput.java:73:   * @throws IOException Indicates error in serialization
./src/core/org/apache/hadoop/record/RecordOutput.java:79:   * @param s String to be serialized
./src/core/org/apache/hadoop/record/RecordOutput.java:80:   * @param tag Used by tagged serialization formats (such as XML)
./src/core/org/apache/hadoop/record/RecordOutput.java:81:   * @throws IOException Indicates error in serialization
./src/core/org/apache/hadoop/record/RecordOutput.java:87:   * @param buf Buffer to be serialized
./src/core/org/apache/hadoop/record/RecordOutput.java:88:   * @param tag Used by tagged serialization formats (such as XML)
./src/core/org/apache/hadoop/record/RecordOutput.java:89:   * @throws IOException Indicates error in serialization
./src/core/org/apache/hadoop/record/RecordOutput.java:96:   * @param r Record to be serialized
./src/core/org/apache/hadoop/record/RecordOutput.java:97:   * @param tag Used by tagged serialization formats (such as XML)
./src/core/org/apache/hadoop/record/RecordOutput.java:98:   * @throws IOException Indicates error in serialization
./src/core/org/apache/hadoop/record/RecordOutput.java:104:   * @param r Record to be serialized
./src/core/org/apache/hadoop/record/RecordOutput.java:105:   * @param tag Used by tagged serialization formats (such as XML)
./src/core/org/apache/hadoop/record/RecordOutput.java:106:   * @throws IOException Indicates error in serialization
./src/core/org/apache/hadoop/record/RecordOutput.java:112:   * @param v Vector to be serialized
./src/core/org/apache/hadoop/record/RecordOutput.java:113:   * @param tag Used by tagged serialization formats (such as XML)
./src/core/org/apache/hadoop/record/RecordOutput.java:114:   * @throws IOException Indicates error in serialization
./src/core/org/apache/hadoop/record/RecordOutput.java:120:   * @param v Vector to be serialized
./src/core/org/apache/hadoop/record/RecordOutput.java:121:   * @param tag Used by tagged serialization formats (such as XML)
./src/core/org/apache/hadoop/record/RecordOutput.java:122:   * @throws IOException Indicates error in serialization
./src/core/org/apache/hadoop/record/RecordOutput.java:128:   * @param m Map to be serialized
./src/core/org/apache/hadoop/record/RecordOutput.java:129:   * @param tag Used by tagged serialization formats (such as XML)
./src/core/org/apache/hadoop/record/RecordOutput.java:130:   * @throws IOException Indicates error in serialization
./src/core/org/apache/hadoop/record/RecordOutput.java:136:   * @param m Map to be serialized
./src/core/org/apache/hadoop/record/RecordOutput.java:137:   * @param tag Used by tagged serialization formats (such as XML)
./src/core/org/apache/hadoop/record/RecordOutput.java:138:   * @throws IOException Indicates error in serialization
./src/core/org/apache/hadoop/record/Utils.java:41:   * @param s
./src/core/org/apache/hadoop/record/Utils.java:42:   * @return
./src/core/org/apache/hadoop/record/Utils.java:82:   * @param s
./src/core/org/apache/hadoop/record/Utils.java:83:   * @return
./src/core/org/apache/hadoop/record/Utils.java:105:   * @param s
./src/core/org/apache/hadoop/record/Utils.java:106:   * @return
./src/core/org/apache/hadoop/record/Utils.java:142:   * @param s
./src/core/org/apache/hadoop/record/Utils.java:143:   * @throws java.io.IOException
./src/core/org/apache/hadoop/record/Utils.java:144:   * @return
./src/core/org/apache/hadoop/record/Utils.java:182:   * @param s
./src/core/org/apache/hadoop/record/Utils.java:183:   * @return
./src/core/org/apache/hadoop/record/Utils.java:191:   * @param s
./src/core/org/apache/hadoop/record/Utils.java:192:   * @throws java.io.IOException
./src/core/org/apache/hadoop/record/Utils.java:193:   * @return
./src/core/org/apache/hadoop/record/Utils.java:210:   * @param buf
./src/core/org/apache/hadoop/record/Utils.java:211:   * @return
./src/core/org/apache/hadoop/record/Utils.java:222:   * @param s CSV-serialized representation of buffer
./src/core/org/apache/hadoop/record/Utils.java:223:   * @throws java.io.IOException
./src/core/org/apache/hadoop/record/Utils.java:224:   * @return Deserialized Buffer
./src/core/org/apache/hadoop/record/Utils.java:407:   * @param bytes byte array with decode long
./src/core/org/apache/hadoop/record/Utils.java:408:   * @param start starting index
./src/core/org/apache/hadoop/record/Utils.java:409:   * @throws java.io.IOException
./src/core/org/apache/hadoop/record/Utils.java:410:   * @return deserialized long
./src/core/org/apache/hadoop/record/Utils.java:418:   * @param bytes byte array with the encoded integer
./src/core/org/apache/hadoop/record/Utils.java:419:   * @param start start index
./src/core/org/apache/hadoop/record/Utils.java:420:   * @throws java.io.IOException
./src/core/org/apache/hadoop/record/Utils.java:421:   * @return deserialized integer
./src/core/org/apache/hadoop/record/Utils.java:429:   * @param in input stream
./src/core/org/apache/hadoop/record/Utils.java:430:   * @throws java.io.IOException
./src/core/org/apache/hadoop/record/Utils.java:431:   * @return deserialized long
./src/core/org/apache/hadoop/record/Utils.java:439:   * @param in input stream
./src/core/org/apache/hadoop/record/Utils.java:440:   * @throws java.io.IOException
./src/core/org/apache/hadoop/record/Utils.java:441:   * @return deserialized integer
./src/core/org/apache/hadoop/record/Utils.java:449:   * @return the encoded length
./src/core/org/apache/hadoop/record/Utils.java:466:   * @param stream Binary output stream
./src/core/org/apache/hadoop/record/Utils.java:467:   * @param i Long to be serialized
./src/core/org/apache/hadoop/record/Utils.java:468:   * @throws java.io.IOException
./src/core/org/apache/hadoop/record/Utils.java:477:   * @param stream Binary output stream
./src/core/org/apache/hadoop/record/Utils.java:478:   * @param i int to be serialized
./src/core/org/apache/hadoop/record/Utils.java:479:   * @throws java.io.IOException
./src/core/org/apache/hadoop/security/AccessControlException.java:26:  //Required by {@link java.io.Serializable}.
./src/core/org/apache/hadoop/security/AccessControlException.java:31:   * {@link org.apache.hadoop.ipc.RemoteException}.
./src/core/org/apache/hadoop/security/AccessControlException.java:38:   * Constructs an {@link AccessControlException}
./src/core/org/apache/hadoop/security/AccessControlException.java:40:   * @param s the detail message.
./src/core/org/apache/hadoop/security/UnixUserGroupInformation.java:42:  /** Create an immutable {@link UnixUserGroupInformation} object. */
./src/core/org/apache/hadoop/security/UnixUserGroupInformation.java:62:   * @param userName a user's name
./src/core/org/apache/hadoop/security/UnixUserGroupInformation.java:63:   * @param groupNames groups list, first of which is the default group
./src/core/org/apache/hadoop/security/UnixUserGroupInformation.java:64:   * @exception IllegalArgumentException if any argument is null
./src/core/org/apache/hadoop/security/UnixUserGroupInformation.java:72:   * @param ugi an array containing user/group names, the first
./src/core/org/apache/hadoop/security/UnixUserGroupInformation.java:75:   * @exception IllegalArgumentException if the array size is less than 2 
./src/core/org/apache/hadoop/security/UnixUserGroupInformation.java:90:   * @param userName a user's name
./src/core/org/apache/hadoop/security/UnixUserGroupInformation.java:91:   * @param groupNames groups list, the first of which is the default group
./src/core/org/apache/hadoop/security/UnixUserGroupInformation.java:92:   * @exception IllegalArgumentException if any argument is null
./src/core/org/apache/hadoop/security/UnixUserGroupInformation.java:128:   *  @param in input stream
./src/core/org/apache/hadoop/security/UnixUserGroupInformation.java:129:   *  @exception IOException is thrown if encounter any error when reading
./src/core/org/apache/hadoop/security/UnixUserGroupInformation.java:152:   * @param out output stream
./src/core/org/apache/hadoop/security/UnixUserGroupInformation.java:153:   * @exception IOException if encounter any error during writing
./src/core/org/apache/hadoop/security/UnixUserGroupInformation.java:176:   * @param conf configuration
./src/core/org/apache/hadoop/security/UnixUserGroupInformation.java:177:   * @param attr property name
./src/core/org/apache/hadoop/security/UnixUserGroupInformation.java:178:   * @param ugi a UnixUserGroupInformation
./src/core/org/apache/hadoop/security/UnixUserGroupInformation.java:196:   * @param conf configuration
./src/core/org/apache/hadoop/security/UnixUserGroupInformation.java:197:   * @param attr property name
./src/core/org/apache/hadoop/security/UnixUserGroupInformation.java:198:   * @return a UnixUGI
./src/core/org/apache/hadoop/security/UnixUserGroupInformation.java:199:   * @throws LoginException if the stored string is ill-formatted.
./src/core/org/apache/hadoop/security/UnixUserGroupInformation.java:266:   *  @param conf either a job configuration or client's configuration
./src/core/org/apache/hadoop/security/UnixUserGroupInformation.java:267:   *  @param save saving it to conf?
./src/core/org/apache/hadoop/security/UnixUserGroupInformation.java:268:   *  @return UnixUserGroupInformation a user/group information
./src/core/org/apache/hadoop/security/UnixUserGroupInformation.java:269:   *  @exception LoginException if not able to get the user/group information
./src/core/org/apache/hadoop/security/UnixUserGroupInformation.java:301:   * @return current user's name
./src/core/org/apache/hadoop/security/UnixUserGroupInformation.java:302:   * @throws IOException if encounter any error while running the command
./src/core/org/apache/hadoop/security/UnixUserGroupInformation.java:316:   * @return the groups list that the current user belongs to
./src/core/org/apache/hadoop/security/UnixUserGroupInformation.java:317:   * @throws IOException if encounter any error when running the command
./src/core/org/apache/hadoop/security/UnixUserGroupInformation.java:339:   * @param other other object
./src/core/org/apache/hadoop/security/UnixUserGroupInformation.java:340:   * @return true if they are the same; false otherwise.
./src/core/org/apache/hadoop/security/UnixUserGroupInformation.java:383:   * @return  a hash code value for this UGI.
./src/core/org/apache/hadoop/security/UnixUserGroupInformation.java:391:   * @return a comma separated string containing the user name and group names
./src/core/org/apache/hadoop/security/UserGroupInformation.java:29:/** A {@link Writable} abstract class for storing user and groups information.
./src/core/org/apache/hadoop/security/UserGroupInformation.java:38:  /** @return the {@link UserGroupInformation} for the current thread */ 
./src/core/org/apache/hadoop/security/UserGroupInformation.java:43:  /** Set the {@link UserGroupInformation} for the current thread */ 
./src/core/org/apache/hadoop/security/UserGroupInformation.java:53:   * @return the user's name
./src/core/org/apache/hadoop/security/UserGroupInformation.java:59:   * @return an array of group names
./src/core/org/apache/hadoop/security/UserGroupInformation.java:72:  /** Read a {@link UserGroupInformation} from conf */
./src/core/org/apache/hadoop/util/Daemon.java:21:/** A thread that has called {@link Thread#setDaemon(boolean) } with true.*/
./src/core/org/apache/hadoop/util/DataChecksum.java:63:   * @return DataChecksum of the type in the array or null in case of an error.
./src/core/org/apache/hadoop/util/DataChecksum.java:117:   * @return number of bytes written. Will be equal to getChecksumSize();
./src/core/org/apache/hadoop/util/DataChecksum.java:141:    * @return number of bytes written. Will be equal to getChecksumSize();
./src/core/org/apache/hadoop/util/DataChecksum.java:168:    * @return true if the checksum matches and false otherwise.
./src/core/org/apache/hadoop/util/DiskChecker.java:52:   * @param dir
./src/core/org/apache/hadoop/util/DiskChecker.java:53:   * @return true on success, false on failure
./src/core/org/apache/hadoop/util/GenericOptionsParser.java:101: * @see Tool
./src/core/org/apache/hadoop/util/GenericOptionsParser.java:102: * @see ToolRunner
./src/core/org/apache/hadoop/util/GenericOptionsParser.java:115:   * obtained by {@link #getRemainingArgs()}.
./src/core/org/apache/hadoop/util/GenericOptionsParser.java:117:   * @param conf the <code>Configuration</code> to modify.
./src/core/org/apache/hadoop/util/GenericOptionsParser.java:118:   * @param args command-line arguments.
./src/core/org/apache/hadoop/util/GenericOptionsParser.java:129:   * {@link #getCommandLine()}.
./src/core/org/apache/hadoop/util/GenericOptionsParser.java:131:   * @param conf the configuration to modify  
./src/core/org/apache/hadoop/util/GenericOptionsParser.java:132:   * @param options options built by the caller 
./src/core/org/apache/hadoop/util/GenericOptionsParser.java:133:   * @param args User-specified arguments
./src/core/org/apache/hadoop/util/GenericOptionsParser.java:142:   * @return array of <code>String</code>s containing the un-parsed arguments
./src/core/org/apache/hadoop/util/GenericOptionsParser.java:154:   * {@link #GenericOptionsParser(Configuration, String[])}, then returned 
./src/core/org/apache/hadoop/util/GenericOptionsParser.java:157:   * @return <code>CommandLine</code> representing list of arguments 
./src/core/org/apache/hadoop/util/GenericOptionsParser.java:167:  @SuppressWarnings("static-access")
./src/core/org/apache/hadoop/util/GenericOptionsParser.java:214:   * @param conf Configuration to be modified
./src/core/org/apache/hadoop/util/GenericOptionsParser.java:215:   * @param line User-specified generic options
./src/core/org/apache/hadoop/util/GenericOptionsParser.java:265:   * @param conf
./src/core/org/apache/hadoop/util/GenericOptionsParser.java:266:   * @return libjar urls
./src/core/org/apache/hadoop/util/GenericOptionsParser.java:267:   * @throws IOException
./src/core/org/apache/hadoop/util/GenericOptionsParser.java:290:   * @param files
./src/core/org/apache/hadoop/util/GenericOptionsParser.java:291:   * @return
./src/core/org/apache/hadoop/util/GenericOptionsParser.java:335:   * @param conf Configuration to be modified
./src/core/org/apache/hadoop/util/GenericOptionsParser.java:336:   * @param args User-specified arguments
./src/core/org/apache/hadoop/util/GenericOptionsParser.java:337:   * @return Command-specific arguments
./src/core/org/apache/hadoop/util/GenericOptionsParser.java:359:   * @param out stream to print the usage message to.
./src/core/org/apache/hadoop/util/GenericsUtil.java:32:   * @param <T> The type of the argument
./src/core/org/apache/hadoop/util/GenericsUtil.java:33:   * @param t the object to get it class
./src/core/org/apache/hadoop/util/GenericsUtil.java:34:   * @return <code>Class&lt;T&gt;</code>
./src/core/org/apache/hadoop/util/GenericsUtil.java:37:    @SuppressWarnings("unchecked")
./src/core/org/apache/hadoop/util/GenericsUtil.java:45:   * @param c the Class object of the items in the list
./src/core/org/apache/hadoop/util/GenericsUtil.java:46:   * @param list the list to convert
./src/core/org/apache/hadoop/util/GenericsUtil.java:50:    @SuppressWarnings("unchecked")
./src/core/org/apache/hadoop/util/GenericsUtil.java:62:   * @param list the list to convert
./src/core/org/apache/hadoop/util/GenericsUtil.java:63:   * @throws ArrayIndexOutOfBoundsException if the list is empty. 
./src/core/org/apache/hadoop/util/GenericsUtil.java:64:   * Use {@link #toArray(Class, List)} if the list may be empty.
./src/core/org/apache/hadoop/util/HeapSort.java:44:   * {@inheritDoc}
./src/core/org/apache/hadoop/util/HeapSort.java:51:   * {@inheritDoc}
./src/core/org/apache/hadoop/util/IndexedSortable.java:21: * Interface for collections capable of being sorted by {@link IndexedSorter}
./src/core/org/apache/hadoop/util/IndexedSortable.java:28:   * {@link java.util.Comparable#compare}.
./src/core/org/apache/hadoop/util/IndexedSorter.java:21: * Interface for sort algorithms accepting {@link IndexedSortable} items.
./src/core/org/apache/hadoop/util/IndexedSorter.java:24: * {@link IndexedSortable#compare} and {@link IndexedSortable#swap} items
./src/core/org/apache/hadoop/util/IndexedSorter.java:34:   * @see IndexedSortable#compare
./src/core/org/apache/hadoop/util/IndexedSorter.java:35:   * @see IndexedSortable#swap
./src/core/org/apache/hadoop/util/IndexedSorter.java:40:   * Same as {@link #sort(IndexedSortable,int,int)}, but indicate progress
./src/core/org/apache/hadoop/util/IndexedSorter.java:42:   * @see #sort(IndexedSortable,int,int)
./src/core/org/apache/hadoop/util/LineReader.java:43:   * @param in The input stream
./src/core/org/apache/hadoop/util/LineReader.java:44:   * @throws IOException
./src/core/org/apache/hadoop/util/LineReader.java:53:   * @param in The input stream
./src/core/org/apache/hadoop/util/LineReader.java:54:   * @param bufferSize Size of the read buffer
./src/core/org/apache/hadoop/util/LineReader.java:55:   * @throws IOException
./src/core/org/apache/hadoop/util/LineReader.java:67:   * @param in input stream
./src/core/org/apache/hadoop/util/LineReader.java:68:   * @param conf configuration
./src/core/org/apache/hadoop/util/LineReader.java:69:   * @throws IOException
./src/core/org/apache/hadoop/util/LineReader.java:77:   * @return was there more data?
./src/core/org/apache/hadoop/util/LineReader.java:78:   * @throws IOException
./src/core/org/apache/hadoop/util/LineReader.java:88:   * @throws IOException
./src/core/org/apache/hadoop/util/LineReader.java:96:   * @param str the object to store the given line
./src/core/org/apache/hadoop/util/LineReader.java:97:   * @param maxLineLength the maximum number of bytes to store into str.
./src/core/org/apache/hadoop/util/LineReader.java:98:   * @param maxBytesToConsume the maximum number of bytes to consume in this call.
./src/core/org/apache/hadoop/util/LineReader.java:99:   * @return the number of bytes read including the newline
./src/core/org/apache/hadoop/util/LineReader.java:100:   * @throws IOException if the underlying stream throws
./src/core/org/apache/hadoop/util/LineReader.java:161:   * @param str the object to store the given line
./src/core/org/apache/hadoop/util/LineReader.java:162:   * @param maxLineLength the maximum number of bytes to store into str.
./src/core/org/apache/hadoop/util/LineReader.java:163:   * @return the number of bytes read including the newline
./src/core/org/apache/hadoop/util/LineReader.java:164:   * @throws IOException if the underlying stream throws
./src/core/org/apache/hadoop/util/LineReader.java:172:   * @param str the object to store the given line
./src/core/org/apache/hadoop/util/LineReader.java:173:   * @return the number of bytes read including the newline
./src/core/org/apache/hadoop/util/LineReader.java:174:   * @throws IOException if the underlying stream throws
./src/core/org/apache/hadoop/util/NativeCodeLoader.java:60:   * @return <code>true</code> if native-hadoop is loaded, 
./src/core/org/apache/hadoop/util/NativeCodeLoader.java:69:   * @param conf configuration
./src/core/org/apache/hadoop/util/NativeCodeLoader.java:71:   * @return <code>true</code> if native hadoop libraries, if present, can be 
./src/core/org/apache/hadoop/util/NativeCodeLoader.java:81:   * @param conf configuration
./src/core/org/apache/hadoop/util/NativeCodeLoader.java:82:   * @param loadNativeLibraries can native hadoop libraries be loaded
./src/core/org/apache/hadoop/util/PlatformName.java:36:   * @return returns the complete platform as per the java-vm.
./src/core/org/apache/hadoop/util/PrintJarMainClass.java:29:   * @param args
./src/core/org/apache/hadoop/util/PriorityQueue.java:34:  @SuppressWarnings("unchecked")
./src/core/org/apache/hadoop/util/PriorityQueue.java:56:   * @param element
./src/core/org/apache/hadoop/util/PriorityQueue.java:57:   * @return true if element is added, false otherwise.
./src/core/org/apache/hadoop/util/ProcfsBasedProcessTree.java:70:   * @return true if ProcfsBasedProcessTree is available. False otherwise.
./src/core/org/apache/hadoop/util/ProcfsBasedProcessTree.java:90:   * @return the process-tree with latest state.
./src/core/org/apache/hadoop/util/ProcfsBasedProcessTree.java:147:   * @return true if the process-true is alive, false otherwise.
./src/core/org/apache/hadoop/util/ProcfsBasedProcessTree.java:191:   * @return cumulative virtual memory used by the process-tree in kilobytes.
./src/core/org/apache/hadoop/util/ProcfsBasedProcessTree.java:206:   * @param pidFileName
./src/core/org/apache/hadoop/util/ProcfsBasedProcessTree.java:208:   * @return the PID string read from the pid-file. Returns null if the
./src/core/org/apache/hadoop/util/ProgramDriver.java:34:   * @date april 2006
./src/core/org/apache/hadoop/util/ProgramDriver.java:48:     * @param mainClass the class with the main for the example program
./src/core/org/apache/hadoop/util/ProgramDriver.java:49:     * @param description a string to display to the user in help messages
./src/core/org/apache/hadoop/util/ProgramDriver.java:50:     * @throws SecurityException if we can't use reflection
./src/core/org/apache/hadoop/util/ProgramDriver.java:51:     * @throws NoSuchMethodException if the class doesn't have a main method
./src/core/org/apache/hadoop/util/ProgramDriver.java:62:     * @param args the arguments for the application
./src/core/org/apache/hadoop/util/ProgramDriver.java:63:     * @throws Throwable The exception thrown by the invoked method
./src/core/org/apache/hadoop/util/ProgramDriver.java:92:   * @param name The name of the string you want the class instance to be called with
./src/core/org/apache/hadoop/util/ProgramDriver.java:93:   * @param mainClass The class that you want to add to the repository
./src/core/org/apache/hadoop/util/ProgramDriver.java:94:   * @param description The description of the class
./src/core/org/apache/hadoop/util/ProgramDriver.java:95:   * @throws NoSuchMethodException 
./src/core/org/apache/hadoop/util/ProgramDriver.java:96:   * @throws SecurityException 
./src/core/org/apache/hadoop/util/ProgramDriver.java:108:   * @param args The argument from the user. args[0] is the command to run.
./src/core/org/apache/hadoop/util/ProgramDriver.java:109:   * @throws NoSuchMethodException 
./src/core/org/apache/hadoop/util/ProgramDriver.java:110:   * @throws SecurityException 
./src/core/org/apache/hadoop/util/ProgramDriver.java:111:   * @throws IllegalAccessException 
./src/core/org/apache/hadoop/util/ProgramDriver.java:112:   * @throws IllegalArgumentException 
./src/core/org/apache/hadoop/util/ProgramDriver.java:113:   * @throws Throwable Anything thrown by the example program's main
./src/core/org/apache/hadoop/util/Progress.java:24: * a hierarchy of {@link Progress} instances, each modelling a phase of
./src/core/org/apache/hadoop/util/Progress.java:25: * execution.  The root is constructed with {@link #Progress()}.  Nodes for
./src/core/org/apache/hadoop/util/Progress.java:26: * sub-phases are created by calling {@link #addPhase()}.
./src/core/org/apache/hadoop/util/QuickSort.java:47:   * {@inheritDoc} If the recursion depth falls below {@link #getMaxDepth},
./src/core/org/apache/hadoop/util/QuickSort.java:48:   * then switch to {@link HeapSort}.
./src/core/org/apache/hadoop/util/QuickSort.java:55:   * {@inheritDoc}
./src/core/org/apache/hadoop/util/ReflectionUtils.java:48:   * @param theObject object for which to set configuration
./src/core/org/apache/hadoop/util/ReflectionUtils.java:49:   * @param conf Configuration
./src/core/org/apache/hadoop/util/ReflectionUtils.java:65:   * @param theClass class of which an object is created
./src/core/org/apache/hadoop/util/ReflectionUtils.java:66:   * @param conf Configuration
./src/core/org/apache/hadoop/util/ReflectionUtils.java:67:   * @return a new object
./src/core/org/apache/hadoop/util/ReflectionUtils.java:69:  @SuppressWarnings("unchecked")
./src/core/org/apache/hadoop/util/ReflectionUtils.java:104:   * @param stream the stream to
./src/core/org/apache/hadoop/util/ReflectionUtils.java:105:   * @param title a string title for the stack trace
./src/core/org/apache/hadoop/util/ReflectionUtils.java:151:   * @param log the logger that logs the stack trace
./src/core/org/apache/hadoop/util/ReflectionUtils.java:152:   * @param title a descriptive title for the call stacks
./src/core/org/apache/hadoop/util/ReflectionUtils.java:153:   * @param minInterval the minimum time from the last 
./src/core/org/apache/hadoop/util/ReflectionUtils.java:176:   * Return the correctly-typed {@link Class} of the given object.
./src/core/org/apache/hadoop/util/ReflectionUtils.java:178:   * @param o object whose correctly-typed <code>Class</code> is to be obtained
./src/core/org/apache/hadoop/util/ReflectionUtils.java:179:   * @return the correctly typed <code>Class</code> of the given object.
./src/core/org/apache/hadoop/util/ReflectionUtils.java:181:  @SuppressWarnings("unchecked")
./src/core/org/apache/hadoop/util/ServletUtil.java:61:   * @return the HTML footer.
./src/core/org/apache/hadoop/util/ServletUtil.java:71:   * @param perc The percentage value for which graph is to be generated
./src/core/org/apache/hadoop/util/ServletUtil.java:72:   * @param width The width of the display table
./src/core/org/apache/hadoop/util/ServletUtil.java:73:   * @return HTML String representation of the percentage graph
./src/core/org/apache/hadoop/util/ServletUtil.java:74:   * @throws IOException
./src/core/org/apache/hadoop/util/ServletUtil.java:97:   * @param perc The percentage value for which graph is to be generated
./src/core/org/apache/hadoop/util/ServletUtil.java:98:   * @param width The width of the display table
./src/core/org/apache/hadoop/util/ServletUtil.java:99:   * @return HTML String representation of the percentage graph
./src/core/org/apache/hadoop/util/ServletUtil.java:100:   * @throws IOException
./src/core/org/apache/hadoop/util/Shell.java:61:   * process from within the {@link org.apache.hadoop.mapred.Mapper} or the 
./src/core/org/apache/hadoop/util/Shell.java:62:   * {@link org.apache.hadoop.mapred.Reducer} implementations 
./src/core/org/apache/hadoop/util/Shell.java:63:   * e.g. <a href="{@docRoot}/org/apache/hadoop/mapred/pipes/package-summary.html">Hadoop Pipes</a> 
./src/core/org/apache/hadoop/util/Shell.java:64:   * or <a href="{@docRoot}/org/apache/hadoop/streaming/package-summary.html">Hadoop Streaming</a>.
./src/core/org/apache/hadoop/util/Shell.java:68:   * @param conf configuration
./src/core/org/apache/hadoop/util/Shell.java:69:   * @return a <code>String[]</code> with the ulimit command arguments or 
./src/core/org/apache/hadoop/util/Shell.java:107:   * @param interval the minimum duration to wait before re-executing the 
./src/core/org/apache/hadoop/util/Shell.java:116:   * @param env Mapping of environment variables
./src/core/org/apache/hadoop/util/Shell.java:123:   * @param dir The directory where the command would be executed
./src/core/org/apache/hadoop/util/Shell.java:161:      @Override
./src/core/org/apache/hadoop/util/Shell.java:227:   * @return process executing the command
./src/core/org/apache/hadoop/util/Shell.java:234:   * @return the exit code of the process
./src/core/org/apache/hadoop/util/Shell.java:312:     * @return a string representation of the object.
./src/core/org/apache/hadoop/util/Shell.java:333:   * @param cmd shell command to execute.
./src/core/org/apache/hadoop/util/Shell.java:334:   * @return the output of the executed command.
./src/core/org/apache/hadoop/util/StringUtils.java:45:   * @param e The exception to stringify
./src/core/org/apache/hadoop/util/StringUtils.java:46:   * @return A string with exception name and call stack.
./src/core/org/apache/hadoop/util/StringUtils.java:58:   * @param fullHostname the full hostname
./src/core/org/apache/hadoop/util/StringUtils.java:59:   * @return the hostname to the first dot
./src/core/org/apache/hadoop/util/StringUtils.java:75:   * @param number the number to format
./src/core/org/apache/hadoop/util/StringUtils.java:76:   * @return a human readable form of the integer
./src/core/org/apache/hadoop/util/StringUtils.java:99:   * @param done the percentage to format (0.0 to 1.0)
./src/core/org/apache/hadoop/util/StringUtils.java:100:   * @param digits the number of digits past the decimal point
./src/core/org/apache/hadoop/util/StringUtils.java:101:   * @return a string representation of the percentage
./src/core/org/apache/hadoop/util/StringUtils.java:115:   * @param strs Array of strings
./src/core/org/apache/hadoop/util/StringUtils.java:116:   * @return Empty string if strs.length is 0, comma separated list of strings
./src/core/org/apache/hadoop/util/StringUtils.java:134:   * @param bytes
./src/core/org/apache/hadoop/util/StringUtils.java:135:   * @param start start index, inclusively
./src/core/org/apache/hadoop/util/StringUtils.java:136:   * @param end end index, exclusively
./src/core/org/apache/hadoop/util/StringUtils.java:137:   * @return hex string representation of the byte array
./src/core/org/apache/hadoop/util/StringUtils.java:158:   * @param hex the hex String array
./src/core/org/apache/hadoop/util/StringUtils.java:159:   * @return a byte array that is a hex string representation of the given
./src/core/org/apache/hadoop/util/StringUtils.java:171:   * @param uris
./src/core/org/apache/hadoop/util/StringUtils.java:187:   * @param str
./src/core/org/apache/hadoop/util/StringUtils.java:207:   * @param str
./src/core/org/apache/hadoop/util/StringUtils.java:225:   * @param finishTime finish time
./src/core/org/apache/hadoop/util/StringUtils.java:226:   * @param startTime start time
./src/core/org/apache/hadoop/util/StringUtils.java:238:   * @param timeDiff The time difference to format
./src/core/org/apache/hadoop/util/StringUtils.java:266:   * @param dateFormat date format to use
./src/core/org/apache/hadoop/util/StringUtils.java:267:   * @param finishTime fnish time
./src/core/org/apache/hadoop/util/StringUtils.java:268:   * @param startTime start time
./src/core/org/apache/hadoop/util/StringUtils.java:269:   * @return formatted value. 
./src/core/org/apache/hadoop/util/StringUtils.java:285:   * @param str the comma seperated string values
./src/core/org/apache/hadoop/util/StringUtils.java:286:   * @return the arraylist of the comma seperated string values
./src/core/org/apache/hadoop/util/StringUtils.java:298:   * @param str comma seperated string values
./src/core/org/apache/hadoop/util/StringUtils.java:299:   * @return an <code>ArrayList</code> of string values
./src/core/org/apache/hadoop/util/StringUtils.java:319:   * @param str a string that may have escaped separator
./src/core/org/apache/hadoop/util/StringUtils.java:320:   * @return an array of strings
./src/core/org/apache/hadoop/util/StringUtils.java:328:   * @param str a string that may have escaped separator
./src/core/org/apache/hadoop/util/StringUtils.java:329:   * @param escapeChar a char that be used to escape the separator
./src/core/org/apache/hadoop/util/StringUtils.java:330:   * @param separator a separator char
./src/core/org/apache/hadoop/util/StringUtils.java:331:   * @return an array of strings
./src/core/org/apache/hadoop/util/StringUtils.java:359:   * @param str the source string
./src/core/org/apache/hadoop/util/StringUtils.java:360:   * @param separator the character to find
./src/core/org/apache/hadoop/util/StringUtils.java:361:   * @param escapeChar character used to escape
./src/core/org/apache/hadoop/util/StringUtils.java:362:   * @param start from where to search
./src/core/org/apache/hadoop/util/StringUtils.java:363:   * @param split used to pass back the extracted string
./src/core/org/apache/hadoop/util/StringUtils.java:384:   * @param str a string
./src/core/org/apache/hadoop/util/StringUtils.java:385:   * @return an escaped string
./src/core/org/apache/hadoop/util/StringUtils.java:395:   * @param str string
./src/core/org/apache/hadoop/util/StringUtils.java:396:   * @param escapeChar escape char
./src/core/org/apache/hadoop/util/StringUtils.java:397:   * @param charToEscape the char to be escaped
./src/core/org/apache/hadoop/util/StringUtils.java:398:   * @return an escaped string
./src/core/org/apache/hadoop/util/StringUtils.java:416:   * @param charsToEscape array of characters to be escaped
./src/core/org/apache/hadoop/util/StringUtils.java:437:   * @param str a string
./src/core/org/apache/hadoop/util/StringUtils.java:438:   * @return an unescaped string
./src/core/org/apache/hadoop/util/StringUtils.java:448:   * @param str string
./src/core/org/apache/hadoop/util/StringUtils.java:449:   * @param escapeChar escape char
./src/core/org/apache/hadoop/util/StringUtils.java:450:   * @param charToEscape the escaped char
./src/core/org/apache/hadoop/util/StringUtils.java:451:   * @return an unescaped string
./src/core/org/apache/hadoop/util/StringUtils.java:459:   * @param charsToEscape array of characters to unescape
./src/core/org/apache/hadoop/util/StringUtils.java:499:   * @return hostname
./src/core/org/apache/hadoop/util/StringUtils.java:508:   * @param prefix prefix keyword for the message
./src/core/org/apache/hadoop/util/StringUtils.java:509:   * @param msg content of the message
./src/core/org/apache/hadoop/util/StringUtils.java:510:   * @return a message for logging
./src/core/org/apache/hadoop/util/StringUtils.java:523:   * @param clazz the class of the server
./src/core/org/apache/hadoop/util/StringUtils.java:524:   * @param args arguments
./src/core/org/apache/hadoop/util/StringUtils.java:525:   * @param LOG the target log object
./src/core/org/apache/hadoop/util/StringUtils.java:574:     * @return The TraditionalBinaryPrefix object corresponding to the symbol.
./src/core/org/apache/hadoop/util/StringUtils.java:595:     * @param s input string
./src/core/org/apache/hadoop/util/StringUtils.java:596:     * @return a long value represented by the input string.
./src/core/org/apache/hadoop/util/StringUtils.java:617:     * @param string
./src/core/org/apache/hadoop/util/StringUtils.java:618:     * @return HTML Escaped String representation
./src/core/org/apache/hadoop/util/Tool.java:28: * <a href="{@docRoot}/org/apache/hadoop/util/GenericOptionsParser.html#GenericOptions">
./src/core/org/apache/hadoop/util/Tool.java:29: * standard command-line options</a> to {@link ToolRunner#run(Tool, String[])} 
./src/core/org/apache/hadoop/util/Tool.java:67: * @see GenericOptionsParser
./src/core/org/apache/hadoop/util/Tool.java:68: * @see ToolRunner
./src/core/org/apache/hadoop/util/Tool.java:74:   * @param args command specific arguments.
./src/core/org/apache/hadoop/util/Tool.java:75:   * @return exit code.
./src/core/org/apache/hadoop/util/Tool.java:76:   * @throws Exception
./src/core/org/apache/hadoop/util/ToolRunner.java:25: * A utility to help run {@link Tool}s.
./src/core/org/apache/hadoop/util/ToolRunner.java:29: * {@link GenericOptionsParser} to parse the 
./src/core/org/apache/hadoop/util/ToolRunner.java:30: * <a href="{@docRoot}/org/apache/hadoop/util/GenericOptionsParser.html#GenericOptions">
./src/core/org/apache/hadoop/util/ToolRunner.java:36: * @see Tool
./src/core/org/apache/hadoop/util/ToolRunner.java:37: * @see GenericOptionsParser
./src/core/org/apache/hadoop/util/ToolRunner.java:42:   * Runs the given <code>Tool</code> by {@link Tool#run(String[])}, after 
./src/core/org/apache/hadoop/util/ToolRunner.java:49:   * @param conf <code>Configuration</code> for the <code>Tool</code>.
./src/core/org/apache/hadoop/util/ToolRunner.java:50:   * @param tool <code>Tool</code> to run.
./src/core/org/apache/hadoop/util/ToolRunner.java:51:   * @param args command-line arguments to the tool.
./src/core/org/apache/hadoop/util/ToolRunner.java:52:   * @return exit code of the {@link Tool#run(String[])} method.
./src/core/org/apache/hadoop/util/ToolRunner.java:73:   * @param tool <code>Tool</code> to run.
./src/core/org/apache/hadoop/util/ToolRunner.java:74:   * @param args command-line arguments to the tool.
./src/core/org/apache/hadoop/util/ToolRunner.java:75:   * @return exit code of the {@link Tool#run(String[])} method.
./src/core/org/apache/hadoop/util/ToolRunner.java:85:   *  @param out stream to write usage information to.
./src/core/org/apache/hadoop/util/UTF8ByteArrayUtils.java:24:   * @param utf a byte array containing a UTF-8 encoded string
./src/core/org/apache/hadoop/util/UTF8ByteArrayUtils.java:25:   * @param start starting offset
./src/core/org/apache/hadoop/util/UTF8ByteArrayUtils.java:26:   * @param end ending position
./src/core/org/apache/hadoop/util/UTF8ByteArrayUtils.java:27:   * @param b the byte to find
./src/core/org/apache/hadoop/util/UTF8ByteArrayUtils.java:28:   * @return position that first byte occures otherwise -1
./src/core/org/apache/hadoop/util/UTF8ByteArrayUtils.java:41:   * @param utf a byte array containing a UTF-8 encoded string
./src/core/org/apache/hadoop/util/UTF8ByteArrayUtils.java:42:   * @param start starting offset
./src/core/org/apache/hadoop/util/UTF8ByteArrayUtils.java:43:   * @param end ending position
./src/core/org/apache/hadoop/util/UTF8ByteArrayUtils.java:44:   * @param b the bytes to find
./src/core/org/apache/hadoop/util/UTF8ByteArrayUtils.java:45:   * @return position that first byte occures otherwise -1
./src/core/org/apache/hadoop/util/UTF8ByteArrayUtils.java:66:   * @param utf a byte array containing a UTF-8 encoded string
./src/core/org/apache/hadoop/util/UTF8ByteArrayUtils.java:67:   * @param start starting offset
./src/core/org/apache/hadoop/util/UTF8ByteArrayUtils.java:68:   * @param length the length of byte array
./src/core/org/apache/hadoop/util/UTF8ByteArrayUtils.java:69:   * @param b the byte to find
./src/core/org/apache/hadoop/util/UTF8ByteArrayUtils.java:70:   * @param n the desired occurrence of the given byte
./src/core/org/apache/hadoop/util/UTF8ByteArrayUtils.java:71:   * @return position that nth occurrence of the given byte if exists; otherwise -1
./src/core/org/apache/hadoop/util/UTF8ByteArrayUtils.java:88:   * @param utf a byte array containing a UTF-8 encoded string
./src/core/org/apache/hadoop/util/UTF8ByteArrayUtils.java:89:   * @param b the byte to find
./src/core/org/apache/hadoop/util/UTF8ByteArrayUtils.java:90:   * @param n the desired occurrence of the given byte
./src/core/org/apache/hadoop/util/UTF8ByteArrayUtils.java:91:   * @return position that nth occurrence of the given byte if exists; otherwise -1
./src/core/org/apache/hadoop/util/VersionInfo.java:38:   * @return
./src/core/org/apache/hadoop/util/VersionInfo.java:46:   * @return the Hadoop version string, eg. "0.6.3-dev"
./src/core/org/apache/hadoop/util/VersionInfo.java:54:   * @return the revision number, eg. "451451"
./src/core/org/apache/hadoop/util/VersionInfo.java:62:   * @return the compilation date in unix date format
./src/core/org/apache/hadoop/util/VersionInfo.java:70:   * @return the username of the user
./src/core/org/apache/hadoop/util/XMLUtils.java:33:   * @param styleSheet the style-sheet
./src/core/org/apache/hadoop/util/XMLUtils.java:34:   * @param xml input xml data
./src/core/org/apache/hadoop/util/XMLUtils.java:35:   * @param out output
./src/core/org/apache/hadoop/util/XMLUtils.java:36:   * @throws TransformerConfigurationException
./src/core/org/apache/hadoop/util/XMLUtils.java:37:   * @throws TransformerException
./src/hdfs/org/apache/hadoop/hdfs/ChecksumDistributedFileSystem.java:43:  /** @deprecated */
./src/hdfs/org/apache/hadoop/hdfs/ChecksumDistributedFileSystem.java:75:   * @see org.apache.hadoop.hdfs.protocol.ClientProtocol#setSafeMode(FSConstants.SafeModeAction)
./src/hdfs/org/apache/hadoop/hdfs/ChecksumDistributedFileSystem.java:124:  @Override
./src/hdfs/org/apache/hadoop/hdfs/DFSClient.java:218:   * @return the default block size in bytes
./src/hdfs/org/apache/hadoop/hdfs/DFSClient.java:246:   *  @deprecated Use getBlockLocations instead
./src/hdfs/org/apache/hadoop/hdfs/DFSClient.java:259:  @Deprecated
./src/hdfs/org/apache/hadoop/hdfs/DFSClient.java:344:   * @param src stream name
./src/hdfs/org/apache/hadoop/hdfs/DFSClient.java:345:   * @param overwrite do not check for file existence if true
./src/hdfs/org/apache/hadoop/hdfs/DFSClient.java:346:   * @return output stream
./src/hdfs/org/apache/hadoop/hdfs/DFSClient.java:347:   * @throws IOException
./src/hdfs/org/apache/hadoop/hdfs/DFSClient.java:359:   * @param src stream name
./src/hdfs/org/apache/hadoop/hdfs/DFSClient.java:360:   * @param overwrite do not check for file existence if true
./src/hdfs/org/apache/hadoop/hdfs/DFSClient.java:361:   * @return output stream
./src/hdfs/org/apache/hadoop/hdfs/DFSClient.java:362:   * @throws IOException
./src/hdfs/org/apache/hadoop/hdfs/DFSClient.java:375:   * @param src stream name
./src/hdfs/org/apache/hadoop/hdfs/DFSClient.java:376:   * @param overwrite do not check for file existence if true
./src/hdfs/org/apache/hadoop/hdfs/DFSClient.java:377:   * @param replication block replication
./src/hdfs/org/apache/hadoop/hdfs/DFSClient.java:378:   * @return output stream
./src/hdfs/org/apache/hadoop/hdfs/DFSClient.java:379:   * @throws IOException
./src/hdfs/org/apache/hadoop/hdfs/DFSClient.java:395:   * @param src stream name
./src/hdfs/org/apache/hadoop/hdfs/DFSClient.java:396:   * @param overwrite do not check for file existence if true
./src/hdfs/org/apache/hadoop/hdfs/DFSClient.java:397:   * @param replication block replication
./src/hdfs/org/apache/hadoop/hdfs/DFSClient.java:398:   * @return output stream
./src/hdfs/org/apache/hadoop/hdfs/DFSClient.java:399:   * @throws IOException
./src/hdfs/org/apache/hadoop/hdfs/DFSClient.java:412:   * {@link #create(String,FsPermission,boolean,short,long,Progressable,int)}
./src/hdfs/org/apache/hadoop/hdfs/DFSClient.java:414:   * @see FsPermission#getDefault()
./src/hdfs/org/apache/hadoop/hdfs/DFSClient.java:431:   * @param src stream name
./src/hdfs/org/apache/hadoop/hdfs/DFSClient.java:432:   * @param permission The permission of the directory being created.
./src/hdfs/org/apache/hadoop/hdfs/DFSClient.java:433:   * If permission == null, use {@link FsPermission#getDefault()}.
./src/hdfs/org/apache/hadoop/hdfs/DFSClient.java:434:   * @param overwrite do not check for file existence if true
./src/hdfs/org/apache/hadoop/hdfs/DFSClient.java:435:   * @param replication block replication
./src/hdfs/org/apache/hadoop/hdfs/DFSClient.java:436:   * @return output stream
./src/hdfs/org/apache/hadoop/hdfs/DFSClient.java:437:   * @throws IOException
./src/hdfs/org/apache/hadoop/hdfs/DFSClient.java:438:   * @see ClientProtocol#create(String, FsPermission, String, boolean, short, long)
./src/hdfs/org/apache/hadoop/hdfs/DFSClient.java:464:   * @param src file name
./src/hdfs/org/apache/hadoop/hdfs/DFSClient.java:465:   * @param buffersize buffer size
./src/hdfs/org/apache/hadoop/hdfs/DFSClient.java:466:   * @param progress for reporting write-progress
./src/hdfs/org/apache/hadoop/hdfs/DFSClient.java:467:   * @return an output stream for writing into the file
./src/hdfs/org/apache/hadoop/hdfs/DFSClient.java:468:   * @throws IOException
./src/hdfs/org/apache/hadoop/hdfs/DFSClient.java:469:   * @see {@link ClientProtocol#append(String, String)}
./src/hdfs/org/apache/hadoop/hdfs/DFSClient.java:493:   * @see ClientProtocol#setReplication(String, short)
./src/hdfs/org/apache/hadoop/hdfs/DFSClient.java:494:   * @param replication
./src/hdfs/org/apache/hadoop/hdfs/DFSClient.java:495:   * @throws IOException
./src/hdfs/org/apache/hadoop/hdfs/DFSClient.java:496:   * @return true is successful or false if file does not exist 
./src/hdfs/org/apache/hadoop/hdfs/DFSClient.java:511:   * See {@link ClientProtocol#rename(String, String)}. 
./src/hdfs/org/apache/hadoop/hdfs/DFSClient.java:525:   * See {@link ClientProtocol#delete(String)}. 
./src/hdfs/org/apache/hadoop/hdfs/DFSClient.java:527:  @Deprecated
./src/hdfs/org/apache/hadoop/hdfs/DFSClient.java:554:  /** @deprecated Use getFileStatus() instead */
./src/hdfs/org/apache/hadoop/hdfs/DFSClient.java:555:  @Deprecated
./src/hdfs/org/apache/hadoop/hdfs/DFSClient.java:586:   * @param src The file path
./src/hdfs/org/apache/hadoop/hdfs/DFSClient.java:587:   * @return The checksum 
./src/hdfs/org/apache/hadoop/hdfs/DFSClient.java:588:   * @see DistributedFileSystem#getFileChecksum(Path)
./src/hdfs/org/apache/hadoop/hdfs/DFSClient.java:597:   * @param src The file path
./src/hdfs/org/apache/hadoop/hdfs/DFSClient.java:598:   * @return The checksum 
./src/hdfs/org/apache/hadoop/hdfs/DFSClient.java:701:   * @param src path name.
./src/hdfs/org/apache/hadoop/hdfs/DFSClient.java:702:   * @param permission
./src/hdfs/org/apache/hadoop/hdfs/DFSClient.java:703:   * @throws <code>FileNotFoundException</code> is file does not exist.
./src/hdfs/org/apache/hadoop/hdfs/DFSClient.java:718:   * @param src path name.
./src/hdfs/org/apache/hadoop/hdfs/DFSClient.java:719:   * @param username user id.
./src/hdfs/org/apache/hadoop/hdfs/DFSClient.java:720:   * @param groupname user group.
./src/hdfs/org/apache/hadoop/hdfs/DFSClient.java:721:   * @throws <code>FileNotFoundException</code> is file does not exist.
./src/hdfs/org/apache/hadoop/hdfs/DFSClient.java:759:   * See {@link ClientProtocol#setSafeMode(FSConstants.SafeModeAction)} 
./src/hdfs/org/apache/hadoop/hdfs/DFSClient.java:762:   * @see ClientProtocol#setSafeMode(FSConstants.SafeModeAction)
./src/hdfs/org/apache/hadoop/hdfs/DFSClient.java:770:   * See {@link ClientProtocol#refreshNodes()} 
./src/hdfs/org/apache/hadoop/hdfs/DFSClient.java:773:   * @see ClientProtocol#refreshNodes()
./src/hdfs/org/apache/hadoop/hdfs/DFSClient.java:781:   * See {@link ClientProtocol#metaSave(String)} 
./src/hdfs/org/apache/hadoop/hdfs/DFSClient.java:784:   * @see ClientProtocol#metaSave(String)
./src/hdfs/org/apache/hadoop/hdfs/DFSClient.java:791:   * @see ClientProtocol#finalizeUpgrade()
./src/hdfs/org/apache/hadoop/hdfs/DFSClient.java:798:   * @see ClientProtocol#distributedUpgradeProgress(FSConstants.UpgradeAction)
./src/hdfs/org/apache/hadoop/hdfs/DFSClient.java:815:   * @param src The path of the directory being created
./src/hdfs/org/apache/hadoop/hdfs/DFSClient.java:816:   * @param permission The permission of the directory being created.
./src/hdfs/org/apache/hadoop/hdfs/DFSClient.java:817:   * If permission == null, use {@link FsPermission#getDefault()}.
./src/hdfs/org/apache/hadoop/hdfs/DFSClient.java:818:   * @return True if the operation success.
./src/hdfs/org/apache/hadoop/hdfs/DFSClient.java:819:   * @see ClientProtocol#mkdirs(String, FsPermission)
./src/hdfs/org/apache/hadoop/hdfs/DFSClient.java:847:   * @see org.apache.hadoop.hdfs.protocol.ClientProtocol#setQuota(String, long, long)
./src/hdfs/org/apache/hadoop/hdfs/DFSClient.java:873:   * @throws FileNotFoundException if the path is not a file
./src/hdfs/org/apache/hadoop/hdfs/DFSClient.java:989:    /** {@inheritDoc} */
./src/hdfs/org/apache/hadoop/hdfs/DFSClient.java:993:        return s + "@" + DFSClient.this + ": "
./src/hdfs/org/apache/hadoop/hdfs/DFSClient.java:1043:    @Override
./src/hdfs/org/apache/hadoop/hdfs/DFSClient.java:1078:    @Override
./src/hdfs/org/apache/hadoop/hdfs/DFSClient.java:1098:    @Override
./src/hdfs/org/apache/hadoop/hdfs/DFSClient.java:1104:    @Override
./src/hdfs/org/apache/hadoop/hdfs/DFSClient.java:1113:    @Override
./src/hdfs/org/apache/hadoop/hdfs/DFSClient.java:1118:    @Override
./src/hdfs/org/apache/hadoop/hdfs/DFSClient.java:1140:    @Override
./src/hdfs/org/apache/hadoop/hdfs/DFSClient.java:1316:    @Override
./src/hdfs/org/apache/hadoop/hdfs/DFSClient.java:1440:     * @param offset
./src/hdfs/org/apache/hadoop/hdfs/DFSClient.java:1441:     * @return located block
./src/hdfs/org/apache/hadoop/hdfs/DFSClient.java:1442:     * @throws IOException
./src/hdfs/org/apache/hadoop/hdfs/DFSClient.java:1468:     * @param offset
./src/hdfs/org/apache/hadoop/hdfs/DFSClient.java:1469:     * @param length
./src/hdfs/org/apache/hadoop/hdfs/DFSClient.java:1470:     * @return consequent segment of located blocks
./src/hdfs/org/apache/hadoop/hdfs/DFSClient.java:1471:     * @throws IOException
./src/hdfs/org/apache/hadoop/hdfs/DFSClient.java:1571:    @Override
./src/hdfs/org/apache/hadoop/hdfs/DFSClient.java:1591:    @Override
./src/hdfs/org/apache/hadoop/hdfs/DFSClient.java:1652:    @Override
./src/hdfs/org/apache/hadoop/hdfs/DFSClient.java:1786:     * @param position start read from this position
./src/hdfs/org/apache/hadoop/hdfs/DFSClient.java:1787:     * @param buffer read buffer
./src/hdfs/org/apache/hadoop/hdfs/DFSClient.java:1788:     * @param offset offset into buffer
./src/hdfs/org/apache/hadoop/hdfs/DFSClient.java:1789:     * @param length number of bytes to read
./src/hdfs/org/apache/hadoop/hdfs/DFSClient.java:1791:     * @return actual number of bytes read
./src/hdfs/org/apache/hadoop/hdfs/DFSClient.java:1793:    @Override
./src/hdfs/org/apache/hadoop/hdfs/DFSClient.java:1830:    @Override
./src/hdfs/org/apache/hadoop/hdfs/DFSClient.java:1847:    @Override
./src/hdfs/org/apache/hadoop/hdfs/DFSClient.java:1880:     * Same as {@link #seekToNewSource(long)} except that it does not exclude
./src/hdfs/org/apache/hadoop/hdfs/DFSClient.java:1894:    @Override
./src/hdfs/org/apache/hadoop/hdfs/DFSClient.java:1915:    @Override
./src/hdfs/org/apache/hadoop/hdfs/DFSClient.java:1922:    @Override
./src/hdfs/org/apache/hadoop/hdfs/DFSClient.java:1933:    @Override
./src/hdfs/org/apache/hadoop/hdfs/DFSClient.java:1937:    @Override
./src/hdfs/org/apache/hadoop/hdfs/DFSClient.java:1940:    @Override
./src/hdfs/org/apache/hadoop/hdfs/DFSClient.java:2577:     * @see ClientProtocol#create(String, FsPermission, String, boolean, short, long)
./src/hdfs/org/apache/hadoop/hdfs/DFSClient.java:2598:     * @see ClientProtocol#create(String, FsPermission, String, boolean, short, long)
./src/hdfs/org/apache/hadoop/hdfs/DFSClient.java:2849:    // @see FSOutputSummer#writeChunk()
./src/hdfs/org/apache/hadoop/hdfs/DFSClient.java:2850:    @Override
./src/hdfs/org/apache/hadoop/hdfs/DFSClient.java:3051:    @Override
./src/hdfs/org/apache/hadoop/hdfs/DFSClient.java:3178:  /** {@inheritDoc} */
./src/hdfs/org/apache/hadoop/hdfs/DistributedFileSystem.java:54:  /** @deprecated */
./src/hdfs/org/apache/hadoop/hdfs/DistributedFileSystem.java:60:  /** @deprecated */
./src/hdfs/org/apache/hadoop/hdfs/DistributedFileSystem.java:123:  /** {@inheritDoc} */
./src/hdfs/org/apache/hadoop/hdfs/DistributedFileSystem.java:191:  @Deprecated
./src/hdfs/org/apache/hadoop/hdfs/DistributedFileSystem.java:204:  /** {@inheritDoc} */
./src/hdfs/org/apache/hadoop/hdfs/DistributedFileSystem.java:210:   * @see org.apache.hadoop.hdfs.protocol.ClientProtocol#setQuota(String, long, long) 
./src/hdfs/org/apache/hadoop/hdfs/DistributedFileSystem.java:239:  /** {@inheritDoc} */
./src/hdfs/org/apache/hadoop/hdfs/DistributedFileSystem.java:305:   * @see org.apache.hadoop.hdfs.protocol.ClientProtocol#setSafeMode(
./src/hdfs/org/apache/hadoop/hdfs/DistributedFileSystem.java:323:   * @throws IOException
./src/hdfs/org/apache/hadoop/hdfs/DistributedFileSystem.java:387:   * @throws FileNotFoundException if the file does not exist.
./src/hdfs/org/apache/hadoop/hdfs/DistributedFileSystem.java:398:  /** {@inheritDoc} */
./src/hdfs/org/apache/hadoop/hdfs/DistributedFileSystem.java:403:  /** {@inheritDoc }*/
./src/hdfs/org/apache/hadoop/hdfs/DistributedFileSystem.java:409:  /** {@inheritDoc }*/
./src/hdfs/org/apache/hadoop/hdfs/DistributedFileSystem.java:418:  /** {@inheritDoc }*/
./src/hdfs/org/apache/hadoop/hdfs/HftpFileSystem.java:64: * @see org.apache.hadoop.hdfs.server.namenode.ListPathsServlet
./src/hdfs/org/apache/hadoop/hdfs/HftpFileSystem.java:65: * @see org.apache.hadoop.hdfs.server.namenode.FileDataServlet
./src/hdfs/org/apache/hadoop/hdfs/HftpFileSystem.java:77:  @Override
./src/hdfs/org/apache/hadoop/hdfs/HftpFileSystem.java:89:  @Override
./src/hdfs/org/apache/hadoop/hdfs/HftpFileSystem.java:101:   * @param path The path component of the URL
./src/hdfs/org/apache/hadoop/hdfs/HftpFileSystem.java:102:   * @param query The query component of the URL
./src/hdfs/org/apache/hadoop/hdfs/HftpFileSystem.java:118:  @Override
./src/hdfs/org/apache/hadoop/hdfs/HftpFileSystem.java:230:  @Override
./src/hdfs/org/apache/hadoop/hdfs/HftpFileSystem.java:236:  @Override
./src/hdfs/org/apache/hadoop/hdfs/HftpFileSystem.java:245:    /** {@inheritDoc} */
./src/hdfs/org/apache/hadoop/hdfs/HftpFileSystem.java:282:  /** {@inheritDoc} */
./src/hdfs/org/apache/hadoop/hdfs/HftpFileSystem.java:287:  @Override
./src/hdfs/org/apache/hadoop/hdfs/HftpFileSystem.java:292:  @Override
./src/hdfs/org/apache/hadoop/hdfs/HftpFileSystem.java:301:  @Override
./src/hdfs/org/apache/hadoop/hdfs/HftpFileSystem.java:309:  @Override
./src/hdfs/org/apache/hadoop/hdfs/HftpFileSystem.java:314:  @Override
./src/hdfs/org/apache/hadoop/hdfs/HftpFileSystem.java:316:   * @deprecated Use delete(path, boolean)
./src/hdfs/org/apache/hadoop/hdfs/HftpFileSystem.java:318:  @Deprecated
./src/hdfs/org/apache/hadoop/hdfs/HftpFileSystem.java:323:  @Override 
./src/hdfs/org/apache/hadoop/hdfs/HftpFileSystem.java:328:  @Override
./src/hdfs/org/apache/hadoop/hdfs/HsftpFileSystem.java:33: * @see org.apache.hadoop.hdfs.server.namenode.ListPathsServlet
./src/hdfs/org/apache/hadoop/hdfs/HsftpFileSystem.java:34: * @see org.apache.hadoop.hdfs.server.namenode.FileDataServlet
./src/hdfs/org/apache/hadoop/hdfs/HsftpFileSystem.java:38:  @Override
./src/hdfs/org/apache/hadoop/hdfs/HsftpFileSystem.java:50:  @Override
./src/hdfs/org/apache/hadoop/hdfs/protocol/Block.java:153:  /** {@inheritDoc} */
./src/hdfs/org/apache/hadoop/hdfs/protocol/Block.java:168:  /** {@inheritDoc} */
./src/hdfs/org/apache/hadoop/hdfs/protocol/Block.java:180:  /** {@inheritDoc} */
./src/hdfs/org/apache/hadoop/hdfs/protocol/BlockListAsLongs.java:48:   * @param blockArray - the input array block[]
./src/hdfs/org/apache/hadoop/hdfs/protocol/BlockListAsLongs.java:49:   * @return the output array of long[]
./src/hdfs/org/apache/hadoop/hdfs/protocol/BlockListAsLongs.java:66:   * @param iBlockList - BlockListALongs create from this long[] parameter
./src/hdfs/org/apache/hadoop/hdfs/protocol/BlockListAsLongs.java:83:   * @return - the number of blocks
./src/hdfs/org/apache/hadoop/hdfs/protocol/BlockListAsLongs.java:92:   * @param index - the block whose block-id is desired
./src/hdfs/org/apache/hadoop/hdfs/protocol/BlockListAsLongs.java:93:   * @return the block-id
./src/hdfs/org/apache/hadoop/hdfs/protocol/BlockListAsLongs.java:101:   * @param index - the block whose block-len is desired
./src/hdfs/org/apache/hadoop/hdfs/protocol/BlockListAsLongs.java:102:   * @return - the block-len
./src/hdfs/org/apache/hadoop/hdfs/protocol/BlockListAsLongs.java:110:   * @param index - the block whose block-len is desired
./src/hdfs/org/apache/hadoop/hdfs/protocol/BlockListAsLongs.java:111:   * @return - the generation stamp
./src/hdfs/org/apache/hadoop/hdfs/protocol/BlockListAsLongs.java:119:   * @param index - the index of the block to set
./src/hdfs/org/apache/hadoop/hdfs/protocol/BlockListAsLongs.java:120:   * @param b - the block is set to the value of the this block
./src/hdfs/org/apache/hadoop/hdfs/protocol/ClientDatanodeProtocol.java:37:   * @param block the specified block
./src/hdfs/org/apache/hadoop/hdfs/protocol/ClientDatanodeProtocol.java:38:   * @param keepLength keep the block length
./src/hdfs/org/apache/hadoop/hdfs/protocol/ClientDatanodeProtocol.java:39:   * @param targets the list of possible locations of specified block
./src/hdfs/org/apache/hadoop/hdfs/protocol/ClientDatanodeProtocol.java:40:   * @return the new blockid if recovery successful and the generation stamp
./src/hdfs/org/apache/hadoop/hdfs/protocol/ClientDatanodeProtocol.java:43:   * @throws IOException
./src/hdfs/org/apache/hadoop/hdfs/protocol/ClientProtocol.java:32: * {@link org.apache.hadoop.hdfs.DistributedFileSystem} class to communicate 
./src/hdfs/org/apache/hadoop/hdfs/protocol/ClientProtocol.java:55:   * Return {@link LocatedBlocks} which contains
./src/hdfs/org/apache/hadoop/hdfs/protocol/ClientProtocol.java:63:   * @param src file name
./src/hdfs/org/apache/hadoop/hdfs/protocol/ClientProtocol.java:64:   * @param offset range start offset
./src/hdfs/org/apache/hadoop/hdfs/protocol/ClientProtocol.java:65:   * @param length range length
./src/hdfs/org/apache/hadoop/hdfs/protocol/ClientProtocol.java:66:   * @return file length and array of blocks with their locations
./src/hdfs/org/apache/hadoop/hdfs/protocol/ClientProtocol.java:67:   * @throws IOException
./src/hdfs/org/apache/hadoop/hdfs/protocol/ClientProtocol.java:81:   * Although, other clients cannot {@link #delete(String)}, re-create or 
./src/hdfs/org/apache/hadoop/hdfs/protocol/ClientProtocol.java:82:   * {@link #rename(String, String)} it until the file is completed
./src/hdfs/org/apache/hadoop/hdfs/protocol/ClientProtocol.java:86:   * create multi-block files must also use {@link #addBlock(String, String)}.
./src/hdfs/org/apache/hadoop/hdfs/protocol/ClientProtocol.java:88:   * @param src path of the file being created.
./src/hdfs/org/apache/hadoop/hdfs/protocol/ClientProtocol.java:89:   * @param masked masked permission.
./src/hdfs/org/apache/hadoop/hdfs/protocol/ClientProtocol.java:90:   * @param clientName name of the current client.
./src/hdfs/org/apache/hadoop/hdfs/protocol/ClientProtocol.java:91:   * @param overwrite indicates whether the file should be 
./src/hdfs/org/apache/hadoop/hdfs/protocol/ClientProtocol.java:93:   * @param replication block replication factor.
./src/hdfs/org/apache/hadoop/hdfs/protocol/ClientProtocol.java:94:   * @param blockSize maximum block size.
./src/hdfs/org/apache/hadoop/hdfs/protocol/ClientProtocol.java:96:   * @throws AccessControlException if permission to create file is 
./src/hdfs/org/apache/hadoop/hdfs/protocol/ClientProtocol.java:98:   * be wrapped into {@link org.apache.hadoop.ipc.RemoteException}.
./src/hdfs/org/apache/hadoop/hdfs/protocol/ClientProtocol.java:99:   * @throws QuotaExceededException if the file creation violates 
./src/hdfs/org/apache/hadoop/hdfs/protocol/ClientProtocol.java:101:   * @throws IOException if other errors occur.
./src/hdfs/org/apache/hadoop/hdfs/protocol/ClientProtocol.java:113:   * @param src path of the file being created.
./src/hdfs/org/apache/hadoop/hdfs/protocol/ClientProtocol.java:114:   * @param clientName name of the current client.
./src/hdfs/org/apache/hadoop/hdfs/protocol/ClientProtocol.java:115:   * @return information about the last partial block if any.
./src/hdfs/org/apache/hadoop/hdfs/protocol/ClientProtocol.java:116:   * @throws AccessControlException if permission to append file is 
./src/hdfs/org/apache/hadoop/hdfs/protocol/ClientProtocol.java:118:   * be wrapped into {@link org.apache.hadoop.ipc.RemoteException}.
./src/hdfs/org/apache/hadoop/hdfs/protocol/ClientProtocol.java:119:   * @throws IOException if other errors occur.
./src/hdfs/org/apache/hadoop/hdfs/protocol/ClientProtocol.java:131:   * @param src file name
./src/hdfs/org/apache/hadoop/hdfs/protocol/ClientProtocol.java:132:   * @param replication new replication
./src/hdfs/org/apache/hadoop/hdfs/protocol/ClientProtocol.java:133:   * @throws IOException
./src/hdfs/org/apache/hadoop/hdfs/protocol/ClientProtocol.java:134:   * @return true if successful;
./src/hdfs/org/apache/hadoop/hdfs/protocol/ClientProtocol.java:150:   * @param src
./src/hdfs/org/apache/hadoop/hdfs/protocol/ClientProtocol.java:151:   * @param username If it is null, the original username remains unchanged.
./src/hdfs/org/apache/hadoop/hdfs/protocol/ClientProtocol.java:152:   * @param groupname If it is null, the original groupname remains unchanged.
./src/hdfs/org/apache/hadoop/hdfs/protocol/ClientProtocol.java:174:   * @return LocatedBlock allocated block information.
./src/hdfs/org/apache/hadoop/hdfs/protocol/ClientProtocol.java:195:   * @param blocks Array of located blocks to report
./src/hdfs/org/apache/hadoop/hdfs/protocol/ClientProtocol.java:205:   * @param src existing file or directory name.
./src/hdfs/org/apache/hadoop/hdfs/protocol/ClientProtocol.java:206:   * @param dst new name.
./src/hdfs/org/apache/hadoop/hdfs/protocol/ClientProtocol.java:207:   * @return true if successful, or false if the old name does not exist
./src/hdfs/org/apache/hadoop/hdfs/protocol/ClientProtocol.java:209:   * @throws IOException if the new name is invalid.
./src/hdfs/org/apache/hadoop/hdfs/protocol/ClientProtocol.java:210:   * @throws QuotaExceededException if the rename would violate 
./src/hdfs/org/apache/hadoop/hdfs/protocol/ClientProtocol.java:220:   * @param src existing name.
./src/hdfs/org/apache/hadoop/hdfs/protocol/ClientProtocol.java:221:   * @return true only if the existing file or directory was actually removed 
./src/hdfs/org/apache/hadoop/hdfs/protocol/ClientProtocol.java:231:   * @param src existing name
./src/hdfs/org/apache/hadoop/hdfs/protocol/ClientProtocol.java:232:   * @param recursive if true deletes a non empty directory recursively,
./src/hdfs/org/apache/hadoop/hdfs/protocol/ClientProtocol.java:234:   * @return true only if the existing file or directory was actually removed 
./src/hdfs/org/apache/hadoop/hdfs/protocol/ClientProtocol.java:243:   * @param src The path of the directory being created
./src/hdfs/org/apache/hadoop/hdfs/protocol/ClientProtocol.java:244:   * @param masked The masked permission of the directory being created
./src/hdfs/org/apache/hadoop/hdfs/protocol/ClientProtocol.java:245:   * @return True if the operation success.
./src/hdfs/org/apache/hadoop/hdfs/protocol/ClientProtocol.java:246:   * @throws {@link AccessControlException} if permission to create file is 
./src/hdfs/org/apache/hadoop/hdfs/protocol/ClientProtocol.java:248:   * be wraped into {@link org.apache.hadoop.ipc.RemoteException}.
./src/hdfs/org/apache/hadoop/hdfs/protocol/ClientProtocol.java:249:   * @throws QuotaExceededException if the operation would violate 
./src/hdfs/org/apache/hadoop/hdfs/protocol/ClientProtocol.java:303:   * @param filename The name of the file
./src/hdfs/org/apache/hadoop/hdfs/protocol/ClientProtocol.java:304:   * @return The number of bytes in each block
./src/hdfs/org/apache/hadoop/hdfs/protocol/ClientProtocol.java:305:   * @throws IOException
./src/hdfs/org/apache/hadoop/hdfs/protocol/ClientProtocol.java:319:   * {@link #setSafeMode(FSConstants.SafeModeAction) setSafeMode(SafeModeAction.SAFEMODE_GET)}.
./src/hdfs/org/apache/hadoop/hdfs/protocol/ClientProtocol.java:335:   * {@link #setSafeMode(FSConstants.SafeModeAction) setSafeMode(SafeModeAction.SAFEMODE_ENTER)}
./src/hdfs/org/apache/hadoop/hdfs/protocol/ClientProtocol.java:337:   * using {@link #setSafeMode(FSConstants.SafeModeAction) setSafeMode(SafeModeAction.SAFEMODE_LEAVE)}.
./src/hdfs/org/apache/hadoop/hdfs/protocol/ClientProtocol.java:339:   * {@link #setSafeMode(FSConstants.SafeModeAction) setSafeMode(SafeModeAction.SAFEMODE_GET)}
./src/hdfs/org/apache/hadoop/hdfs/protocol/ClientProtocol.java:354:   * @param action  <ul> <li>0 leave safe mode;</li>
./src/hdfs/org/apache/hadoop/hdfs/protocol/ClientProtocol.java:357:   * @return <ul><li>0 if the safe mode is OFF or</li> 
./src/hdfs/org/apache/hadoop/hdfs/protocol/ClientProtocol.java:359:   * @throws IOException
./src/hdfs/org/apache/hadoop/hdfs/protocol/ClientProtocol.java:365:   * @throws IOException
./src/hdfs/org/apache/hadoop/hdfs/protocol/ClientProtocol.java:374:   * @throws IOException
./src/hdfs/org/apache/hadoop/hdfs/protocol/ClientProtocol.java:381:   * @param action {@link FSConstants.UpgradeAction} to perform
./src/hdfs/org/apache/hadoop/hdfs/protocol/ClientProtocol.java:382:   * @return upgrade status information or null if no upgrades are in progress
./src/hdfs/org/apache/hadoop/hdfs/protocol/ClientProtocol.java:383:   * @throws IOException
./src/hdfs/org/apache/hadoop/hdfs/protocol/ClientProtocol.java:391:   * @throws IOException
./src/hdfs/org/apache/hadoop/hdfs/protocol/ClientProtocol.java:397:   * @param src The string representation of the path to the file
./src/hdfs/org/apache/hadoop/hdfs/protocol/ClientProtocol.java:398:   * @throws IOException if permission to access file is denied by the system 
./src/hdfs/org/apache/hadoop/hdfs/protocol/ClientProtocol.java:399:   * @return object containing information regarding the file
./src/hdfs/org/apache/hadoop/hdfs/protocol/ClientProtocol.java:405:   * Get {@link ContentSummary} rooted at the specified directory.
./src/hdfs/org/apache/hadoop/hdfs/protocol/ClientProtocol.java:406:   * @param path The string representation of the path
./src/hdfs/org/apache/hadoop/hdfs/protocol/ClientProtocol.java:412:   * @param path  The string representation of the path to the directory
./src/hdfs/org/apache/hadoop/hdfs/protocol/ClientProtocol.java:413:   * @param namespaceQuota Limit on the number of names in the tree rooted 
./src/hdfs/org/apache/hadoop/hdfs/protocol/ClientProtocol.java:415:   * @param diskspaceQuota Limit on disk space occupied all the files under
./src/hdfs/org/apache/hadoop/hdfs/protocol/ClientProtocol.java:420:   * the quota to that value, (2) {@link FSConstants#QUOTA_DONT_SET}  implies 
./src/hdfs/org/apache/hadoop/hdfs/protocol/ClientProtocol.java:421:   * the quota will not be changed, and (3) {@link FSConstants#QUOTA_RESET} 
./src/hdfs/org/apache/hadoop/hdfs/protocol/ClientProtocol.java:424:   * @throws FileNotFoundException if the path is a file or 
./src/hdfs/org/apache/hadoop/hdfs/protocol/ClientProtocol.java:426:   * @throws QuotaExceededException if the directory size 
./src/hdfs/org/apache/hadoop/hdfs/protocol/ClientProtocol.java:435:   * @param src The string representation of the path
./src/hdfs/org/apache/hadoop/hdfs/protocol/ClientProtocol.java:436:   * @param client The string representation of the client
./src/hdfs/org/apache/hadoop/hdfs/protocol/ClientProtocol.java:442:   * @param src The string representation of the path
./src/hdfs/org/apache/hadoop/hdfs/protocol/ClientProtocol.java:443:   * @param mtime The number of milliseconds since Jan 1, 1970.
./src/hdfs/org/apache/hadoop/hdfs/protocol/ClientProtocol.java:446:   * @param atime The number of milliseconds since Jan 1, 1970.
./src/hdfs/org/apache/hadoop/hdfs/protocol/DatanodeID.java:51:   * @param from
./src/hdfs/org/apache/hadoop/hdfs/protocol/DatanodeID.java:62:   * @param nodeName (hostname:portNumber) 
./src/hdfs/org/apache/hadoop/hdfs/protocol/DatanodeID.java:63:   * @param storageID data storage ID
./src/hdfs/org/apache/hadoop/hdfs/protocol/DatanodeID.java:64:   * @param infoPort info server port 
./src/hdfs/org/apache/hadoop/hdfs/protocol/DatanodeID.java:65:   * @param ipcPort ipc server port
./src/hdfs/org/apache/hadoop/hdfs/protocol/DatanodeID.java:76:   * @return hostname:portNumber.
./src/hdfs/org/apache/hadoop/hdfs/protocol/DatanodeID.java:83:   * @return data storage ID.
./src/hdfs/org/apache/hadoop/hdfs/protocol/DatanodeID.java:90:   * @return infoPort (the port at which the HTTP server bound to)
./src/hdfs/org/apache/hadoop/hdfs/protocol/DatanodeID.java:97:   * @return ipcPort (the port at which the IPC server bound to)
./src/hdfs/org/apache/hadoop/hdfs/protocol/DatanodeID.java:111:   * @return hostname and no :portNumber.
./src/hdfs/org/apache/hadoop/hdfs/protocol/DatanodeID.java:161:   * @param that
./src/hdfs/org/apache/hadoop/hdfs/protocol/DatanodeID.java:162:   * @return as specified by Comparable.
./src/hdfs/org/apache/hadoop/hdfs/protocol/DatanodeID.java:171:  /** {@inheritDoc} */
./src/hdfs/org/apache/hadoop/hdfs/protocol/DatanodeID.java:178:  /** {@inheritDoc} */
./src/hdfs/org/apache/hadoop/hdfs/protocol/DatanodeInfo.java:312:  /** {@inheritDoc} */
./src/hdfs/org/apache/hadoop/hdfs/protocol/DatanodeInfo.java:329:  /** {@inheritDoc} */
./src/hdfs/org/apache/hadoop/hdfs/protocol/FSConstants.java:54:  //TODO mb@media-style.com: should be conf injected?
./src/hdfs/org/apache/hadoop/hdfs/protocol/LocatedBlocks.java:92:   * @return block if found, or null otherwise.
./src/hdfs/org/apache/hadoop/hdfs/server/balancer/Balancer.java:286:     * @return true if a proxy is found; otherwise false
./src/hdfs/org/apache/hadoop/hdfs/server/balancer/Balancer.java:789:   * @param args
./src/hdfs/org/apache/hadoop/hdfs/server/balancer/Balancer.java:907:   * @return the total number of bytes that are 
./src/hdfs/org/apache/hadoop/hdfs/server/balancer/Balancer.java:909:   * @param datanodes a set of datanodes
./src/hdfs/org/apache/hadoop/hdfs/server/balancer/Balancer.java:1368:   * @param args arguments to a Balancer
./src/hdfs/org/apache/hadoop/hdfs/server/balancer/Balancer.java:1369:   * @exception any exception occurs during datanode balancing
./src/hdfs/org/apache/hadoop/hdfs/server/common/GenerationStamp.java:93:  /** {@inheritDoc} */
./src/hdfs/org/apache/hadoop/hdfs/server/common/GenerationStamp.java:98:  /** {@inheritDoc} */
./src/hdfs/org/apache/hadoop/hdfs/server/common/GenerationStamp.java:110:  /** {@inheritDoc} */
./src/hdfs/org/apache/hadoop/hdfs/server/common/Storage.java:213:     * @throws IOException if file cannot be read or contains inconsistent data
./src/hdfs/org/apache/hadoop/hdfs/server/common/Storage.java:239:     * @throws IOException
./src/hdfs/org/apache/hadoop/hdfs/server/common/Storage.java:287:     * @throws IOException
./src/hdfs/org/apache/hadoop/hdfs/server/common/Storage.java:329:     * @param startOpt a startup option.
./src/hdfs/org/apache/hadoop/hdfs/server/common/Storage.java:331:     * @return state {@link StorageState} of the storage directory 
./src/hdfs/org/apache/hadoop/hdfs/server/common/Storage.java:332:     * @throws {@link InconsistentFSStateException} if directory state is not 
./src/hdfs/org/apache/hadoop/hdfs/server/common/Storage.java:439:     * @param curState specifies what/how the state should be recovered
./src/hdfs/org/apache/hadoop/hdfs/server/common/Storage.java:440:     * @throws IOException
./src/hdfs/org/apache/hadoop/hdfs/server/common/Storage.java:502:     * @throws IOException if locking fails
./src/hdfs/org/apache/hadoop/hdfs/server/common/Storage.java:517:     * @return A lock object representing the newly-acquired lock or
./src/hdfs/org/apache/hadoop/hdfs/server/common/Storage.java:519:     * @throws IOException if locking fails.
./src/hdfs/org/apache/hadoop/hdfs/server/common/Storage.java:542:     * @throws IOException
./src/hdfs/org/apache/hadoop/hdfs/server/common/Storage.java:600:   * @param oldVersion
./src/hdfs/org/apache/hadoop/hdfs/server/common/Storage.java:624:   * @param props
./src/hdfs/org/apache/hadoop/hdfs/server/common/Storage.java:625:   * @throws IOException
./src/hdfs/org/apache/hadoop/hdfs/server/common/Storage.java:659:   * @param props
./src/hdfs/org/apache/hadoop/hdfs/server/common/Storage.java:660:   * @throws IOException
./src/hdfs/org/apache/hadoop/hdfs/server/common/Storage.java:684:   * @throws IOException
./src/hdfs/org/apache/hadoop/hdfs/server/common/Storage.java:695:   * @throws IOException
./src/hdfs/org/apache/hadoop/hdfs/server/common/Storage.java:706:   * @return <code>true</code> if exclusive locks are supported or
./src/hdfs/org/apache/hadoop/hdfs/server/common/Storage.java:708:   * @throws IOException
./src/hdfs/org/apache/hadoop/hdfs/server/common/Storage.java:709:   * @see StorageDirectory#lock()
./src/hdfs/org/apache/hadoop/hdfs/server/common/Upgradeable.java:30: * one returned by {@link #getVersion()} must be upgraded with this object.
./src/hdfs/org/apache/hadoop/hdfs/server/common/Upgradeable.java:35:   * @return layout version
./src/hdfs/org/apache/hadoop/hdfs/server/common/Upgradeable.java:41:   * @return type
./src/hdfs/org/apache/hadoop/hdfs/server/common/Upgradeable.java:47:   * @return description
./src/hdfs/org/apache/hadoop/hdfs/server/common/Upgradeable.java:59:   * @return integer value in the range [0, 100].
./src/hdfs/org/apache/hadoop/hdfs/server/common/Upgradeable.java:71:   * @return an UpgradeCommand for broadcasting.
./src/hdfs/org/apache/hadoop/hdfs/server/common/Upgradeable.java:72:   * @throws IOException
./src/hdfs/org/apache/hadoop/hdfs/server/common/Upgradeable.java:85:   * @throws IOException
./src/hdfs/org/apache/hadoop/hdfs/server/common/Upgradeable.java:92:   * @param details true if upgradeStatus details need to be included, 
./src/hdfs/org/apache/hadoop/hdfs/server/common/Upgradeable.java:94:   * @return {@link UpgradeStatusReport}
./src/hdfs/org/apache/hadoop/hdfs/server/common/Upgradeable.java:95:   * @throws IOException
./src/hdfs/org/apache/hadoop/hdfs/server/common/UpgradeManager.java:29: * {@link #broadcastCommand} is the command that should be 
./src/hdfs/org/apache/hadoop/hdfs/server/common/UpgradeObject.java:27: * Contains default implementation of common methods of {@link Upgradeable}
./src/hdfs/org/apache/hadoop/hdfs/server/common/UpgradeStatusReport.java:53:   * @return layout version
./src/hdfs/org/apache/hadoop/hdfs/server/common/UpgradeStatusReport.java:62:   * @see Upgradeable#getUpgradeStatus() 
./src/hdfs/org/apache/hadoop/hdfs/server/common/UpgradeStatusReport.java:70:   * @return true if finalized or false otherwise.
./src/hdfs/org/apache/hadoop/hdfs/server/common/UpgradeStatusReport.java:80:   * @param details true if upgradeStatus details need to be included, 
./src/hdfs/org/apache/hadoop/hdfs/server/common/UpgradeStatusReport.java:82:   * @return text
./src/hdfs/org/apache/hadoop/hdfs/server/common/Util.java:23:   * @return current time in msec.
./src/hdfs/org/apache/hadoop/hdfs/server/datanode/BlockMetadataHeader.java:64:   * @param in 
./src/hdfs/org/apache/hadoop/hdfs/server/datanode/BlockMetadataHeader.java:65:   * @return Metadata Header
./src/hdfs/org/apache/hadoop/hdfs/server/datanode/BlockMetadataHeader.java:66:   * @throws IOException
./src/hdfs/org/apache/hadoop/hdfs/server/datanode/BlockMetadataHeader.java:75:   * @param dataset
./src/hdfs/org/apache/hadoop/hdfs/server/datanode/BlockMetadataHeader.java:76:   * @param block
./src/hdfs/org/apache/hadoop/hdfs/server/datanode/BlockMetadataHeader.java:77:   * @return
./src/hdfs/org/apache/hadoop/hdfs/server/datanode/BlockMetadataHeader.java:78:   * @throws IOException
./src/hdfs/org/apache/hadoop/hdfs/server/datanode/BlockMetadataHeader.java:100:   * @param out DataOutputStream
./src/hdfs/org/apache/hadoop/hdfs/server/datanode/BlockMetadataHeader.java:101:   * @param header 
./src/hdfs/org/apache/hadoop/hdfs/server/datanode/BlockMetadataHeader.java:102:   * @return 
./src/hdfs/org/apache/hadoop/hdfs/server/datanode/BlockMetadataHeader.java:103:   * @throws IOException
./src/hdfs/org/apache/hadoop/hdfs/server/datanode/BlockMetadataHeader.java:114:   * @param out
./src/hdfs/org/apache/hadoop/hdfs/server/datanode/BlockMetadataHeader.java:115:   * @param checksum
./src/hdfs/org/apache/hadoop/hdfs/server/datanode/BlockMetadataHeader.java:116:   * @throws IOException
./src/hdfs/org/apache/hadoop/hdfs/server/datanode/BlockReceiver.java:152:   * @throws IOException
./src/hdfs/org/apache/hadoop/hdfs/server/datanode/BlockReceiver.java:698:     * @param seqno
./src/hdfs/org/apache/hadoop/hdfs/server/datanode/BlockReceiver.java:699:     * @param lastPacketInBlock
./src/hdfs/org/apache/hadoop/hdfs/server/datanode/BlockReceiver.java:814:     * @see java.lang.Runnable#run()
./src/hdfs/org/apache/hadoop/hdfs/server/datanode/BlockSender.java:207:   * {@link SocketOutputStream} and tries 
./src/hdfs/org/apache/hadoop/hdfs/server/datanode/BlockSender.java:208:   * {@link SocketOutputStream#transferToFully(FileChannel, long, int)} to
./src/hdfs/org/apache/hadoop/hdfs/server/datanode/BlockSender.java:331:   * @param out  stream to which the block is written to
./src/hdfs/org/apache/hadoop/hdfs/server/datanode/BlockSender.java:332:   * @param baseStream optional. if non-null, <code>out</code> is assumed to 
./src/hdfs/org/apache/hadoop/hdfs/server/datanode/BlockSender.java:335:   *        {@link SocketOutputStream#transferToFully(FileChannel, 
./src/hdfs/org/apache/hadoop/hdfs/server/datanode/BlockSender.java:337:   * @param throttler for sending data.
./src/hdfs/org/apache/hadoop/hdfs/server/datanode/BlockSender.java:338:   * @return total bytes reads, including crc.
./src/hdfs/org/apache/hadoop/hdfs/server/datanode/BlockTransferThrottler.java:35:   * @param bandwidthPerSec bandwidth allowed in bytes per second. 
./src/hdfs/org/apache/hadoop/hdfs/server/datanode/BlockTransferThrottler.java:43:   * @param period in milliseconds. Bandwidth is enforced over this
./src/hdfs/org/apache/hadoop/hdfs/server/datanode/BlockTransferThrottler.java:45:   * @param bandwidthPerSec bandwidth allowed in bytes per second. 
./src/hdfs/org/apache/hadoop/hdfs/server/datanode/BlockTransferThrottler.java:55:   * @return current throttle bandwidth in bytes per second.
./src/hdfs/org/apache/hadoop/hdfs/server/datanode/BlockTransferThrottler.java:65:   * @param bytesPerSecond 
./src/hdfs/org/apache/hadoop/hdfs/server/datanode/BlockTransferThrottler.java:78:   * @param numOfBytes
./src/hdfs/org/apache/hadoop/hdfs/server/datanode/DataBlockScanner.java:285:  /** @return the last scan time */
./src/hdfs/org/apache/hadoop/hdfs/server/datanode/DataBlockScanner.java:728:     * @param dir where the logs files are located.
./src/hdfs/org/apache/hadoop/hdfs/server/datanode/DataBlockScanner.java:729:     * @param filePrefix prefix of the file.
./src/hdfs/org/apache/hadoop/hdfs/server/datanode/DataBlockScanner.java:730:     * @param maxNumLines max lines in a file (its a soft limit).
./src/hdfs/org/apache/hadoop/hdfs/server/datanode/DataBlockScanner.java:731:     * @throws IOException
./src/hdfs/org/apache/hadoop/hdfs/server/datanode/DataNode.java:137:   * Use {@link NetUtils#createSocketAddr(String)} instead.
./src/hdfs/org/apache/hadoop/hdfs/server/datanode/DataNode.java:139:  @Deprecated
./src/hdfs/org/apache/hadoop/hdfs/server/datanode/DataNode.java:189:   * @return current time in msec.
./src/hdfs/org/apache/hadoop/hdfs/server/datanode/DataNode.java:216:   * @param conf - the configuration
./src/hdfs/org/apache/hadoop/hdfs/server/datanode/DataNode.java:220:   * @param dataDirs - only for a non-simulated storage data node
./src/hdfs/org/apache/hadoop/hdfs/server/datanode/DataNode.java:221:   * @throws IOException
./src/hdfs/org/apache/hadoop/hdfs/server/datanode/DataNode.java:499:   * @see FSNamesystem#registerDatanode(DatanodeRegistration)
./src/hdfs/org/apache/hadoop/hdfs/server/datanode/DataNode.java:500:   * @throws IOException
./src/hdfs/org/apache/hadoop/hdfs/server/datanode/DataNode.java:782:     * @param cmd
./src/hdfs/org/apache/hadoop/hdfs/server/datanode/DataNode.java:783:     * @return true if further processing may be required or false otherwise. 
./src/hdfs/org/apache/hadoop/hdfs/server/datanode/DataNode.java:784:     * @throws IOException
./src/hdfs/org/apache/hadoop/hdfs/server/datanode/DataNode.java:1136:   *  {@link DataNode#runDatanodeDaemon(DataNode)} subsequently. 
./src/hdfs/org/apache/hadoop/hdfs/server/datanode/DataNode.java:1179:   * @param dataDirs List of directories, where the new DataNode instance should
./src/hdfs/org/apache/hadoop/hdfs/server/datanode/DataNode.java:1181:   * @param conf Configuration instance to use.
./src/hdfs/org/apache/hadoop/hdfs/server/datanode/DataNode.java:1182:   * @return DataNode instance for given list of data dirs and conf, or null if
./src/hdfs/org/apache/hadoop/hdfs/server/datanode/DataNode.java:1184:   * @throws IOException
./src/hdfs/org/apache/hadoop/hdfs/server/datanode/DataNode.java:1204:  @Override
./src/hdfs/org/apache/hadoop/hdfs/server/datanode/DataNode.java:1222:   * @return false if passed argements are incorrect
./src/hdfs/org/apache/hadoop/hdfs/server/datanode/DataNode.java:1273:   * @return the fsdataset that stores the blocks
./src/hdfs/org/apache/hadoop/hdfs/server/datanode/DataNode.java:1294:  /** {@inheritDoc} */
./src/hdfs/org/apache/hadoop/hdfs/server/datanode/DataNode.java:1337:  /** {@inheritDoc} */
./src/hdfs/org/apache/hadoop/hdfs/server/datanode/DataNode.java:1353:  /** {@inheritDoc} */
./src/hdfs/org/apache/hadoop/hdfs/server/datanode/DataNode.java:1377:    /** {@inheritDoc} */
./src/hdfs/org/apache/hadoop/hdfs/server/datanode/DataNode.java:1502:  /** {@inheritDoc} */
./src/hdfs/org/apache/hadoop/hdfs/server/datanode/DataStorage.java:50: * @see Storage
./src/hdfs/org/apache/hadoop/hdfs/server/datanode/DataStorage.java:89:   * @param nsInfo namespace information
./src/hdfs/org/apache/hadoop/hdfs/server/datanode/DataStorage.java:90:   * @param dataDirs array of data storage directories
./src/hdfs/org/apache/hadoop/hdfs/server/datanode/DataStorage.java:91:   * @param startOpt startup option
./src/hdfs/org/apache/hadoop/hdfs/server/datanode/DataStorage.java:92:   * @throws IOException
./src/hdfs/org/apache/hadoop/hdfs/server/datanode/DataStorage.java:217:   * @param sd  storage directory
./src/hdfs/org/apache/hadoop/hdfs/server/datanode/DataStorage.java:218:   * @param nsInfo  namespace info
./src/hdfs/org/apache/hadoop/hdfs/server/datanode/DataStorage.java:219:   * @param startOpt  startup option
./src/hdfs/org/apache/hadoop/hdfs/server/datanode/DataStorage.java:220:   * @throws IOException
./src/hdfs/org/apache/hadoop/hdfs/server/datanode/DataStorage.java:260:   * @param sd  storage directory
./src/hdfs/org/apache/hadoop/hdfs/server/datanode/DataStorage.java:261:   * @throws IOException
./src/hdfs/org/apache/hadoop/hdfs/server/datanode/DataStorage.java:427:   * @param oldFileName
./src/hdfs/org/apache/hadoop/hdfs/server/datanode/DataStorage.java:428:   * @return the new metadata file name with the default generation stamp.
./src/hdfs/org/apache/hadoop/hdfs/server/datanode/DataXceiver.java:142:   * @param in The stream to read from
./src/hdfs/org/apache/hadoop/hdfs/server/datanode/DataXceiver.java:143:   * @throws IOException
./src/hdfs/org/apache/hadoop/hdfs/server/datanode/DataXceiver.java:215:   * @param in The stream to read from
./src/hdfs/org/apache/hadoop/hdfs/server/datanode/DataXceiver.java:216:   * @throws IOException
./src/hdfs/org/apache/hadoop/hdfs/server/datanode/DataXceiver.java:389:   * @param in
./src/hdfs/org/apache/hadoop/hdfs/server/datanode/DataXceiver.java:427:   * @param in
./src/hdfs/org/apache/hadoop/hdfs/server/datanode/DataXceiver.java:471:   * @param in The stream to read from
./src/hdfs/org/apache/hadoop/hdfs/server/datanode/DataXceiver.java:472:   * @throws IOException
./src/hdfs/org/apache/hadoop/hdfs/server/datanode/DataXceiver.java:529:   * @param in The stream to read from
./src/hdfs/org/apache/hadoop/hdfs/server/datanode/DataXceiver.java:530:   * @throws IOException
./src/hdfs/org/apache/hadoop/hdfs/server/datanode/DataXceiver.java:623:   * @param s socket to write to
./src/hdfs/org/apache/hadoop/hdfs/server/datanode/DataXceiver.java:624:   * @param opStatus status message to write
./src/hdfs/org/apache/hadoop/hdfs/server/datanode/DataXceiver.java:625:   * @param timeout send timeout
./src/hdfs/org/apache/hadoop/hdfs/server/datanode/DataXceiverServer.java:69:    * @param bandwidth Total amount of bandwidth can be used for balancing 
./src/hdfs/org/apache/hadoop/hdfs/server/datanode/FSDataset.java:215:     * @throws DiskErrorException
./src/hdfs/org/apache/hadoop/hdfs/server/datanode/FSDataset.java:638:  /** {@inheritDoc} */
./src/hdfs/org/apache/hadoop/hdfs/server/datanode/FSDataset.java:777:   * @param block Block
./src/hdfs/org/apache/hadoop/hdfs/server/datanode/FSDataset.java:778:   * @param numLinks Detach if the number of links exceed this value
./src/hdfs/org/apache/hadoop/hdfs/server/datanode/FSDataset.java:779:   * @throws IOException
./src/hdfs/org/apache/hadoop/hdfs/server/datanode/FSDataset.java:780:   * @return - true if the specified block was detached
./src/hdfs/org/apache/hadoop/hdfs/server/datanode/FSDataset.java:799:  /** {@inheritDoc} */
./src/hdfs/org/apache/hadoop/hdfs/server/datanode/FSDataset.java:831:   * @return ongoing create threads if there is any. Otherwise, return null.
./src/hdfs/org/apache/hadoop/hdfs/server/datanode/FSDataset.java:1187:  /** {@inheritDoc} */
./src/hdfs/org/apache/hadoop/hdfs/server/datanode/FSDataset.java:1316:   * @throws DiskErrorException
./src/hdfs/org/apache/hadoop/hdfs/server/datanode/FSDatasetInterface.java:47:   * @param b - the block for which the metadata length is desired
./src/hdfs/org/apache/hadoop/hdfs/server/datanode/FSDatasetInterface.java:48:   * @return the length of the metadata file for the specified block.
./src/hdfs/org/apache/hadoop/hdfs/server/datanode/FSDatasetInterface.java:49:   * @throws IOException
./src/hdfs/org/apache/hadoop/hdfs/server/datanode/FSDatasetInterface.java:72:   * @param b - the block
./src/hdfs/org/apache/hadoop/hdfs/server/datanode/FSDatasetInterface.java:73:   * @return the metadata input stream; 
./src/hdfs/org/apache/hadoop/hdfs/server/datanode/FSDatasetInterface.java:74:   * @throws IOException
./src/hdfs/org/apache/hadoop/hdfs/server/datanode/FSDatasetInterface.java:81:   * @param b - the block
./src/hdfs/org/apache/hadoop/hdfs/server/datanode/FSDatasetInterface.java:82:   * @return true of the metafile for specified block exits
./src/hdfs/org/apache/hadoop/hdfs/server/datanode/FSDatasetInterface.java:83:   * @throws IOException
./src/hdfs/org/apache/hadoop/hdfs/server/datanode/FSDatasetInterface.java:90:   * @param b
./src/hdfs/org/apache/hadoop/hdfs/server/datanode/FSDatasetInterface.java:91:   * @return   the specified block's on-disk length (excluding metadta)
./src/hdfs/org/apache/hadoop/hdfs/server/datanode/FSDatasetInterface.java:92:   * @throws IOException
./src/hdfs/org/apache/hadoop/hdfs/server/datanode/FSDatasetInterface.java:97:   * @return the generation stamp stored with the block.
./src/hdfs/org/apache/hadoop/hdfs/server/datanode/FSDatasetInterface.java:103:   * @param b
./src/hdfs/org/apache/hadoop/hdfs/server/datanode/FSDatasetInterface.java:104:   * @return an input stream to read the contents of the specified block
./src/hdfs/org/apache/hadoop/hdfs/server/datanode/FSDatasetInterface.java:105:   * @throws IOException
./src/hdfs/org/apache/hadoop/hdfs/server/datanode/FSDatasetInterface.java:111:   * @param b
./src/hdfs/org/apache/hadoop/hdfs/server/datanode/FSDatasetInterface.java:112:   * @param seekOffset
./src/hdfs/org/apache/hadoop/hdfs/server/datanode/FSDatasetInterface.java:113:   * @return an input stream to read the contents of the specified block,
./src/hdfs/org/apache/hadoop/hdfs/server/datanode/FSDatasetInterface.java:115:   * @throws IOException
./src/hdfs/org/apache/hadoop/hdfs/server/datanode/FSDatasetInterface.java:123:   * @param b
./src/hdfs/org/apache/hadoop/hdfs/server/datanode/FSDatasetInterface.java:124:   * @param blkoff
./src/hdfs/org/apache/hadoop/hdfs/server/datanode/FSDatasetInterface.java:125:   * @param ckoff
./src/hdfs/org/apache/hadoop/hdfs/server/datanode/FSDatasetInterface.java:126:   * @return an input stream to read the contents of the specified block,
./src/hdfs/org/apache/hadoop/hdfs/server/datanode/FSDatasetInterface.java:128:   * @throws IOException
./src/hdfs/org/apache/hadoop/hdfs/server/datanode/FSDatasetInterface.java:162:    /** {@inheritDoc} */
./src/hdfs/org/apache/hadoop/hdfs/server/datanode/FSDatasetInterface.java:171:   * @param b
./src/hdfs/org/apache/hadoop/hdfs/server/datanode/FSDatasetInterface.java:172:   * @param isRecovery True if this is part of erro recovery, otherwise false
./src/hdfs/org/apache/hadoop/hdfs/server/datanode/FSDatasetInterface.java:173:   * @return a BlockWriteStreams object to allow writing the block data
./src/hdfs/org/apache/hadoop/hdfs/server/datanode/FSDatasetInterface.java:175:   * @throws IOException
./src/hdfs/org/apache/hadoop/hdfs/server/datanode/FSDatasetInterface.java:188:   * @param b
./src/hdfs/org/apache/hadoop/hdfs/server/datanode/FSDatasetInterface.java:189:   * @throws IOException
./src/hdfs/org/apache/hadoop/hdfs/server/datanode/FSDatasetInterface.java:196:   * @param b
./src/hdfs/org/apache/hadoop/hdfs/server/datanode/FSDatasetInterface.java:197:   * @throws IOException
./src/hdfs/org/apache/hadoop/hdfs/server/datanode/FSDatasetInterface.java:203:   * @return - the block report - the full list of blocks stored
./src/hdfs/org/apache/hadoop/hdfs/server/datanode/FSDatasetInterface.java:209:   * @param b
./src/hdfs/org/apache/hadoop/hdfs/server/datanode/FSDatasetInterface.java:210:   * @return - true if the specified block is valid
./src/hdfs/org/apache/hadoop/hdfs/server/datanode/FSDatasetInterface.java:216:   * @param invalidBlks - the blocks to be invalidated
./src/hdfs/org/apache/hadoop/hdfs/server/datanode/FSDatasetInterface.java:217:   * @throws IOException
./src/hdfs/org/apache/hadoop/hdfs/server/datanode/FSDatasetInterface.java:223:     * @throws DiskErrorException
./src/hdfs/org/apache/hadoop/hdfs/server/datanode/FSDatasetInterface.java:239:   * @param b
./src/hdfs/org/apache/hadoop/hdfs/server/datanode/FSDatasetInterface.java:240:   * @param stream The stream to the data file and checksum file
./src/hdfs/org/apache/hadoop/hdfs/server/datanode/FSDatasetInterface.java:241:   * @return the position of the file pointer in the data stream
./src/hdfs/org/apache/hadoop/hdfs/server/datanode/FSDatasetInterface.java:242:   * @throws IOException
./src/hdfs/org/apache/hadoop/hdfs/server/datanode/FSDatasetInterface.java:249:   * @param b
./src/hdfs/org/apache/hadoop/hdfs/server/datanode/FSDatasetInterface.java:250:   * @param stream The stream for the data file and checksum file
./src/hdfs/org/apache/hadoop/hdfs/server/datanode/FSDatasetInterface.java:251:   * @param dataOffset The position to which the file pointre for the data stream
./src/hdfs/org/apache/hadoop/hdfs/server/datanode/FSDatasetInterface.java:253:   * @param ckOffset The position to which the file pointre for the checksum stream
./src/hdfs/org/apache/hadoop/hdfs/server/datanode/FSDatasetInterface.java:255:   * @throws IOException
./src/hdfs/org/apache/hadoop/hdfs/server/datanode/FSDatasetInterface.java:263:   * @param b The block to be verified.
./src/hdfs/org/apache/hadoop/hdfs/server/datanode/FSDatasetInterface.java:264:   * @throws IOException
./src/hdfs/org/apache/hadoop/hdfs/server/datanode/metrics/DataNodeMetrics.java:40: *  <p> {@link #blocksRead}.inc()
./src/hdfs/org/apache/hadoop/hdfs/server/datanode/metrics/DataNodeStatistics.java:35:   * @param dataNodeMetrics - the metrics from which the mbean gets its info
./src/hdfs/org/apache/hadoop/hdfs/server/datanode/metrics/DataNodeStatistics.java:58:   * @inheritDoc
./src/hdfs/org/apache/hadoop/hdfs/server/datanode/metrics/DataNodeStatistics.java:65:   * @inheritDoc
./src/hdfs/org/apache/hadoop/hdfs/server/datanode/metrics/DataNodeStatistics.java:72:   * @inheritDoc
./src/hdfs/org/apache/hadoop/hdfs/server/datanode/metrics/DataNodeStatistics.java:79:   * @inheritDoc
./src/hdfs/org/apache/hadoop/hdfs/server/datanode/metrics/DataNodeStatistics.java:86:   * @inheritDoc
./src/hdfs/org/apache/hadoop/hdfs/server/datanode/metrics/DataNodeStatistics.java:93:   * @inheritDoc
./src/hdfs/org/apache/hadoop/hdfs/server/datanode/metrics/DataNodeStatistics.java:100:   *   {@inheritDoc}
./src/hdfs/org/apache/hadoop/hdfs/server/datanode/metrics/DataNodeStatistics.java:107:   * @inheritDoc
./src/hdfs/org/apache/hadoop/hdfs/server/datanode/metrics/DataNodeStatistics.java:114:   * @inheritDoc
./src/hdfs/org/apache/hadoop/hdfs/server/datanode/metrics/DataNodeStatistics.java:121:   * @inheritDoc
./src/hdfs/org/apache/hadoop/hdfs/server/datanode/metrics/DataNodeStatistics.java:128:   * @inheritDoc
./src/hdfs/org/apache/hadoop/hdfs/server/datanode/metrics/DataNodeStatistics.java:135:   * @inheritDoc
./src/hdfs/org/apache/hadoop/hdfs/server/datanode/metrics/DataNodeStatistics.java:142:   * @inheritDoc
./src/hdfs/org/apache/hadoop/hdfs/server/datanode/metrics/DataNodeStatistics.java:149:   * @inheritDoc
./src/hdfs/org/apache/hadoop/hdfs/server/datanode/metrics/DataNodeStatistics.java:156:   * @inheritDoc
./src/hdfs/org/apache/hadoop/hdfs/server/datanode/metrics/DataNodeStatistics.java:163:   * @inheritDoc
./src/hdfs/org/apache/hadoop/hdfs/server/datanode/metrics/DataNodeStatistics.java:170:   * @inheritDoc
./src/hdfs/org/apache/hadoop/hdfs/server/datanode/metrics/DataNodeStatistics.java:177:   * @inheritDoc
./src/hdfs/org/apache/hadoop/hdfs/server/datanode/metrics/DataNodeStatistics.java:184:   * @inheritDoc
./src/hdfs/org/apache/hadoop/hdfs/server/datanode/metrics/DataNodeStatistics.java:191:   * @inheritDoc
./src/hdfs/org/apache/hadoop/hdfs/server/datanode/metrics/DataNodeStatistics.java:198:   * @inheritDoc
./src/hdfs/org/apache/hadoop/hdfs/server/datanode/metrics/DataNodeStatistics.java:205:   * @inheritDoc
./src/hdfs/org/apache/hadoop/hdfs/server/datanode/metrics/DataNodeStatistics.java:212:   * @inheritDoc
./src/hdfs/org/apache/hadoop/hdfs/server/datanode/metrics/DataNodeStatistics.java:219:   * @inheritDoc
./src/hdfs/org/apache/hadoop/hdfs/server/datanode/metrics/DataNodeStatistics.java:226:   * @inheritDoc
./src/hdfs/org/apache/hadoop/hdfs/server/datanode/metrics/DataNodeStatistics.java:233:   * @inheritDoc
./src/hdfs/org/apache/hadoop/hdfs/server/datanode/metrics/DataNodeStatistics.java:240:   * @inheritDoc
./src/hdfs/org/apache/hadoop/hdfs/server/datanode/metrics/DataNodeStatistics.java:247:   * @inheritDoc
./src/hdfs/org/apache/hadoop/hdfs/server/datanode/metrics/DataNodeStatistics.java:254:   * @inheritDoc
./src/hdfs/org/apache/hadoop/hdfs/server/datanode/metrics/DataNodeStatistics.java:261:   * @inheritDoc
./src/hdfs/org/apache/hadoop/hdfs/server/datanode/metrics/DataNodeStatistics.java:268:   * @inheritDoc
./src/hdfs/org/apache/hadoop/hdfs/server/datanode/metrics/DataNodeStatistics.java:275:   * @inheritDoc
./src/hdfs/org/apache/hadoop/hdfs/server/datanode/metrics/DataNodeStatistics.java:282:   * @inheritDoc
./src/hdfs/org/apache/hadoop/hdfs/server/datanode/metrics/DataNodeStatistics.java:289:   * @inheritDoc
./src/hdfs/org/apache/hadoop/hdfs/server/datanode/metrics/DataNodeStatistics.java:296:   * @inheritDoc
./src/hdfs/org/apache/hadoop/hdfs/server/datanode/metrics/DataNodeStatistics.java:303:   * @inheritDoc
./src/hdfs/org/apache/hadoop/hdfs/server/datanode/metrics/DataNodeStatistics.java:310:   * @inheritDoc
./src/hdfs/org/apache/hadoop/hdfs/server/datanode/metrics/DataNodeStatistics.java:317:   * @inheritDoc
./src/hdfs/org/apache/hadoop/hdfs/server/datanode/metrics/DataNodeStatistics.java:324:   * @inheritDoc
./src/hdfs/org/apache/hadoop/hdfs/server/datanode/metrics/DataNodeStatistics.java:331:   * @inheritDoc
./src/hdfs/org/apache/hadoop/hdfs/server/datanode/metrics/DataNodeStatistics.java:338:   * @inheritDoc
./src/hdfs/org/apache/hadoop/hdfs/server/datanode/metrics/DataNodeStatistics.java:345:   * @inheritDoc
./src/hdfs/org/apache/hadoop/hdfs/server/datanode/metrics/DataNodeStatistics.java:352:   * @inheritDoc
./src/hdfs/org/apache/hadoop/hdfs/server/datanode/metrics/DataNodeStatistics.java:359:   * @inheritDoc
./src/hdfs/org/apache/hadoop/hdfs/server/datanode/metrics/DataNodeStatistics.java:366:   * @inheritDoc
./src/hdfs/org/apache/hadoop/hdfs/server/datanode/UpgradeManagerDatanode.java:68:   * @return true if distributed upgrade is required or false otherwise
./src/hdfs/org/apache/hadoop/hdfs/server/datanode/UpgradeManagerDatanode.java:69:   * @throws IOException
./src/hdfs/org/apache/hadoop/hdfs/server/datanode/UpgradeObjectDatanode.java:51:   * @throws IOException
./src/hdfs/org/apache/hadoop/hdfs/server/datanode/UpgradeObjectDatanode.java:70:   * @param nsInfo name-node versions, verify that the upgrade
./src/hdfs/org/apache/hadoop/hdfs/server/datanode/UpgradeObjectDatanode.java:73:   * @throws IOException
./src/hdfs/org/apache/hadoop/hdfs/server/datanode/UpgradeObjectDatanode.java:74:   * @return true if data-node itself should start the upgrade or 
./src/hdfs/org/apache/hadoop/hdfs/server/namenode/INode.java:109:   * @param other Other node to be copied
./src/hdfs/org/apache/hadoop/hdfs/server/namenode/INode.java:126:  /** Set the {@link PermissionStatus} */
./src/hdfs/org/apache/hadoop/hdfs/server/namenode/INode.java:132:  /** Get the {@link PermissionStatus} */
./src/hdfs/org/apache/hadoop/hdfs/server/namenode/INode.java:160:  /** Get the {@link FsPermission} */
./src/hdfs/org/apache/hadoop/hdfs/server/namenode/INode.java:168:  /** Set the {@link FsPermission} of this {@link INode} */
./src/hdfs/org/apache/hadoop/hdfs/server/namenode/INode.java:184:  /** Compute {@link ContentSummary}. */
./src/hdfs/org/apache/hadoop/hdfs/server/namenode/INode.java:191:   * @return an array of three longs. 
./src/hdfs/org/apache/hadoop/hdfs/server/namenode/INode.java:198:   * @return the quota if it is set; -1 otherwise
./src/hdfs/org/apache/hadoop/hdfs/server/namenode/INode.java:221:   * @return local file name
./src/hdfs/org/apache/hadoop/hdfs/server/namenode/INode.java:229:   * @return local file name
./src/hdfs/org/apache/hadoop/hdfs/server/namenode/INode.java:249:  /** {@inheritDoc} */
./src/hdfs/org/apache/hadoop/hdfs/server/namenode/INode.java:256:   * @return parent INode
./src/hdfs/org/apache/hadoop/hdfs/server/namenode/INode.java:264:   * @return access time
./src/hdfs/org/apache/hadoop/hdfs/server/namenode/INode.java:290:   * @return access time
./src/hdfs/org/apache/hadoop/hdfs/server/namenode/INode.java:312:   * @param path
./src/hdfs/org/apache/hadoop/hdfs/server/namenode/INode.java:313:   * @return array of byte arrays each of which represents 
./src/hdfs/org/apache/hadoop/hdfs/server/namenode/INode.java:333:   * @param path
./src/hdfs/org/apache/hadoop/hdfs/server/namenode/INode.java:334:   * @return array of names 
./src/hdfs/org/apache/hadoop/hdfs/server/namenode/INode.java:378:   * @return a negative integer, zero, or a positive integer 
./src/hdfs/org/apache/hadoop/hdfs/server/namenode/INode.java:379:   * as defined by {@link #compareTo(byte[])}.
./src/hdfs/org/apache/hadoop/hdfs/server/namenode/BlocksMap.java:113:     *      * @return first free triplet index.
./src/hdfs/org/apache/hadoop/hdfs/server/namenode/BlocksMap.java:181:     * @param dn
./src/hdfs/org/apache/hadoop/hdfs/server/namenode/BlocksMap.java:182:     * @return index or -1 if not found.
./src/hdfs/org/apache/hadoop/hdfs/server/namenode/BlocksMap.java:200:     * @return current block as the new head of the list.
./src/hdfs/org/apache/hadoop/hdfs/server/namenode/BlocksMap.java:219:     * @return the new head of the list or null if the list becomes
./src/hdfs/org/apache/hadoop/hdfs/server/namenode/CorruptReplicasMap.java:42:   * @param blk Block to be added to CorruptReplicasMap
./src/hdfs/org/apache/hadoop/hdfs/server/namenode/CorruptReplicasMap.java:43:   * @param dn DatanodeDescriptor which holds the corrupt replica
./src/hdfs/org/apache/hadoop/hdfs/server/namenode/CorruptReplicasMap.java:73:   * @param blk Block to be removed
./src/hdfs/org/apache/hadoop/hdfs/server/namenode/CorruptReplicasMap.java:87:   * @param blk block to be removed
./src/hdfs/org/apache/hadoop/hdfs/server/namenode/CorruptReplicasMap.java:88:   * @param datanode datanode where the block is located
./src/hdfs/org/apache/hadoop/hdfs/server/namenode/CorruptReplicasMap.java:89:   * @return true if the removal is successful; 
./src/hdfs/org/apache/hadoop/hdfs/server/namenode/CorruptReplicasMap.java:110:   * @param blk Block for which nodes are requested
./src/hdfs/org/apache/hadoop/hdfs/server/namenode/CorruptReplicasMap.java:111:   * @return collection of nodes. Null if does not exists
./src/hdfs/org/apache/hadoop/hdfs/server/namenode/CorruptReplicasMap.java:120:   * @param blk Block to check
./src/hdfs/org/apache/hadoop/hdfs/server/namenode/CorruptReplicasMap.java:121:   * @param node DatanodeDescriptor which holds the replica
./src/hdfs/org/apache/hadoop/hdfs/server/namenode/CorruptReplicasMap.java:122:   * @return true if replica is corrupt, false if does not exists in this map
./src/hdfs/org/apache/hadoop/hdfs/server/namenode/DatanodeDescriptor.java:114:   * @param nodeID id of the data node
./src/hdfs/org/apache/hadoop/hdfs/server/namenode/DatanodeDescriptor.java:122:   * @param nodeID id of the data node
./src/hdfs/org/apache/hadoop/hdfs/server/namenode/DatanodeDescriptor.java:123:   * @param networkLocation location of the data node in network
./src/hdfs/org/apache/hadoop/hdfs/server/namenode/DatanodeDescriptor.java:132:   * @param nodeID id of the data node
./src/hdfs/org/apache/hadoop/hdfs/server/namenode/DatanodeDescriptor.java:133:   * @param networkLocation location of the data node in network
./src/hdfs/org/apache/hadoop/hdfs/server/namenode/DatanodeDescriptor.java:134:   * @param hostName it could be different from host specified for DatanodeID
./src/hdfs/org/apache/hadoop/hdfs/server/namenode/DatanodeDescriptor.java:144:   * @param nodeID id of the data node
./src/hdfs/org/apache/hadoop/hdfs/server/namenode/DatanodeDescriptor.java:145:   * @param capacity capacity of the data node
./src/hdfs/org/apache/hadoop/hdfs/server/namenode/DatanodeDescriptor.java:146:   * @param dfsUsed space used by the data node
./src/hdfs/org/apache/hadoop/hdfs/server/namenode/DatanodeDescriptor.java:147:   * @param remaining remaing capacity of the data node
./src/hdfs/org/apache/hadoop/hdfs/server/namenode/DatanodeDescriptor.java:148:   * @param xceiverCount # of data transfers at the data node
./src/hdfs/org/apache/hadoop/hdfs/server/namenode/DatanodeDescriptor.java:161:   * @param nodeID id of the data node
./src/hdfs/org/apache/hadoop/hdfs/server/namenode/DatanodeDescriptor.java:162:   * @param networkLocation location of the data node in network
./src/hdfs/org/apache/hadoop/hdfs/server/namenode/DatanodeDescriptor.java:163:   * @param capacity capacity of the data node, including space used by non-dfs
./src/hdfs/org/apache/hadoop/hdfs/server/namenode/DatanodeDescriptor.java:164:   * @param dfsUsed the used space by dfs datanode
./src/hdfs/org/apache/hadoop/hdfs/server/namenode/DatanodeDescriptor.java:165:   * @param remaining remaing capacity of the data node
./src/hdfs/org/apache/hadoop/hdfs/server/namenode/DatanodeDescriptor.java:166:   * @param xceiverCount # of data transfers at the data node
./src/hdfs/org/apache/hadoop/hdfs/server/namenode/DatanodeDescriptor.java:432:   * @return Approximate number of blocks currently scheduled to be written 
./src/hdfs/org/apache/hadoop/hdfs/server/namenode/DfsServlet.java:48:  /** Get {@link UserGroupInformation} from request */
./src/hdfs/org/apache/hadoop/hdfs/server/namenode/DfsServlet.java:61:   * Create a {@link NameNode} proxy from the current {@link ServletContext}. 
./src/hdfs/org/apache/hadoop/hdfs/server/namenode/EditLogInputStream.java:28: * into the #{@link EditLogOutputStream}.
./src/hdfs/org/apache/hadoop/hdfs/server/namenode/EditLogInputStream.java:34:   * @return name of the stream
./src/hdfs/org/apache/hadoop/hdfs/server/namenode/EditLogInputStream.java:38:  /** {@inheritDoc} */
./src/hdfs/org/apache/hadoop/hdfs/server/namenode/EditLogInputStream.java:41:  /** {@inheritDoc} */
./src/hdfs/org/apache/hadoop/hdfs/server/namenode/EditLogInputStream.java:44:  /** {@inheritDoc} */
./src/hdfs/org/apache/hadoop/hdfs/server/namenode/EditLogInputStream.java:47:  /** {@inheritDoc} */
./src/hdfs/org/apache/hadoop/hdfs/server/namenode/EditLogOutputStream.java:41:   * @return name of the stream
./src/hdfs/org/apache/hadoop/hdfs/server/namenode/EditLogOutputStream.java:45:  /** {@inheritDoc} */
./src/hdfs/org/apache/hadoop/hdfs/server/namenode/EditLogOutputStream.java:53:   * @param op operation
./src/hdfs/org/apache/hadoop/hdfs/server/namenode/EditLogOutputStream.java:54:   * @param writables array of Writable arguments
./src/hdfs/org/apache/hadoop/hdfs/server/namenode/EditLogOutputStream.java:55:   * @throws IOException
./src/hdfs/org/apache/hadoop/hdfs/server/namenode/EditLogOutputStream.java:62:   * @throws IOException
./src/hdfs/org/apache/hadoop/hdfs/server/namenode/EditLogOutputStream.java:66:  /** {@inheritDoc} */
./src/hdfs/org/apache/hadoop/hdfs/server/namenode/EditLogOutputStream.java:77:   * {@link #setReadyToFlush()} into underlying persistent store.
./src/hdfs/org/apache/hadoop/hdfs/server/namenode/EditLogOutputStream.java:78:   * @throws IOException
./src/hdfs/org/apache/hadoop/hdfs/server/namenode/EditLogOutputStream.java:105:   * Return total time spent in {@link #flushAndSync()}
./src/hdfs/org/apache/hadoop/hdfs/server/namenode/EditLogOutputStream.java:112:   * Return number of calls to {@link #flushAndSync()}
./src/hdfs/org/apache/hadoop/hdfs/server/namenode/FileChecksumServlets.java:52:    /** {@inheritDoc} */
./src/hdfs/org/apache/hadoop/hdfs/server/namenode/FileChecksumServlets.java:76:    /** {@inheritDoc} */
./src/hdfs/org/apache/hadoop/hdfs/server/namenode/FileDataServlet.java:34: * @see org.apache.hadoop.hdfs.HftpFileSystem
./src/hdfs/org/apache/hadoop/hdfs/server/namenode/FileDataServlet.java:75:   * {@code
./src/hdfs/org/apache/hadoop/hdfs/server/namenode/FsckServlet.java:38:  @SuppressWarnings("unchecked")
./src/hdfs/org/apache/hadoop/hdfs/server/namenode/FSDirectory.java:336:   * @see #unprotectedRenameTo(String, String, long)
./src/hdfs/org/apache/hadoop/hdfs/server/namenode/FSDirectory.java:353:   * @param src source path
./src/hdfs/org/apache/hadoop/hdfs/server/namenode/FSDirectory.java:354:   * @param dst destination path
./src/hdfs/org/apache/hadoop/hdfs/server/namenode/FSDirectory.java:355:   * @return true if rename succeeds; false otherwise
./src/hdfs/org/apache/hadoop/hdfs/server/namenode/FSDirectory.java:356:   * @throws QuotaExceededException if the operation violates any quota limit
./src/hdfs/org/apache/hadoop/hdfs/server/namenode/FSDirectory.java:448:   * @param src file name
./src/hdfs/org/apache/hadoop/hdfs/server/namenode/FSDirectory.java:449:   * @param replication new replication
./src/hdfs/org/apache/hadoop/hdfs/server/namenode/FSDirectory.java:450:   * @param oldReplication old replication - output parameter
./src/hdfs/org/apache/hadoop/hdfs/server/namenode/FSDirectory.java:451:   * @return array of file blocks
./src/hdfs/org/apache/hadoop/hdfs/server/namenode/FSDirectory.java:452:   * @throws IOException
./src/hdfs/org/apache/hadoop/hdfs/server/namenode/FSDirectory.java:496:   * @param filename the filename
./src/hdfs/org/apache/hadoop/hdfs/server/namenode/FSDirectory.java:497:   * @return the number of bytes 
./src/hdfs/org/apache/hadoop/hdfs/server/namenode/FSDirectory.java:498:   * @throws IOException if it is a directory or does not exist.
./src/hdfs/org/apache/hadoop/hdfs/server/namenode/FSDirectory.java:595:   * @param src a string representation of a path to an inode
./src/hdfs/org/apache/hadoop/hdfs/server/namenode/FSDirectory.java:596:   * @param modificationTime the time the inode is removed
./src/hdfs/org/apache/hadoop/hdfs/server/namenode/FSDirectory.java:597:   * @param deletedBlocks the place holder for the blocks to be removed
./src/hdfs/org/apache/hadoop/hdfs/server/namenode/FSDirectory.java:598:   * @return if the deletion succeeds
./src/hdfs/org/apache/hadoop/hdfs/server/namenode/FSDirectory.java:650:   * @see #replaceNode(String, INodeFile, INodeFile)
./src/hdfs/org/apache/hadoop/hdfs/server/namenode/FSDirectory.java:724:   * @param src The string representation of the path to the file
./src/hdfs/org/apache/hadoop/hdfs/server/namenode/FSDirectory.java:725:   * @return object containing information regarding the file
./src/hdfs/org/apache/hadoop/hdfs/server/namenode/FSDirectory.java:757:   * Get {@link INode} associated with the file.
./src/hdfs/org/apache/hadoop/hdfs/server/namenode/FSDirectory.java:771:   * @param path the path to explore
./src/hdfs/org/apache/hadoop/hdfs/server/namenode/FSDirectory.java:772:   * @return INodes array containing the existing INodes in the order they
./src/hdfs/org/apache/hadoop/hdfs/server/namenode/FSDirectory.java:778:   * @see INodeDirectory#getExistingPathINodes(byte[][], INode[])
./src/hdfs/org/apache/hadoop/hdfs/server/namenode/FSDirectory.java:815:   * @param path path for the file.
./src/hdfs/org/apache/hadoop/hdfs/server/namenode/FSDirectory.java:816:   * @param nsDelta the delta change of namespace
./src/hdfs/org/apache/hadoop/hdfs/server/namenode/FSDirectory.java:817:   * @param dsDelta the delta change of diskspace
./src/hdfs/org/apache/hadoop/hdfs/server/namenode/FSDirectory.java:818:   * @throws QuotaExceededException if the new count violates any quota limit
./src/hdfs/org/apache/hadoop/hdfs/server/namenode/FSDirectory.java:819:   * @throws FileNotFound if path does not exist.
./src/hdfs/org/apache/hadoop/hdfs/server/namenode/FSDirectory.java:837:   * @param inodes an array of inodes on a path
./src/hdfs/org/apache/hadoop/hdfs/server/namenode/FSDirectory.java:838:   * @param numOfINodes the number of inodes to update starting from index 0
./src/hdfs/org/apache/hadoop/hdfs/server/namenode/FSDirectory.java:839:   * @param nsDelta the delta change of namespace
./src/hdfs/org/apache/hadoop/hdfs/server/namenode/FSDirectory.java:840:   * @param dsDelta the delta change of diskspace
./src/hdfs/org/apache/hadoop/hdfs/server/namenode/FSDirectory.java:841:   * @throws QuotaExceededException if the new count violates any quota limit
./src/hdfs/org/apache/hadoop/hdfs/server/namenode/FSDirectory.java:891:   * @param src string representation of the path to the directory
./src/hdfs/org/apache/hadoop/hdfs/server/namenode/FSDirectory.java:892:   * @param permissions the permission of the directory
./src/hdfs/org/apache/hadoop/hdfs/server/namenode/FSDirectory.java:893:   * @param inheritPermission if the permission of the directory should inherit
./src/hdfs/org/apache/hadoop/hdfs/server/namenode/FSDirectory.java:896:   * @param now creation time
./src/hdfs/org/apache/hadoop/hdfs/server/namenode/FSDirectory.java:897:   * @return true if the operation succeeds false otherwise
./src/hdfs/org/apache/hadoop/hdfs/server/namenode/FSDirectory.java:898:   * @throws FileNotFoundException if an ancestor or itself is a file
./src/hdfs/org/apache/hadoop/hdfs/server/namenode/FSDirectory.java:899:   * @throws QuotaExceededException if directory creation violates 
./src/hdfs/org/apache/hadoop/hdfs/server/namenode/FSDirectory.java:1074:   * @param dir the root of the tree that represents the directory
./src/hdfs/org/apache/hadoop/hdfs/server/namenode/FSDirectory.java:1075:   * @param counters counters for name space and disk space
./src/hdfs/org/apache/hadoop/hdfs/server/namenode/FSDirectory.java:1076:   * @param nodesInPath INodes for the each of components in the path.
./src/hdfs/org/apache/hadoop/hdfs/server/namenode/FSDirectory.java:1077:   * @return the size of the tree
./src/hdfs/org/apache/hadoop/hdfs/server/namenode/FSDirectory.java:1133:   * See {@link ClientProtocol#setQuota(String, long, long)} for the contract.
./src/hdfs/org/apache/hadoop/hdfs/server/namenode/FSDirectory.java:1135:   * @returns INodeDirectory if any of the quotas have changed. null other wise.
./src/hdfs/org/apache/hadoop/hdfs/server/namenode/FSDirectory.java:1136:   * @throws FileNotFoundException if the path does not exist or is a file
./src/hdfs/org/apache/hadoop/hdfs/server/namenode/FSDirectory.java:1137:   * @throws QuotaExceededException if the directory tree size is 
./src/hdfs/org/apache/hadoop/hdfs/server/namenode/FSDirectory.java:1187:   * See {@link ClientProtocol#setQuota(String, long, long)} for the 
./src/hdfs/org/apache/hadoop/hdfs/server/namenode/FSDirectory.java:1189:   * @see #unprotectedSetQuota(String, long, long)
./src/hdfs/org/apache/hadoop/hdfs/server/namenode/FSEditLog.java:59:  @Deprecated private static final byte OP_DATANODE_ADD = 5;
./src/hdfs/org/apache/hadoop/hdfs/server/namenode/FSEditLog.java:60:  @Deprecated private static final byte OP_DATANODE_REMOVE = 6;
./src/hdfs/org/apache/hadoop/hdfs/server/namenode/FSEditLog.java:109:   * An implementation of the abstract class {@link EditLogOutputStream},
./src/hdfs/org/apache/hadoop/hdfs/server/namenode/FSEditLog.java:131:    @Override
./src/hdfs/org/apache/hadoop/hdfs/server/namenode/FSEditLog.java:136:    /** {@inheritDoc} */
./src/hdfs/org/apache/hadoop/hdfs/server/namenode/FSEditLog.java:137:    @Override
./src/hdfs/org/apache/hadoop/hdfs/server/namenode/FSEditLog.java:142:    /** {@inheritDoc} */
./src/hdfs/org/apache/hadoop/hdfs/server/namenode/FSEditLog.java:143:    @Override
./src/hdfs/org/apache/hadoop/hdfs/server/namenode/FSEditLog.java:154:    @Override
./src/hdfs/org/apache/hadoop/hdfs/server/namenode/FSEditLog.java:163:    @Override
./src/hdfs/org/apache/hadoop/hdfs/server/namenode/FSEditLog.java:187:    @Override
./src/hdfs/org/apache/hadoop/hdfs/server/namenode/FSEditLog.java:201:    @Override
./src/hdfs/org/apache/hadoop/hdfs/server/namenode/FSEditLog.java:213:    @Override
./src/hdfs/org/apache/hadoop/hdfs/server/namenode/FSEditLog.java:222:    @Override
./src/hdfs/org/apache/hadoop/hdfs/server/namenode/FSEditLog.java:259:    @Override
./src/hdfs/org/apache/hadoop/hdfs/server/namenode/FSEditLog.java:264:    @Override
./src/hdfs/org/apache/hadoop/hdfs/server/namenode/FSEditLog.java:269:    @Override
./src/hdfs/org/apache/hadoop/hdfs/server/namenode/FSEditLog.java:274:    @Override
./src/hdfs/org/apache/hadoop/hdfs/server/namenode/FSEditLog.java:279:    @Override
./src/hdfs/org/apache/hadoop/hdfs/server/namenode/FSEditLog.java:284:    @Override
./src/hdfs/org/apache/hadoop/hdfs/server/namenode/FSEditLog.java:326:   * @throws IOException
./src/hdfs/org/apache/hadoop/hdfs/server/namenode/FSEditLog.java:1041:   * @param src the string representation of the path to a directory
./src/hdfs/org/apache/hadoop/hdfs/server/namenode/FSEditLog.java:1042:   * @param quota the directory size limit
./src/hdfs/org/apache/hadoop/hdfs/server/namenode/FSImage.java:235:   * @param dataDirs
./src/hdfs/org/apache/hadoop/hdfs/server/namenode/FSImage.java:236:   * @param startOpt startup option
./src/hdfs/org/apache/hadoop/hdfs/server/namenode/FSImage.java:237:   * @throws IOException
./src/hdfs/org/apache/hadoop/hdfs/server/namenode/FSImage.java:238:   * @return true if the image needs to be saved or false otherwise
./src/hdfs/org/apache/hadoop/hdfs/server/namenode/FSImage.java:493:   * @throws IOException
./src/hdfs/org/apache/hadoop/hdfs/server/namenode/FSImage.java:564:   * @param sd storage directory
./src/hdfs/org/apache/hadoop/hdfs/server/namenode/FSImage.java:565:   * @throws IOException
./src/hdfs/org/apache/hadoop/hdfs/server/namenode/FSImage.java:583:   * @param sd
./src/hdfs/org/apache/hadoop/hdfs/server/namenode/FSImage.java:584:   * @throws IOException
./src/hdfs/org/apache/hadoop/hdfs/server/namenode/FSImage.java:714:   * @return whether the image should be saved
./src/hdfs/org/apache/hadoop/hdfs/server/namenode/FSImage.java:715:   * @throws IOException
./src/hdfs/org/apache/hadoop/hdfs/server/namenode/FSImage.java:965:   * @param sd storage directory
./src/hdfs/org/apache/hadoop/hdfs/server/namenode/FSImage.java:966:   * @return number of edits loaded
./src/hdfs/org/apache/hadoop/hdfs/server/namenode/FSImage.java:967:   * @throws IOException
./src/hdfs/org/apache/hadoop/hdfs/server/namenode/FSImage.java:1053:   * @return new namespaceID
./src/hdfs/org/apache/hadoop/hdfs/server/namenode/FSNamesystem.java:151:   * Done by storing a set of {@link DatanodeDescriptor} objects, sorted by 
./src/hdfs/org/apache/hadoop/hdfs/server/namenode/FSNamesystem.java:162:   * The list of the {@link DatanodeDescriptor}s in the map is checkpointed
./src/hdfs/org/apache/hadoop/hdfs/server/namenode/FSNamesystem.java:163:   * in the namespace image file. Only the {@link DatanodeInfo} part is 
./src/hdfs/org/apache/hadoop/hdfs/server/namenode/FSNamesystem.java:202:   * This is a subset of {@link #datanodeMap}, containing nodes that are 
./src/hdfs/org/apache/hadoop/hdfs/server/namenode/FSNamesystem.java:204:   * The {@link HeartbeatMonitor} periodically checks for outdated entries,
./src/hdfs/org/apache/hadoop/hdfs/server/namenode/FSNamesystem.java:622:   * @param datanode on which blocks are located
./src/hdfs/org/apache/hadoop/hdfs/server/namenode/FSNamesystem.java:623:   * @param size total size of blocks
./src/hdfs/org/apache/hadoop/hdfs/server/namenode/FSNamesystem.java:696:   * @throws IOException
./src/hdfs/org/apache/hadoop/hdfs/server/namenode/FSNamesystem.java:713:   * @throws IOException
./src/hdfs/org/apache/hadoop/hdfs/server/namenode/FSNamesystem.java:740:   * @see #getBlockLocations(String, long, long)
./src/hdfs/org/apache/hadoop/hdfs/server/namenode/FSNamesystem.java:762:   * @see ClientProtocol#getBlockLocations(String, long, long)
./src/hdfs/org/apache/hadoop/hdfs/server/namenode/FSNamesystem.java:771:   * @see ClientProtocol#getBlockLocations(String, long, long)
./src/hdfs/org/apache/hadoop/hdfs/server/namenode/FSNamesystem.java:902:   * @see ClientProtocol#setReplication(String, short)
./src/hdfs/org/apache/hadoop/hdfs/server/namenode/FSNamesystem.java:903:   * @param src file name
./src/hdfs/org/apache/hadoop/hdfs/server/namenode/FSNamesystem.java:904:   * @param replication new replication
./src/hdfs/org/apache/hadoop/hdfs/server/namenode/FSNamesystem.java:905:   * @return true if successful; 
./src/hdfs/org/apache/hadoop/hdfs/server/namenode/FSNamesystem.java:986:   * @see ClientProtocol#create(String, FsPermission, String, boolean, short, long)
./src/hdfs/org/apache/hadoop/hdfs/server/namenode/FSNamesystem.java:988:   * @throws IOException if file name is invalid
./src/hdfs/org/apache/hadoop/hdfs/server/namenode/FSNamesystem.java:989:   *         {@link FSDirectory#isValidToCreate(String)}.
./src/hdfs/org/apache/hadoop/hdfs/server/namenode/FSNamesystem.java:1431:   * @param src path to the file
./src/hdfs/org/apache/hadoop/hdfs/server/namenode/FSNamesystem.java:1432:   * @param indoes INode representing each of the components of src. 
./src/hdfs/org/apache/hadoop/hdfs/server/namenode/FSNamesystem.java:1478:   * @param n datanode
./src/hdfs/org/apache/hadoop/hdfs/server/namenode/FSNamesystem.java:1487:   * @param b block
./src/hdfs/org/apache/hadoop/hdfs/server/namenode/FSNamesystem.java:1488:   * @param n datanode
./src/hdfs/org/apache/hadoop/hdfs/server/namenode/FSNamesystem.java:1499:   * @param b block
./src/hdfs/org/apache/hadoop/hdfs/server/namenode/FSNamesystem.java:1500:   * @param n datanode
./src/hdfs/org/apache/hadoop/hdfs/server/namenode/FSNamesystem.java:1542:   * @param blk Block to be marked as corrupt
./src/hdfs/org/apache/hadoop/hdfs/server/namenode/FSNamesystem.java:1543:   * @param dn Datanode which holds the corrupt replica
./src/hdfs/org/apache/hadoop/hdfs/server/namenode/FSNamesystem.java:1716:   * @param src The string representation of the path to the file
./src/hdfs/org/apache/hadoop/hdfs/server/namenode/FSNamesystem.java:1717:   * @throws IOException if permission to access file is denied by the system 
./src/hdfs/org/apache/hadoop/hdfs/server/namenode/FSNamesystem.java:1718:   * @return object containing information regarding the file
./src/hdfs/org/apache/hadoop/hdfs/server/namenode/FSNamesystem.java:1787:   * See {@link ClientProtocol#setQuota(String, long, long)} for the 
./src/hdfs/org/apache/hadoop/hdfs/server/namenode/FSNamesystem.java:1800:   * @param src The string representation of the path
./src/hdfs/org/apache/hadoop/hdfs/server/namenode/FSNamesystem.java:1801:   * @param clientName The string representation of the client
./src/hdfs/org/apache/hadoop/hdfs/server/namenode/FSNamesystem.java:1802:   * @throws IOException if path does not exist
./src/hdfs/org/apache/hadoop/hdfs/server/namenode/FSNamesystem.java:1819:   * @param src The filename
./src/hdfs/org/apache/hadoop/hdfs/server/namenode/FSNamesystem.java:1820:   * @param holder The datanode that was creating the file
./src/hdfs/org/apache/hadoop/hdfs/server/namenode/FSNamesystem.java:2009:   * @see org.apache.hadoop.hdfs.server.datanode.DataNode#register()
./src/hdfs/org/apache/hadoop/hdfs/server/namenode/FSNamesystem.java:2153:   * @see #registerDatanode(DatanodeRegistration)
./src/hdfs/org/apache/hadoop/hdfs/server/namenode/FSNamesystem.java:2154:   * @see FSImage#newNamespaceID()
./src/hdfs/org/apache/hadoop/hdfs/server/namenode/FSNamesystem.java:2155:   * @return registration ID
./src/hdfs/org/apache/hadoop/hdfs/server/namenode/FSNamesystem.java:2164:   * @return unique storage ID
./src/hdfs/org/apache/hadoop/hdfs/server/namenode/FSNamesystem.java:2196:   * @return a datanode command 
./src/hdfs/org/apache/hadoop/hdfs/server/namenode/FSNamesystem.java:2197:   * @throws IOException
./src/hdfs/org/apache/hadoop/hdfs/server/namenode/FSNamesystem.java:2323:   * @return number of blocks scheduled for replication or removal.
./src/hdfs/org/apache/hadoop/hdfs/server/namenode/FSNamesystem.java:2363:   * Scan blocks in {@link #neededReplications} and assign replication
./src/hdfs/org/apache/hadoop/hdfs/server/namenode/FSNamesystem.java:2369:   * @return number of blocks scheduled for replication during this iteration.
./src/hdfs/org/apache/hadoop/hdfs/server/namenode/FSNamesystem.java:2554:   * in {@link #recentInvalidateSets}.
./src/hdfs/org/apache/hadoop/hdfs/server/namenode/FSNamesystem.java:2556:   * @return number of blocks scheduled for removal during this iteration.
./src/hdfs/org/apache/hadoop/hdfs/server/namenode/FSNamesystem.java:2631:   * @param nodeID datanode ID
./src/hdfs/org/apache/hadoop/hdfs/server/namenode/FSNamesystem.java:2646:   * @param nodeInfo datanode descriptor
./src/hdfs/org/apache/hadoop/hdfs/server/namenode/FSNamesystem.java:2689:   * @param nodeID node
./src/hdfs/org/apache/hadoop/hdfs/server/namenode/FSNamesystem.java:2810:   * @return the block that is stored in blockMap.
./src/hdfs/org/apache/hadoop/hdfs/server/namenode/FSNamesystem.java:2977:   * add them to {@link #recentInvalidateSets} so that they could be further
./src/hdfs/org/apache/hadoop/hdfs/server/namenode/FSNamesystem.java:2984:   * @param blk Block whose corrupt replicas need to be invalidated
./src/hdfs/org/apache/hadoop/hdfs/server/namenode/FSNamesystem.java:3790:   * @param nodeID
./src/hdfs/org/apache/hadoop/hdfs/server/namenode/FSNamesystem.java:3791:   * @return DatanodeDescriptor or null if the node is not found.
./src/hdfs/org/apache/hadoop/hdfs/server/namenode/FSNamesystem.java:3792:   * @throws IOException
./src/hdfs/org/apache/hadoop/hdfs/server/namenode/FSNamesystem.java:3809:  @Deprecated
./src/hdfs/org/apache/hadoop/hdfs/server/namenode/FSNamesystem.java:3821:  @Deprecated
./src/hdfs/org/apache/hadoop/hdfs/server/namenode/FSNamesystem.java:3850:   * An instance of {@link SafeModeInfo} is created when the name node
./src/hdfs/org/apache/hadoop/hdfs/server/namenode/FSNamesystem.java:3853:   * During name node startup {@link SafeModeInfo} counts the number of
./src/hdfs/org/apache/hadoop/hdfs/server/namenode/FSNamesystem.java:3857:   * {@link FSNamesystem#blocksMap}. When the ratio reaches the
./src/hdfs/org/apache/hadoop/hdfs/server/namenode/FSNamesystem.java:3858:   * {@link #threshold} it starts the {@link SafeModeMonitor} daemon in order
./src/hdfs/org/apache/hadoop/hdfs/server/namenode/FSNamesystem.java:3859:   * to monitor whether the safe mode {@link #extension} is passed.
./src/hdfs/org/apache/hadoop/hdfs/server/namenode/FSNamesystem.java:3866:   * @see ClientProtocol#setSafeMode(FSConstants.SafeModeAction)
./src/hdfs/org/apache/hadoop/hdfs/server/namenode/FSNamesystem.java:3867:   * @see SafeModeMonitor
./src/hdfs/org/apache/hadoop/hdfs/server/namenode/FSNamesystem.java:3896:     * @param conf configuration
./src/hdfs/org/apache/hadoop/hdfs/server/namenode/FSNamesystem.java:3909:     * The {@link #threshold} is set to 1.5 so that it could never be reached.
./src/hdfs/org/apache/hadoop/hdfs/server/namenode/FSNamesystem.java:3910:     * {@link #blockTotal} is set to -1 to indicate that safe mode is manual.
./src/hdfs/org/apache/hadoop/hdfs/server/namenode/FSNamesystem.java:3912:     * @see SafeModeInfo
./src/hdfs/org/apache/hadoop/hdfs/server/namenode/FSNamesystem.java:3927:     * @return true if in safe mode
./src/hdfs/org/apache/hadoop/hdfs/server/namenode/FSNamesystem.java:3991:     * @return true if can leave or false otherwise.
./src/hdfs/org/apache/hadoop/hdfs/server/namenode/FSNamesystem.java:4005:     * if DFS is empty or {@link #threshold} == 0
./src/hdfs/org/apache/hadoop/hdfs/server/namenode/FSNamesystem.java:4056:     * @param replication current replication 
./src/hdfs/org/apache/hadoop/hdfs/server/namenode/FSNamesystem.java:4067:     * @param replication current replication 
./src/hdfs/org/apache/hadoop/hdfs/server/namenode/FSNamesystem.java:4155:    /** interval in msec for checking safe mode: {@value} */
./src/hdfs/org/apache/hadoop/hdfs/server/namenode/FSNamesystem.java:4176:   * @return current time in msec.
./src/hdfs/org/apache/hadoop/hdfs/server/namenode/FSNamesystem.java:4199:   * @return true if safe mode is ON, false otherwise
./src/hdfs/org/apache/hadoop/hdfs/server/namenode/FSNamesystem.java:4209:   * @param replication current replication 
./src/hdfs/org/apache/hadoop/hdfs/server/namenode/FSNamesystem.java:4244:   * @throws IOException
./src/hdfs/org/apache/hadoop/hdfs/server/namenode/FSNamesystem.java:4256:   * @throws IOException
./src/hdfs/org/apache/hadoop/hdfs/server/namenode/FSNamesystem.java:4377:   * {@link PermissionChecker#checkPermission(String, INodeDirectory, boolean, FsAction, FsAction, FsAction, FsAction)}.
./src/hdfs/org/apache/hadoop/hdfs/server/namenode/FSNamesystem.java:4469:   * @return Number of live data nodes
./src/hdfs/org/apache/hadoop/hdfs/server/namenode/FSNamesystem.java:4488:   * @return Number of dead data nodes
./src/hdfs/org/apache/hadoop/hdfs/server/namenode/GetImageServlet.java:38:  @SuppressWarnings("unchecked")
./src/hdfs/org/apache/hadoop/hdfs/server/namenode/Host2NodesMap.java:128:   * @return DatanodeDescriptor if found; otherwise null.
./src/hdfs/org/apache/hadoop/hdfs/server/namenode/Host2NodesMap.java:156:   * @return DatanodeDescriptor if found or null otherwise 
./src/hdfs/org/apache/hadoop/hdfs/server/namenode/INodeDirectory.java:57:   * @param other
./src/hdfs/org/apache/hadoop/hdfs/server/namenode/INodeDirectory.java:83:   * @param newChild Child node to be added
./src/hdfs/org/apache/hadoop/hdfs/server/namenode/INodeDirectory.java:157:   * @param components array of path component name
./src/hdfs/org/apache/hadoop/hdfs/server/namenode/INodeDirectory.java:158:   * @param existing INode array to fill with existing INodes
./src/hdfs/org/apache/hadoop/hdfs/server/namenode/INodeDirectory.java:159:   * @return number of existing INodes in the path
./src/hdfs/org/apache/hadoop/hdfs/server/namenode/INodeDirectory.java:187:   * @param path the path to explore
./src/hdfs/org/apache/hadoop/hdfs/server/namenode/INodeDirectory.java:188:   * @return INodes array containing the existing INodes in the order they
./src/hdfs/org/apache/hadoop/hdfs/server/namenode/INodeDirectory.java:194:   * @see #getExistingPathINodes(byte[][], INode[])
./src/hdfs/org/apache/hadoop/hdfs/server/namenode/INodeDirectory.java:208:   * @param node INode to insert
./src/hdfs/org/apache/hadoop/hdfs/server/namenode/INodeDirectory.java:209:   * @param inheritPermission inherit permission from parent?
./src/hdfs/org/apache/hadoop/hdfs/server/namenode/INodeDirectory.java:210:   * @return  null if the child with this name already exists; 
./src/hdfs/org/apache/hadoop/hdfs/server/namenode/INodeDirectory.java:242:   * @see #addNode(String, INode, boolean)
./src/hdfs/org/apache/hadoop/hdfs/server/namenode/INodeDirectory.java:251:   * @param path file path
./src/hdfs/org/apache/hadoop/hdfs/server/namenode/INodeDirectory.java:252:   * @param newNode INode to be added
./src/hdfs/org/apache/hadoop/hdfs/server/namenode/INodeDirectory.java:253:   * @param inheritPermission If true, copy the parent's permission to newNode.
./src/hdfs/org/apache/hadoop/hdfs/server/namenode/INodeDirectory.java:254:   * @return null if the node already exists; inserted INode, otherwise
./src/hdfs/org/apache/hadoop/hdfs/server/namenode/INodeDirectory.java:255:   * @throws FileNotFoundException if parent does not exist or 
./src/hdfs/org/apache/hadoop/hdfs/server/namenode/INodeDirectory.java:269:   * @return  parent INode if new inode is inserted
./src/hdfs/org/apache/hadoop/hdfs/server/namenode/INodeDirectory.java:271:   * @throws  FileNotFoundException if parent does not exist or 
./src/hdfs/org/apache/hadoop/hdfs/server/namenode/INodeDirectory.java:305:  /** {@inheritDoc} */
./src/hdfs/org/apache/hadoop/hdfs/server/namenode/INodeDirectory.java:316:  /** {@inheritDoc} */
./src/hdfs/org/apache/hadoop/hdfs/server/namenode/INodeDirectoryWithQuota.java:34:   * @param nsQuota Namespace quota to be assigned to this inode
./src/hdfs/org/apache/hadoop/hdfs/server/namenode/INodeDirectoryWithQuota.java:35:   * @param dsQuota Diskspace quota to be assigned to this indoe
./src/hdfs/org/apache/hadoop/hdfs/server/namenode/INodeDirectoryWithQuota.java:36:   * @param other The other inode from which all other properties are copied
./src/hdfs/org/apache/hadoop/hdfs/server/namenode/INodeDirectoryWithQuota.java:68:   * @return this directory's namespace quota
./src/hdfs/org/apache/hadoop/hdfs/server/namenode/INodeDirectoryWithQuota.java:75:   * @return this directory's diskspace quota
./src/hdfs/org/apache/hadoop/hdfs/server/namenode/INodeDirectoryWithQuota.java:83:   * @param nsQuota Namespace quota to be set
./src/hdfs/org/apache/hadoop/hdfs/server/namenode/INodeDirectoryWithQuota.java:84:   * @param dsQuota diskspace quota to be set
./src/hdfs/org/apache/hadoop/hdfs/server/namenode/INodeDirectoryWithQuota.java:85:   * @throws QuotaExceededException if quota is modified and the modified quota
./src/hdfs/org/apache/hadoop/hdfs/server/namenode/INodeDirectoryWithQuota.java:102:  @Override
./src/hdfs/org/apache/hadoop/hdfs/server/namenode/INodeDirectoryWithQuota.java:110:   * @return the size of the subtree rooted at this directory
./src/hdfs/org/apache/hadoop/hdfs/server/namenode/INodeDirectoryWithQuota.java:122:   * @param nsDelta the change of the tree size
./src/hdfs/org/apache/hadoop/hdfs/server/namenode/INodeDirectoryWithQuota.java:123:   * @param dsDelta change to disk space occupied
./src/hdfs/org/apache/hadoop/hdfs/server/namenode/INodeDirectoryWithQuota.java:124:   * @throws QuotaExceededException if the changed size is greater 
./src/hdfs/org/apache/hadoop/hdfs/server/namenode/INodeDirectoryWithQuota.java:143:   * @param namespace size of the directory to be set
./src/hdfs/org/apache/hadoop/hdfs/server/namenode/INodeDirectoryWithQuota.java:144:   * @param diskspace disk space take by all the nodes under this directory
./src/hdfs/org/apache/hadoop/hdfs/server/namenode/INodeDirectoryWithQuota.java:152:   * @throws QuotaExceededException if the given quota is less than the count
./src/hdfs/org/apache/hadoop/hdfs/server/namenode/INodeFile.java:59:   * Set the {@link FsPermission} of this {@link INodeFile}.
./src/hdfs/org/apache/hadoop/hdfs/server/namenode/INodeFile.java:61:   * the {@link FsAction#EXECUTE} action, if any, is ignored.
./src/hdfs/org/apache/hadoop/hdfs/server/namenode/INodeFile.java:73:   * @return block replication
./src/hdfs/org/apache/hadoop/hdfs/server/namenode/INodeFile.java:85:   * @return file blocks
./src/hdfs/org/apache/hadoop/hdfs/server/namenode/INodeFile.java:125:  /** {@inheritDoc} */
./src/hdfs/org/apache/hadoop/hdfs/server/namenode/INodeFile.java:139:  @Override
./src/hdfs/org/apache/hadoop/hdfs/server/namenode/INodeFile.java:169:   * @return the number of bytes
./src/hdfs/org/apache/hadoop/hdfs/server/namenode/INodeFileUnderConstruction.java:86:  @Override
./src/hdfs/org/apache/hadoop/hdfs/server/namenode/INodeFileUnderConstruction.java:176:   * @return true if lastRecoveryTimeis updated. 
./src/hdfs/org/apache/hadoop/hdfs/server/namenode/LeaseManager.java:88:  /** @return the lease containing src */
./src/hdfs/org/apache/hadoop/hdfs/server/namenode/LeaseManager.java:91:  /** @return the number of leases currently in the system */
./src/hdfs/org/apache/hadoop/hdfs/server/namenode/LeaseManager.java:94:  /** @return the number of paths contained in all leases */
./src/hdfs/org/apache/hadoop/hdfs/server/namenode/LeaseManager.java:200:    /** @return true if the Hard Limit Timer has expired */
./src/hdfs/org/apache/hadoop/hdfs/server/namenode/LeaseManager.java:205:    /** @return true if the Soft Limit Timer has expired */
./src/hdfs/org/apache/hadoop/hdfs/server/namenode/LeaseManager.java:211:     * @return the path associated with the pendingFile and null if not found.
./src/hdfs/org/apache/hadoop/hdfs/server/namenode/LeaseManager.java:230:    /** {@inheritDoc} */
./src/hdfs/org/apache/hadoop/hdfs/server/namenode/LeaseManager.java:236:    /** {@inheritDoc} */
./src/hdfs/org/apache/hadoop/hdfs/server/namenode/LeaseManager.java:251:    /** {@inheritDoc} */
./src/hdfs/org/apache/hadoop/hdfs/server/namenode/LeaseManager.java:264:    /** {@inheritDoc} */
./src/hdfs/org/apache/hadoop/hdfs/server/namenode/LeaseManager.java:378:  /** {@inheritDoc} */
./src/hdfs/org/apache/hadoop/hdfs/server/namenode/ListPathsServlet.java:44: * @see org.apache.hadoop.hdfs.HftpFileSystem
./src/hdfs/org/apache/hadoop/hdfs/server/namenode/ListPathsServlet.java:104:   * {@code
./src/hdfs/org/apache/hadoop/hdfs/server/namenode/ListPathsServlet.java:114:   * {@code
./src/hdfs/org/apache/hadoop/hdfs/server/namenode/metrics/FSNamesystemMetrics.java:36: *  <p> {@link #filesTotal}.set()
./src/hdfs/org/apache/hadoop/hdfs/server/namenode/metrics/NameNodeMetrics.java:40: *  <p> {@link #syncs}.inc()
./src/hdfs/org/apache/hadoop/hdfs/server/namenode/metrics/NameNodeStatistics.java:36:   * @param nameNodeMetrics - the metrics from which the mbean gets its info
./src/hdfs/org/apache/hadoop/hdfs/server/namenode/metrics/NameNodeStatistics.java:53:   * @inheritDoc
./src/hdfs/org/apache/hadoop/hdfs/server/namenode/metrics/NameNodeStatistics.java:60:   * @inheritDoc
./src/hdfs/org/apache/hadoop/hdfs/server/namenode/metrics/NameNodeStatistics.java:67:   * @inheritDoc
./src/hdfs/org/apache/hadoop/hdfs/server/namenode/metrics/NameNodeStatistics.java:74:   * @inheritDoc
./src/hdfs/org/apache/hadoop/hdfs/server/namenode/metrics/NameNodeStatistics.java:81:   * @inheritDoc
./src/hdfs/org/apache/hadoop/hdfs/server/namenode/metrics/NameNodeStatistics.java:88:   * @inheritDoc
./src/hdfs/org/apache/hadoop/hdfs/server/namenode/metrics/NameNodeStatistics.java:95:   * @inheritDoc
./src/hdfs/org/apache/hadoop/hdfs/server/namenode/metrics/NameNodeStatistics.java:102:   * @inheritDoc
./src/hdfs/org/apache/hadoop/hdfs/server/namenode/metrics/NameNodeStatistics.java:109:   * @inheritDoc
./src/hdfs/org/apache/hadoop/hdfs/server/namenode/metrics/NameNodeStatistics.java:116:   * @inheritDoc
./src/hdfs/org/apache/hadoop/hdfs/server/namenode/metrics/NameNodeStatistics.java:124:   * @inheritDoc
./src/hdfs/org/apache/hadoop/hdfs/server/namenode/metrics/NameNodeStatistics.java:131:   * @inheritDoc
./src/hdfs/org/apache/hadoop/hdfs/server/namenode/metrics/NameNodeStatistics.java:139:   * @inheritDoc
./src/hdfs/org/apache/hadoop/hdfs/server/namenode/metrics/NameNodeStatistics.java:146:   * @inheritDoc
./src/hdfs/org/apache/hadoop/hdfs/server/namenode/metrics/NameNodeStatistics.java:153:   * @inheritDoc
./src/hdfs/org/apache/hadoop/hdfs/server/namenode/metrics/NameNodeStatistics.java:160:   * @inheritDoc
./src/hdfs/org/apache/hadoop/hdfs/server/namenode/metrics/NameNodeStatistics.java:167:   *@deprecated call getNumGetListingOps() instead
./src/hdfs/org/apache/hadoop/hdfs/server/namenode/metrics/NameNodeStatistics.java:169:  @Deprecated
./src/hdfs/org/apache/hadoop/hdfs/server/namenode/metrics/NameNodeStatistics.java:175:   * @inheritDoc
./src/hdfs/org/apache/hadoop/hdfs/server/namenode/metrics/NameNodeStatistics.java:182:   * @inheritDoc
./src/hdfs/org/apache/hadoop/hdfs/server/namenode/metrics/NameNodeStatistics.java:189:   * @inheritDoc
./src/hdfs/org/apache/hadoop/hdfs/server/namenode/metrics/NameNodeStatistics.java:196:   * @inheritDoc
./src/hdfs/org/apache/hadoop/hdfs/server/namenode/metrics/NameNodeStatistics.java:202:  /** @inheritDoc */
./src/hdfs/org/apache/hadoop/hdfs/server/namenode/metrics/NameNodeStatistics.java:208:   * @inheritDoc
./src/hdfs/org/apache/hadoop/hdfs/server/namenode/metrics/NameNodeStatistics.java:215:   * @inheritDoc
./src/hdfs/org/apache/hadoop/hdfs/server/namenode/NameNode.java:146:   * @param address hostname:port to bind to
./src/hdfs/org/apache/hadoop/hdfs/server/namenode/NameNode.java:147:   * @param conf the configuration
./src/hdfs/org/apache/hadoop/hdfs/server/namenode/NameNode.java:176:   * <li>{@link org.apache.hadoop.hdfs.server.common.HdfsConstants.StartupOption#REGULAR REGULAR} - normal startup</li>
./src/hdfs/org/apache/hadoop/hdfs/server/namenode/NameNode.java:177:   * <li>{@link org.apache.hadoop.hdfs.server.common.HdfsConstants.StartupOption#FORMAT FORMAT} - format name node</li>
./src/hdfs/org/apache/hadoop/hdfs/server/namenode/NameNode.java:178:   * <li>{@link org.apache.hadoop.hdfs.server.common.HdfsConstants.StartupOption#UPGRADE UPGRADE} - start the cluster  
./src/hdfs/org/apache/hadoop/hdfs/server/namenode/NameNode.java:180:   * <li>{@link org.apache.hadoop.hdfs.server.common.HdfsConstants.StartupOption#ROLLBACK ROLLBACK} - roll the  
./src/hdfs/org/apache/hadoop/hdfs/server/namenode/NameNode.java:190:   * @param conf  confirguration
./src/hdfs/org/apache/hadoop/hdfs/server/namenode/NameNode.java:191:   * @throws IOException
./src/hdfs/org/apache/hadoop/hdfs/server/namenode/NameNode.java:251:   * @param datanode on which blocks are located
./src/hdfs/org/apache/hadoop/hdfs/server/namenode/NameNode.java:252:   * @param size total size of blocks
./src/hdfs/org/apache/hadoop/hdfs/server/namenode/NameNode.java:267:  /** {@inheritDoc} */
./src/hdfs/org/apache/hadoop/hdfs/server/namenode/NameNode.java:284:  /** {@inheritDoc} */
./src/hdfs/org/apache/hadoop/hdfs/server/namenode/NameNode.java:309:  /** {@inheritDoc} */
./src/hdfs/org/apache/hadoop/hdfs/server/namenode/NameNode.java:326:  /** {@inheritDoc} */
./src/hdfs/org/apache/hadoop/hdfs/server/namenode/NameNode.java:333:  /** {@inheritDoc} */
./src/hdfs/org/apache/hadoop/hdfs/server/namenode/NameNode.java:339:  /** {@inheritDoc} */
./src/hdfs/org/apache/hadoop/hdfs/server/namenode/NameNode.java:369:  /** {@inheritDoc} */
./src/hdfs/org/apache/hadoop/hdfs/server/namenode/NameNode.java:400:  /** {@inheritDoc} */
./src/hdfs/org/apache/hadoop/hdfs/server/namenode/NameNode.java:405:  /** {@inheritDoc} */
./src/hdfs/org/apache/hadoop/hdfs/server/namenode/NameNode.java:435:  @Deprecated
./src/hdfs/org/apache/hadoop/hdfs/server/namenode/NameNode.java:440:  /** {@inheritDoc} */
./src/hdfs/org/apache/hadoop/hdfs/server/namenode/NameNode.java:464:  /** {@inheritDoc} */
./src/hdfs/org/apache/hadoop/hdfs/server/namenode/NameNode.java:494:   * @param src The string representation of the path to the file
./src/hdfs/org/apache/hadoop/hdfs/server/namenode/NameNode.java:495:   * @throws IOException if permission to access file is denied by the system
./src/hdfs/org/apache/hadoop/hdfs/server/namenode/NameNode.java:496:   * @return object containing information regarding the file
./src/hdfs/org/apache/hadoop/hdfs/server/namenode/NameNode.java:503:  /** @inheritDoc */
./src/hdfs/org/apache/hadoop/hdfs/server/namenode/NameNode.java:520:   * @inheritDoc
./src/hdfs/org/apache/hadoop/hdfs/server/namenode/NameNode.java:579:  /** {@inheritDoc} */
./src/hdfs/org/apache/hadoop/hdfs/server/namenode/NameNode.java:584:  /** {@inheritDoc} */
./src/hdfs/org/apache/hadoop/hdfs/server/namenode/NameNode.java:590:  /** {@inheritDoc} */
./src/hdfs/org/apache/hadoop/hdfs/server/namenode/NameNode.java:595:  /** @inheritDoc */
./src/hdfs/org/apache/hadoop/hdfs/server/namenode/NameNode.java:684:   * @param nodeReg data node registration
./src/hdfs/org/apache/hadoop/hdfs/server/namenode/NameNode.java:685:   * @throws IOException
./src/hdfs/org/apache/hadoop/hdfs/server/namenode/NameNode.java:696:   * @param version
./src/hdfs/org/apache/hadoop/hdfs/server/namenode/NameNode.java:697:   * @throws IOException
./src/hdfs/org/apache/hadoop/hdfs/server/namenode/NameNode.java:725:   * @return the address on which the NameNodes is listening to.
./src/hdfs/org/apache/hadoop/hdfs/server/namenode/NameNode.java:740:   * @param conf
./src/hdfs/org/apache/hadoop/hdfs/server/namenode/NameNode.java:741:   * @param isConfirmationNeeded
./src/hdfs/org/apache/hadoop/hdfs/server/namenode/NameNode.java:742:   * @return true if formatting was aborted, false otherwise
./src/hdfs/org/apache/hadoop/hdfs/server/namenode/NameNode.java:743:   * @throws IOException
./src/hdfs/org/apache/hadoop/hdfs/server/namenode/NamenodeFsck.java:55: *      <li>none ({@link #FIXING_NONE})</li>
./src/hdfs/org/apache/hadoop/hdfs/server/namenode/NamenodeFsck.java:57: *      ({@link #FIXING_MOVE}). Remaining data blocks are saved as a
./src/hdfs/org/apache/hadoop/hdfs/server/namenode/NamenodeFsck.java:59: *      <li>delete corrupted files ({@link #FIXING_DELETE})</li>
./src/hdfs/org/apache/hadoop/hdfs/server/namenode/NamenodeFsck.java:95:   * @param conf configuration (namenode config)
./src/hdfs/org/apache/hadoop/hdfs/server/namenode/NamenodeFsck.java:96:   * @param nn namenode that this fsck is going to use
./src/hdfs/org/apache/hadoop/hdfs/server/namenode/NamenodeFsck.java:97:   * @param pmap key=value[] map that is passed to the http servlet as url parameters
./src/hdfs/org/apache/hadoop/hdfs/server/namenode/NamenodeFsck.java:98:   * @param response the object into which  this servelet writes the url contents
./src/hdfs/org/apache/hadoop/hdfs/server/namenode/NamenodeFsck.java:99:   * @throws IOException
./src/hdfs/org/apache/hadoop/hdfs/server/namenode/NamenodeFsck.java:123:   * @throws Exception
./src/hdfs/org/apache/hadoop/hdfs/server/namenode/NamenodeFsck.java:372:   * XXX (ab) Bulk of this method is copied verbatim from {@link DFSClient}, which is
./src/hdfs/org/apache/hadoop/hdfs/server/namenode/NamenodeFsck.java:502:   * @param args
./src/hdfs/org/apache/hadoop/hdfs/server/namenode/PermissionChecker.java:29:/** Perform permission checking in {@link FSNamesystem}. */
./src/hdfs/org/apache/hadoop/hdfs/server/namenode/PermissionChecker.java:75:   * @param doCheckOwner Require user to be the owner of the path?
./src/hdfs/org/apache/hadoop/hdfs/server/namenode/PermissionChecker.java:76:   * @param ancestorAccess The access required by the ancestor of the path.
./src/hdfs/org/apache/hadoop/hdfs/server/namenode/PermissionChecker.java:77:   * @param parentAccess The access required by the parent of the path.
./src/hdfs/org/apache/hadoop/hdfs/server/namenode/PermissionChecker.java:78:   * @param access The access required by the path.
./src/hdfs/org/apache/hadoop/hdfs/server/namenode/PermissionChecker.java:79:   * @param subAccess If path is a directory,
./src/hdfs/org/apache/hadoop/hdfs/server/namenode/PermissionChecker.java:82:   * @return a PermissionChecker object which caches data for later use.
./src/hdfs/org/apache/hadoop/hdfs/server/namenode/PermissionChecker.java:83:   * @throws AccessControlException
./src/hdfs/org/apache/hadoop/hdfs/server/namenode/ReplicationTargetChooser.java:60:   * @param numOfReplicas: number of replicas wanted.
./src/hdfs/org/apache/hadoop/hdfs/server/namenode/ReplicationTargetChooser.java:61:   * @param writer: the writer's machine, null if not in the cluster.
./src/hdfs/org/apache/hadoop/hdfs/server/namenode/ReplicationTargetChooser.java:62:   * @param excludedNodes: datanodesthat should not be considered targets.
./src/hdfs/org/apache/hadoop/hdfs/server/namenode/ReplicationTargetChooser.java:63:   * @param blocksize: size of the data to be written.
./src/hdfs/org/apache/hadoop/hdfs/server/namenode/ReplicationTargetChooser.java:64:   * @return array of DatanodeDescriptor instances chosen as targets
./src/hdfs/org/apache/hadoop/hdfs/server/namenode/ReplicationTargetChooser.java:84:   * @param numOfReplicas: additional number of replicas wanted.
./src/hdfs/org/apache/hadoop/hdfs/server/namenode/ReplicationTargetChooser.java:85:   * @param writer: the writer's machine, null if not in the cluster.
./src/hdfs/org/apache/hadoop/hdfs/server/namenode/ReplicationTargetChooser.java:86:   * @param choosenNodes: datanodes that have been choosen as targets.
./src/hdfs/org/apache/hadoop/hdfs/server/namenode/ReplicationTargetChooser.java:87:   * @param excludedNodes: datanodesthat should not be considered targets.
./src/hdfs/org/apache/hadoop/hdfs/server/namenode/ReplicationTargetChooser.java:88:   * @param blocksize: size of the data to be written.
./src/hdfs/org/apache/hadoop/hdfs/server/namenode/ReplicationTargetChooser.java:89:   * @return array of DatanodeDescriptor instances chosen as target 
./src/hdfs/org/apache/hadoop/hdfs/server/namenode/ReplicationTargetChooser.java:193:   * @return the choosen node
./src/hdfs/org/apache/hadoop/hdfs/server/namenode/ReplicationTargetChooser.java:227:   * @return the choosen node
./src/hdfs/org/apache/hadoop/hdfs/server/namenode/ReplicationTargetChooser.java:302:   * @return the choosen node
./src/hdfs/org/apache/hadoop/hdfs/server/namenode/ReplicationTargetChooser.java:357:   * @return the choosen nodes
./src/hdfs/org/apache/hadoop/hdfs/server/namenode/ReplicationTargetChooser.java:488:   * @param lBlk block with locations
./src/hdfs/org/apache/hadoop/hdfs/server/namenode/ReplicationTargetChooser.java:489:   * @param cluster 
./src/hdfs/org/apache/hadoop/hdfs/server/namenode/ReplicationTargetChooser.java:490:   * @return 1 if the block must be relicated on additional rack,
./src/hdfs/org/apache/hadoop/hdfs/server/namenode/ReplicationTargetChooser.java:504:   * @param lBlk block with locations
./src/hdfs/org/apache/hadoop/hdfs/server/namenode/ReplicationTargetChooser.java:505:   * @param minRacks number of racks the block should be replicated to
./src/hdfs/org/apache/hadoop/hdfs/server/namenode/ReplicationTargetChooser.java:506:   * @param cluster 
./src/hdfs/org/apache/hadoop/hdfs/server/namenode/ReplicationTargetChooser.java:507:   * @return the difference between the required and the actual number of racks
./src/hdfs/org/apache/hadoop/hdfs/server/namenode/SecondaryNameNode.java:244:   * @throws IOException
./src/hdfs/org/apache/hadoop/hdfs/server/namenode/SecondaryNameNode.java:354:   * @param argv The parameters passed to this program.
./src/hdfs/org/apache/hadoop/hdfs/server/namenode/SecondaryNameNode.java:355:   * @exception Exception if the filesystem does not exist.
./src/hdfs/org/apache/hadoop/hdfs/server/namenode/SecondaryNameNode.java:356:   * @return 0 on success, non zero on error.
./src/hdfs/org/apache/hadoop/hdfs/server/namenode/SecondaryNameNode.java:438:   * @param cmd The command that is being executed.
./src/hdfs/org/apache/hadoop/hdfs/server/namenode/SecondaryNameNode.java:456:   * @param argv Command line parameters.
./src/hdfs/org/apache/hadoop/hdfs/server/namenode/SecondaryNameNode.java:457:   * @exception Exception if the filesystem does not exist.
./src/hdfs/org/apache/hadoop/hdfs/server/namenode/SecondaryNameNode.java:480:    @Override
./src/hdfs/org/apache/hadoop/hdfs/server/namenode/SecondaryNameNode.java:491:     * @param dataDirs
./src/hdfs/org/apache/hadoop/hdfs/server/namenode/SecondaryNameNode.java:492:     * @param editsDirs
./src/hdfs/org/apache/hadoop/hdfs/server/namenode/SecondaryNameNode.java:493:     * @throws IOException
./src/hdfs/org/apache/hadoop/hdfs/server/namenode/SecondaryNameNode.java:543:     * @throws IOException
./src/hdfs/org/apache/hadoop/hdfs/server/namenode/SerialNumberManager.java:24:  /** This is the only instance of {@link SerialNumberManager}.*/
./src/hdfs/org/apache/hadoop/hdfs/server/namenode/SerialNumberManager.java:67:    /** {@inheritDoc} */
./src/hdfs/org/apache/hadoop/hdfs/server/namenode/StringBytesWritable.java:49:  /** {@inheritDoc} */
./src/hdfs/org/apache/hadoop/hdfs/server/namenode/TransferFsImage.java:44:   * @param pmap key=value[] map that is passed to the http servlet as 
./src/hdfs/org/apache/hadoop/hdfs/server/namenode/TransferFsImage.java:46:   * @param request the object from which this servelet reads the url contents
./src/hdfs/org/apache/hadoop/hdfs/server/namenode/TransferFsImage.java:47:   * @param response the object into which this servelet writes the url contents
./src/hdfs/org/apache/hadoop/hdfs/server/namenode/TransferFsImage.java:48:   * @throws IOException
./src/hdfs/org/apache/hadoop/hdfs/server/namenode/UnderReplicatedBlocks.java:66:   * @param block a under replication block
./src/hdfs/org/apache/hadoop/hdfs/server/namenode/UnderReplicatedBlocks.java:67:   * @param curReplicas current number of replicas of the block
./src/hdfs/org/apache/hadoop/hdfs/server/namenode/UnderReplicatedBlocks.java:68:   * @param expectedReplicas expected number of replicas of the block
./src/hdfs/org/apache/hadoop/hdfs/server/namenode/UnderReplicatedBlocks.java:93:   * @param block a under replication block
./src/hdfs/org/apache/hadoop/hdfs/server/namenode/UnderReplicatedBlocks.java:94:   * @param curReplicas current number of replicas of the block
./src/hdfs/org/apache/hadoop/hdfs/server/namenode/UnderReplicatedBlocks.java:95:   * @param expectedReplicas expected number of replicas of the block
./src/hdfs/org/apache/hadoop/hdfs/server/namenode/UpgradeManagerNamenode.java:52:   * @return true if distributed upgrade is required or false otherwise
./src/hdfs/org/apache/hadoop/hdfs/server/namenode/UpgradeManagerNamenode.java:53:   * @throws IOException
./src/hdfs/org/apache/hadoop/hdfs/server/namenode/UpgradeObjectNamenode.java:40:   * @param command
./src/hdfs/org/apache/hadoop/hdfs/server/namenode/UpgradeObjectNamenode.java:41:   * @return the reply command which is analyzed on the client side.
./src/hdfs/org/apache/hadoop/hdfs/server/protocol/BlockCommand.java:45:   * @param blocktargetlist    blocks to be transferred 
./src/hdfs/org/apache/hadoop/hdfs/server/protocol/BlockCommand.java:63:   * @param blocks blocks related to the action
./src/hdfs/org/apache/hadoop/hdfs/server/protocol/BlockMetaDataInfo.java:47:  /** {@inheritDoc} */
./src/hdfs/org/apache/hadoop/hdfs/server/protocol/BlockMetaDataInfo.java:53:  /** {@inheritDoc} */
./src/hdfs/org/apache/hadoop/hdfs/server/protocol/DatanodeProtocol.java:68:   * @see org.apache.hadoop.hdfs.server.datanode.DataNode#dnRegistration
./src/hdfs/org/apache/hadoop/hdfs/server/protocol/DatanodeProtocol.java:69:   * @see org.apache.hadoop.hdfs.server.namenode.FSNamesystem#registerDatanode(DatanodeRegistration)
./src/hdfs/org/apache/hadoop/hdfs/server/protocol/DatanodeProtocol.java:71:   * @return updated {@link org.apache.hadoop.hdfs.server.protocol.DatanodeRegistration}, which contains 
./src/hdfs/org/apache/hadoop/hdfs/server/protocol/DatanodeProtocol.java:96:   * @param registration
./src/hdfs/org/apache/hadoop/hdfs/server/protocol/DatanodeProtocol.java:97:   * @param blocks - the block list as an array of longs.
./src/hdfs/org/apache/hadoop/hdfs/server/protocol/DatanodeProtocol.java:101:   * @return - the next command for DN to process.
./src/hdfs/org/apache/hadoop/hdfs/server/protocol/DatanodeProtocol.java:102:   * @throws IOException
./src/hdfs/org/apache/hadoop/hdfs/server/protocol/DatanodeProtocol.java:137:   * @return a reply in the form of an upgrade command
./src/hdfs/org/apache/hadoop/hdfs/server/protocol/DatanodeProtocol.java:142:   * same as {@link org.apache.hadoop.hdfs.protocol.ClientProtocol#reportBadBlocks(LocatedBlock[])}
./src/hdfs/org/apache/hadoop/hdfs/server/protocol/DatanodeProtocol.java:148:   * @return the next GenerationStamp to be associated with the specified
./src/hdfs/org/apache/hadoop/hdfs/server/protocol/DatanodeRegistration.java:105:  /** {@inheritDoc} */
./src/hdfs/org/apache/hadoop/hdfs/server/protocol/DatanodeRegistration.java:117:  /** {@inheritDoc} */
./src/hdfs/org/apache/hadoop/hdfs/server/protocol/InterDatanodeProtocol.java:38:  /** @return the BlockMetaDataInfo of a block;
./src/hdfs/org/apache/hadoop/hdfs/server/protocol/NamenodeProtocol.java:39:   * @param datanode  a data node
./src/hdfs/org/apache/hadoop/hdfs/server/protocol/NamenodeProtocol.java:40:   * @param size      requested size
./src/hdfs/org/apache/hadoop/hdfs/server/protocol/NamenodeProtocol.java:41:   * @return          a list of blocks & their locations
./src/hdfs/org/apache/hadoop/hdfs/server/protocol/NamenodeProtocol.java:42:   * @throws RemoteException if size is less than or equal to 0 or
./src/hdfs/org/apache/hadoop/hdfs/server/protocol/NamenodeProtocol.java:50:   * @return The number of bytes in the current edit log.
./src/hdfs/org/apache/hadoop/hdfs/server/protocol/NamenodeProtocol.java:51:   * @throws IOException
./src/hdfs/org/apache/hadoop/hdfs/server/protocol/NamenodeProtocol.java:58:   * @throws IOException
./src/hdfs/org/apache/hadoop/hdfs/server/protocol/NamenodeProtocol.java:59:   * @return a unique token to identify this transaction.
./src/hdfs/org/apache/hadoop/hdfs/server/protocol/NamenodeProtocol.java:67:   * @throws IOException
./src/hdfs/org/apache/hadoop/hdfs/tools/DFSAdmin.java:84:     * @param cmd A string representation of a command starting with "-"
./src/hdfs/org/apache/hadoop/hdfs/tools/DFSAdmin.java:85:     * @return true if this is a clrQuota command; false otherwise
./src/hdfs/org/apache/hadoop/hdfs/tools/DFSAdmin.java:91:    @Override
./src/hdfs/org/apache/hadoop/hdfs/tools/DFSAdmin.java:96:    @Override
./src/hdfs/org/apache/hadoop/hdfs/tools/DFSAdmin.java:131:     * @param cmd A string representation of a command starting with "-"
./src/hdfs/org/apache/hadoop/hdfs/tools/DFSAdmin.java:132:     * @return true if this is a count command; false otherwise
./src/hdfs/org/apache/hadoop/hdfs/tools/DFSAdmin.java:138:    @Override
./src/hdfs/org/apache/hadoop/hdfs/tools/DFSAdmin.java:143:    @Override
./src/hdfs/org/apache/hadoop/hdfs/tools/DFSAdmin.java:170:     * @param cmd A string representation of a command starting with "-"
./src/hdfs/org/apache/hadoop/hdfs/tools/DFSAdmin.java:171:     * @return true if this is a clrQuota command; false otherwise
./src/hdfs/org/apache/hadoop/hdfs/tools/DFSAdmin.java:177:    @Override
./src/hdfs/org/apache/hadoop/hdfs/tools/DFSAdmin.java:182:    @Override
./src/hdfs/org/apache/hadoop/hdfs/tools/DFSAdmin.java:219:     * @param cmd A string representation of a command starting with "-"
./src/hdfs/org/apache/hadoop/hdfs/tools/DFSAdmin.java:220:     * @return true if this is a count command; false otherwise
./src/hdfs/org/apache/hadoop/hdfs/tools/DFSAdmin.java:226:    @Override
./src/hdfs/org/apache/hadoop/hdfs/tools/DFSAdmin.java:231:    @Override
./src/hdfs/org/apache/hadoop/hdfs/tools/DFSAdmin.java:253:   * @exception IOException if the filesystem does not exist.
./src/hdfs/org/apache/hadoop/hdfs/tools/DFSAdmin.java:310:   * @param argv List of of command line parameters.
./src/hdfs/org/apache/hadoop/hdfs/tools/DFSAdmin.java:311:   * @param idx The index of the command that is being processed.
./src/hdfs/org/apache/hadoop/hdfs/tools/DFSAdmin.java:312:   * @exception IOException if the filesystem does not exist.
./src/hdfs/org/apache/hadoop/hdfs/tools/DFSAdmin.java:364:   * @exception IOException 
./src/hdfs/org/apache/hadoop/hdfs/tools/DFSAdmin.java:480:   * @exception IOException 
./src/hdfs/org/apache/hadoop/hdfs/tools/DFSAdmin.java:502:   * @exception IOException 
./src/hdfs/org/apache/hadoop/hdfs/tools/DFSAdmin.java:538:   * @param argv List of of command line parameters.
./src/hdfs/org/apache/hadoop/hdfs/tools/DFSAdmin.java:539:   * @param idx The index of the command that is being processed.
./src/hdfs/org/apache/hadoop/hdfs/tools/DFSAdmin.java:540:   * @exception IOException if an error accoured wile accessing
./src/hdfs/org/apache/hadoop/hdfs/tools/DFSAdmin.java:554:   * @param cmd The command that is being executed.
./src/hdfs/org/apache/hadoop/hdfs/tools/DFSAdmin.java:606:   * @param argv The parameters passed to this program.
./src/hdfs/org/apache/hadoop/hdfs/tools/DFSAdmin.java:607:   * @exception Exception if the filesystem does not exist.
./src/hdfs/org/apache/hadoop/hdfs/tools/DFSAdmin.java:608:   * @return 0 on success, non zero on error.
./src/hdfs/org/apache/hadoop/hdfs/tools/DFSAdmin.java:610:  @Override
./src/hdfs/org/apache/hadoop/hdfs/tools/DFSAdmin.java:730:   * @param argv Command line parameters.
./src/hdfs/org/apache/hadoop/hdfs/tools/DFSAdmin.java:731:   * @exception Exception if the filesystem does not exist.
./src/hdfs/org/apache/hadoop/hdfs/tools/DFSck.java:43: *      <li>none ({@link org.apache.hadoop.hdfs.server.namenode.NamenodeFsck#FIXING_NONE})</li>
./src/hdfs/org/apache/hadoop/hdfs/tools/DFSck.java:45: *      ({@link org.apache.hadoop.hdfs.server.namenode.NamenodeFsck#FIXING_MOVE}). Remaining data blocks are saved as a
./src/hdfs/org/apache/hadoop/hdfs/tools/DFSck.java:47: *      <li>delete corrupted files ({@link org.apache.hadoop.hdfs.server.namenode.NamenodeFsck#FIXING_DELETE})</li>
./src/hdfs/org/apache/hadoop/hdfs/tools/DFSck.java:64:   * @param conf current Configuration
./src/hdfs/org/apache/hadoop/hdfs/tools/DFSck.java:65:   * @throws Exception
./src/hdfs/org/apache/hadoop/hdfs/tools/DFSck.java:96:   * @param args
./src/mapred/org/apache/hadoop/mapred/ClusterStatus.java:48: * {@link JobClient#getClusterStatus()}.</p>
./src/mapred/org/apache/hadoop/mapred/ClusterStatus.java:50: * @see JobClient
./src/mapred/org/apache/hadoop/mapred/ClusterStatus.java:66:   * @param trackers no. of tasktrackers in the cluster
./src/mapred/org/apache/hadoop/mapred/ClusterStatus.java:67:   * @param maps no. of currently running map-tasks in the cluster
./src/mapred/org/apache/hadoop/mapred/ClusterStatus.java:68:   * @param reduces no. of currently running reduce-tasks in the cluster
./src/mapred/org/apache/hadoop/mapred/ClusterStatus.java:69:   * @param max the maximum no. of tasks in the cluster
./src/mapred/org/apache/hadoop/mapred/ClusterStatus.java:70:   * @param state the {@link JobTracker.State} of the <code>JobTracker</code>
./src/mapred/org/apache/hadoop/mapred/ClusterStatus.java:86:   * @return the number of task trackers in the cluster.
./src/mapred/org/apache/hadoop/mapred/ClusterStatus.java:95:   * @return the number of currently running map tasks in the cluster.
./src/mapred/org/apache/hadoop/mapred/ClusterStatus.java:104:   * @return the number of currently running reduce tasks in the cluster.
./src/mapred/org/apache/hadoop/mapred/ClusterStatus.java:113:   * @return the maximum capacity for running map tasks in the cluster.
./src/mapred/org/apache/hadoop/mapred/ClusterStatus.java:122:   * @return the maximum capacity for running reduce tasks in the cluster.
./src/mapred/org/apache/hadoop/mapred/ClusterStatus.java:130:   * as {@link JobTracker.State}
./src/mapred/org/apache/hadoop/mapred/ClusterStatus.java:132:   * @return the current state of the <code>JobTracker</code>.
./src/mapred/org/apache/hadoop/mapred/CommitTaskAction.java:25: * Represents a directive from the {@link org.apache.hadoop.mapred.JobTracker} 
./src/mapred/org/apache/hadoop/mapred/CommitTaskAction.java:26: * to the {@link org.apache.hadoop.mapred.TaskTracker} to commit the output
./src/mapred/org/apache/hadoop/mapred/CompletedJobStatusStore.java:81:   * @return TRUE if active, FALSE otherwise.
./src/mapred/org/apache/hadoop/mapred/CompletedJobStatusStore.java:132:   * @param job the job about to be 'retired'
./src/mapred/org/apache/hadoop/mapred/CompletedJobStatusStore.java:217:   * @param jobId the jobId for which jobStatus is queried
./src/mapred/org/apache/hadoop/mapred/CompletedJobStatusStore.java:218:   * @return JobStatus object, null if not able to retrieve
./src/mapred/org/apache/hadoop/mapred/CompletedJobStatusStore.java:240:   * @param jobId the jobId for which jobProfile is queried
./src/mapred/org/apache/hadoop/mapred/CompletedJobStatusStore.java:241:   * @return JobProfile object, null if not able to retrieve
./src/mapred/org/apache/hadoop/mapred/CompletedJobStatusStore.java:264:   * @param jobId the jobId for which Counters is queried
./src/mapred/org/apache/hadoop/mapred/CompletedJobStatusStore.java:265:   * @return Counters object, null if not able to retrieve
./src/mapred/org/apache/hadoop/mapred/CompletedJobStatusStore.java:289:   * @param jobId       the jobId for which TaskCompletionEvents is queried
./src/mapred/org/apache/hadoop/mapred/CompletedJobStatusStore.java:290:   * @param fromEventId events offset
./src/mapred/org/apache/hadoop/mapred/CompletedJobStatusStore.java:291:   * @param maxEvents   max number of events
./src/mapred/org/apache/hadoop/mapred/CompletedJobStatusStore.java:292:   * @return TaskCompletionEvent[], empty array if not able to retrieve
./src/mapred/org/apache/hadoop/mapred/Counters.java:46: * any {@link Enum} type.</p>
./src/mapred/org/apache/hadoop/mapred/Counters.java:48: * <p><code>Counters</code> are bunched into {@link Group}s, each comprising of
./src/mapred/org/apache/hadoop/mapred/Counters.java:112:     * @return the internal name of the counter
./src/mapred/org/apache/hadoop/mapred/Counters.java:120:     * @return the user facing name of the counter
./src/mapred/org/apache/hadoop/mapred/Counters.java:170:     * @return the current value
./src/mapred/org/apache/hadoop/mapred/Counters.java:178:     * @param incr the value to increase this counter by
./src/mapred/org/apache/hadoop/mapred/Counters.java:187:   *  counter {@link Enum} class.  
./src/mapred/org/apache/hadoop/mapred/Counters.java:214:     * @throws MissingResourceException if the bundle isn't found
./src/mapred/org/apache/hadoop/mapred/Counters.java:309:     * @param id the numeric id of the counter within the group
./src/mapred/org/apache/hadoop/mapred/Counters.java:310:     * @param name the internal counter name
./src/mapred/org/apache/hadoop/mapred/Counters.java:311:     * @return the counter
./src/mapred/org/apache/hadoop/mapred/Counters.java:312:     * @deprecated use {@link #getCounter(String)} instead
./src/mapred/org/apache/hadoop/mapred/Counters.java:314:    @Deprecated
./src/mapred/org/apache/hadoop/mapred/Counters.java:321:     * @param name the internal counter name
./src/mapred/org/apache/hadoop/mapred/Counters.java:322:     * @return the counter
./src/mapred/org/apache/hadoop/mapred/Counters.java:393:   * @return Set of counter names.
./src/mapred/org/apache/hadoop/mapred/Counters.java:419:   * @param key the counter key
./src/mapred/org/apache/hadoop/mapred/Counters.java:420:   * @return the matching counter object
./src/mapred/org/apache/hadoop/mapred/Counters.java:434:   * @param group the name of the group
./src/mapred/org/apache/hadoop/mapred/Counters.java:435:   * @param name the internal name of the counter
./src/mapred/org/apache/hadoop/mapred/Counters.java:436:   * @return the counter for that name
./src/mapred/org/apache/hadoop/mapred/Counters.java:444:   * @param group the name of the group
./src/mapred/org/apache/hadoop/mapred/Counters.java:445:   * @param id the id of the counter within the group (0 to N-1)
./src/mapred/org/apache/hadoop/mapred/Counters.java:446:   * @param name the internal name of the counter
./src/mapred/org/apache/hadoop/mapred/Counters.java:447:   * @return the counter for that name
./src/mapred/org/apache/hadoop/mapred/Counters.java:448:   * @deprecated
./src/mapred/org/apache/hadoop/mapred/Counters.java:450:  @Deprecated
./src/mapred/org/apache/hadoop/mapred/Counters.java:458:   * @param key identifies a counter
./src/mapred/org/apache/hadoop/mapred/Counters.java:459:   * @param amount amount by which counter is to be incremented
./src/mapred/org/apache/hadoop/mapred/Counters.java:468:   * @param group the name of the group
./src/mapred/org/apache/hadoop/mapred/Counters.java:469:   * @param counter the internal name of the counter
./src/mapred/org/apache/hadoop/mapred/Counters.java:470:   * @param amount amount by which counter is to be incremented
./src/mapred/org/apache/hadoop/mapred/Counters.java:487:   * @param other the other Counters instance
./src/mapred/org/apache/hadoop/mapred/Counters.java:561:   * @param log The log to use.
./src/mapred/org/apache/hadoop/mapred/Counters.java:591:   * @return the string with "name=value" for each counter and separated by ","
./src/mapred/org/apache/hadoop/mapred/Counters.java:616:   * @return the string in the following format
./src/mapred/org/apache/hadoop/mapred/Counters.java:655:   * {@link #makeEscapedCompactString()}. 
./src/mapred/org/apache/hadoop/mapred/Counters.java:656:   * @return a Counter
./src/mapred/org/apache/hadoop/mapred/DefaultJobHistoryParser.java:42:   * @param jobHistoryFile history file for this job. 
./src/mapred/org/apache/hadoop/mapred/DefaultJobHistoryParser.java:43:   * @param job a precreated JobInfo object, should be non-null. 
./src/mapred/org/apache/hadoop/mapred/DefaultJobHistoryParser.java:44:   * @param fs FileSystem where historyFile is present. 
./src/mapred/org/apache/hadoop/mapred/DefaultJobHistoryParser.java:45:   * @throws IOException
./src/mapred/org/apache/hadoop/mapred/EagerTaskInitializationListener.java:32: * A {@link JobInProgressListener} which initializes the tasks for a job as soon
./src/mapred/org/apache/hadoop/mapred/EagerTaskInitializationListener.java:33: * as the job is added (using the {@link #jobAdded(JobInProgress)} method).
./src/mapred/org/apache/hadoop/mapred/EagerTaskInitializationListener.java:95:  @Override
./src/mapred/org/apache/hadoop/mapred/EagerTaskInitializationListener.java:128:  @Override
./src/mapred/org/apache/hadoop/mapred/EagerTaskInitializationListener.java:135:  @Override
./src/mapred/org/apache/hadoop/mapred/FileInputFormat.java:38: * A base class for file-based {@link InputFormat}.
./src/mapred/org/apache/hadoop/mapred/FileInputFormat.java:42: * {@link #getSplits(JobConf, int)}.
./src/mapred/org/apache/hadoop/mapred/FileInputFormat.java:44: * {@link #isSplitable(FileSystem, Path)} method to ensure input-files are
./src/mapred/org/apache/hadoop/mapred/FileInputFormat.java:45: * not split-up and are processed as a whole by {@link Mapper}s.
./src/mapred/org/apache/hadoop/mapred/FileInputFormat.java:93:   * so that {@link Mapper}s process entire files.
./src/mapred/org/apache/hadoop/mapred/FileInputFormat.java:95:   * @param fs the file system that the file is on
./src/mapred/org/apache/hadoop/mapred/FileInputFormat.java:96:   * @param filename the file name to check
./src/mapred/org/apache/hadoop/mapred/FileInputFormat.java:97:   * @return is this file splitable?
./src/mapred/org/apache/hadoop/mapred/FileInputFormat.java:111:   * @param filter the PathFilter class use for filtering the input paths.
./src/mapred/org/apache/hadoop/mapred/FileInputFormat.java:121:   * @return the PathFilter instance set for the job, NULL if none has been set.
./src/mapred/org/apache/hadoop/mapred/FileInputFormat.java:134:   * @param job the job to list input paths for
./src/mapred/org/apache/hadoop/mapred/FileInputFormat.java:135:   * @return array of FileStatus objects
./src/mapred/org/apache/hadoop/mapred/FileInputFormat.java:136:   * @throws IOException if zero items.
./src/mapred/org/apache/hadoop/mapred/FileInputFormat.java:185:  /** Splits files returned by {@link #listStatus(JobConf)} when
./src/mapred/org/apache/hadoop/mapred/FileInputFormat.java:187:  @SuppressWarnings("deprecation")
./src/mapred/org/apache/hadoop/mapred/FileInputFormat.java:263:   * @param conf Configuration of the job
./src/mapred/org/apache/hadoop/mapred/FileInputFormat.java:264:   * @param commaSeparatedPaths Comma separated paths to be set as 
./src/mapred/org/apache/hadoop/mapred/FileInputFormat.java:276:   * @param conf The configuration of the job 
./src/mapred/org/apache/hadoop/mapred/FileInputFormat.java:277:   * @param commaSeparatedPaths Comma separated paths to be added to
./src/mapred/org/apache/hadoop/mapred/FileInputFormat.java:287:   * Set the array of {@link Path}s as the list of inputs
./src/mapred/org/apache/hadoop/mapred/FileInputFormat.java:290:   * @param conf Configuration of the job. 
./src/mapred/org/apache/hadoop/mapred/FileInputFormat.java:291:   * @param inputPaths the {@link Path}s of the input directories/files 
./src/mapred/org/apache/hadoop/mapred/FileInputFormat.java:306:   * Add a {@link Path} to the list of inputs for the map-reduce job.
./src/mapred/org/apache/hadoop/mapred/FileInputFormat.java:308:   * @param conf The configuration of the job 
./src/mapred/org/apache/hadoop/mapred/FileInputFormat.java:309:   * @param path {@link Path} to be added to the list of inputs for 
./src/mapred/org/apache/hadoop/mapred/FileInputFormat.java:360:   * Get the list of input {@link Path}s for the map-reduce job.
./src/mapred/org/apache/hadoop/mapred/FileInputFormat.java:362:   * @param conf The configuration of the job 
./src/mapred/org/apache/hadoop/mapred/FileInputFormat.java:363:   * @return the list of input {@link Path}s for the map-reduce job.
./src/mapred/org/apache/hadoop/mapred/FileOutputCommitter.java:31:/** An {@link OutputCommitter} that commits files specified 
./src/mapred/org/apache/hadoop/mapred/FileOutputFormat.java:29:/** A base class for {@link OutputFormat}. */
./src/mapred/org/apache/hadoop/mapred/FileOutputFormat.java:34:   * @param conf the {@link JobConf} to modify
./src/mapred/org/apache/hadoop/mapred/FileOutputFormat.java:35:   * @param compress should the output of the job be compressed?
./src/mapred/org/apache/hadoop/mapred/FileOutputFormat.java:43:   * @param conf the {@link JobConf} to look in
./src/mapred/org/apache/hadoop/mapred/FileOutputFormat.java:44:   * @return <code>true</code> if the job output should be compressed,
./src/mapred/org/apache/hadoop/mapred/FileOutputFormat.java:52:   * Set the {@link CompressionCodec} to be used to compress job outputs.
./src/mapred/org/apache/hadoop/mapred/FileOutputFormat.java:53:   * @param conf the {@link JobConf} to modify
./src/mapred/org/apache/hadoop/mapred/FileOutputFormat.java:54:   * @param codecClass the {@link CompressionCodec} to be used to
./src/mapred/org/apache/hadoop/mapred/FileOutputFormat.java:66:   * Get the {@link CompressionCodec} for compressing the job outputs.
./src/mapred/org/apache/hadoop/mapred/FileOutputFormat.java:67:   * @param conf the {@link JobConf} to look in
./src/mapred/org/apache/hadoop/mapred/FileOutputFormat.java:68:   * @param defaultValue the {@link CompressionCodec} to return if not set
./src/mapred/org/apache/hadoop/mapred/FileOutputFormat.java:69:   * @return the {@link CompressionCodec} to be used to compress the 
./src/mapred/org/apache/hadoop/mapred/FileOutputFormat.java:71:   * @throws IllegalArgumentException if the class was specified, but not found
./src/mapred/org/apache/hadoop/mapred/FileOutputFormat.java:111:   * Set the {@link Path} of the output directory for the map-reduce job.
./src/mapred/org/apache/hadoop/mapred/FileOutputFormat.java:113:   * @param conf The configuration of the job.
./src/mapred/org/apache/hadoop/mapred/FileOutputFormat.java:114:   * @param outputDir the {@link Path} of the output directory for 
./src/mapred/org/apache/hadoop/mapred/FileOutputFormat.java:123:   * Set the {@link Path} of the task's temporary output directory 
./src/mapred/org/apache/hadoop/mapred/FileOutputFormat.java:128:   * @param conf The configuration of the job.
./src/mapred/org/apache/hadoop/mapred/FileOutputFormat.java:129:   * @param outputDir the {@link Path} of the output directory 
./src/mapred/org/apache/hadoop/mapred/FileOutputFormat.java:139:   * Get the {@link Path} to the output directory for the map-reduce job.
./src/mapred/org/apache/hadoop/mapred/FileOutputFormat.java:141:   * @return the {@link Path} to the output directory for the map-reduce job.
./src/mapred/org/apache/hadoop/mapred/FileOutputFormat.java:142:   * @see FileOutputFormat#getWorkOutputPath(JobConf)
./src/mapred/org/apache/hadoop/mapred/FileOutputFormat.java:150:   *  Get the {@link Path} to the task's temporary output directory 
./src/mapred/org/apache/hadoop/mapred/FileOutputFormat.java:155:   * <p><i>Note:</i> The following is valid only if the {@link OutputCommitter}
./src/mapred/org/apache/hadoop/mapred/FileOutputFormat.java:156:   *  is {@link FileOutputCommitter}. If <code>OutputCommitter</code> is not 
./src/mapred/org/apache/hadoop/mapred/FileOutputFormat.java:158:   *  directory is same as {@link #getOutputPath(JobConf)} i.e.
./src/mapred/org/apache/hadoop/mapred/FileOutputFormat.java:182:   * of his reduce-task i.e. via {@link #getWorkOutputPath(JobConf)}, and the 
./src/mapred/org/apache/hadoop/mapred/FileOutputFormat.java:190:   * path  returned by {@link #getWorkOutputPath(JobConf)} from map/reduce 
./src/mapred/org/apache/hadoop/mapred/FileOutputFormat.java:197:   * @return the {@link Path} to the task's temporary output directory 
./src/mapred/org/apache/hadoop/mapred/FileOutputFormat.java:209:   * @param conf job-configuration
./src/mapred/org/apache/hadoop/mapred/FileOutputFormat.java:210:   * @param name temporary task-output filename
./src/mapred/org/apache/hadoop/mapred/FileOutputFormat.java:211:   * @return path to the task's temporary output file
./src/mapred/org/apache/hadoop/mapred/FileOutputFormat.java:212:   * @throws IOException
./src/mapred/org/apache/hadoop/mapred/FileOutputFormat.java:247:   * @param conf the configuration for the job.
./src/mapred/org/apache/hadoop/mapred/FileOutputFormat.java:248:   * @param name the name to make unique.
./src/mapred/org/apache/hadoop/mapred/FileOutputFormat.java:249:   * @return a unique name accross all tasks of the job.
./src/mapred/org/apache/hadoop/mapred/FileOutputFormat.java:268:   * Helper function to generate a {@link Path} for a file that is unique for
./src/mapred/org/apache/hadoop/mapred/FileOutputFormat.java:275:   * <p>This method uses the {@link #getUniqueName} method to make the file name
./src/mapred/org/apache/hadoop/mapred/FileOutputFormat.java:278:   * @param conf the configuration for the job.
./src/mapred/org/apache/hadoop/mapred/FileOutputFormat.java:279:   * @param name the name for the file.
./src/mapred/org/apache/hadoop/mapred/FileOutputFormat.java:280:   * @return a unique path accross all tasks of the job.
./src/mapred/org/apache/hadoop/mapred/FileSplit.java:30:/** A section of an input file.  Returned by {@link
./src/mapred/org/apache/hadoop/mapred/FileSplit.java:32: * {@link InputFormat#getRecordReader(InputSplit,JobConf,Reporter)}. */
./src/mapred/org/apache/hadoop/mapred/FileSplit.java:42:   * @deprecated
./src/mapred/org/apache/hadoop/mapred/FileSplit.java:43:   * @param file the file name
./src/mapred/org/apache/hadoop/mapred/FileSplit.java:44:   * @param start the position of the first byte in the file to process
./src/mapred/org/apache/hadoop/mapred/FileSplit.java:45:   * @param length the number of bytes in the file to process
./src/mapred/org/apache/hadoop/mapred/FileSplit.java:47:  @Deprecated
./src/mapred/org/apache/hadoop/mapred/FileSplit.java:54:   * @param file the file name
./src/mapred/org/apache/hadoop/mapred/FileSplit.java:55:   * @param start the position of the first byte in the file to process
./src/mapred/org/apache/hadoop/mapred/FileSplit.java:56:   * @param length the number of bytes in the file to process
./src/mapred/org/apache/hadoop/mapred/FileSplit.java:57:   * @param hosts the list of hosts containing the block, possibly null
./src/mapred/org/apache/hadoop/mapred/HeartbeatResponse.java:33: * The response sent by the {@link JobTracker} to the hearbeat sent
./src/mapred/org/apache/hadoop/mapred/HeartbeatResponse.java:34: * periodically by the {@link TaskTracker}
./src/mapred/org/apache/hadoop/mapred/ID.java:29: * as an integer. This is the super class of {@link JobID}, 
./src/mapred/org/apache/hadoop/mapred/ID.java:30: * {@link TaskID} and {@link TaskAttemptID}.
./src/mapred/org/apache/hadoop/mapred/ID.java:32: * @see JobID
./src/mapred/org/apache/hadoop/mapred/ID.java:33: * @see TaskID
./src/mapred/org/apache/hadoop/mapred/ID.java:34: * @see TaskAttemptID
./src/mapred/org/apache/hadoop/mapred/ID.java:52:  @Override
./src/mapred/org/apache/hadoop/mapred/ID.java:57:  @Override
./src/mapred/org/apache/hadoop/mapred/ID.java:62:  @Override
./src/mapred/org/apache/hadoop/mapred/ID.java:96:   * @return constructed Id object or null if the given String is null
./src/mapred/org/apache/hadoop/mapred/ID.java:97:   * @throws IllegalArgumentException if the given string is malformed
./src/mapred/org/apache/hadoop/mapred/IFile.java:240:     * @param conf Configuration File 
./src/mapred/org/apache/hadoop/mapred/IFile.java:241:     * @param fs  FileSystem
./src/mapred/org/apache/hadoop/mapred/IFile.java:242:     * @param file Path of the file to be opened. This file should have
./src/mapred/org/apache/hadoop/mapred/IFile.java:244:     * @param codec codec
./src/mapred/org/apache/hadoop/mapred/IFile.java:245:     * @throws IOException
./src/mapred/org/apache/hadoop/mapred/IFile.java:258:     * @param conf Configuration File 
./src/mapred/org/apache/hadoop/mapred/IFile.java:259:     * @param in   The input stream
./src/mapred/org/apache/hadoop/mapred/IFile.java:260:     * @param length Length of the data in the stream, including the checksum
./src/mapred/org/apache/hadoop/mapred/IFile.java:262:     * @param codec codec
./src/mapred/org/apache/hadoop/mapred/IFile.java:263:     * @throws IOException
./src/mapred/org/apache/hadoop/mapred/IFile.java:292:     * @param buf buffer 
./src/mapred/org/apache/hadoop/mapred/IFile.java:293:     * @param off offset
./src/mapred/org/apache/hadoop/mapred/IFile.java:294:     * @param len length of buffer
./src/mapred/org/apache/hadoop/mapred/IFile.java:295:     * @return the no. of bytes read
./src/mapred/org/apache/hadoop/mapred/IFile.java:296:     * @throws IOException
./src/mapred/org/apache/hadoop/mapred/IFile.java:443:    @Override
./src/mapred/org/apache/hadoop/mapred/IFile.java:451:    @Override
./src/mapred/org/apache/hadoop/mapred/IFileInputStream.java:29: * Used to validate the checksum of files created by {@link IFileOutputStream}. 
./src/mapred/org/apache/hadoop/mapred/IFileInputStream.java:45:   * @param in The input stream to be verified for checksum.
./src/mapred/org/apache/hadoop/mapred/IFileInputStream.java:46:   * @param len The length of the input stream including checksum bytes.
./src/mapred/org/apache/hadoop/mapred/IFileInputStream.java:61:  @Override
./src/mapred/org/apache/hadoop/mapred/IFileInputStream.java:66:  @Override
./src/mapred/org/apache/hadoop/mapred/IFileInputStream.java:160:  @Override
./src/mapred/org/apache/hadoop/mapred/IFileOutputStream.java:43:   * @param out
./src/mapred/org/apache/hadoop/mapred/IFileOutputStream.java:52:  @Override
./src/mapred/org/apache/hadoop/mapred/IFileOutputStream.java:66:  @Override
./src/mapred/org/apache/hadoop/mapred/IFileOutputStream.java:72:  @Override
./src/mapred/org/apache/hadoop/mapred/IndexCache.java:53:   * @param mapId
./src/mapred/org/apache/hadoop/mapred/IndexCache.java:54:   * @param reduce
./src/mapred/org/apache/hadoop/mapred/IndexCache.java:55:   * @param fileName The file to read the index information from if it is not
./src/mapred/org/apache/hadoop/mapred/IndexCache.java:57:   * @return The Index Information
./src/mapred/org/apache/hadoop/mapred/IndexCache.java:58:   * @throws IOException
./src/mapred/org/apache/hadoop/mapred/IndexCache.java:132:   * @param mapId The taskID of this map.
./src/mapred/org/apache/hadoop/mapred/InputFormat.java:35: *   Split-up the input file(s) into logical {@link InputSplit}s, each of 
./src/mapred/org/apache/hadoop/mapred/InputFormat.java:36: *   which is then assigned to an individual {@link Mapper}.
./src/mapred/org/apache/hadoop/mapred/InputFormat.java:39: *   Provide the {@link RecordReader} implementation to be used to glean
./src/mapred/org/apache/hadoop/mapred/InputFormat.java:41: *   the {@link Mapper}.
./src/mapred/org/apache/hadoop/mapred/InputFormat.java:45: * <p>The default behavior of file-based {@link InputFormat}s, typically 
./src/mapred/org/apache/hadoop/mapred/InputFormat.java:46: * sub-classes of {@link FileInputFormat}, is to split the 
./src/mapred/org/apache/hadoop/mapred/InputFormat.java:47: * input into <i>logical</i> {@link InputSplit}s based on the total size, in 
./src/mapred/org/apache/hadoop/mapred/InputFormat.java:48: * bytes, of the input files. However, the {@link FileSystem} blocksize of  
./src/mapred/org/apache/hadoop/mapred/InputFormat.java:51: * <a href="{@docRoot}/../hadoop-default.html#mapred.min.split.size">
./src/mapred/org/apache/hadoop/mapred/InputFormat.java:56: * application has to also implement a {@link RecordReader} on whom lies the
./src/mapred/org/apache/hadoop/mapred/InputFormat.java:60: * @see InputSplit
./src/mapred/org/apache/hadoop/mapred/InputFormat.java:61: * @see RecordReader
./src/mapred/org/apache/hadoop/mapred/InputFormat.java:62: * @see JobClient
./src/mapred/org/apache/hadoop/mapred/InputFormat.java:63: * @see FileInputFormat
./src/mapred/org/apache/hadoop/mapred/InputFormat.java:70:   * <p>Each {@link InputSplit} is then assigned to an individual {@link Mapper}
./src/mapred/org/apache/hadoop/mapred/InputFormat.java:77:   * @param job job configuration.
./src/mapred/org/apache/hadoop/mapred/InputFormat.java:78:   * @param numSplits the desired number of splits, a hint.
./src/mapred/org/apache/hadoop/mapred/InputFormat.java:79:   * @return an array of {@link InputSplit}s for the job.
./src/mapred/org/apache/hadoop/mapred/InputFormat.java:84:   * Get the {@link RecordReader} for the given {@link InputSplit}.
./src/mapred/org/apache/hadoop/mapred/InputFormat.java:90:   * @param split the {@link InputSplit}
./src/mapred/org/apache/hadoop/mapred/InputFormat.java:91:   * @param job the job that this split belongs to
./src/mapred/org/apache/hadoop/mapred/InputFormat.java:92:   * @return a {@link RecordReader}
./src/mapred/org/apache/hadoop/mapred/InputSplit.java:26: * individual {@link Mapper}. 
./src/mapred/org/apache/hadoop/mapred/InputSplit.java:29: * responsibility of {@link RecordReader} of the job to process this and present
./src/mapred/org/apache/hadoop/mapred/InputSplit.java:32: * @see InputFormat
./src/mapred/org/apache/hadoop/mapred/InputSplit.java:33: * @see RecordReader
./src/mapred/org/apache/hadoop/mapred/InputSplit.java:40:   * @return the number of bytes in the input split.
./src/mapred/org/apache/hadoop/mapred/InputSplit.java:41:   * @throws IOException
./src/mapred/org/apache/hadoop/mapred/InputSplit.java:48:   * @return list of hostnames where data of the <code>InputSplit</code> is
./src/mapred/org/apache/hadoop/mapred/InputSplit.java:50:   * @throws IOException
./src/mapred/org/apache/hadoop/mapred/InterTrackerProtocol.java:33:   * {@link #heartbeat(TaskTrackerStatus, boolean, boolean, short)}
./src/mapred/org/apache/hadoop/mapred/InterTrackerProtocol.java:62:   * Called regularly by the {@link TaskTracker} to update the status of its 
./src/mapred/org/apache/hadoop/mapred/InterTrackerProtocol.java:63:   * tasks within the job tracker. {@link JobTracker} responds with a 
./src/mapred/org/apache/hadoop/mapred/InterTrackerProtocol.java:64:   * {@link HeartbeatResponse} that directs the 
./src/mapred/org/apache/hadoop/mapred/InterTrackerProtocol.java:65:   * {@link TaskTracker} to undertake a series of 'actions' 
./src/mapred/org/apache/hadoop/mapred/InterTrackerProtocol.java:66:   * (see {@link org.apache.hadoop.mapred.TaskTrackerAction.ActionType}).  
./src/mapred/org/apache/hadoop/mapred/InterTrackerProtocol.java:68:   * {@link TaskTracker} must also indicate whether this is the first 
./src/mapred/org/apache/hadoop/mapred/InterTrackerProtocol.java:70:   * it recieved from the {@link JobTracker} 
./src/mapred/org/apache/hadoop/mapred/InterTrackerProtocol.java:72:   * @param status the status update
./src/mapred/org/apache/hadoop/mapred/InterTrackerProtocol.java:73:   * @param initialContact <code>true</code> if this is first interaction since
./src/mapred/org/apache/hadoop/mapred/InterTrackerProtocol.java:75:   * @param acceptNewTasks <code>true</code> if the {@link TaskTracker} is
./src/mapred/org/apache/hadoop/mapred/InterTrackerProtocol.java:77:   * @param responseId the last responseId successfully acted upon by the
./src/mapred/org/apache/hadoop/mapred/InterTrackerProtocol.java:78:   *                   {@link TaskTracker}.
./src/mapred/org/apache/hadoop/mapred/InterTrackerProtocol.java:79:   * @return a {@link org.apache.hadoop.mapred.HeartbeatResponse} with 
./src/mapred/org/apache/hadoop/mapred/InterTrackerProtocol.java:94:   * @param taskTracker the name of the task tracker
./src/mapred/org/apache/hadoop/mapred/InterTrackerProtocol.java:95:   * @param errorClass the kind of error (eg. the class that was thrown)
./src/mapred/org/apache/hadoop/mapred/InterTrackerProtocol.java:96:   * @param errorMessage the human readable error message
./src/mapred/org/apache/hadoop/mapred/InterTrackerProtocol.java:97:   * @throws IOException if there was a problem in communication or on the
./src/mapred/org/apache/hadoop/mapred/InterTrackerProtocol.java:106:   * @param jobid job id 
./src/mapred/org/apache/hadoop/mapred/InterTrackerProtocol.java:107:   * @param fromEventId event id to start from. 
./src/mapred/org/apache/hadoop/mapred/InterTrackerProtocol.java:108:   * @param maxEvents the max number of events we want to look at
./src/mapred/org/apache/hadoop/mapred/InterTrackerProtocol.java:109:   * @return array of task completion events. 
./src/mapred/org/apache/hadoop/mapred/InterTrackerProtocol.java:110:   * @throws IOException
./src/mapred/org/apache/hadoop/mapred/InterTrackerProtocol.java:118:   * @return the system directory where job-specific files are to be placed.
./src/mapred/org/apache/hadoop/mapred/InvalidInputException.java:34:   * @param probs the list of problems to report. this list is not copied.
./src/mapred/org/apache/hadoop/mapred/InvalidInputException.java:42:   * @return the list of problems, which must not be modified
./src/mapred/org/apache/hadoop/mapred/InvalidInputException.java:50:   * @return the concatenated messages from all of the problems.
./src/mapred/org/apache/hadoop/mapred/IsolationRunner.java:132:   * @param fs the filesystem to create the files in
./src/mapred/org/apache/hadoop/mapred/IsolationRunner.java:133:   * @param dir the directory name to create the files in
./src/mapred/org/apache/hadoop/mapred/IsolationRunner.java:134:   * @param conf the jobconf
./src/mapred/org/apache/hadoop/mapred/IsolationRunner.java:135:   * @throws IOException if something goes wrong writing
./src/mapred/org/apache/hadoop/mapred/IsolationRunner.java:160:   * @param args the first argument is the task directory
./src/mapred/org/apache/hadoop/mapred/JobChangeEvent.java:21: * {@link JobChangeEvent} is used to capture state changes in a job. A job can 
./src/mapred/org/apache/hadoop/mapred/JobClient.java:72: * with the {@link JobTracker}.
./src/mapred/org/apache/hadoop/mapred/JobClient.java:84: *   Computing the {@link InputSplit}s for the job.
./src/mapred/org/apache/hadoop/mapred/JobClient.java:87: *   Setup the requisite accounting information for the {@link DistributedCache} 
./src/mapred/org/apache/hadoop/mapred/JobClient.java:101: * job via {@link JobConf} and then uses the <code>JobClient</code> to submit 
./src/mapred/org/apache/hadoop/mapred/JobClient.java:134: *   {@link #runJob(JobConf)} : submits the job and returns only after 
./src/mapred/org/apache/hadoop/mapred/JobClient.java:138: *   {@link #submitJob(JobConf)} : only submits the job, then poll the 
./src/mapred/org/apache/hadoop/mapred/JobClient.java:139: *   returned handle to the {@link RunningJob} to query status and make 
./src/mapred/org/apache/hadoop/mapred/JobClient.java:143: *   {@link JobConf#setJobEndNotificationURI(String)} : setup a notification
./src/mapred/org/apache/hadoop/mapred/JobClient.java:148: * @see JobConf
./src/mapred/org/apache/hadoop/mapred/JobClient.java:149: * @see ClusterStatus
./src/mapred/org/apache/hadoop/mapred/JobClient.java:150: * @see Tool
./src/mapred/org/apache/hadoop/mapred/JobClient.java:151: * @see DistributedCache
./src/mapred/org/apache/hadoop/mapred/JobClient.java:195:     * @throws IOException
./src/mapred/org/apache/hadoop/mapred/JobClient.java:209:    /** @deprecated This method is deprecated and will be removed. Applications should 
./src/mapred/org/apache/hadoop/mapred/JobClient.java:210:     * rather use {@link #getID()}.*/
./src/mapred/org/apache/hadoop/mapred/JobClient.java:211:    @Deprecated
./src/mapred/org/apache/hadoop/mapred/JobClient.java:320:    * @param priority new priority of the job. 
./src/mapred/org/apache/hadoop/mapred/JobClient.java:329:     * @param taskId the id of the task to kill.
./src/mapred/org/apache/hadoop/mapred/JobClient.java:330:     * @param shouldFail if true the task is failed and added to failed tasks list, otherwise
./src/mapred/org/apache/hadoop/mapred/JobClient.java:337:    /** @deprecated Applications should rather use {@link #killTask(TaskAttemptID, boolean)}*/
./src/mapred/org/apache/hadoop/mapred/JobClient.java:338:    @Deprecated
./src/mapred/org/apache/hadoop/mapred/JobClient.java:355:    @Override
./src/mapred/org/apache/hadoop/mapred/JobClient.java:390:   * Build a job client with the given {@link JobConf}, and connect to the 
./src/mapred/org/apache/hadoop/mapred/JobClient.java:391:   * default {@link JobTracker}.
./src/mapred/org/apache/hadoop/mapred/JobClient.java:393:   * @param conf the job configuration.
./src/mapred/org/apache/hadoop/mapred/JobClient.java:394:   * @throws IOException
./src/mapred/org/apache/hadoop/mapred/JobClient.java:405:   * @param conf the configuration object to set.
./src/mapred/org/apache/hadoop/mapred/JobClient.java:420:   * Connect to the default {@link JobTracker}.
./src/mapred/org/apache/hadoop/mapred/JobClient.java:421:   * @param conf the job configuration.
./src/mapred/org/apache/hadoop/mapred/JobClient.java:422:   * @throws IOException
./src/mapred/org/apache/hadoop/mapred/JobClient.java:443:   * @param jobTrackAddr the job tracker to connect to.
./src/mapred/org/apache/hadoop/mapred/JobClient.java:444:   * @param conf configuration.
./src/mapred/org/apache/hadoop/mapred/JobClient.java:464:   * @return the filesystem handle.
./src/mapred/org/apache/hadoop/mapred/JobClient.java:538:   * @param conf
./src/mapred/org/apache/hadoop/mapred/JobClient.java:539:   * @throws IOException
./src/mapred/org/apache/hadoop/mapred/JobClient.java:726:   * This returns a handle to the {@link RunningJob} which can be used to track
./src/mapred/org/apache/hadoop/mapred/JobClient.java:729:   * @param jobFile the job configuration.
./src/mapred/org/apache/hadoop/mapred/JobClient.java:730:   * @return a handle to the {@link RunningJob} which can be used to track the
./src/mapred/org/apache/hadoop/mapred/JobClient.java:732:   * @throws FileNotFoundException
./src/mapred/org/apache/hadoop/mapred/JobClient.java:733:   * @throws InvalidJobConfException
./src/mapred/org/apache/hadoop/mapred/JobClient.java:734:   * @throws IOException
./src/mapred/org/apache/hadoop/mapred/JobClient.java:754:   * This returns a handle to the {@link RunningJob} which can be used to track
./src/mapred/org/apache/hadoop/mapred/JobClient.java:757:   * @param job the job configuration.
./src/mapred/org/apache/hadoop/mapred/JobClient.java:758:   * @return a handle to the {@link RunningJob} which can be used to track the
./src/mapred/org/apache/hadoop/mapred/JobClient.java:760:   * @throws FileNotFoundException
./src/mapred/org/apache/hadoop/mapred/JobClient.java:761:   * @throws InvalidJobConfException
./src/mapred/org/apache/hadoop/mapred/JobClient.java:762:   * @throws IOException
./src/mapred/org/apache/hadoop/mapred/JobClient.java:936:   * @param splits the input splits to write out
./src/mapred/org/apache/hadoop/mapred/JobClient.java:937:   * @param out the stream to write to
./src/mapred/org/apache/hadoop/mapred/JobClient.java:958:   * @param in the stream to read from
./src/mapred/org/apache/hadoop/mapred/JobClient.java:959:   * @return the complete list of splits
./src/mapred/org/apache/hadoop/mapred/JobClient.java:960:   * @throws IOException
./src/mapred/org/apache/hadoop/mapred/JobClient.java:982:   * Get an {@link RunningJob} object to track an ongoing job.  Returns
./src/mapred/org/apache/hadoop/mapred/JobClient.java:985:   * @param jobid the jobid of the job.
./src/mapred/org/apache/hadoop/mapred/JobClient.java:986:   * @return the {@link RunningJob} handle to track the job, null if the 
./src/mapred/org/apache/hadoop/mapred/JobClient.java:988:   * @throws IOException
./src/mapred/org/apache/hadoop/mapred/JobClient.java:999:  /**@deprecated Applications should rather use {@link #getJob(JobID)}. 
./src/mapred/org/apache/hadoop/mapred/JobClient.java:1001:  @Deprecated
./src/mapred/org/apache/hadoop/mapred/JobClient.java:1009:   * @param jobId the job to query.
./src/mapred/org/apache/hadoop/mapred/JobClient.java:1010:   * @return the list of all of the map tips.
./src/mapred/org/apache/hadoop/mapred/JobClient.java:1011:   * @throws IOException
./src/mapred/org/apache/hadoop/mapred/JobClient.java:1017:  /**@deprecated Applications should rather use {@link #getMapTaskReports(JobID)}*/
./src/mapred/org/apache/hadoop/mapred/JobClient.java:1018:  @Deprecated
./src/mapred/org/apache/hadoop/mapred/JobClient.java:1026:   * @param jobId the job to query.
./src/mapred/org/apache/hadoop/mapred/JobClient.java:1027:   * @return the list of all of the reduce tips.
./src/mapred/org/apache/hadoop/mapred/JobClient.java:1028:   * @throws IOException
./src/mapred/org/apache/hadoop/mapred/JobClient.java:1037:   * @param jobId the job to query.
./src/mapred/org/apache/hadoop/mapred/JobClient.java:1038:   * @return the list of all of the cleanup tips.
./src/mapred/org/apache/hadoop/mapred/JobClient.java:1039:   * @throws IOException
./src/mapred/org/apache/hadoop/mapred/JobClient.java:1048:   * @param jobId the job to query.
./src/mapred/org/apache/hadoop/mapred/JobClient.java:1049:   * @return the list of all of the setup tips.
./src/mapred/org/apache/hadoop/mapred/JobClient.java:1050:   * @throws IOException
./src/mapred/org/apache/hadoop/mapred/JobClient.java:1056:  /**@deprecated Applications should rather use {@link #getReduceTaskReports(JobID)}*/
./src/mapred/org/apache/hadoop/mapred/JobClient.java:1057:  @Deprecated
./src/mapred/org/apache/hadoop/mapred/JobClient.java:1065:   * @return the status information about the Map-Reduce cluster as an object
./src/mapred/org/apache/hadoop/mapred/JobClient.java:1066:   *         of {@link ClusterStatus}.
./src/mapred/org/apache/hadoop/mapred/JobClient.java:1067:   * @throws IOException
./src/mapred/org/apache/hadoop/mapred/JobClient.java:1077:   * @return array of {@link JobStatus} for the running/to-be-run jobs.
./src/mapred/org/apache/hadoop/mapred/JobClient.java:1078:   * @throws IOException
./src/mapred/org/apache/hadoop/mapred/JobClient.java:1097:   * @return array of {@link JobStatus} for the submitted jobs.
./src/mapred/org/apache/hadoop/mapred/JobClient.java:1098:   * @throws IOException
./src/mapred/org/apache/hadoop/mapred/JobClient.java:1108:   * @param job the job configuration.
./src/mapred/org/apache/hadoop/mapred/JobClient.java:1109:   * @throws IOException
./src/mapred/org/apache/hadoop/mapred/JobClient.java:1295:   * @param newValue task filter.
./src/mapred/org/apache/hadoop/mapred/JobClient.java:1297:  @Deprecated
./src/mapred/org/apache/hadoop/mapred/JobClient.java:1305:   * @param job the JobConf to examine.
./src/mapred/org/apache/hadoop/mapred/JobClient.java:1306:   * @return the filter level.
./src/mapred/org/apache/hadoop/mapred/JobClient.java:1316:   * @param job the JobConf to modify.
./src/mapred/org/apache/hadoop/mapred/JobClient.java:1317:   * @param newValue the value to set.
./src/mapred/org/apache/hadoop/mapred/JobClient.java:1326:   * @return task filter. 
./src/mapred/org/apache/hadoop/mapred/JobClient.java:1328:  @Deprecated
./src/mapred/org/apache/hadoop/mapred/JobClient.java:1603:   * @param jobId the job id for the job's events to list
./src/mapred/org/apache/hadoop/mapred/JobClient.java:1604:   * @throws IOException
./src/mapred/org/apache/hadoop/mapred/JobClient.java:1622:   * @throws IOException
./src/mapred/org/apache/hadoop/mapred/JobClient.java:1635:   * @throws IOException
./src/mapred/org/apache/hadoop/mapred/JobClient.java:1659:   * @return the max available Maps in the cluster
./src/mapred/org/apache/hadoop/mapred/JobClient.java:1660:   * @throws IOException
./src/mapred/org/apache/hadoop/mapred/JobClient.java:1669:   * @return the max available Reduces in the cluster
./src/mapred/org/apache/hadoop/mapred/JobClient.java:1670:   * @throws IOException
./src/mapred/org/apache/hadoop/mapred/JobClient.java:1679:   * @return the system directory where job-specific files are to be placed.
./src/mapred/org/apache/hadoop/mapred/JobClient.java:1693:   * @return Array of JobQueueInfo objects
./src/mapred/org/apache/hadoop/mapred/JobClient.java:1694:   * @throws IOException
./src/mapred/org/apache/hadoop/mapred/JobClient.java:1703:   * @param queueName name of the Job Queue
./src/mapred/org/apache/hadoop/mapred/JobClient.java:1704:   * @return Array of jobs present in the job queue
./src/mapred/org/apache/hadoop/mapred/JobClient.java:1705:   * @throws IOException
./src/mapred/org/apache/hadoop/mapred/JobClient.java:1715:   * @param queueName name of the job queue.
./src/mapred/org/apache/hadoop/mapred/JobClient.java:1716:   * @return Queue information associated to particular queue.
./src/mapred/org/apache/hadoop/mapred/JobClient.java:1717:   * @throws IOException
./src/mapred/org/apache/hadoop/mapred/JobConf.java:56: *   <a href="{@docRoot}/org/apache/hadoop/conf/Configuration.html#FinalParams">
./src/mapred/org/apache/hadoop/mapred/JobConf.java:61: *   (e.g. {@link #setNumReduceTasks(int)}), some parameters interact subtly 
./src/mapred/org/apache/hadoop/mapred/JobConf.java:63: *   complex for the user to control finely (e.g. {@link #setNumMapTasks(int)}).
./src/mapred/org/apache/hadoop/mapred/JobConf.java:67: * <p><code>JobConf</code> typically specifies the {@link Mapper}, combiner 
./src/mapred/org/apache/hadoop/mapred/JobConf.java:68: * (if any), {@link Partitioner}, {@link Reducer}, {@link InputFormat} and 
./src/mapred/org/apache/hadoop/mapred/JobConf.java:69: * {@link OutputFormat} implementations to be used etc.
./src/mapred/org/apache/hadoop/mapred/JobConf.java:73: * the {@link DistributedCache}, whether or not intermediate and/or job outputs 
./src/mapred/org/apache/hadoop/mapred/JobConf.java:75: * ( {@link #setMapDebugScript(String)}/{@link #setReduceDebugScript(String)}),
./src/mapred/org/apache/hadoop/mapred/JobConf.java:98: * @see JobClient
./src/mapred/org/apache/hadoop/mapred/JobConf.java:99: * @see ClusterStatus
./src/mapred/org/apache/hadoop/mapred/JobConf.java:100: * @see Tool
./src/mapred/org/apache/hadoop/mapred/JobConf.java:101: * @see DistributedCache
./src/mapred/org/apache/hadoop/mapred/JobConf.java:127:   * @param exampleClass a class whose containing jar is used as the job's jar.
./src/mapred/org/apache/hadoop/mapred/JobConf.java:136:   * @param conf a Configuration whose settings will be inherited.
./src/mapred/org/apache/hadoop/mapred/JobConf.java:145:   * @param conf a Configuration whose settings will be inherited.
./src/mapred/org/apache/hadoop/mapred/JobConf.java:146:   * @param exampleClass a class whose containing jar is used as the job's jar.
./src/mapred/org/apache/hadoop/mapred/JobConf.java:156:   * @param config a Configuration-format XML job description file.
./src/mapred/org/apache/hadoop/mapred/JobConf.java:164:   * @param config a Configuration-format XML job description file.
./src/mapred/org/apache/hadoop/mapred/JobConf.java:174:   * If the parameter {@code loadDefaults} is false, the new instance
./src/mapred/org/apache/hadoop/mapred/JobConf.java:177:   * @param loadDefaults specifies whether to load from the default files
./src/mapred/org/apache/hadoop/mapred/JobConf.java:186:   * @return the user jar for the map-reduce job.
./src/mapred/org/apache/hadoop/mapred/JobConf.java:193:   * @param jar the user jar for the map-reduce job.
./src/mapred/org/apache/hadoop/mapred/JobConf.java:200:   * @param cls the example class.
./src/mapred/org/apache/hadoop/mapred/JobConf.java:238:   * @return the username
./src/mapred/org/apache/hadoop/mapred/JobConf.java:247:   * @param user the username for this job.
./src/mapred/org/apache/hadoop/mapred/JobConf.java:259:   * @param keep <code>true</code> if framework should keep the intermediate files 
./src/mapred/org/apache/hadoop/mapred/JobConf.java:270:   * @return should the files be kept?
./src/mapred/org/apache/hadoop/mapred/JobConf.java:281:   * @param pattern the java.util.regex.Pattern to match against the 
./src/mapred/org/apache/hadoop/mapred/JobConf.java:292:   * @return the pattern as a string, if it was set, othewise null.
./src/mapred/org/apache/hadoop/mapred/JobConf.java:301:   * @param dir the new current working directory.
./src/mapred/org/apache/hadoop/mapred/JobConf.java:311:   * @return the directory name.
./src/mapred/org/apache/hadoop/mapred/JobConf.java:331:   * @param numTasks the number of tasks to execute; defaults to 1;
./src/mapred/org/apache/hadoop/mapred/JobConf.java:346:   * Get the {@link InputFormat} implementation for the map-reduce job,
./src/mapred/org/apache/hadoop/mapred/JobConf.java:347:   * defaults to {@link TextInputFormat} if not specified explicity.
./src/mapred/org/apache/hadoop/mapred/JobConf.java:349:   * @return the {@link InputFormat} implementation for the map-reduce job.
./src/mapred/org/apache/hadoop/mapred/JobConf.java:359:   * Set the {@link InputFormat} implementation for the map-reduce job.
./src/mapred/org/apache/hadoop/mapred/JobConf.java:361:   * @param theClass the {@link InputFormat} implementation for the map-reduce 
./src/mapred/org/apache/hadoop/mapred/JobConf.java:369:   * Get the {@link OutputFormat} implementation for the map-reduce job,
./src/mapred/org/apache/hadoop/mapred/JobConf.java:370:   * defaults to {@link TextOutputFormat} if not specified explicity.
./src/mapred/org/apache/hadoop/mapred/JobConf.java:372:   * @return the {@link OutputFormat} implementation for the map-reduce job.
./src/mapred/org/apache/hadoop/mapred/JobConf.java:382:   * Get the {@link OutputCommitter} implementation for the map-reduce job,
./src/mapred/org/apache/hadoop/mapred/JobConf.java:383:   * defaults to {@link FileOutputCommitter} if not specified explicitly.
./src/mapred/org/apache/hadoop/mapred/JobConf.java:385:   * @return the {@link OutputCommitter} implementation for the map-reduce job.
./src/mapred/org/apache/hadoop/mapred/JobConf.java:394:   * Set the {@link OutputCommitter} implementation for the map-reduce job.
./src/mapred/org/apache/hadoop/mapred/JobConf.java:396:   * @param theClass the {@link OutputCommitter} implementation for the map-reduce 
./src/mapred/org/apache/hadoop/mapred/JobConf.java:404:   * Set the {@link OutputFormat} implementation for the map-reduce job.
./src/mapred/org/apache/hadoop/mapred/JobConf.java:406:   * @param theClass the {@link OutputFormat} implementation for the map-reduce 
./src/mapred/org/apache/hadoop/mapred/JobConf.java:417:   * @param compress should the map outputs be compressed?
./src/mapred/org/apache/hadoop/mapred/JobConf.java:426:   * @return <code>true</code> if the outputs of the maps are to be compressed,
./src/mapred/org/apache/hadoop/mapred/JobConf.java:434:   * Set the given class as the  {@link CompressionCodec} for the map outputs.
./src/mapred/org/apache/hadoop/mapred/JobConf.java:436:   * @param codecClass the {@link CompressionCodec} class that will compress  
./src/mapred/org/apache/hadoop/mapred/JobConf.java:447:   * Get the {@link CompressionCodec} for compressing the map outputs.
./src/mapred/org/apache/hadoop/mapred/JobConf.java:449:   * @param defaultValue the {@link CompressionCodec} to return if not set
./src/mapred/org/apache/hadoop/mapred/JobConf.java:450:   * @return the {@link CompressionCodec} class that should be used to compress the 
./src/mapred/org/apache/hadoop/mapred/JobConf.java:452:   * @throws IllegalArgumentException if the class was specified, but not found
./src/mapred/org/apache/hadoop/mapred/JobConf.java:474:   * @return the map output key class.
./src/mapred/org/apache/hadoop/mapred/JobConf.java:489:   * @param theClass the map output key class.
./src/mapred/org/apache/hadoop/mapred/JobConf.java:500:   * @return the map output value class.
./src/mapred/org/apache/hadoop/mapred/JobConf.java:516:   * @param theClass the map output value class.
./src/mapred/org/apache/hadoop/mapred/JobConf.java:525:   * @return the key class for the job output data.
./src/mapred/org/apache/hadoop/mapred/JobConf.java:535:   * @param theClass the key class for the job output data.
./src/mapred/org/apache/hadoop/mapred/JobConf.java:542:   * Get the {@link RawComparator} comparator used to compare keys.
./src/mapred/org/apache/hadoop/mapred/JobConf.java:544:   * @return the {@link RawComparator} comparator used to compare keys.
./src/mapred/org/apache/hadoop/mapred/JobConf.java:555:   * Set the {@link RawComparator} comparator used to compare keys.
./src/mapred/org/apache/hadoop/mapred/JobConf.java:557:   * @param theClass the {@link RawComparator} comparator used to 
./src/mapred/org/apache/hadoop/mapred/JobConf.java:559:   * @see #setOutputValueGroupingComparator(Class)                 
./src/mapred/org/apache/hadoop/mapred/JobConf.java:567:   * Set the {@link KeyFieldBasedComparator} options used to compare keys.
./src/mapred/org/apache/hadoop/mapred/JobConf.java:569:   * @param keySpec the key specification of the form -k pos1[,pos2], where,
./src/mapred/org/apache/hadoop/mapred/JobConf.java:587:   * Get the {@link KeyFieldBasedComparator} options
./src/mapred/org/apache/hadoop/mapred/JobConf.java:594:   * Set the {@link KeyFieldBasedPartitioner} options used for 
./src/mapred/org/apache/hadoop/mapred/JobConf.java:595:   * {@link Partitioner}
./src/mapred/org/apache/hadoop/mapred/JobConf.java:597:   * @param keySpec the key specification of the form -k pos1[,pos2], where,
./src/mapred/org/apache/hadoop/mapred/JobConf.java:612:   * Get the {@link KeyFieldBasedPartitioner} options
./src/mapred/org/apache/hadoop/mapred/JobConf.java:619:   * Get the user defined {@link WritableComparable} comparator for 
./src/mapred/org/apache/hadoop/mapred/JobConf.java:622:   * @return comparator set by the user for grouping values.
./src/mapred/org/apache/hadoop/mapred/JobConf.java:623:   * @see #setOutputValueGroupingComparator(Class) for details.  
./src/mapred/org/apache/hadoop/mapred/JobConf.java:636:   * Set the user defined {@link RawComparator} comparator for 
./src/mapred/org/apache/hadoop/mapred/JobConf.java:642:   * {@link Reducer#reduce(Object, java.util.Iterator, OutputCollector, Reporter)}.</p>
./src/mapred/org/apache/hadoop/mapred/JobConf.java:647:   * <p>Since {@link #setOutputKeyComparatorClass(Class)} can be used to control 
./src/mapred/org/apache/hadoop/mapred/JobConf.java:656:   * @param theClass the comparator class to be used for grouping keys. 
./src/mapred/org/apache/hadoop/mapred/JobConf.java:658:   * @see #setOutputKeyComparatorClass(Class)                 
./src/mapred/org/apache/hadoop/mapred/JobConf.java:669:   * @return the value class for job outputs.
./src/mapred/org/apache/hadoop/mapred/JobConf.java:678:   * @param theClass the value class for job outputs.
./src/mapred/org/apache/hadoop/mapred/JobConf.java:685:   * Get the {@link Mapper} class for the job.
./src/mapred/org/apache/hadoop/mapred/JobConf.java:687:   * @return the {@link Mapper} class for the job.
./src/mapred/org/apache/hadoop/mapred/JobConf.java:694:   * Set the {@link Mapper} class for the job.
./src/mapred/org/apache/hadoop/mapred/JobConf.java:696:   * @param theClass the {@link Mapper} class for the job.
./src/mapred/org/apache/hadoop/mapred/JobConf.java:703:   * Get the {@link MapRunnable} class for the job.
./src/mapred/org/apache/hadoop/mapred/JobConf.java:705:   * @return the {@link MapRunnable} class for the job.
./src/mapred/org/apache/hadoop/mapred/JobConf.java:713:   * Expert: Set the {@link MapRunnable} class for the job.
./src/mapred/org/apache/hadoop/mapred/JobConf.java:715:   * Typically used to exert greater control on {@link Mapper}s.
./src/mapred/org/apache/hadoop/mapred/JobConf.java:717:   * @param theClass the {@link MapRunnable} class for the job.
./src/mapred/org/apache/hadoop/mapred/JobConf.java:724:   * Get the {@link Partitioner} used to partition {@link Mapper}-outputs 
./src/mapred/org/apache/hadoop/mapred/JobConf.java:725:   * to be sent to the {@link Reducer}s.
./src/mapred/org/apache/hadoop/mapred/JobConf.java:727:   * @return the {@link Partitioner} used to partition map-outputs.
./src/mapred/org/apache/hadoop/mapred/JobConf.java:735:   * Set the {@link Partitioner} class used to partition 
./src/mapred/org/apache/hadoop/mapred/JobConf.java:736:   * {@link Mapper}-outputs to be sent to the {@link Reducer}s.
./src/mapred/org/apache/hadoop/mapred/JobConf.java:738:   * @param theClass the {@link Partitioner} used to partition map-outputs.
./src/mapred/org/apache/hadoop/mapred/JobConf.java:745:   * Get the {@link Reducer} class for the job.
./src/mapred/org/apache/hadoop/mapred/JobConf.java:747:   * @return the {@link Reducer} class for the job.
./src/mapred/org/apache/hadoop/mapred/JobConf.java:755:   * Set the {@link Reducer} class for the job.
./src/mapred/org/apache/hadoop/mapred/JobConf.java:757:   * @param theClass the {@link Reducer} class for the job.
./src/mapred/org/apache/hadoop/mapred/JobConf.java:766:   * the {@link Reducer} for the job i.e. {@link #getReducerClass()}.
./src/mapred/org/apache/hadoop/mapred/JobConf.java:768:   * @return the user-defined combiner class used to combine map-outputs.
./src/mapred/org/apache/hadoop/mapred/JobConf.java:779:   * helps to cut down the amount of data transferred from the {@link Mapper} to
./src/mapred/org/apache/hadoop/mapred/JobConf.java:780:   * the {@link Reducer}, leading to better performance.</p>
./src/mapred/org/apache/hadoop/mapred/JobConf.java:783:   * job i.e. {@link #setReducerClass(Class)}.</p>
./src/mapred/org/apache/hadoop/mapred/JobConf.java:785:   * @param theClass the user-defined combiner class used to combine 
./src/mapred/org/apache/hadoop/mapred/JobConf.java:796:   * @return <code>true</code> if speculative execution be used for this job,
./src/mapred/org/apache/hadoop/mapred/JobConf.java:806:   * @param speculativeExecution <code>true</code> if speculative execution 
./src/mapred/org/apache/hadoop/mapred/JobConf.java:818:   * @return <code>true</code> if speculative execution be 
./src/mapred/org/apache/hadoop/mapred/JobConf.java:829:   * @param speculativeExecution <code>true</code> if speculative execution 
./src/mapred/org/apache/hadoop/mapred/JobConf.java:841:   * @return <code>true</code> if speculative execution be used 
./src/mapred/org/apache/hadoop/mapred/JobConf.java:852:   * @param speculativeExecution <code>true</code> if speculative execution 
./src/mapred/org/apache/hadoop/mapred/JobConf.java:865:   * @return the number of reduce tasks for this job.
./src/mapred/org/apache/hadoop/mapred/JobConf.java:873:   * number of spawned map tasks depends on the number of {@link InputSplit}s 
./src/mapred/org/apache/hadoop/mapred/JobConf.java:874:   * generated by the job's {@link InputFormat#getSplits(JobConf, int)}.
./src/mapred/org/apache/hadoop/mapred/JobConf.java:876:   * A custom {@link InputFormat} is typically used to accurately control 
./src/mapred/org/apache/hadoop/mapred/JobConf.java:889:   * <p>The default behavior of file-based {@link InputFormat}s is to split the 
./src/mapred/org/apache/hadoop/mapred/JobConf.java:890:   * input into <i>logical</i> {@link InputSplit}s based on the total size, in 
./src/mapred/org/apache/hadoop/mapred/JobConf.java:891:   * bytes, of input files. However, the {@link FileSystem} blocksize of the 
./src/mapred/org/apache/hadoop/mapred/JobConf.java:894:   * <a href="{@docRoot}/../hadoop-default.html#mapred.min.split.size">
./src/mapred/org/apache/hadoop/mapred/JobConf.java:898:   * you'll end up with 82,000 maps, unless {@link #setNumMapTasks(int)} is 
./src/mapred/org/apache/hadoop/mapred/JobConf.java:901:   * @param n the number of map tasks for this job.
./src/mapred/org/apache/hadoop/mapred/JobConf.java:902:   * @see InputFormat#getSplits(JobConf, int)
./src/mapred/org/apache/hadoop/mapred/JobConf.java:903:   * @see FileInputFormat
./src/mapred/org/apache/hadoop/mapred/JobConf.java:904:   * @see FileSystem#getDefaultBlockSize()
./src/mapred/org/apache/hadoop/mapred/JobConf.java:905:   * @see FileStatus#getBlockSize()
./src/mapred/org/apache/hadoop/mapred/JobConf.java:913:   * @return the number of reduce tasks for this job.
./src/mapred/org/apache/hadoop/mapred/JobConf.java:924:   * <a href="{@docRoot}/../hadoop-default.html#mapred.tasktracker.reduce.tasks.maximum">
./src/mapred/org/apache/hadoop/mapred/JobConf.java:946:   * {@link FileOutputFormat#setOutputPath(JobConf, Path)}. Also, the 
./src/mapred/org/apache/hadoop/mapred/JobConf.java:949:   * @param n the number of reduce tasks for this job.
./src/mapred/org/apache/hadoop/mapred/JobConf.java:958:   * @return the max number of attempts per map task.
./src/mapred/org/apache/hadoop/mapred/JobConf.java:968:   * @param n the number of attempts per map task.
./src/mapred/org/apache/hadoop/mapred/JobConf.java:979:   * @return the max number of attempts per reduce task.
./src/mapred/org/apache/hadoop/mapred/JobConf.java:988:   * @param n the number of attempts per reduce task.
./src/mapred/org/apache/hadoop/mapred/JobConf.java:998:   * @return the job's name, defaulting to "".
./src/mapred/org/apache/hadoop/mapred/JobConf.java:1007:   * @param name the job's new name.
./src/mapred/org/apache/hadoop/mapred/JobConf.java:1026:   * @return the session identifier, defaulting to "".
./src/mapred/org/apache/hadoop/mapred/JobConf.java:1035:   * @param sessionId the new session id.
./src/mapred/org/apache/hadoop/mapred/JobConf.java:1046:   * @param noFailures maximum no. of failures of a given job per tasktracker.
./src/mapred/org/apache/hadoop/mapred/JobConf.java:1057:   * @return the maximum no. of failures of a given job per tasktracker.
./src/mapred/org/apache/hadoop/mapred/JobConf.java:1067:   * Each map task is executed a minimum of {@link #getMaxMapAttempts()} 
./src/mapred/org/apache/hadoop/mapred/JobConf.java:1071:   * the job being declared as {@link JobStatus#FAILED}.
./src/mapred/org/apache/hadoop/mapred/JobConf.java:1073:   * @return the maximum percentage of map tasks that can fail without
./src/mapred/org/apache/hadoop/mapred/JobConf.java:1084:   * Each map task is executed a minimum of {@link #getMaxMapAttempts} attempts 
./src/mapred/org/apache/hadoop/mapred/JobConf.java:1087:   * @param percent the maximum percentage of map tasks that can fail without 
./src/mapred/org/apache/hadoop/mapred/JobConf.java:1098:   * Each reduce task is executed a minimum of {@link #getMaxReduceAttempts()} 
./src/mapred/org/apache/hadoop/mapred/JobConf.java:1102:   * in the job being declared as {@link JobStatus#FAILED}.
./src/mapred/org/apache/hadoop/mapred/JobConf.java:1104:   * @return the maximum percentage of reduce tasks that can fail without
./src/mapred/org/apache/hadoop/mapred/JobConf.java:1115:   * Each reduce task is executed a minimum of {@link #getMaxReduceAttempts()} 
./src/mapred/org/apache/hadoop/mapred/JobConf.java:1118:   * @param percent the maximum percentage of reduce tasks that can fail without 
./src/mapred/org/apache/hadoop/mapred/JobConf.java:1126:   * Set {@link JobPriority} for this job.
./src/mapred/org/apache/hadoop/mapred/JobConf.java:1128:   * @param prio the {@link JobPriority} for this job.
./src/mapred/org/apache/hadoop/mapred/JobConf.java:1135:   * Get the {@link JobPriority} for this job.
./src/mapred/org/apache/hadoop/mapred/JobConf.java:1137:   * @return the {@link JobPriority} for this job.
./src/mapred/org/apache/hadoop/mapred/JobConf.java:1150:   * @return true if some tasks will be profiled
./src/mapred/org/apache/hadoop/mapred/JobConf.java:1160:   * @param newValue true means it should be gathered
./src/mapred/org/apache/hadoop/mapred/JobConf.java:1172:   * @return the parameters to pass to the task child to configure profiling
./src/mapred/org/apache/hadoop/mapred/JobConf.java:1187:   * @param value the configuration string
./src/mapred/org/apache/hadoop/mapred/JobConf.java:1195:   * @param isMap is the task a map?
./src/mapred/org/apache/hadoop/mapred/JobConf.java:1196:   * @return the task ranges
./src/mapred/org/apache/hadoop/mapred/JobConf.java:1206:   * @param newValue a set of integer ranges of the map ids
./src/mapred/org/apache/hadoop/mapred/JobConf.java:1226:   * <p> The script file is distributed through {@link DistributedCache} 
./src/mapred/org/apache/hadoop/mapred/JobConf.java:1236:   * @param mDbgScript the script name
./src/mapred/org/apache/hadoop/mapred/JobConf.java:1245:   * @return the debug Script for the mapred job for failed map tasks.
./src/mapred/org/apache/hadoop/mapred/JobConf.java:1246:   * @see #setMapDebugScript(String)
./src/mapred/org/apache/hadoop/mapred/JobConf.java:1263:   * <p> The script file is distributed through {@link DistributedCache} 
./src/mapred/org/apache/hadoop/mapred/JobConf.java:1273:   * @param rDbgScript the script name
./src/mapred/org/apache/hadoop/mapred/JobConf.java:1282:   * @return the debug script for the mapred job for failed reduce tasks.
./src/mapred/org/apache/hadoop/mapred/JobConf.java:1283:   * @see #setReduceDebugScript(String)
./src/mapred/org/apache/hadoop/mapred/JobConf.java:1293:   * @return the job end notification uri, <code>null</code> if it hasn't
./src/mapred/org/apache/hadoop/mapred/JobConf.java:1295:   * @see #setJobEndNotificationURI(String)
./src/mapred/org/apache/hadoop/mapred/JobConf.java:1312:   * @param uri the job end notification uri
./src/mapred/org/apache/hadoop/mapred/JobConf.java:1313:   * @see JobStatus
./src/mapred/org/apache/hadoop/mapred/JobConf.java:1314:   * @see <a href="{@docRoot}/org/apache/hadoop/mapred/JobClient.html#JobCompletionAndChaining">Job Completion and Chaining</a>
./src/mapred/org/apache/hadoop/mapred/JobConf.java:1333:   * @return The localized job specific shared directory
./src/mapred/org/apache/hadoop/mapred/JobConf.java:1346:   * If set to {@link #DISABLED_VIRTUAL_MEMORY_LIMIT}, tasks are assured 
./src/mapred/org/apache/hadoop/mapred/JobConf.java:1351:   * @return The maximum amount of memory any task of this job will use, in kilobytes.
./src/mapred/org/apache/hadoop/mapred/JobConf.java:1352:   * @see #getMaxVirtualMemoryForTasks()
./src/mapred/org/apache/hadoop/mapred/JobConf.java:1361:   * @param vmem Maximum amount of memory in kilobytes any task of this job 
./src/mapred/org/apache/hadoop/mapred/JobConf.java:1363:   * @see #getMaxVirtualMemoryForTask()
./src/mapred/org/apache/hadoop/mapred/JobConf.java:1373:   * @return name of the queue
./src/mapred/org/apache/hadoop/mapred/JobConf.java:1382:   * @param queueName Name of the queue
./src/mapred/org/apache/hadoop/mapred/JobConf.java:1393:   * @param my_class the class to find.
./src/mapred/org/apache/hadoop/mapred/JobConf.java:1394:   * @return a jar file that contains the class, or null.
./src/mapred/org/apache/hadoop/mapred/JobConf.java:1395:   * @throws IOException
./src/mapred/org/apache/hadoop/mapred/JobContext.java:39:   * @return JobConf
./src/mapred/org/apache/hadoop/mapred/JobContext.java:48:   * @return progress mechanism 
./src/mapred/org/apache/hadoop/mapred/jobcontrol/Job.java:69:   * @param jobConf a mapred job configuration representing a job to be executed.
./src/mapred/org/apache/hadoop/mapred/jobcontrol/Job.java:70:   * @param dependingJobs an array of jobs the current job depends on
./src/mapred/org/apache/hadoop/mapred/jobcontrol/Job.java:86:   * @param jobConf mapred job configuration representing a job to be executed.
./src/mapred/org/apache/hadoop/mapred/jobcontrol/Job.java:87:   * @throws IOException
./src/mapred/org/apache/hadoop/mapred/jobcontrol/Job.java:93:  @Override
./src/mapred/org/apache/hadoop/mapred/jobcontrol/Job.java:116:   * @return the job name of this job
./src/mapred/org/apache/hadoop/mapred/jobcontrol/Job.java:124:   * @param jobName the job name
./src/mapred/org/apache/hadoop/mapred/jobcontrol/Job.java:131:   * @return the job ID of this job assigned by JobControl
./src/mapred/org/apache/hadoop/mapred/jobcontrol/Job.java:139:   * @param id the job ID
./src/mapred/org/apache/hadoop/mapred/jobcontrol/Job.java:146:   * @return the mapred ID of this job
./src/mapred/org/apache/hadoop/mapred/jobcontrol/Job.java:147:   * @deprecated use {@link #getAssignedJobID()} instead
./src/mapred/org/apache/hadoop/mapred/jobcontrol/Job.java:149:  @Deprecated
./src/mapred/org/apache/hadoop/mapred/jobcontrol/Job.java:156:   * @param mapredJobID the mapred job ID for this job.
./src/mapred/org/apache/hadoop/mapred/jobcontrol/Job.java:157:   * @deprecated use {@link #setAssignedJobID(JobID)} instead
./src/mapred/org/apache/hadoop/mapred/jobcontrol/Job.java:159:  @Deprecated
./src/mapred/org/apache/hadoop/mapred/jobcontrol/Job.java:165:   * @return the mapred ID of this job as assigned by the 
./src/mapred/org/apache/hadoop/mapred/jobcontrol/Job.java:175:   * @param mapredJobID the mapred job ID for this job.
./src/mapred/org/apache/hadoop/mapred/jobcontrol/Job.java:182:   * @return the mapred job conf of this job
./src/mapred/org/apache/hadoop/mapred/jobcontrol/Job.java:191:   * @param jobConf the mapred job conf for this job.
./src/mapred/org/apache/hadoop/mapred/jobcontrol/Job.java:198:   * @return the state of this job
./src/mapred/org/apache/hadoop/mapred/jobcontrol/Job.java:206:   * @param state the new state for this job.
./src/mapred/org/apache/hadoop/mapred/jobcontrol/Job.java:213:   * @return the message of this job
./src/mapred/org/apache/hadoop/mapred/jobcontrol/Job.java:221:   * @param message the message for this job.
./src/mapred/org/apache/hadoop/mapred/jobcontrol/Job.java:229:   * @return the job client of this job
./src/mapred/org/apache/hadoop/mapred/jobcontrol/Job.java:236:   * @return the depending jobs of this job
./src/mapred/org/apache/hadoop/mapred/jobcontrol/Job.java:246:   * @param dependingJob Job that this Job depends on.
./src/mapred/org/apache/hadoop/mapred/jobcontrol/Job.java:247:   * @return <tt>true</tt> if the Job was added.
./src/mapred/org/apache/hadoop/mapred/jobcontrol/Job.java:261:   * @return true if this job is in a complete state
./src/mapred/org/apache/hadoop/mapred/jobcontrol/Job.java:270:   * @return true if this job is in READY state
./src/mapred/org/apache/hadoop/mapred/jobcontrol/JobControl.java:62:   * @param groupName a name identifying this group
./src/mapred/org/apache/hadoop/mapred/jobcontrol/JobControl.java:86:   * @return the jobs in the waiting state
./src/mapred/org/apache/hadoop/mapred/jobcontrol/JobControl.java:93:   * @return the jobs in the running state
./src/mapred/org/apache/hadoop/mapred/jobcontrol/JobControl.java:100:   * @return the jobs in the ready state
./src/mapred/org/apache/hadoop/mapred/jobcontrol/JobControl.java:107:   * @return the jobs in the success state
./src/mapred/org/apache/hadoop/mapred/jobcontrol/JobControl.java:151:   * @param aJob the the new job
./src/mapred/org/apache/hadoop/mapred/jobcontrol/JobControl.java:164:   * @param jobs
./src/mapred/org/apache/hadoop/mapred/jobcontrol/JobControl.java:173:   * @return the thread state
./src/mapred/org/apache/hadoop/mapred/JobEndNotifier.java:217:    @Override
./src/mapred/org/apache/hadoop/mapred/JobEndNotifier.java:228:    @Override
./src/mapred/org/apache/hadoop/mapred/JobEndNotifier.java:233:    @Override
./src/mapred/org/apache/hadoop/mapred/JobHistory.java:134:   * @param conf Jobconf of the job tracker.
./src/mapred/org/apache/hadoop/mapred/JobHistory.java:135:   * @param hostname jobtracker's hostname
./src/mapred/org/apache/hadoop/mapred/JobHistory.java:136:   * @param jobTrackerStartTime jobtracker's start time
./src/mapred/org/apache/hadoop/mapred/JobHistory.java:137:   * @return true if intialized properly
./src/mapred/org/apache/hadoop/mapred/JobHistory.java:214:     * @param jobId job id, assigned by jobtracker. 
./src/mapred/org/apache/hadoop/mapred/JobHistory.java:227:  /** Escapes the string especially for {@link JobHistory}
./src/mapred/org/apache/hadoop/mapred/JobHistory.java:238:   * @param path path to history file
./src/mapred/org/apache/hadoop/mapred/JobHistory.java:239:   * @param l Listener for history events 
./src/mapred/org/apache/hadoop/mapred/JobHistory.java:240:   * @param fs FileSystem where history file is present
./src/mapred/org/apache/hadoop/mapred/JobHistory.java:241:   * @throws IOException
./src/mapred/org/apache/hadoop/mapred/JobHistory.java:285:   * @param line
./src/mapred/org/apache/hadoop/mapred/JobHistory.java:286:   * @param l
./src/mapred/org/apache/hadoop/mapred/JobHistory.java:287:   * @throws IOException
./src/mapred/org/apache/hadoop/mapred/JobHistory.java:317:   * @param recordType type of log event
./src/mapred/org/apache/hadoop/mapred/JobHistory.java:318:   * @param key key
./src/mapred/org/apache/hadoop/mapred/JobHistory.java:319:   * @param value value
./src/mapred/org/apache/hadoop/mapred/JobHistory.java:332:   * @param recordType type of log event
./src/mapred/org/apache/hadoop/mapred/JobHistory.java:333:   * @param keys type of log event
./src/mapred/org/apache/hadoop/mapred/JobHistory.java:334:   * @param values type of log event
./src/mapred/org/apache/hadoop/mapred/JobHistory.java:359:   * @return true if history logging is disabled, false otherwise. 
./src/mapred/org/apache/hadoop/mapred/JobHistory.java:368:   * @param disableHistory true if history should be disabled, false otherwise. 
./src/mapred/org/apache/hadoop/mapred/JobHistory.java:384:     * @param k 
./src/mapred/org/apache/hadoop/mapred/JobHistory.java:385:     * @return if null it returns empty string - "" 
./src/mapred/org/apache/hadoop/mapred/JobHistory.java:394:     * @param k key 
./src/mapred/org/apache/hadoop/mapred/JobHistory.java:406:     * @param k
./src/mapred/org/apache/hadoop/mapred/JobHistory.java:417:     * @param k
./src/mapred/org/apache/hadoop/mapred/JobHistory.java:418:     * @param s
./src/mapred/org/apache/hadoop/mapred/JobHistory.java:425:     * @param m
./src/mapred/org/apache/hadoop/mapred/JobHistory.java:432:     * @param values
./src/mapred/org/apache/hadoop/mapred/JobHistory.java:464:     * @param jobId id of the job
./src/mapred/org/apache/hadoop/mapred/JobHistory.java:465:     * @return the path of the job file on the local file system 
./src/mapred/org/apache/hadoop/mapred/JobHistory.java:476:     * @param logFile path of the job-history file
./src/mapred/org/apache/hadoop/mapred/JobHistory.java:477:     * @return URL encoded path
./src/mapred/org/apache/hadoop/mapred/JobHistory.java:478:     * @throws IOException
./src/mapred/org/apache/hadoop/mapred/JobHistory.java:501:     * @param logFileName file name of the job-history file
./src/mapred/org/apache/hadoop/mapred/JobHistory.java:502:     * @return URL encoded filename
./src/mapred/org/apache/hadoop/mapred/JobHistory.java:503:     * @throws IOException
./src/mapred/org/apache/hadoop/mapred/JobHistory.java:523:     * @param logFileName file name of the job-history file
./src/mapred/org/apache/hadoop/mapred/JobHistory.java:524:     * @return URL decoded filename
./src/mapred/org/apache/hadoop/mapred/JobHistory.java:525:     * @throws IOException
./src/mapred/org/apache/hadoop/mapred/JobHistory.java:621:     * @param jobConf the job conf
./src/mapred/org/apache/hadoop/mapred/JobHistory.java:622:     * @param id job id
./src/mapred/org/apache/hadoop/mapred/JobHistory.java:683:     * @param fileName the history filename that needs checkpointing
./src/mapred/org/apache/hadoop/mapred/JobHistory.java:684:     * @param conf Job conf
./src/mapred/org/apache/hadoop/mapred/JobHistory.java:685:     * @throws IOException
./src/mapred/org/apache/hadoop/mapred/JobHistory.java:712:     * @param conf job conf
./src/mapred/org/apache/hadoop/mapred/JobHistory.java:713:     * @param logFilePath Path of the log file
./src/mapred/org/apache/hadoop/mapred/JobHistory.java:714:     * @throws IOException 
./src/mapred/org/apache/hadoop/mapred/JobHistory.java:740:     * @param id Job id  
./src/mapred/org/apache/hadoop/mapred/JobHistory.java:741:     * @param conf the job conf
./src/mapred/org/apache/hadoop/mapred/JobHistory.java:742:     * @throws IOException
./src/mapred/org/apache/hadoop/mapred/JobHistory.java:782:     * @param jobId job id assigned by job tracker.
./src/mapred/org/apache/hadoop/mapred/JobHistory.java:783:     * @param jobConf job conf of the job
./src/mapred/org/apache/hadoop/mapred/JobHistory.java:784:     * @param jobConfPath path to job conf xml file in HDFS.
./src/mapred/org/apache/hadoop/mapred/JobHistory.java:785:     * @param submitTime time when job tracker received the job
./src/mapred/org/apache/hadoop/mapred/JobHistory.java:786:     * @throws IOException
./src/mapred/org/apache/hadoop/mapred/JobHistory.java:933:     * @param jobId job id, assigned by jobtracker. 
./src/mapred/org/apache/hadoop/mapred/JobHistory.java:934:     * @param startTime start time of job. 
./src/mapred/org/apache/hadoop/mapred/JobHistory.java:935:     * @param totalMaps total maps assigned by jobtracker. 
./src/mapred/org/apache/hadoop/mapred/JobHistory.java:936:     * @param totalReduces total reduces. 
./src/mapred/org/apache/hadoop/mapred/JobHistory.java:959:     * @param jobId job id, assigned by jobtracker. 
./src/mapred/org/apache/hadoop/mapred/JobHistory.java:960:     * @param startTime start time of job. 
./src/mapred/org/apache/hadoop/mapred/JobHistory.java:961:     * @param totalMaps total maps assigned by jobtracker. 
./src/mapred/org/apache/hadoop/mapred/JobHistory.java:962:     * @param totalReduces total reduces. 
./src/mapred/org/apache/hadoop/mapred/JobHistory.java:963:     * @deprecated Use {@link #logInited(JobID, long, int, int)} and 
./src/mapred/org/apache/hadoop/mapred/JobHistory.java:964:     * {@link #logStarted(JobID)}
./src/mapred/org/apache/hadoop/mapred/JobHistory.java:966:    @Deprecated
./src/mapred/org/apache/hadoop/mapred/JobHistory.java:974:     * @param jobId job id, assigned by jobtracker. 
./src/mapred/org/apache/hadoop/mapred/JobHistory.java:992:     * @param jobId job id, assigned by jobtracker. 
./src/mapred/org/apache/hadoop/mapred/JobHistory.java:993:     * @param finishTime finish time of job in ms. 
./src/mapred/org/apache/hadoop/mapred/JobHistory.java:994:     * @param finishedMaps no of maps successfully finished. 
./src/mapred/org/apache/hadoop/mapred/JobHistory.java:995:     * @param finishedReduces no of reduces finished sucessfully. 
./src/mapred/org/apache/hadoop/mapred/JobHistory.java:996:     * @param failedMaps no of failed map tasks. 
./src/mapred/org/apache/hadoop/mapred/JobHistory.java:997:     * @param failedReduces no of failed reduce tasks. 
./src/mapred/org/apache/hadoop/mapred/JobHistory.java:998:     * @param counters the counters from the job
./src/mapred/org/apache/hadoop/mapred/JobHistory.java:1034:     * @param jobid job id
./src/mapred/org/apache/hadoop/mapred/JobHistory.java:1035:     * @param timestamp time when job failure was detected in ms.  
./src/mapred/org/apache/hadoop/mapred/JobHistory.java:1036:     * @param finishedMaps no finished map tasks. 
./src/mapred/org/apache/hadoop/mapred/JobHistory.java:1037:     * @param finishedReduces no of finished reduce tasks. 
./src/mapred/org/apache/hadoop/mapred/JobHistory.java:1059:     * @param jobid
./src/mapred/org/apache/hadoop/mapred/JobHistory.java:1061:     * @param timestamp
./src/mapred/org/apache/hadoop/mapred/JobHistory.java:1063:     * @param finishedMaps
./src/mapred/org/apache/hadoop/mapred/JobHistory.java:1065:     * @param finishedReduces
./src/mapred/org/apache/hadoop/mapred/JobHistory.java:1089:     * @param jobid job id
./src/mapred/org/apache/hadoop/mapred/JobHistory.java:1090:     * @param priority Jobs priority 
./src/mapred/org/apache/hadoop/mapred/JobHistory.java:1106:     * @param jobid job id
./src/mapred/org/apache/hadoop/mapred/JobHistory.java:1107:     * @param submitTime job's submit time
./src/mapred/org/apache/hadoop/mapred/JobHistory.java:1108:     * @param launchTime job's launch time
./src/mapred/org/apache/hadoop/mapred/JobHistory.java:1109:     * @param restartCount number of times the job got restarted
./src/mapred/org/apache/hadoop/mapred/JobHistory.java:1140:     * @param taskId task id
./src/mapred/org/apache/hadoop/mapred/JobHistory.java:1141:     * @param taskType MAP or REDUCE
./src/mapred/org/apache/hadoop/mapred/JobHistory.java:1142:     * @param startTime startTime of tip. 
./src/mapred/org/apache/hadoop/mapred/JobHistory.java:1162:     * @param taskId task id
./src/mapred/org/apache/hadoop/mapred/JobHistory.java:1163:     * @param taskType MAP or REDUCE
./src/mapred/org/apache/hadoop/mapred/JobHistory.java:1164:     * @param finishTime finish timeof task in ms
./src/mapred/org/apache/hadoop/mapred/JobHistory.java:1185:     * @param taskId task id
./src/mapred/org/apache/hadoop/mapred/JobHistory.java:1186:     * @param taskType MAP or REDUCE.
./src/mapred/org/apache/hadoop/mapred/JobHistory.java:1187:     * @param time timestamp when job failed detected. 
./src/mapred/org/apache/hadoop/mapred/JobHistory.java:1188:     * @param error error message for failure. 
./src/mapred/org/apache/hadoop/mapred/JobHistory.java:1195:     * @param failedDueToAttempt The attempt that caused the failure, if any
./src/mapred/org/apache/hadoop/mapred/JobHistory.java:1239:     * @param taskAttemptId task attempt id
./src/mapred/org/apache/hadoop/mapred/JobHistory.java:1240:     * @param startTime start time of task attempt as reported by task tracker. 
./src/mapred/org/apache/hadoop/mapred/JobHistory.java:1241:     * @param hostName host name of the task attempt. 
./src/mapred/org/apache/hadoop/mapred/JobHistory.java:1242:     * @deprecated Use 
./src/mapred/org/apache/hadoop/mapred/JobHistory.java:1243:     *             {@link #logStarted(TaskAttemptID, long, String, int, String)}
./src/mapred/org/apache/hadoop/mapred/JobHistory.java:1245:    @Deprecated
./src/mapred/org/apache/hadoop/mapred/JobHistory.java:1253:     * @param taskAttemptId task attempt id
./src/mapred/org/apache/hadoop/mapred/JobHistory.java:1254:     * @param startTime start time of task attempt as reported by task tracker. 
./src/mapred/org/apache/hadoop/mapred/JobHistory.java:1255:     * @param trackerName name of the tracker executing the task attempt.
./src/mapred/org/apache/hadoop/mapred/JobHistory.java:1256:     * @param httpPort http port of the task tracker executing the task attempt
./src/mapred/org/apache/hadoop/mapred/JobHistory.java:1257:     * @param taskType Whether the attempt is cleanup or setup or map 
./src/mapred/org/apache/hadoop/mapred/JobHistory.java:1282:     * @param taskAttemptId task attempt id 
./src/mapred/org/apache/hadoop/mapred/JobHistory.java:1283:     * @param finishTime finish time
./src/mapred/org/apache/hadoop/mapred/JobHistory.java:1284:     * @param hostName host name 
./src/mapred/org/apache/hadoop/mapred/JobHistory.java:1285:     * @deprecated Use 
./src/mapred/org/apache/hadoop/mapred/JobHistory.java:1286:     * {@link #logFinished(TaskAttemptID, long, String, String, String, Counters)}
./src/mapred/org/apache/hadoop/mapred/JobHistory.java:1288:    @Deprecated
./src/mapred/org/apache/hadoop/mapred/JobHistory.java:1298:     * @param taskAttemptId task attempt id 
./src/mapred/org/apache/hadoop/mapred/JobHistory.java:1299:     * @param finishTime finish time
./src/mapred/org/apache/hadoop/mapred/JobHistory.java:1300:     * @param hostName host name 
./src/mapred/org/apache/hadoop/mapred/JobHistory.java:1301:     * @param taskType Whether the attempt is cleanup or setup or map 
./src/mapred/org/apache/hadoop/mapred/JobHistory.java:1302:     * @param stateString state string of the task attempt
./src/mapred/org/apache/hadoop/mapred/JobHistory.java:1303:     * @param counter counters of the task attempt
./src/mapred/org/apache/hadoop/mapred/JobHistory.java:1334:     * @param taskAttemptId task attempt id
./src/mapred/org/apache/hadoop/mapred/JobHistory.java:1335:     * @param timestamp timestamp
./src/mapred/org/apache/hadoop/mapred/JobHistory.java:1336:     * @param hostName hostname of this task attempt.
./src/mapred/org/apache/hadoop/mapred/JobHistory.java:1337:     * @param error error message if any for this task attempt.
./src/mapred/org/apache/hadoop/mapred/JobHistory.java:1338:     * @deprecated Use
./src/mapred/org/apache/hadoop/mapred/JobHistory.java:1339:     * {@link #logFailed(TaskAttemptID, long, String, String, String)} 
./src/mapred/org/apache/hadoop/mapred/JobHistory.java:1341:    @Deprecated
./src/mapred/org/apache/hadoop/mapred/JobHistory.java:1351:     * @param taskAttemptId task attempt id
./src/mapred/org/apache/hadoop/mapred/JobHistory.java:1352:     * @param timestamp timestamp
./src/mapred/org/apache/hadoop/mapred/JobHistory.java:1353:     * @param hostName hostname of this task attempt.
./src/mapred/org/apache/hadoop/mapred/JobHistory.java:1354:     * @param error error message if any for this task attempt. 
./src/mapred/org/apache/hadoop/mapred/JobHistory.java:1355:     * @param taskType Whether the attempt is cleanup or setup or map 
./src/mapred/org/apache/hadoop/mapred/JobHistory.java:1381:     * @param taskAttemptId task attempt id
./src/mapred/org/apache/hadoop/mapred/JobHistory.java:1382:     * @param timestamp timestamp
./src/mapred/org/apache/hadoop/mapred/JobHistory.java:1383:     * @param hostName hostname of this task attempt.
./src/mapred/org/apache/hadoop/mapred/JobHistory.java:1384:     * @param error error message if any for this task attempt. 
./src/mapred/org/apache/hadoop/mapred/JobHistory.java:1385:     * @deprecated Use 
./src/mapred/org/apache/hadoop/mapred/JobHistory.java:1386:     * {@link #logKilled(TaskAttemptID, long, String, String, String)}
./src/mapred/org/apache/hadoop/mapred/JobHistory.java:1388:    @Deprecated
./src/mapred/org/apache/hadoop/mapred/JobHistory.java:1397:     * @param taskAttemptId task attempt id
./src/mapred/org/apache/hadoop/mapred/JobHistory.java:1398:     * @param timestamp timestamp
./src/mapred/org/apache/hadoop/mapred/JobHistory.java:1399:     * @param hostName hostname of this task attempt.
./src/mapred/org/apache/hadoop/mapred/JobHistory.java:1400:     * @param error error message if any for this task attempt. 
./src/mapred/org/apache/hadoop/mapred/JobHistory.java:1401:     * @param taskType Whether the attempt is cleanup or setup or map 
./src/mapred/org/apache/hadoop/mapred/JobHistory.java:1433:     * @param taskAttemptId task attempt id
./src/mapred/org/apache/hadoop/mapred/JobHistory.java:1434:     * @param startTime start time
./src/mapred/org/apache/hadoop/mapred/JobHistory.java:1435:     * @param hostName host name 
./src/mapred/org/apache/hadoop/mapred/JobHistory.java:1436:     * @deprecated Use 
./src/mapred/org/apache/hadoop/mapred/JobHistory.java:1437:     * {@link #logStarted(TaskAttemptID, long, String, int, String)}
./src/mapred/org/apache/hadoop/mapred/JobHistory.java:1439:    @Deprecated
./src/mapred/org/apache/hadoop/mapred/JobHistory.java:1448:     * @param taskAttemptId task attempt id
./src/mapred/org/apache/hadoop/mapred/JobHistory.java:1449:     * @param startTime start time
./src/mapred/org/apache/hadoop/mapred/JobHistory.java:1450:     * @param trackerName tracker name 
./src/mapred/org/apache/hadoop/mapred/JobHistory.java:1451:     * @param httpPort the http port of the tracker executing the task attempt
./src/mapred/org/apache/hadoop/mapred/JobHistory.java:1452:     * @param taskType Whether the attempt is cleanup or setup or reduce 
./src/mapred/org/apache/hadoop/mapred/JobHistory.java:1478:     * @param taskAttemptId task attempt id
./src/mapred/org/apache/hadoop/mapred/JobHistory.java:1479:     * @param shuffleFinished shuffle finish time
./src/mapred/org/apache/hadoop/mapred/JobHistory.java:1480:     * @param sortFinished sort finish time
./src/mapred/org/apache/hadoop/mapred/JobHistory.java:1481:     * @param finishTime finish time of task
./src/mapred/org/apache/hadoop/mapred/JobHistory.java:1482:     * @param hostName host name where task attempt executed
./src/mapred/org/apache/hadoop/mapred/JobHistory.java:1483:     * @deprecated Use 
./src/mapred/org/apache/hadoop/mapred/JobHistory.java:1484:     * {@link #logFinished(TaskAttemptID, long, long, long, String, String, String, Counters)}
./src/mapred/org/apache/hadoop/mapred/JobHistory.java:1486:    @Deprecated
./src/mapred/org/apache/hadoop/mapred/JobHistory.java:1498:     * @param taskAttemptId task attempt id
./src/mapred/org/apache/hadoop/mapred/JobHistory.java:1499:     * @param shuffleFinished shuffle finish time
./src/mapred/org/apache/hadoop/mapred/JobHistory.java:1500:     * @param sortFinished sort finish time
./src/mapred/org/apache/hadoop/mapred/JobHistory.java:1501:     * @param finishTime finish time of task
./src/mapred/org/apache/hadoop/mapred/JobHistory.java:1502:     * @param hostName host name where task attempt executed
./src/mapred/org/apache/hadoop/mapred/JobHistory.java:1503:     * @param taskType Whether the attempt is cleanup or setup or reduce 
./src/mapred/org/apache/hadoop/mapred/JobHistory.java:1504:     * @param stateString the state string of the attempt
./src/mapred/org/apache/hadoop/mapred/JobHistory.java:1505:     * @param counter counters of the attempt
./src/mapred/org/apache/hadoop/mapred/JobHistory.java:1538:     * @param taskAttemptId task attempt id
./src/mapred/org/apache/hadoop/mapred/JobHistory.java:1539:     * @param timestamp time stamp when task failed
./src/mapred/org/apache/hadoop/mapred/JobHistory.java:1540:     * @param hostName host name of the task attempt.  
./src/mapred/org/apache/hadoop/mapred/JobHistory.java:1541:     * @param error error message of the task.
./src/mapred/org/apache/hadoop/mapred/JobHistory.java:1542:     * @deprecated Use 
./src/mapred/org/apache/hadoop/mapred/JobHistory.java:1543:     * {@link #logFailed(TaskAttemptID, long, String, String, String)} 
./src/mapred/org/apache/hadoop/mapred/JobHistory.java:1545:    @Deprecated
./src/mapred/org/apache/hadoop/mapred/JobHistory.java:1554:     * @param taskAttemptId task attempt id
./src/mapred/org/apache/hadoop/mapred/JobHistory.java:1555:     * @param timestamp time stamp when task failed
./src/mapred/org/apache/hadoop/mapred/JobHistory.java:1556:     * @param hostName host name of the task attempt.  
./src/mapred/org/apache/hadoop/mapred/JobHistory.java:1557:     * @param error error message of the task. 
./src/mapred/org/apache/hadoop/mapred/JobHistory.java:1558:     * @param taskType Whether the attempt is cleanup or setup or reduce 
./src/mapred/org/apache/hadoop/mapred/JobHistory.java:1584:     * @param taskAttemptId task attempt id
./src/mapred/org/apache/hadoop/mapred/JobHistory.java:1585:     * @param timestamp time stamp when task failed
./src/mapred/org/apache/hadoop/mapred/JobHistory.java:1586:     * @param hostName host name of the task attempt.  
./src/mapred/org/apache/hadoop/mapred/JobHistory.java:1587:     * @param error error message of the task.
./src/mapred/org/apache/hadoop/mapred/JobHistory.java:1588:     * @deprecated Use 
./src/mapred/org/apache/hadoop/mapred/JobHistory.java:1589:     * {@link #logKilled(TaskAttemptID, long, String, String, String)} 
./src/mapred/org/apache/hadoop/mapred/JobHistory.java:1591:    @Deprecated
./src/mapred/org/apache/hadoop/mapred/JobHistory.java:1600:     * @param taskAttemptId task attempt id
./src/mapred/org/apache/hadoop/mapred/JobHistory.java:1601:     * @param timestamp time stamp when task failed
./src/mapred/org/apache/hadoop/mapred/JobHistory.java:1602:     * @param hostName host name of the task attempt.  
./src/mapred/org/apache/hadoop/mapred/JobHistory.java:1603:     * @param error error message of the task. 
./src/mapred/org/apache/hadoop/mapred/JobHistory.java:1604:     * @param taskType Whether the attempt is cleanup or setup or reduce 
./src/mapred/org/apache/hadoop/mapred/JobHistory.java:1638:     * @param recType type of record, which is the first entry in the line. 
./src/mapred/org/apache/hadoop/mapred/JobHistory.java:1639:     * @param values a map of key-value pairs as thry appear in history.
./src/mapred/org/apache/hadoop/mapred/JobHistory.java:1640:     * @throws IOException
./src/mapred/org/apache/hadoop/mapred/JobHistory.java:1692:   * @param attempt
./src/mapred/org/apache/hadoop/mapred/JobHistory.java:1693:   * @return the taskLogsUrl. null if http-port or tracker-name or
./src/mapred/org/apache/hadoop/mapred/JobID.java:40: * use appropriate constructors or {@link #forName(String)} method. 
./src/mapred/org/apache/hadoop/mapred/JobID.java:42: * @see TaskID
./src/mapred/org/apache/hadoop/mapred/JobID.java:43: * @see TaskAttemptID
./src/mapred/org/apache/hadoop/mapred/JobID.java:44: * @see JobTracker#getNewJobId()
./src/mapred/org/apache/hadoop/mapred/JobID.java:45: * @see JobTracker#getStartTime()
./src/mapred/org/apache/hadoop/mapred/JobID.java:60:   * @param jtIdentifier jobTracker identifier
./src/mapred/org/apache/hadoop/mapred/JobID.java:61:   * @param id job number
./src/mapred/org/apache/hadoop/mapred/JobID.java:74:  @Override
./src/mapred/org/apache/hadoop/mapred/JobID.java:87:  @Override
./src/mapred/org/apache/hadoop/mapred/JobID.java:97:  @Override
./src/mapred/org/apache/hadoop/mapred/JobID.java:112:  @Override
./src/mapred/org/apache/hadoop/mapred/JobID.java:117:  @Override
./src/mapred/org/apache/hadoop/mapred/JobID.java:123:  @Override
./src/mapred/org/apache/hadoop/mapred/JobID.java:136:   * @return constructed JobId object or null if the given String is null
./src/mapred/org/apache/hadoop/mapred/JobID.java:137:   * @throws IllegalArgumentException if the given string is malformed
./src/mapred/org/apache/hadoop/mapred/JobID.java:165:   * @param jtIdentifier jobTracker identifier, or null
./src/mapred/org/apache/hadoop/mapred/JobID.java:166:   * @param jobId job number, or null
./src/mapred/org/apache/hadoop/mapred/JobID.java:167:   * @return a regex pattern matching JobIDs
./src/mapred/org/apache/hadoop/mapred/JobInProgress.java:330:   * @return <code>true</code> if the job has been initialized, 
./src/mapred/org/apache/hadoop/mapred/JobInProgress.java:563:   * @return the raw array of maps for this job
./src/mapred/org/apache/hadoop/mapred/JobInProgress.java:571:   * @return the array of cleanup tasks for the job
./src/mapred/org/apache/hadoop/mapred/JobInProgress.java:579:   * @return the array of setup tasks for the job
./src/mapred/org/apache/hadoop/mapred/JobInProgress.java:587:   * @return the raw array of reduce tasks for this job
./src/mapred/org/apache/hadoop/mapred/JobInProgress.java:595:   * @return
./src/mapred/org/apache/hadoop/mapred/JobInProgress.java:604:   * @return
./src/mapred/org/apache/hadoop/mapred/JobInProgress.java:613:   * @return
./src/mapred/org/apache/hadoop/mapred/JobInProgress.java:622:   * @return the job's configuration
./src/mapred/org/apache/hadoop/mapred/JobInProgress.java:823:   * @return the job-level counters.
./src/mapred/org/apache/hadoop/mapred/JobInProgress.java:856:   * @param counters the counters to increment
./src/mapred/org/apache/hadoop/mapred/JobInProgress.java:857:   * @param tips the tasks to add in to counters
./src/mapred/org/apache/hadoop/mapred/JobInProgress.java:858:   * @return counters the same object passed in as counters
./src/mapred/org/apache/hadoop/mapred/JobInProgress.java:950:   * @return true/false
./src/mapred/org/apache/hadoop/mapred/JobInProgress.java:1032:   * @return true/false
./src/mapred/org/apache/hadoop/mapred/JobInProgress.java:1084:   * @param tip The tip for which the task is added
./src/mapred/org/apache/hadoop/mapred/JobInProgress.java:1085:   * @param id The attempt-id for the task
./src/mapred/org/apache/hadoop/mapred/JobInProgress.java:1086:   * @param tts task-tracker status
./src/mapred/org/apache/hadoop/mapred/JobInProgress.java:1087:   * @param isScheduled Whether this task is scheduled from the JT or has 
./src/mapred/org/apache/hadoop/mapred/JobInProgress.java:1198:   * @param trackerName task-tracker on which a task failed
./src/mapred/org/apache/hadoop/mapred/JobInProgress.java:1227:   * @return the no. of 'flaky' tasktrackers for a given job.
./src/mapred/org/apache/hadoop/mapred/JobInProgress.java:1237:   * @return the map of tasktrackers and no. of errors which occurred
./src/mapred/org/apache/hadoop/mapred/JobInProgress.java:1251:   * @param tip the tip that needs to be retired
./src/mapred/org/apache/hadoop/mapred/JobInProgress.java:1290:   * @param tip the tip that needs to be retired
./src/mapred/org/apache/hadoop/mapred/JobInProgress.java:1306:   * @param tip the tip that needs to be scheduled as running
./src/mapred/org/apache/hadoop/mapred/JobInProgress.java:1344:   * @param tip the tip that needs to be scheduled as running
./src/mapred/org/apache/hadoop/mapred/JobInProgress.java:1360:   * @param tip the tip that needs to be failed
./src/mapred/org/apache/hadoop/mapred/JobInProgress.java:1398:   * @param tip the tip that needs to be failed
./src/mapred/org/apache/hadoop/mapred/JobInProgress.java:1411:   * @param tips a collection of TIPs
./src/mapred/org/apache/hadoop/mapred/JobInProgress.java:1412:   * @param ttStatus the status of tracker that has requested a task to run
./src/mapred/org/apache/hadoop/mapred/JobInProgress.java:1413:   * @param numUniqueHosts number of unique hosts that run trask trackers
./src/mapred/org/apache/hadoop/mapred/JobInProgress.java:1414:   * @param removeFailedTip whether to remove the failed tips
./src/mapred/org/apache/hadoop/mapred/JobInProgress.java:1455:   * @param list a list of tips
./src/mapred/org/apache/hadoop/mapred/JobInProgress.java:1456:   * @param taskTracker the tracker that has requested a tip
./src/mapred/org/apache/hadoop/mapred/JobInProgress.java:1457:   * @param avgProgress the average progress for speculation
./src/mapred/org/apache/hadoop/mapred/JobInProgress.java:1458:   * @param currentTime current time in milliseconds
./src/mapred/org/apache/hadoop/mapred/JobInProgress.java:1459:   * @param shouldRemove whether to remove the tips
./src/mapred/org/apache/hadoop/mapred/JobInProgress.java:1460:   * @return a tip that can be speculated on the tracker
./src/mapred/org/apache/hadoop/mapred/JobInProgress.java:1500:   * @param tts The task tracker that is asking for a task
./src/mapred/org/apache/hadoop/mapred/JobInProgress.java:1501:   * @param clusterSize The number of task trackers in the cluster
./src/mapred/org/apache/hadoop/mapred/JobInProgress.java:1502:   * @param numUniqueHosts The number of hosts that run task trackers
./src/mapred/org/apache/hadoop/mapred/JobInProgress.java:1503:   * @param avgProgress The average progress of this kind of task in this job
./src/mapred/org/apache/hadoop/mapred/JobInProgress.java:1504:   * @return the index in tasks of the selected task (or -1 for no task)
./src/mapred/org/apache/hadoop/mapred/JobInProgress.java:1691:   * @param tts The task tracker that is asking for a task
./src/mapred/org/apache/hadoop/mapred/JobInProgress.java:1692:   * @param clusterSize The number of task trackers in the cluster
./src/mapred/org/apache/hadoop/mapred/JobInProgress.java:1693:   * @param numUniqueHosts The number of hosts that run task trackers
./src/mapred/org/apache/hadoop/mapred/JobInProgress.java:1694:   * @param avgProgress The average progress of this kind of task in this job
./src/mapred/org/apache/hadoop/mapred/JobInProgress.java:1695:   * @return the index in tasks of the selected task (or -1 for no task)
./src/mapred/org/apache/hadoop/mapred/JobInProgress.java:1894:   * @param metrics job-tracker metrics
./src/mapred/org/apache/hadoop/mapred/JobInProgress.java:1948:   * @param jobTerminationState job termination state
./src/mapred/org/apache/hadoop/mapred/JobInProgress.java:2199:   * @param tip The task's tip
./src/mapred/org/apache/hadoop/mapred/JobInProgress.java:2200:   * @param taskid The task id
./src/mapred/org/apache/hadoop/mapred/JobInProgress.java:2201:   * @param reason The reason that the task failed
./src/mapred/org/apache/hadoop/mapred/JobInProgress.java:2202:   * @param trackerName The task tracker the task failed on
./src/mapred/org/apache/hadoop/mapred/JobInProgress.java:2303:   * @param mapId the id of the map
./src/mapred/org/apache/hadoop/mapred/JobInProgress.java:2304:   * @return the task status of the completed task
./src/mapred/org/apache/hadoop/mapred/JobInProgress.java:2364:   * @return The JobID of this JobInProgress.
./src/mapred/org/apache/hadoop/mapred/JobInProgressListener.java:21: * A listener for changes in a {@link JobInProgress job}'s lifecycle in the
./src/mapred/org/apache/hadoop/mapred/JobInProgressListener.java:22: * {@link JobTracker}.
./src/mapred/org/apache/hadoop/mapred/JobInProgressListener.java:27:   * Invoked when a new job has been added to the {@link JobTracker}.
./src/mapred/org/apache/hadoop/mapred/JobInProgressListener.java:28:   * @param job The added job.
./src/mapred/org/apache/hadoop/mapred/JobInProgressListener.java:33:   * Invoked when a job has been removed from the {@link JobTracker}.
./src/mapred/org/apache/hadoop/mapred/JobInProgressListener.java:34:   * @param job The removed job.
./src/mapred/org/apache/hadoop/mapred/JobInProgressListener.java:39:   * Invoked when a job has been updated in the {@link JobTracker}.
./src/mapred/org/apache/hadoop/mapred/JobInProgressListener.java:40:   * This change in the job is tracker using {@link JobChangeEvent}.
./src/mapred/org/apache/hadoop/mapred/JobInProgressListener.java:41:   * @param event the event that tracks the change
./src/mapred/org/apache/hadoop/mapred/JobProfile.java:53:   * Construct an empty {@link JobProfile}.
./src/mapred/org/apache/hadoop/mapred/JobProfile.java:59:   * Construct a {@link JobProfile} the userid, jobid, 
./src/mapred/org/apache/hadoop/mapred/JobProfile.java:62:   * @param user userid of the person who submitted the job.
./src/mapred/org/apache/hadoop/mapred/JobProfile.java:63:   * @param jobid id of the job.
./src/mapred/org/apache/hadoop/mapred/JobProfile.java:64:   * @param jobFile job configuration file. 
./src/mapred/org/apache/hadoop/mapred/JobProfile.java:65:   * @param url link to the web-ui for details of the job.
./src/mapred/org/apache/hadoop/mapred/JobProfile.java:66:   * @param name user-specified job name.
./src/mapred/org/apache/hadoop/mapred/JobProfile.java:74:   * Construct a {@link JobProfile} the userid, jobid, 
./src/mapred/org/apache/hadoop/mapred/JobProfile.java:77:   * @param user userid of the person who submitted the job.
./src/mapred/org/apache/hadoop/mapred/JobProfile.java:78:   * @param jobid id of the job.
./src/mapred/org/apache/hadoop/mapred/JobProfile.java:79:   * @param jobFile job configuration file. 
./src/mapred/org/apache/hadoop/mapred/JobProfile.java:80:   * @param url link to the web-ui for details of the job.
./src/mapred/org/apache/hadoop/mapred/JobProfile.java:81:   * @param name user-specified job name.
./src/mapred/org/apache/hadoop/mapred/JobProfile.java:82:   * @param queueName name of the queue to which the job is submitted
./src/mapred/org/apache/hadoop/mapred/JobProfile.java:95:   * @deprecated use JobProfile(String, JobID, String, String, String) instead
./src/mapred/org/apache/hadoop/mapred/JobProfile.java:97:  @Deprecated
./src/mapred/org/apache/hadoop/mapred/JobProfile.java:118:   * @deprecated use getJobID() instead
./src/mapred/org/apache/hadoop/mapred/JobProfile.java:120:  @Deprecated
./src/mapred/org/apache/hadoop/mapred/JobProfile.java:152:   * @return name of the queue.
./src/mapred/org/apache/hadoop/mapred/JobQueueClient.java:27: * to get JobQueue related information from the {@link JobTracker}
./src/mapred/org/apache/hadoop/mapred/JobQueueClient.java:51:  @Override
./src/mapred/org/apache/hadoop/mapred/JobQueueClient.java:102:   * registered with the {@link QueueManager}. Display of the Jobs is 
./src/mapred/org/apache/hadoop/mapred/JobQueueClient.java:105:   * @throws IOException
./src/mapred/org/apache/hadoop/mapred/JobQueueClient.java:127:   * with the {@link QueueManager}
./src/mapred/org/apache/hadoop/mapred/JobQueueClient.java:129:   * @throws IOException
./src/mapred/org/apache/hadoop/mapred/JobQueueInfo.java:52:   * @param queueName Name of the job queue
./src/mapred/org/apache/hadoop/mapred/JobQueueInfo.java:53:   * @param schedulingInfo Scheduling Information associated with the job
./src/mapred/org/apache/hadoop/mapred/JobQueueInfo.java:65:   * @param queueName Name of the job queue.
./src/mapred/org/apache/hadoop/mapred/JobQueueInfo.java:74:   * @return queue name
./src/mapred/org/apache/hadoop/mapred/JobQueueInfo.java:83:   * @param schedulingInfo
./src/mapred/org/apache/hadoop/mapred/JobQueueInfo.java:93:   * @return Scheduling information associated to particular Job Queue
./src/mapred/org/apache/hadoop/mapred/JobQueueInfo.java:103:  @Override
./src/mapred/org/apache/hadoop/mapred/JobQueueInfo.java:109:  @Override
./src/mapred/org/apache/hadoop/mapred/JobQueueJobInProgressListener.java:29: * A {@link JobInProgressListener} that maintains the jobs being managed in
./src/mapred/org/apache/hadoop/mapred/JobQueueJobInProgressListener.java:32: * {@link #JobQueueJobInProgressListener(Collection)} constructor.
./src/mapred/org/apache/hadoop/mapred/JobQueueJobInProgressListener.java:36:  /** A class that groups all the information from a {@link JobInProgress} that 
./src/mapred/org/apache/hadoop/mapred/JobQueueJobInProgressListener.java:86:   * @param jobQueue A collection whose iterator returns jobs in priority order.
./src/mapred/org/apache/hadoop/mapred/JobQueueJobInProgressListener.java:100:  @Override
./src/mapred/org/apache/hadoop/mapred/JobQueueJobInProgressListener.java:106:  @Override
./src/mapred/org/apache/hadoop/mapred/JobQueueJobInProgressListener.java:113:  @Override
./src/mapred/org/apache/hadoop/mapred/JobQueueTaskScheduler.java:28: * A {@link TaskScheduler} that keeps jobs in a queue in priority order (FIFO
./src/mapred/org/apache/hadoop/mapred/JobQueueTaskScheduler.java:45:  @Override
./src/mapred/org/apache/hadoop/mapred/JobQueueTaskScheduler.java:55:  @Override
./src/mapred/org/apache/hadoop/mapred/JobQueueTaskScheduler.java:69:  @Override
./src/mapred/org/apache/hadoop/mapred/JobQueueTaskScheduler.java:76:  @Override
./src/mapred/org/apache/hadoop/mapred/JobQueueTaskScheduler.java:214:  @Override
./src/mapred/org/apache/hadoop/mapred/JobStatus.java:69:   * @param jobid The jobid of the job
./src/mapred/org/apache/hadoop/mapred/JobStatus.java:70:   * @param mapProgress The progress made on the maps
./src/mapred/org/apache/hadoop/mapred/JobStatus.java:71:   * @param reduceProgress The progress made on the reduces
./src/mapred/org/apache/hadoop/mapred/JobStatus.java:72:   * @param cleanupProgress The progress made on cleanup
./src/mapred/org/apache/hadoop/mapred/JobStatus.java:73:   * @param runState The current state of the job
./src/mapred/org/apache/hadoop/mapred/JobStatus.java:83:   * @param jobid The jobid of the job
./src/mapred/org/apache/hadoop/mapred/JobStatus.java:84:   * @param mapProgress The progress made on the maps
./src/mapred/org/apache/hadoop/mapred/JobStatus.java:85:   * @param reduceProgress The progress made on the reduces
./src/mapred/org/apache/hadoop/mapred/JobStatus.java:86:   * @param runState The current state of the job
./src/mapred/org/apache/hadoop/mapred/JobStatus.java:95:   * @param jobid The jobid of the job
./src/mapred/org/apache/hadoop/mapred/JobStatus.java:96:   * @param mapProgress The progress made on the maps
./src/mapred/org/apache/hadoop/mapred/JobStatus.java:97:   * @param reduceProgress The progress made on the reduces
./src/mapred/org/apache/hadoop/mapred/JobStatus.java:98:   * @param runState The current state of the job
./src/mapred/org/apache/hadoop/mapred/JobStatus.java:99:   * @param jp Priority of the job.
./src/mapred/org/apache/hadoop/mapred/JobStatus.java:109:   * @param jobid The jobid of the job
./src/mapred/org/apache/hadoop/mapred/JobStatus.java:110:   * @param setupProgress The progress made on the setup
./src/mapred/org/apache/hadoop/mapred/JobStatus.java:111:   * @param mapProgress The progress made on the maps
./src/mapred/org/apache/hadoop/mapred/JobStatus.java:112:   * @param reduceProgress The progress made on the reduces
./src/mapred/org/apache/hadoop/mapred/JobStatus.java:113:   * @param cleanupProgress The progress made on the cleanup
./src/mapred/org/apache/hadoop/mapred/JobStatus.java:114:   * @param runState The current state of the job
./src/mapred/org/apache/hadoop/mapred/JobStatus.java:115:   * @param jp Priority of the job.
./src/mapred/org/apache/hadoop/mapred/JobStatus.java:134:   * @deprecated use getJobID instead
./src/mapred/org/apache/hadoop/mapred/JobStatus.java:136:  @Deprecated
./src/mapred/org/apache/hadoop/mapred/JobStatus.java:140:   * @return The jobid of the Job
./src/mapred/org/apache/hadoop/mapred/JobStatus.java:145:   * @return Percentage of progress in maps 
./src/mapred/org/apache/hadoop/mapred/JobStatus.java:151:   * @param p The value of map progress to set to
./src/mapred/org/apache/hadoop/mapred/JobStatus.java:158:   * @return Percentage of progress in cleanup 
./src/mapred/org/apache/hadoop/mapred/JobStatus.java:164:   * @param p The value of cleanup progress to set to
./src/mapred/org/apache/hadoop/mapred/JobStatus.java:171:   * @return Percentage of progress in setup 
./src/mapred/org/apache/hadoop/mapred/JobStatus.java:177:   * @param p The value of setup progress to set to
./src/mapred/org/apache/hadoop/mapred/JobStatus.java:184:   * @return Percentage of progress in reduce 
./src/mapred/org/apache/hadoop/mapred/JobStatus.java:190:   * @param p The value of reduce progress to set to
./src/mapred/org/apache/hadoop/mapred/JobStatus.java:197:   * @return running state of the job
./src/mapred/org/apache/hadoop/mapred/JobStatus.java:210:   * @param startTime The startTime of the job
./src/mapred/org/apache/hadoop/mapred/JobStatus.java:215:   * @return start time of the job
./src/mapred/org/apache/hadoop/mapred/JobStatus.java:219:  @Override
./src/mapred/org/apache/hadoop/mapred/JobStatus.java:230:   * @param user The username of the job
./src/mapred/org/apache/hadoop/mapred/JobStatus.java:235:   * @return the username of the job
./src/mapred/org/apache/hadoop/mapred/JobStatus.java:241:   * @return the scheduling information of the job
./src/mapred/org/apache/hadoop/mapred/JobStatus.java:250:   * @param schedulingInfo Scheduling information of the job
./src/mapred/org/apache/hadoop/mapred/JobStatus.java:258:   * @return job priority
./src/mapred/org/apache/hadoop/mapred/JobStatus.java:264:   * @param jp new job priority
./src/mapred/org/apache/hadoop/mapred/JobStatusChangeEvent.java:21: * {@link JobStatusChangeEvent} tracks the change in job's status. Job's 
./src/mapred/org/apache/hadoop/mapred/JobStatusChangeEvent.java:45:   * Create a {@link JobStatusChangeEvent} indicating the state has changed. 
./src/mapred/org/apache/hadoop/mapred/JobSubmissionProtocol.java:57:   * @return a unique job name for submitting jobs.
./src/mapred/org/apache/hadoop/mapred/JobSubmissionProtocol.java:58:   * @throws IOException
./src/mapred/org/apache/hadoop/mapred/JobSubmissionProtocol.java:71:   * @return summary of the state of the cluster
./src/mapred/org/apache/hadoop/mapred/JobSubmissionProtocol.java:82:   * @param jobid ID of the job
./src/mapred/org/apache/hadoop/mapred/JobSubmissionProtocol.java:83:   * @param priority Priority to be set for the job
./src/mapred/org/apache/hadoop/mapred/JobSubmissionProtocol.java:89:   * @param taskId the id of the task to kill.
./src/mapred/org/apache/hadoop/mapred/JobSubmissionProtocol.java:90:   * @param shouldFail if true the task is failed and added to failed tasks list, otherwise
./src/mapred/org/apache/hadoop/mapred/JobSubmissionProtocol.java:97:   * @return Profile of the job, or null if not found. 
./src/mapred/org/apache/hadoop/mapred/JobSubmissionProtocol.java:103:   * @return Status of the job, or null if not found.
./src/mapred/org/apache/hadoop/mapred/JobSubmissionProtocol.java:142:   * @return array of JobStatus for the running/to-be-run
./src/mapred/org/apache/hadoop/mapred/JobSubmissionProtocol.java:149:   * @return array of JobStatus for the submitted jobs
./src/mapred/org/apache/hadoop/mapred/JobSubmissionProtocol.java:156:   * @param jobid job id 
./src/mapred/org/apache/hadoop/mapred/JobSubmissionProtocol.java:157:   * @param fromEventId event id to start from.
./src/mapred/org/apache/hadoop/mapred/JobSubmissionProtocol.java:158:   * @param maxEvents the max number of events we want to look at 
./src/mapred/org/apache/hadoop/mapred/JobSubmissionProtocol.java:159:   * @return array of task completion events. 
./src/mapred/org/apache/hadoop/mapred/JobSubmissionProtocol.java:160:   * @throws IOException
./src/mapred/org/apache/hadoop/mapred/JobSubmissionProtocol.java:167:   * @param taskId the id of the task
./src/mapred/org/apache/hadoop/mapred/JobSubmissionProtocol.java:168:   * @return an array of the diagnostic messages
./src/mapred/org/apache/hadoop/mapred/JobSubmissionProtocol.java:176:   * @return the system directory where job-specific files are to be placed.
./src/mapred/org/apache/hadoop/mapred/JobSubmissionProtocol.java:183:   * @return Array of the Job Queue Information Object
./src/mapred/org/apache/hadoop/mapred/JobSubmissionProtocol.java:184:   * @throws IOException 
./src/mapred/org/apache/hadoop/mapred/JobSubmissionProtocol.java:191:   * @param queue Queue Name
./src/mapred/org/apache/hadoop/mapred/JobSubmissionProtocol.java:192:   * @return Scheduling Information of the Queue
./src/mapred/org/apache/hadoop/mapred/JobSubmissionProtocol.java:193:   * @throws IOException 
./src/mapred/org/apache/hadoop/mapred/JobSubmissionProtocol.java:199:   * @param queue Queue name
./src/mapred/org/apache/hadoop/mapred/JobSubmissionProtocol.java:200:   * @return array of JobStatus for the submitted jobs
./src/mapred/org/apache/hadoop/mapred/JobSubmissionProtocol.java:201:   * @throws IOException
./src/mapred/org/apache/hadoop/mapred/join/ArrayListBackedIterator.java:29: * implementation uses an {@link java.util.ArrayList} to store elements
./src/mapred/org/apache/hadoop/mapred/join/ArrayListBackedIterator.java:31: * Prefer {@link StreamBackedIterator}.
./src/mapred/org/apache/hadoop/mapred/join/CompositeInputFormat.java:37: * @see #setFormat
./src/mapred/org/apache/hadoop/mapred/join/CompositeInputFormat.java:45: * @see JoinRecordReader
./src/mapred/org/apache/hadoop/mapred/join/CompositeInputFormat.java:46: * @see MultiFilterRecordReader
./src/mapred/org/apache/hadoop/mapred/join/CompositeInputFormat.java:59:   * {@code
./src/mapred/org/apache/hadoop/mapred/join/CompositeInputFormat.java:62:   *   class ::= @see java.lang.Class#forName(java.lang.String)
./src/mapred/org/apache/hadoop/mapred/join/CompositeInputFormat.java:63:   *   path  ::= @see org.apache.hadoop.fs.Path#Path(java.lang.String)
./src/mapred/org/apache/hadoop/mapred/join/CompositeInputFormat.java:69:   * @see #compose(java.lang.String, java.lang.Class, java.lang.String...)
./src/mapred/org/apache/hadoop/mapred/join/CompositeInputFormat.java:126:  @SuppressWarnings("unchecked") // child types unknown
./src/mapred/org/apache/hadoop/mapred/join/CompositeInputFormat.java:136:   * {@code tbl(<inf>, <p>) }
./src/mapred/org/apache/hadoop/mapred/join/CompositeInputFormat.java:145:   * {@code <op>(tbl(<inf>,<p1>),tbl(<inf>,<p2>),...,tbl(<inf>,<pn>)) }
./src/mapred/org/apache/hadoop/mapred/join/CompositeInputFormat.java:162:   * {@code <op>(tbl(<inf>,<p1>),tbl(<inf>,<p2>),...,tbl(<inf>,<pn>)) }
./src/mapred/org/apache/hadoop/mapred/join/CompositeInputSplit.java:49:   * @throws IOException If capacity was not specified during construction
./src/mapred/org/apache/hadoop/mapred/join/CompositeInputSplit.java:109:   * {@code
./src/mapred/org/apache/hadoop/mapred/join/CompositeInputSplit.java:124:   * {@inheritDoc}
./src/mapred/org/apache/hadoop/mapred/join/CompositeInputSplit.java:125:   * @throws IOException If the child InputSplit cannot be read, typically
./src/mapred/org/apache/hadoop/mapred/join/CompositeInputSplit.java:128:  @SuppressWarnings("unchecked")  // Generic array assignment
./src/mapred/org/apache/hadoop/mapred/join/CompositeRecordReader.java:64:  @SuppressWarnings("unchecked") // Generic array assignment
./src/mapred/org/apache/hadoop/mapred/join/CompositeRecordReader.java:92:   * {@inheritDoc}
./src/mapred/org/apache/hadoop/mapred/join/CompositeRecordReader.java:99:   * {@inheritDoc}
./src/mapred/org/apache/hadoop/mapred/join/CompositeRecordReader.java:159:    @SuppressWarnings("unchecked") // Generic array assignment
./src/mapred/org/apache/hadoop/mapred/join/CompositeRecordReader.java:221:    @SuppressWarnings("unchecked") // No static typeinfo on Tuples
./src/mapred/org/apache/hadoop/mapred/join/CompositeRecordReader.java:266:    @SuppressWarnings("unchecked") // No static typeinfo on Tuples
./src/mapred/org/apache/hadoop/mapred/join/CompositeRecordReader.java:361:  @SuppressWarnings("unchecked") // No values from static EMPTY class
./src/mapred/org/apache/hadoop/mapred/join/CompositeRecordReader.java:401:   * @throws ClassCastException if key classes differ.
./src/mapred/org/apache/hadoop/mapred/join/CompositeRecordReader.java:403:  @SuppressWarnings("unchecked") // Explicit check for key class agreement
./src/mapred/org/apache/hadoop/mapred/join/JoinRecordReader.java:67:  /** {@inheritDoc} */
./src/mapred/org/apache/hadoop/mapred/join/MultiFilterRecordReader.java:60:   * Default implementation offers {@link #emit} every Tuple from the
./src/mapred/org/apache/hadoop/mapred/join/MultiFilterRecordReader.java:67:  /** {@inheritDoc} */
./src/mapred/org/apache/hadoop/mapred/join/MultiFilterRecordReader.java:90:  /** {@inheritDoc} */
./src/mapred/org/apache/hadoop/mapred/join/MultiFilterRecordReader.java:91:  @SuppressWarnings("unchecked") // Explicit check for value class agreement
./src/mapred/org/apache/hadoop/mapred/join/MultiFilterRecordReader.java:108:   * @see MultiFilterDelegationIterator
./src/mapred/org/apache/hadoop/mapred/join/OverrideRecordReader.java:48:  @SuppressWarnings("unchecked") // No static typeinfo on Tuples
./src/mapred/org/apache/hadoop/mapred/join/Parser.java:61: * {@link CompositeRecordReader#combine}) and include a property to map its
./src/mapred/org/apache/hadoop/mapred/join/Parser.java:69:   * @see Parser.TType
./src/mapred/org/apache/hadoop/mapred/join/Parser.java:175:     * @see #addIdentifier(java.lang.String, java.lang.Class[], java.lang.Class, java.lang.Class)
./src/mapred/org/apache/hadoop/mapred/join/Parser.java:176:     * @see CompositeInputFormat#setFormat(org.apache.hadoop.mapred.JobConf)
./src/mapred/org/apache/hadoop/mapred/join/Parser.java:358:     * {@link CompositeInputSplit}.
./src/mapred/org/apache/hadoop/mapred/join/Parser.java:385:    @SuppressWarnings("unchecked") // child types unknowable
./src/mapred/org/apache/hadoop/mapred/join/ResetableIterator.java:27: * Note that this does not extend {@link java.util.Iterator}.
./src/mapred/org/apache/hadoop/mapred/join/ResetableIterator.java:57:   * the same order after a call to {@link #reset} (FIFO).
./src/mapred/org/apache/hadoop/mapred/join/ResetableIterator.java:71:   * calling {@link #add} to avoid a ConcurrentModificationException.
./src/mapred/org/apache/hadoop/mapred/join/TupleWritable.java:32: * Writable type storing multiple {@link org.apache.hadoop.io.Writable}s.
./src/mapred/org/apache/hadoop/mapred/join/TupleWritable.java:41: * @see org.apache.hadoop.io.Writable
./src/mapred/org/apache/hadoop/mapred/join/TupleWritable.java:84:   * {@inheritDoc}
./src/mapred/org/apache/hadoop/mapred/join/TupleWritable.java:159:   * {@code
./src/mapred/org/apache/hadoop/mapred/join/TupleWritable.java:177:   * {@inheritDoc}
./src/mapred/org/apache/hadoop/mapred/join/TupleWritable.java:179:  @SuppressWarnings("unchecked") // No static typeinfo on Tuples
./src/mapred/org/apache/hadoop/mapred/join/WrappedRecordReader.java:71:  /** {@inheritDoc} */
./src/mapred/org/apache/hadoop/mapred/join/WrappedRecordReader.java:123:  @SuppressWarnings("unchecked") // no static type for the slot this sits in
./src/mapred/org/apache/hadoop/mapred/join/WrappedRecordReader.java:195:  @SuppressWarnings("unchecked") // Explicit type check prior to cast
./src/mapred/org/apache/hadoop/mapred/JSPUtil.java:40:   * @param request HTTP request Object.
./src/mapred/org/apache/hadoop/mapred/JSPUtil.java:41:   * @param response HTTP response object.
./src/mapred/org/apache/hadoop/mapred/JSPUtil.java:42:   * @param tracker {@link JobTracker} instance
./src/mapred/org/apache/hadoop/mapred/JSPUtil.java:43:   * @throws IOException
./src/mapred/org/apache/hadoop/mapred/JSPUtil.java:76:   * @param label display heading to be used in the job table.
./src/mapred/org/apache/hadoop/mapred/JSPUtil.java:77:   * @param jobs vector of jobs to be displayed in table.
./src/mapred/org/apache/hadoop/mapred/JSPUtil.java:78:   * @param refresh refresh interval to be used in jobdetails page.
./src/mapred/org/apache/hadoop/mapred/JSPUtil.java:79:   * @param rowId beginning row id to be used in the table.
./src/mapred/org/apache/hadoop/mapred/JSPUtil.java:80:   * @return
./src/mapred/org/apache/hadoop/mapred/JSPUtil.java:81:   * @throws IOException
./src/mapred/org/apache/hadoop/mapred/JVMId.java:69:  @Override
./src/mapred/org/apache/hadoop/mapred/JVMId.java:82:  @Override
./src/mapred/org/apache/hadoop/mapred/JVMId.java:96:  @Override
./src/mapred/org/apache/hadoop/mapred/JVMId.java:101:  @Override
./src/mapred/org/apache/hadoop/mapred/JVMId.java:108:  @Override
./src/mapred/org/apache/hadoop/mapred/JVMId.java:122:   * @return constructed JVMId object or null if the given String is null
./src/mapred/org/apache/hadoop/mapred/JVMId.java:123:   * @throws IllegalArgumentException if the given string is malformed
./src/mapred/org/apache/hadoop/mapred/KeyValueTextInputFormat.java:29: * An {@link InputFormat} for plain text files. Files are broken into lines.
./src/mapred/org/apache/hadoop/mapred/KillJobAction.java:27: * Represents a directive from the {@link org.apache.hadoop.mapred.JobTracker} 
./src/mapred/org/apache/hadoop/mapred/KillJobAction.java:28: * to the {@link org.apache.hadoop.mapred.TaskTracker} to kill the task of 
./src/mapred/org/apache/hadoop/mapred/KillJobAction.java:48:  @Override
./src/mapred/org/apache/hadoop/mapred/KillJobAction.java:53:  @Override
./src/mapred/org/apache/hadoop/mapred/KillTaskAction.java:27: * Represents a directive from the {@link org.apache.hadoop.mapred.JobTracker} 
./src/mapred/org/apache/hadoop/mapred/KillTaskAction.java:28: * to the {@link org.apache.hadoop.mapred.TaskTracker} to kill a task.
./src/mapred/org/apache/hadoop/mapred/KillTaskAction.java:47:  @Override
./src/mapred/org/apache/hadoop/mapred/KillTaskAction.java:52:  @Override
./src/mapred/org/apache/hadoop/mapred/LaunchTaskAction.java:26: * Represents a directive from the {@link org.apache.hadoop.mapred.JobTracker} 
./src/mapred/org/apache/hadoop/mapred/LaunchTaskAction.java:27: * to the {@link org.apache.hadoop.mapred.TaskTracker} to launch a new task.
./src/mapred/org/apache/hadoop/mapred/lib/aggregate/DoubleValueSum.java:44:   * @param val
./src/mapred/org/apache/hadoop/mapred/lib/aggregate/DoubleValueSum.java:55:   * @param val
./src/mapred/org/apache/hadoop/mapred/lib/aggregate/DoubleValueSum.java:64:   * @return the string representation of the aggregated value
./src/mapred/org/apache/hadoop/mapred/lib/aggregate/DoubleValueSum.java:71:   * @return the aggregated value
./src/mapred/org/apache/hadoop/mapred/lib/aggregate/DoubleValueSum.java:85:   * @return return an array of one element. The element is a string
./src/mapred/org/apache/hadoop/mapred/lib/aggregate/LongValueMax.java:43:   * @param val
./src/mapred/org/apache/hadoop/mapred/lib/aggregate/LongValueMax.java:57:   * @param newVal
./src/mapred/org/apache/hadoop/mapred/lib/aggregate/LongValueMax.java:68:   * @return the aggregated value
./src/mapred/org/apache/hadoop/mapred/lib/aggregate/LongValueMax.java:75:   * @return the string representation of the aggregated value
./src/mapred/org/apache/hadoop/mapred/lib/aggregate/LongValueMax.java:89:   * @return return an array of one element. The element is a string
./src/mapred/org/apache/hadoop/mapred/lib/aggregate/LongValueMin.java:43:   * @param val
./src/mapred/org/apache/hadoop/mapred/lib/aggregate/LongValueMin.java:57:   * @param newVal
./src/mapred/org/apache/hadoop/mapred/lib/aggregate/LongValueMin.java:68:   * @return the aggregated value
./src/mapred/org/apache/hadoop/mapred/lib/aggregate/LongValueMin.java:75:   * @return the string representation of the aggregated value
./src/mapred/org/apache/hadoop/mapred/lib/aggregate/LongValueMin.java:89:   * @return return an array of one element. The element is a string
./src/mapred/org/apache/hadoop/mapred/lib/aggregate/LongValueSum.java:43:   * @param val
./src/mapred/org/apache/hadoop/mapred/lib/aggregate/LongValueSum.java:54:   * @param val
./src/mapred/org/apache/hadoop/mapred/lib/aggregate/LongValueSum.java:63:   * @return the aggregated value
./src/mapred/org/apache/hadoop/mapred/lib/aggregate/LongValueSum.java:70:   * @return the string representation of the aggregated value
./src/mapred/org/apache/hadoop/mapred/lib/aggregate/LongValueSum.java:84:   * @return return an array of one element. The element is a string
./src/mapred/org/apache/hadoop/mapred/lib/aggregate/StringValueMax.java:43:   * @param val
./src/mapred/org/apache/hadoop/mapred/lib/aggregate/StringValueMax.java:56:   * @return the aggregated value
./src/mapred/org/apache/hadoop/mapred/lib/aggregate/StringValueMax.java:63:   * @return the string representation of the aggregated value
./src/mapred/org/apache/hadoop/mapred/lib/aggregate/StringValueMax.java:77:   * @return return an array of one element. The element is a string
./src/mapred/org/apache/hadoop/mapred/lib/aggregate/StringValueMin.java:43:   * @param val
./src/mapred/org/apache/hadoop/mapred/lib/aggregate/StringValueMin.java:56:   * @return the aggregated value
./src/mapred/org/apache/hadoop/mapred/lib/aggregate/StringValueMin.java:63:   * @return the string representation of the aggregated value
./src/mapred/org/apache/hadoop/mapred/lib/aggregate/StringValueMin.java:77:   * @return return an array of one element. The element is a string
./src/mapred/org/apache/hadoop/mapred/lib/aggregate/UniqValueCount.java:48:   * @param maxNum the limit in the number of unique values to keep.
./src/mapred/org/apache/hadoop/mapred/lib/aggregate/UniqValueCount.java:62:   * @param n the desired limit on the number of unique values
./src/mapred/org/apache/hadoop/mapred/lib/aggregate/UniqValueCount.java:63:   * @return the new limit on the number of unique values
./src/mapred/org/apache/hadoop/mapred/lib/aggregate/UniqValueCount.java:77:   * @param val
./src/mapred/org/apache/hadoop/mapred/lib/aggregate/UniqValueCount.java:89:   * @return return the number of unique objects aggregated
./src/mapred/org/apache/hadoop/mapred/lib/aggregate/UniqValueCount.java:97:   * @return the set of the unique objects
./src/mapred/org/apache/hadoop/mapred/lib/aggregate/UniqValueCount.java:111:   * @return return an array of the unique objects. The return value is
./src/mapred/org/apache/hadoop/mapred/lib/aggregate/UserDefinedValueAggregatorDescriptor.java:45:   * @param className the name of the class
./src/mapred/org/apache/hadoop/mapred/lib/aggregate/UserDefinedValueAggregatorDescriptor.java:46:   * @return a dynamically created instance of the given class 
./src/mapred/org/apache/hadoop/mapred/lib/aggregate/UserDefinedValueAggregatorDescriptor.java:71:   * @param className the class name of the user defined descriptor class
./src/mapred/org/apache/hadoop/mapred/lib/aggregate/UserDefinedValueAggregatorDescriptor.java:72:   * @param job a configure object used for decriptor configuration
./src/mapred/org/apache/hadoop/mapred/lib/aggregate/UserDefinedValueAggregatorDescriptor.java:83:   * @param key
./src/mapred/org/apache/hadoop/mapred/lib/aggregate/UserDefinedValueAggregatorDescriptor.java:85:   * @param val
./src/mapred/org/apache/hadoop/mapred/lib/aggregate/UserDefinedValueAggregatorDescriptor.java:87:   * @return a list of aggregation id/value pairs. An aggregation id encodes an
./src/mapred/org/apache/hadoop/mapred/lib/aggregate/UserDefinedValueAggregatorDescriptor.java:101:   * @return the string representation of this object.
./src/mapred/org/apache/hadoop/mapred/lib/aggregate/ValueAggregator.java:32:   * @param val the value to be added
./src/mapred/org/apache/hadoop/mapred/lib/aggregate/ValueAggregator.java:43:   * @return the string representation of the agregator
./src/mapred/org/apache/hadoop/mapred/lib/aggregate/ValueAggregator.java:49:   * @return an array of values as the outputs of the combiner.
./src/mapred/org/apache/hadoop/mapred/lib/aggregate/ValueAggregatorBaseDescriptor.java:78:   * @param type the aggregation type
./src/mapred/org/apache/hadoop/mapred/lib/aggregate/ValueAggregatorBaseDescriptor.java:79:   * @param id the aggregation id
./src/mapred/org/apache/hadoop/mapred/lib/aggregate/ValueAggregatorBaseDescriptor.java:80:   * @param val the val associated with the id to be aggregated
./src/mapred/org/apache/hadoop/mapred/lib/aggregate/ValueAggregatorBaseDescriptor.java:81:   * @return an Entry whose key is the aggregation id prefixed with 
./src/mapred/org/apache/hadoop/mapred/lib/aggregate/ValueAggregatorBaseDescriptor.java:91:   * @param type the aggregation type
./src/mapred/org/apache/hadoop/mapred/lib/aggregate/ValueAggregatorBaseDescriptor.java:92:   * @return a value aggregator of the given type.
./src/mapred/org/apache/hadoop/mapred/lib/aggregate/ValueAggregatorBaseDescriptor.java:124:   * @param key
./src/mapred/org/apache/hadoop/mapred/lib/aggregate/ValueAggregatorBaseDescriptor.java:126:   * @param val
./src/mapred/org/apache/hadoop/mapred/lib/aggregate/ValueAggregatorBaseDescriptor.java:128:   * @return a list of aggregation id/value pairs. An aggregation id encodes an
./src/mapred/org/apache/hadoop/mapred/lib/aggregate/ValueAggregatorBaseDescriptor.java:153:   * @param job a job configuration object
./src/mapred/org/apache/hadoop/mapred/lib/aggregate/ValueAggregatorCombiner.java:46:   * @param key the key is expected to be a Text object, whose prefix indicates
./src/mapred/org/apache/hadoop/mapred/lib/aggregate/ValueAggregatorCombiner.java:48:   * @param values the values to combine
./src/mapred/org/apache/hadoop/mapred/lib/aggregate/ValueAggregatorCombiner.java:49:   * @param output to collect combined values
./src/mapred/org/apache/hadoop/mapred/lib/aggregate/ValueAggregatorDescriptor.java:49:   * @param key
./src/mapred/org/apache/hadoop/mapred/lib/aggregate/ValueAggregatorDescriptor.java:51:   * @param val
./src/mapred/org/apache/hadoop/mapred/lib/aggregate/ValueAggregatorDescriptor.java:53:   * @return a list of aggregation id/value pairs. An aggregation id encodes an
./src/mapred/org/apache/hadoop/mapred/lib/aggregate/ValueAggregatorDescriptor.java:63:   * @param job
./src/mapred/org/apache/hadoop/mapred/lib/aggregate/ValueAggregatorJob.java:103:   * @param args the arguments used for job creation. Generic hadoop
./src/mapred/org/apache/hadoop/mapred/lib/aggregate/ValueAggregatorJob.java:105:   * @return a JobConf object ready for submission.
./src/mapred/org/apache/hadoop/mapred/lib/aggregate/ValueAggregatorJob.java:107:   * @throws IOException
./src/mapred/org/apache/hadoop/mapred/lib/aggregate/ValueAggregatorJob.java:108:   * @see GenericOptionsParser
./src/mapred/org/apache/hadoop/mapred/lib/aggregate/ValueAggregatorJob.java:203:   * @param args the arguments used for job creation
./src/mapred/org/apache/hadoop/mapred/lib/aggregate/ValueAggregatorJob.java:204:   * @throws IOException
./src/mapred/org/apache/hadoop/mapred/lib/aggregate/ValueAggregatorReducer.java:40:   * @param key
./src/mapred/org/apache/hadoop/mapred/lib/aggregate/ValueAggregatorReducer.java:46:   * @value the values to be aggregated
./src/mapred/org/apache/hadoop/mapred/lib/aggregate/ValueHistogram.java:44:   * @param val the value to be added. It is expected to be a string
./src/mapred/org/apache/hadoop/mapred/lib/aggregate/ValueHistogram.java:69:   * @return the string representation of this aggregator.
./src/mapred/org/apache/hadoop/mapred/lib/aggregate/ValueHistogram.java:131:   * @return a string representation of the list of value/frequence pairs of 
./src/mapred/org/apache/hadoop/mapred/lib/aggregate/ValueHistogram.java:148:   *  @return a list value/frequence pairs.
./src/mapred/org/apache/hadoop/mapred/lib/aggregate/ValueHistogram.java:166:   * @return a TreeMap representation of the histogram
./src/mapred/org/apache/hadoop/mapred/lib/Chain.java:40: * {@link ChainMapper} and the {@link ChainReducer} classes.
./src/mapred/org/apache/hadoop/mapred/lib/Chain.java:91:   * @param isMap TRUE indicates the chain is for a Mapper, FALSE that is for a
./src/mapred/org/apache/hadoop/mapred/lib/Chain.java:102:   * @param isMap TRUE for Mapper, FALSE for Reducer.
./src/mapred/org/apache/hadoop/mapred/lib/Chain.java:103:   * @return the prefix to use.
./src/mapred/org/apache/hadoop/mapred/lib/Chain.java:110:   * Creates a {@link JobConf} for one of the Maps or Reduce in the chain.
./src/mapred/org/apache/hadoop/mapred/lib/Chain.java:116:   * @param jobConf the chain job's JobConf.
./src/mapred/org/apache/hadoop/mapred/lib/Chain.java:117:   * @param confKey the key for chain element configuration serialized in the
./src/mapred/org/apache/hadoop/mapred/lib/Chain.java:119:   * @return a new JobConf aggregating the chain job's JobConf with the chain
./src/mapred/org/apache/hadoop/mapred/lib/Chain.java:148:   * @param isMap            indicates if the Chain is for a Mapper or for a
./src/mapred/org/apache/hadoop/mapred/lib/Chain.java:150:   * @param jobConf              chain job's JobConf to add the Mapper class.
./src/mapred/org/apache/hadoop/mapred/lib/Chain.java:151:   * @param klass            the Mapper class to add.
./src/mapred/org/apache/hadoop/mapred/lib/Chain.java:152:   * @param inputKeyClass    mapper input key class.
./src/mapred/org/apache/hadoop/mapred/lib/Chain.java:153:   * @param inputValueClass  mapper input value class.
./src/mapred/org/apache/hadoop/mapred/lib/Chain.java:154:   * @param outputKeyClass   mapper output key class.
./src/mapred/org/apache/hadoop/mapred/lib/Chain.java:155:   * @param outputValueClass mapper output value class.
./src/mapred/org/apache/hadoop/mapred/lib/Chain.java:156:   * @param byValue          indicates if key/values should be passed by value
./src/mapred/org/apache/hadoop/mapred/lib/Chain.java:158:   * @param mapperConf       a JobConf with the configuration for the Mapper
./src/mapred/org/apache/hadoop/mapred/lib/Chain.java:256:   * @param jobConf              chain job's JobConf to add the Reducer class.
./src/mapred/org/apache/hadoop/mapred/lib/Chain.java:257:   * @param klass            the Reducer class to add.
./src/mapred/org/apache/hadoop/mapred/lib/Chain.java:258:   * @param inputKeyClass    reducer input key class.
./src/mapred/org/apache/hadoop/mapred/lib/Chain.java:259:   * @param inputValueClass  reducer input value class.
./src/mapred/org/apache/hadoop/mapred/lib/Chain.java:260:   * @param outputKeyClass   reducer output key class.
./src/mapred/org/apache/hadoop/mapred/lib/Chain.java:261:   * @param outputValueClass reducer output value class.
./src/mapred/org/apache/hadoop/mapred/lib/Chain.java:262:   * @param byValue          indicates if key/values should be passed by value
./src/mapred/org/apache/hadoop/mapred/lib/Chain.java:264:   * @param reducerConf      a JobConf with the configuration for the Reducer
./src/mapred/org/apache/hadoop/mapred/lib/Chain.java:317:   * @param jobConf chain job's JobConf.
./src/mapred/org/apache/hadoop/mapred/lib/Chain.java:364:   * @return the chain job conf.
./src/mapred/org/apache/hadoop/mapred/lib/Chain.java:373:   * @return the first Mapper instance in the chain or NULL if none.
./src/mapred/org/apache/hadoop/mapred/lib/Chain.java:382:   * @return the Reducer instance in the chain or NULL if none.
./src/mapred/org/apache/hadoop/mapred/lib/Chain.java:391:   * @param mapperIndex index of the Mapper instance to get the OutputCollector.
./src/mapred/org/apache/hadoop/mapred/lib/Chain.java:392:   * @param output      the original OutputCollector of the task.
./src/mapred/org/apache/hadoop/mapred/lib/Chain.java:393:   * @param reporter    the reporter of the task.
./src/mapred/org/apache/hadoop/mapred/lib/Chain.java:394:   * @return the OutputCollector to be used in the chain.
./src/mapred/org/apache/hadoop/mapred/lib/Chain.java:396:  @SuppressWarnings({"unchecked"})
./src/mapred/org/apache/hadoop/mapred/lib/Chain.java:410:   * @param output   the original OutputCollector of the task.
./src/mapred/org/apache/hadoop/mapred/lib/Chain.java:411:   * @param reporter the reporter of the task.
./src/mapred/org/apache/hadoop/mapred/lib/Chain.java:412:   * @return the OutputCollector to be used in the chain.
./src/mapred/org/apache/hadoop/mapred/lib/Chain.java:414:  @SuppressWarnings({"unchecked"})
./src/mapred/org/apache/hadoop/mapred/lib/Chain.java:425:   * @throws IOException thrown if any of the chain elements threw an
./src/mapred/org/apache/hadoop/mapred/lib/Chain.java:450:   * If it is not the end of the chain, a {@link #collect} invocation invokes
./src/mapred/org/apache/hadoop/mapred/lib/Chain.java:487:    @SuppressWarnings({"unchecked"})
./src/mapred/org/apache/hadoop/mapred/lib/ChainMapper.java:110:   * @param job              job's JobConf to add the Mapper class.
./src/mapred/org/apache/hadoop/mapred/lib/ChainMapper.java:111:   * @param klass            the Mapper class to add.
./src/mapred/org/apache/hadoop/mapred/lib/ChainMapper.java:112:   * @param inputKeyClass    mapper input key class.
./src/mapred/org/apache/hadoop/mapred/lib/ChainMapper.java:113:   * @param inputValueClass  mapper input value class.
./src/mapred/org/apache/hadoop/mapred/lib/ChainMapper.java:114:   * @param outputKeyClass   mapper output key class.
./src/mapred/org/apache/hadoop/mapred/lib/ChainMapper.java:115:   * @param outputValueClass mapper output value class.
./src/mapred/org/apache/hadoop/mapred/lib/ChainMapper.java:116:   * @param byValue          indicates if key/values should be passed by value
./src/mapred/org/apache/hadoop/mapred/lib/ChainMapper.java:118:   * @param mapperConf       a JobConf with the configuration for the Mapper
./src/mapred/org/apache/hadoop/mapred/lib/ChainMapper.java:158:  @SuppressWarnings({"unchecked"})
./src/mapred/org/apache/hadoop/mapred/lib/ChainReducer.java:110:   * @param job              job's JobConf to add the Reducer class.
./src/mapred/org/apache/hadoop/mapred/lib/ChainReducer.java:111:   * @param klass            the Reducer class to add.
./src/mapred/org/apache/hadoop/mapred/lib/ChainReducer.java:112:   * @param inputKeyClass    reducer input key class.
./src/mapred/org/apache/hadoop/mapred/lib/ChainReducer.java:113:   * @param inputValueClass  reducer input value class.
./src/mapred/org/apache/hadoop/mapred/lib/ChainReducer.java:114:   * @param outputKeyClass   reducer output key class.
./src/mapred/org/apache/hadoop/mapred/lib/ChainReducer.java:115:   * @param outputValueClass reducer output value class.
./src/mapred/org/apache/hadoop/mapred/lib/ChainReducer.java:116:   * @param byValue          indicates if key/values should be passed by value
./src/mapred/org/apache/hadoop/mapred/lib/ChainReducer.java:118:   * @param reducerConf      a JobConf with the configuration for the Reducer
./src/mapred/org/apache/hadoop/mapred/lib/ChainReducer.java:154:   * @param job              chain job's JobConf to add the Mapper class.
./src/mapred/org/apache/hadoop/mapred/lib/ChainReducer.java:155:   * @param klass            the Mapper class to add.
./src/mapred/org/apache/hadoop/mapred/lib/ChainReducer.java:156:   * @param inputKeyClass    mapper input key class.
./src/mapred/org/apache/hadoop/mapred/lib/ChainReducer.java:157:   * @param inputValueClass  mapper input value class.
./src/mapred/org/apache/hadoop/mapred/lib/ChainReducer.java:158:   * @param outputKeyClass   mapper output key class.
./src/mapred/org/apache/hadoop/mapred/lib/ChainReducer.java:159:   * @param outputValueClass mapper output value class.
./src/mapred/org/apache/hadoop/mapred/lib/ChainReducer.java:160:   * @param byValue          indicates if key/values should be passed by value
./src/mapred/org/apache/hadoop/mapred/lib/ChainReducer.java:162:   * @param mapperConf       a JobConf with the configuration for the Mapper
./src/mapred/org/apache/hadoop/mapred/lib/ChainReducer.java:202:  @SuppressWarnings({"unchecked"})
./src/mapred/org/apache/hadoop/mapred/lib/db/DBConfiguration.java:32: * {@link DBInputFormat}, and {@link DBOutputFormat}. 
./src/mapred/org/apache/hadoop/mapred/lib/db/DBConfiguration.java:37: * @see DBConfiguration#configureDB(JobConf, String, String, String, String)
./src/mapred/org/apache/hadoop/mapred/lib/db/DBConfiguration.java:38: * @see DBInputFormat#setInput(JobConf, Class, String, String)
./src/mapred/org/apache/hadoop/mapred/lib/db/DBConfiguration.java:39: * @see DBInputFormat#setInput(JobConf, Class, String, String, String, String...)
./src/mapred/org/apache/hadoop/mapred/lib/db/DBConfiguration.java:40: * @see DBOutputFormat#setOutput(JobConf, String, String...)
./src/mapred/org/apache/hadoop/mapred/lib/db/DBConfiguration.java:85:   * @param job the job
./src/mapred/org/apache/hadoop/mapred/lib/db/DBConfiguration.java:86:   * @param driverClass JDBC Driver class name
./src/mapred/org/apache/hadoop/mapred/lib/db/DBConfiguration.java:87:   * @param dbUrl JDBC DB access URL. 
./src/mapred/org/apache/hadoop/mapred/lib/db/DBConfiguration.java:88:   * @param userName DB access username 
./src/mapred/org/apache/hadoop/mapred/lib/db/DBConfiguration.java:89:   * @param passwd DB access passwd
./src/mapred/org/apache/hadoop/mapred/lib/db/DBConfiguration.java:104:   * @param job the job
./src/mapred/org/apache/hadoop/mapred/lib/db/DBConfiguration.java:105:   * @param driverClass JDBC Driver class name
./src/mapred/org/apache/hadoop/mapred/lib/db/DBConfiguration.java:106:   * @param dbUrl JDBC DB access URL. 
./src/mapred/org/apache/hadoop/mapred/lib/db/DBConfiguration.java:119:   * @throws ClassNotFoundException 
./src/mapred/org/apache/hadoop/mapred/lib/db/DBConfiguration.java:120:   * @throws SQLException */
./src/mapred/org/apache/hadoop/mapred/lib/db/DBInputFormat.java:71:     * @param split The InputSplit to read data for
./src/mapred/org/apache/hadoop/mapred/lib/db/DBInputFormat.java:72:     * @throws SQLException 
./src/mapred/org/apache/hadoop/mapred/lib/db/DBInputFormat.java:123:    /** {@inheritDoc} */
./src/mapred/org/apache/hadoop/mapred/lib/db/DBInputFormat.java:134:    /** {@inheritDoc} */
./src/mapred/org/apache/hadoop/mapred/lib/db/DBInputFormat.java:139:    /** {@inheritDoc} */
./src/mapred/org/apache/hadoop/mapred/lib/db/DBInputFormat.java:144:    /** {@inheritDoc} */
./src/mapred/org/apache/hadoop/mapred/lib/db/DBInputFormat.java:149:    /** {@inheritDoc} */
./src/mapred/org/apache/hadoop/mapred/lib/db/DBInputFormat.java:154:    /** {@inheritDoc} */
./src/mapred/org/apache/hadoop/mapred/lib/db/DBInputFormat.java:177:    @Override
./src/mapred/org/apache/hadoop/mapred/lib/db/DBInputFormat.java:179:    @Override
./src/mapred/org/apache/hadoop/mapred/lib/db/DBInputFormat.java:181:    @Override
./src/mapred/org/apache/hadoop/mapred/lib/db/DBInputFormat.java:183:    @Override
./src/mapred/org/apache/hadoop/mapred/lib/db/DBInputFormat.java:202:     * @param start the index of the first row to select
./src/mapred/org/apache/hadoop/mapred/lib/db/DBInputFormat.java:203:     * @param end the index of the last row to select
./src/mapred/org/apache/hadoop/mapred/lib/db/DBInputFormat.java:210:    /** {@inheritDoc} */
./src/mapred/org/apache/hadoop/mapred/lib/db/DBInputFormat.java:217:     * @return The index of the first row to select
./src/mapred/org/apache/hadoop/mapred/lib/db/DBInputFormat.java:224:     * @return The index of the last row to select
./src/mapred/org/apache/hadoop/mapred/lib/db/DBInputFormat.java:231:     * @return The total row count in this split
./src/mapred/org/apache/hadoop/mapred/lib/db/DBInputFormat.java:237:    /** {@inheritDoc} */
./src/mapred/org/apache/hadoop/mapred/lib/db/DBInputFormat.java:243:    /** {@inheritDoc} */
./src/mapred/org/apache/hadoop/mapred/lib/db/DBInputFormat.java:260:  /** {@inheritDoc} */
./src/mapred/org/apache/hadoop/mapred/lib/db/DBInputFormat.java:279:  /** {@inheritDoc} */
./src/mapred/org/apache/hadoop/mapred/lib/db/DBInputFormat.java:280:  @SuppressWarnings("unchecked")
./src/mapred/org/apache/hadoop/mapred/lib/db/DBInputFormat.java:293:  /** {@inheritDoc} */
./src/mapred/org/apache/hadoop/mapred/lib/db/DBInputFormat.java:349:   * @param job The job
./src/mapred/org/apache/hadoop/mapred/lib/db/DBInputFormat.java:350:   * @param inputClass the class object implementing DBWritable, which is the 
./src/mapred/org/apache/hadoop/mapred/lib/db/DBInputFormat.java:352:   * @param tableName The table to read data from
./src/mapred/org/apache/hadoop/mapred/lib/db/DBInputFormat.java:353:   * @param conditions The condition which to select data with, eg. '(updated >
./src/mapred/org/apache/hadoop/mapred/lib/db/DBInputFormat.java:355:   * @param orderBy the fieldNames in the orderBy clause.
./src/mapred/org/apache/hadoop/mapred/lib/db/DBInputFormat.java:356:   * @param fieldNames The field names in the table
./src/mapred/org/apache/hadoop/mapred/lib/db/DBInputFormat.java:357:   * @see #setInput(JobConf, Class, String, String)
./src/mapred/org/apache/hadoop/mapred/lib/db/DBInputFormat.java:374:   * @param job The job
./src/mapred/org/apache/hadoop/mapred/lib/db/DBInputFormat.java:375:   * @param inputClass the class object implementing DBWritable, which is the 
./src/mapred/org/apache/hadoop/mapred/lib/db/DBInputFormat.java:377:   * @param inputQuery the input query to select fields. Example : 
./src/mapred/org/apache/hadoop/mapred/lib/db/DBInputFormat.java:379:   * @param inputCountQuery the input query that returns the number of records in
./src/mapred/org/apache/hadoop/mapred/lib/db/DBInputFormat.java:382:   * @see #setInput(JobConf, Class, String, String, String, String...)
./src/mapred/org/apache/hadoop/mapred/lib/db/DBOutputFormat.java:39: * {@link DBOutputFormat} accepts &lt;key,value&gt; pairs, where 
./src/mapred/org/apache/hadoop/mapred/lib/db/DBOutputFormat.java:40: * key has a type extending DBWritable. Returned {@link RecordWriter} 
./src/mapred/org/apache/hadoop/mapred/lib/db/DBOutputFormat.java:65:    /** {@inheritDoc} */
./src/mapred/org/apache/hadoop/mapred/lib/db/DBOutputFormat.java:89:    /** {@inheritDoc} */
./src/mapred/org/apache/hadoop/mapred/lib/db/DBOutputFormat.java:119:  /** {@inheritDoc} */
./src/mapred/org/apache/hadoop/mapred/lib/db/DBOutputFormat.java:125:  /** {@inheritDoc} */
./src/mapred/org/apache/hadoop/mapred/lib/db/DBOutputFormat.java:148:   * @param job The job
./src/mapred/org/apache/hadoop/mapred/lib/db/DBOutputFormat.java:149:   * @param tableName The table to insert data into
./src/mapred/org/apache/hadoop/mapred/lib/db/DBOutputFormat.java:150:   * @param fieldNames The field names in the table
./src/mapred/org/apache/hadoop/mapred/lib/db/DBWritable.java:11: * <code>DBWritable</code>. DBWritable, is similar to {@link Writable} 
./src/mapred/org/apache/hadoop/mapred/lib/db/DBWritable.java:12: * except that the {@link #write(PreparedStatement)} method takes a 
./src/mapred/org/apache/hadoop/mapred/lib/db/DBWritable.java:13: * {@link PreparedStatement}, and {@link #readFields(ResultSet)} 
./src/mapred/org/apache/hadoop/mapred/lib/db/DBWritable.java:14: * takes a {@link ResultSet}. 
./src/mapred/org/apache/hadoop/mapred/lib/db/DBWritable.java:62:   * Sets the fields of the object in the {@link PreparedStatement}.
./src/mapred/org/apache/hadoop/mapred/lib/db/DBWritable.java:63:   * @param statement the statement that the fields are put into.
./src/mapred/org/apache/hadoop/mapred/lib/db/DBWritable.java:64:   * @throws SQLException
./src/mapred/org/apache/hadoop/mapred/lib/db/DBWritable.java:69:	 * Reads the fields of the object from the {@link ResultSet}. 
./src/mapred/org/apache/hadoop/mapred/lib/db/DBWritable.java:70:	 * @param resultSet the {@link ResultSet} to get the fields from.
./src/mapred/org/apache/hadoop/mapred/lib/db/DBWritable.java:71:	 * @throws SQLException
./src/mapred/org/apache/hadoop/mapred/lib/DelegatingInputFormat.java:40: * An {@link InputFormat} that delegates behaviour of paths to multiple other
./src/mapred/org/apache/hadoop/mapred/lib/DelegatingInputFormat.java:43: * @see MultipleInputs#addInputPath(JobConf, Path, Class, Class)
./src/mapred/org/apache/hadoop/mapred/lib/DelegatingInputFormat.java:114:  @SuppressWarnings("unchecked")
./src/mapred/org/apache/hadoop/mapred/lib/DelegatingMapper.java:31: * An {@link Mapper} that delegates behaviour of paths to multiple other
./src/mapred/org/apache/hadoop/mapred/lib/DelegatingMapper.java:34: * @see MultipleInputs#addInputPath(JobConf, Path, Class, Class)
./src/mapred/org/apache/hadoop/mapred/lib/DelegatingMapper.java:42:  @SuppressWarnings("unchecked")
./src/mapred/org/apache/hadoop/mapred/lib/FieldSelectionMapReduce.java:178:   * @param fieldListSpec an array of field specs
./src/mapred/org/apache/hadoop/mapred/lib/FieldSelectionMapReduce.java:179:   * @param fieldList an array of field numbers extracted from the specs.
./src/mapred/org/apache/hadoop/mapred/lib/FieldSelectionMapReduce.java:180:   * @return number n if some field spec is in the form of "n-", -1 otherwise.
./src/mapred/org/apache/hadoop/mapred/lib/HashPartitioner.java:24:/** Partition keys by their {@link Object#hashCode()}. */
./src/mapred/org/apache/hadoop/mapred/lib/HashPartitioner.java:29:  /** Use {@link Object#hashCode()} to partition. */
./src/mapred/org/apache/hadoop/mapred/lib/InputSampler.java:47: * {@link org.apache.hadoop.mapred.lib.TotalOrderPartitioner}.
./src/mapred/org/apache/hadoop/mapred/lib/InputSampler.java:87:   * Interface to sample using an {@link org.apache.hadoop.mapred.InputFormat}.
./src/mapred/org/apache/hadoop/mapred/lib/InputSampler.java:109:     * @param numSamples Total number of samples to obtain from all selected
./src/mapred/org/apache/hadoop/mapred/lib/InputSampler.java:118:     * @param numSamples Total number of samples to obtain from all selected
./src/mapred/org/apache/hadoop/mapred/lib/InputSampler.java:120:     * @param maxSplitsSampled The maximum number of splits to examine.
./src/mapred/org/apache/hadoop/mapred/lib/InputSampler.java:130:    @SuppressWarnings("unchecked") // ArrayList::toArray doesn't preserve type
./src/mapred/org/apache/hadoop/mapred/lib/InputSampler.java:170:     * @param freq Probability with which a key will be chosen.
./src/mapred/org/apache/hadoop/mapred/lib/InputSampler.java:171:     * @param numSamples Total number of samples to obtain from all selected
./src/mapred/org/apache/hadoop/mapred/lib/InputSampler.java:180:     * @param freq Probability with which a key will be chosen.
./src/mapred/org/apache/hadoop/mapred/lib/InputSampler.java:181:     * @param numSamples Total number of samples to obtain from all selected
./src/mapred/org/apache/hadoop/mapred/lib/InputSampler.java:183:     * @param maxSplitsSampled The maximum number of splits to examine.
./src/mapred/org/apache/hadoop/mapred/lib/InputSampler.java:197:    @SuppressWarnings("unchecked") // ArrayList::toArray doesn't preserve type
./src/mapred/org/apache/hadoop/mapred/lib/InputSampler.java:257:     * @param freq The frequency with which records will be emitted.
./src/mapred/org/apache/hadoop/mapred/lib/InputSampler.java:265:     * @param freq The frequency with which records will be emitted.
./src/mapred/org/apache/hadoop/mapred/lib/InputSampler.java:266:     * @param maxSplitsSampled The maximum number of splits to examine.
./src/mapred/org/apache/hadoop/mapred/lib/InputSampler.java:267:     * @see #getSample
./src/mapred/org/apache/hadoop/mapred/lib/InputSampler.java:279:    @SuppressWarnings("unchecked") // ArrayList::toArray doesn't preserve type
./src/mapred/org/apache/hadoop/mapred/lib/InputSampler.java:310:   * returned from {@link
./src/mapred/org/apache/hadoop/mapred/lib/InputSampler.java:313:  @SuppressWarnings("unchecked") // getInputFormat, getOutputKeyComparator
./src/mapred/org/apache/hadoop/mapred/lib/InputSampler.java:346:   * Configures a JobConf instance and calls {@link #writePartitionFile}.
./src/mapred/org/apache/hadoop/mapred/lib/InverseMapper.java:28:/** A {@link Mapper} that swaps keys and values. */
./src/mapred/org/apache/hadoop/mapred/lib/KeyFieldBasedPartitioner.java:32:  *  {@link KeyFieldBasedComparator}.
./src/mapred/org/apache/hadoop/mapred/lib/KeyFieldHelper.java:29: * This is used in {@link KeyFieldBasedComparator} & 
./src/mapred/org/apache/hadoop/mapred/lib/KeyFieldHelper.java:30: * {@link KeyFieldBasedPartitioner}. Defines all the methods
./src/mapred/org/apache/hadoop/mapred/lib/KeyFieldHelper.java:67:   * {@link KeyFieldBasedPartitioner} */
./src/mapred/org/apache/hadoop/mapred/lib/LongSumReducer.java:31:/** A {@link Reducer} that sums long values. */
./src/mapred/org/apache/hadoop/mapred/lib/MultipleInputs.java:32: * a different {@link InputFormat} and {@link Mapper} for each path 
./src/mapred/org/apache/hadoop/mapred/lib/MultipleInputs.java:36:   * Add a {@link Path} with a custom {@link InputFormat} to the list of
./src/mapred/org/apache/hadoop/mapred/lib/MultipleInputs.java:39:   * @param conf The configuration of the job
./src/mapred/org/apache/hadoop/mapred/lib/MultipleInputs.java:40:   * @param path {@link Path} to be added to the list of inputs for the job
./src/mapred/org/apache/hadoop/mapred/lib/MultipleInputs.java:41:   * @param inputFormatClass {@link InputFormat} class to use for this path
./src/mapred/org/apache/hadoop/mapred/lib/MultipleInputs.java:57:   * Add a {@link Path} with a custom {@link InputFormat} and
./src/mapred/org/apache/hadoop/mapred/lib/MultipleInputs.java:58:   * {@link Mapper} to the list of inputs for the map-reduce job.
./src/mapred/org/apache/hadoop/mapred/lib/MultipleInputs.java:60:   * @param conf The configuration of the job
./src/mapred/org/apache/hadoop/mapred/lib/MultipleInputs.java:61:   * @param path {@link Path} to be added to the list of inputs for the job
./src/mapred/org/apache/hadoop/mapred/lib/MultipleInputs.java:62:   * @param inputFormatClass {@link InputFormat} class to use for this path
./src/mapred/org/apache/hadoop/mapred/lib/MultipleInputs.java:63:   * @param mapperClass {@link Mapper} class to use for this path
./src/mapred/org/apache/hadoop/mapred/lib/MultipleInputs.java:80:   * Retrieves a map of {@link Path}s to the {@link InputFormat} class
./src/mapred/org/apache/hadoop/mapred/lib/MultipleInputs.java:83:   * @param conf The confuration of the job
./src/mapred/org/apache/hadoop/mapred/lib/MultipleInputs.java:84:   * @see #addInputPath(JobConf, Path, Class)
./src/mapred/org/apache/hadoop/mapred/lib/MultipleInputs.java:85:   * @return A map of paths to inputformats for the job
./src/mapred/org/apache/hadoop/mapred/lib/MultipleInputs.java:105:   * Retrieves a map of {@link Path}s to the {@link Mapper} class that
./src/mapred/org/apache/hadoop/mapred/lib/MultipleInputs.java:108:   * @param conf The confuration of the job
./src/mapred/org/apache/hadoop/mapred/lib/MultipleInputs.java:109:   * @see #addInputPath(JobConf, Path, Class, Class)
./src/mapred/org/apache/hadoop/mapred/lib/MultipleInputs.java:110:   * @return A map of paths to mappers for the job
./src/mapred/org/apache/hadoop/mapred/lib/MultipleInputs.java:112:  @SuppressWarnings("unchecked")
./src/mapred/org/apache/hadoop/mapred/lib/MultipleOutputFormat.java:58:   * @param fs
./src/mapred/org/apache/hadoop/mapred/lib/MultipleOutputFormat.java:60:   * @param job
./src/mapred/org/apache/hadoop/mapred/lib/MultipleOutputFormat.java:62:   * @param name
./src/mapred/org/apache/hadoop/mapred/lib/MultipleOutputFormat.java:64:   * @param arg3
./src/mapred/org/apache/hadoop/mapred/lib/MultipleOutputFormat.java:66:   * @return a composite record writer
./src/mapred/org/apache/hadoop/mapred/lib/MultipleOutputFormat.java:67:   * @throws IOException
./src/mapred/org/apache/hadoop/mapred/lib/MultipleOutputFormat.java:120:   * @param name
./src/mapred/org/apache/hadoop/mapred/lib/MultipleOutputFormat.java:122:   * @return the given leaf file name
./src/mapred/org/apache/hadoop/mapred/lib/MultipleOutputFormat.java:133:   * @param key
./src/mapred/org/apache/hadoop/mapred/lib/MultipleOutputFormat.java:135:   * @param name
./src/mapred/org/apache/hadoop/mapred/lib/MultipleOutputFormat.java:137:   * @return generated file name
./src/mapred/org/apache/hadoop/mapred/lib/MultipleOutputFormat.java:147:   * @param key
./src/mapred/org/apache/hadoop/mapred/lib/MultipleOutputFormat.java:149:   * @param value
./src/mapred/org/apache/hadoop/mapred/lib/MultipleOutputFormat.java:151:   * @return the actual key derived from the given key/value
./src/mapred/org/apache/hadoop/mapred/lib/MultipleOutputFormat.java:161:   * @param key
./src/mapred/org/apache/hadoop/mapred/lib/MultipleOutputFormat.java:163:   * @param value
./src/mapred/org/apache/hadoop/mapred/lib/MultipleOutputFormat.java:165:   * @return the actual value derived from the given key/value
./src/mapred/org/apache/hadoop/mapred/lib/MultipleOutputFormat.java:181:   * @param job
./src/mapred/org/apache/hadoop/mapred/lib/MultipleOutputFormat.java:183:   * @param name
./src/mapred/org/apache/hadoop/mapred/lib/MultipleOutputFormat.java:185:   * @return the outfile name based on a given anme and the input file name.
./src/mapred/org/apache/hadoop/mapred/lib/MultipleOutputFormat.java:213:   * @param fs
./src/mapred/org/apache/hadoop/mapred/lib/MultipleOutputFormat.java:215:   * @param job
./src/mapred/org/apache/hadoop/mapred/lib/MultipleOutputFormat.java:217:   * @param name
./src/mapred/org/apache/hadoop/mapred/lib/MultipleOutputFormat.java:220:   * @param arg3
./src/mapred/org/apache/hadoop/mapred/lib/MultipleOutputFormat.java:222:   * @return A RecordWriter object over the given file
./src/mapred/org/apache/hadoop/mapred/lib/MultipleOutputFormat.java:223:   * @throws IOException
./src/mapred/org/apache/hadoop/mapred/lib/MultipleOutputs.java:51: * group is the {@link MultipleOutputs} class name.
./src/mapred/org/apache/hadoop/mapred/lib/MultipleOutputs.java:137:   * @param conf           job conf
./src/mapred/org/apache/hadoop/mapred/lib/MultipleOutputs.java:138:   * @param namedOutput    named output names
./src/mapred/org/apache/hadoop/mapred/lib/MultipleOutputs.java:139:   * @param alreadyDefined whether the existence/non-existence of
./src/mapred/org/apache/hadoop/mapred/lib/MultipleOutputs.java:141:   * @throws IllegalArgumentException if the output name is alreadyDefined or
./src/mapred/org/apache/hadoop/mapred/lib/MultipleOutputs.java:160:   * @param namedOutput named output Name
./src/mapred/org/apache/hadoop/mapred/lib/MultipleOutputs.java:161:   * @throws IllegalArgumentException if the output name is not valid.
./src/mapred/org/apache/hadoop/mapred/lib/MultipleOutputs.java:186:   * @param namedOutput named output Name
./src/mapred/org/apache/hadoop/mapred/lib/MultipleOutputs.java:187:   * @throws IllegalArgumentException if the output name is not valid.
./src/mapred/org/apache/hadoop/mapred/lib/MultipleOutputs.java:201:   * @param conf job conf
./src/mapred/org/apache/hadoop/mapred/lib/MultipleOutputs.java:202:   * @return List of channel Names
./src/mapred/org/apache/hadoop/mapred/lib/MultipleOutputs.java:217:   * @param conf        job conf
./src/mapred/org/apache/hadoop/mapred/lib/MultipleOutputs.java:218:   * @param namedOutput named output
./src/mapred/org/apache/hadoop/mapred/lib/MultipleOutputs.java:219:   * @return <code>true</code> if the name output is multi, <code>false</code>
./src/mapred/org/apache/hadoop/mapred/lib/MultipleOutputs.java:231:   * @param conf        job conf
./src/mapred/org/apache/hadoop/mapred/lib/MultipleOutputs.java:232:   * @param namedOutput named output
./src/mapred/org/apache/hadoop/mapred/lib/MultipleOutputs.java:233:   * @return namedOutput OutputFormat
./src/mapred/org/apache/hadoop/mapred/lib/MultipleOutputs.java:245:   * @param conf        job conf
./src/mapred/org/apache/hadoop/mapred/lib/MultipleOutputs.java:246:   * @param namedOutput named output
./src/mapred/org/apache/hadoop/mapred/lib/MultipleOutputs.java:247:   * @return class for the named output key
./src/mapred/org/apache/hadoop/mapred/lib/MultipleOutputs.java:259:   * @param conf        job conf
./src/mapred/org/apache/hadoop/mapred/lib/MultipleOutputs.java:260:   * @param namedOutput named output
./src/mapred/org/apache/hadoop/mapred/lib/MultipleOutputs.java:261:   * @return class of named output value
./src/mapred/org/apache/hadoop/mapred/lib/MultipleOutputs.java:274:   * @param conf              job conf to add the named output
./src/mapred/org/apache/hadoop/mapred/lib/MultipleOutputs.java:275:   * @param namedOutput       named output name, it has to be a word, letters
./src/mapred/org/apache/hadoop/mapred/lib/MultipleOutputs.java:279:   * @param outputFormatClass OutputFormat class.
./src/mapred/org/apache/hadoop/mapred/lib/MultipleOutputs.java:280:   * @param keyClass          key class
./src/mapred/org/apache/hadoop/mapred/lib/MultipleOutputs.java:281:   * @param valueClass        value class
./src/mapred/org/apache/hadoop/mapred/lib/MultipleOutputs.java:294:   * @param conf              job conf to add the named output
./src/mapred/org/apache/hadoop/mapred/lib/MultipleOutputs.java:295:   * @param namedOutput       named output name, it has to be a word, letters
./src/mapred/org/apache/hadoop/mapred/lib/MultipleOutputs.java:299:   * @param outputFormatClass OutputFormat class.
./src/mapred/org/apache/hadoop/mapred/lib/MultipleOutputs.java:300:   * @param keyClass          key class
./src/mapred/org/apache/hadoop/mapred/lib/MultipleOutputs.java:301:   * @param valueClass        value class
./src/mapred/org/apache/hadoop/mapred/lib/MultipleOutputs.java:314:   * @param conf              job conf to add the named output
./src/mapred/org/apache/hadoop/mapred/lib/MultipleOutputs.java:315:   * @param namedOutput       named output name, it has to be a word, letters
./src/mapred/org/apache/hadoop/mapred/lib/MultipleOutputs.java:319:   * @param multi             indicates if the named output is multi
./src/mapred/org/apache/hadoop/mapred/lib/MultipleOutputs.java:320:   * @param outputFormatClass OutputFormat class.
./src/mapred/org/apache/hadoop/mapred/lib/MultipleOutputs.java:321:   * @param keyClass          key class
./src/mapred/org/apache/hadoop/mapred/lib/MultipleOutputs.java:322:   * @param valueClass        value class
./src/mapred/org/apache/hadoop/mapred/lib/MultipleOutputs.java:344:   * The counters group is the {@link MultipleOutputs} class name.
./src/mapred/org/apache/hadoop/mapred/lib/MultipleOutputs.java:350:   * @param conf    job conf to enableadd the named output.
./src/mapred/org/apache/hadoop/mapred/lib/MultipleOutputs.java:351:   * @param enabled indicates if the counters will be enabled or not.
./src/mapred/org/apache/hadoop/mapred/lib/MultipleOutputs.java:363:   * The counters group is the {@link MultipleOutputs} class name.
./src/mapred/org/apache/hadoop/mapred/lib/MultipleOutputs.java:370:   * @param conf    job conf to enableadd the named output.
./src/mapred/org/apache/hadoop/mapred/lib/MultipleOutputs.java:371:   * @return TRUE if the counters are enabled, FALSE if they are disabled.
./src/mapred/org/apache/hadoop/mapred/lib/MultipleOutputs.java:389:   * @param job the job configuration object
./src/mapred/org/apache/hadoop/mapred/lib/MultipleOutputs.java:403:   * @return iterator with the defined named outputs
./src/mapred/org/apache/hadoop/mapred/lib/MultipleOutputs.java:453:    @SuppressWarnings({"unchecked"})
./src/mapred/org/apache/hadoop/mapred/lib/MultipleOutputs.java:468:   * @param namedOutput the named output name
./src/mapred/org/apache/hadoop/mapred/lib/MultipleOutputs.java:469:   * @param reporter    the reporter
./src/mapred/org/apache/hadoop/mapred/lib/MultipleOutputs.java:470:   * @return the output collector for the given named output
./src/mapred/org/apache/hadoop/mapred/lib/MultipleOutputs.java:471:   * @throws IOException thrown if output collector could not be created
./src/mapred/org/apache/hadoop/mapred/lib/MultipleOutputs.java:473:  @SuppressWarnings({"unchecked"})
./src/mapred/org/apache/hadoop/mapred/lib/MultipleOutputs.java:483:   * @param namedOutput the named output name
./src/mapred/org/apache/hadoop/mapred/lib/MultipleOutputs.java:484:   * @param multiName   the multi name part
./src/mapred/org/apache/hadoop/mapred/lib/MultipleOutputs.java:485:   * @param reporter    the reporter
./src/mapred/org/apache/hadoop/mapred/lib/MultipleOutputs.java:486:   * @return the output collector for the given named output
./src/mapred/org/apache/hadoop/mapred/lib/MultipleOutputs.java:487:   * @throws IOException thrown if output collector could not be created
./src/mapred/org/apache/hadoop/mapred/lib/MultipleOutputs.java:489:  @SuppressWarnings({"unchecked"})
./src/mapred/org/apache/hadoop/mapred/lib/MultipleOutputs.java:516:      @SuppressWarnings({"unchecked"})
./src/mapred/org/apache/hadoop/mapred/lib/MultipleOutputs.java:530:   * @throws java.io.IOException thrown if any of the MultipleOutput files
./src/mapred/org/apache/hadoop/mapred/lib/MultipleOutputs.java:544:    @SuppressWarnings({"unchecked"})
./src/mapred/org/apache/hadoop/mapred/lib/MultipleSequenceFileOutputFormat.java:38:  @Override
./src/mapred/org/apache/hadoop/mapred/lib/MultipleTextOutputFormat.java:38:  @Override
./src/mapred/org/apache/hadoop/mapred/lib/MultithreadedMapRunner.java:36: * Multithreaded implementation for @link org.apache.hadoop.mapred.MapRunnable.
./src/mapred/org/apache/hadoop/mapred/lib/MultithreadedMapRunner.java:39: * @link org.apache.hadoop.mapred.MapRunner, when the Map operation is not CPU
./src/mapred/org/apache/hadoop/mapred/lib/MultithreadedMapRunner.java:64:  @SuppressWarnings("unchecked")
./src/mapred/org/apache/hadoop/mapred/lib/MultithreadedMapRunner.java:206:     * @param key
./src/mapred/org/apache/hadoop/mapred/lib/MultithreadedMapRunner.java:207:     * @param value
./src/mapred/org/apache/hadoop/mapred/lib/MultithreadedMapRunner.java:208:     * @param output
./src/mapred/org/apache/hadoop/mapred/lib/MultithreadedMapRunner.java:209:     * @param reporter
./src/mapred/org/apache/hadoop/mapred/lib/NLineInputFormat.java:76:   * @see org.apache.hadoop.mapred.FileInputFormat#getSplits(JobConf, int)
./src/mapred/org/apache/hadoop/mapred/lib/RegexMapper.java:34:/** A {@link Mapper} that extracts text matching a regular expression. */
./src/mapred/org/apache/hadoop/mapred/lib/TaggedInputSplit.java:34: * An {@link InputSplit} that tags another InputSplit with extra data for use by
./src/mapred/org/apache/hadoop/mapred/lib/TaggedInputSplit.java:35: * {@link DelegatingInputFormat}s and {@link DelegatingMapper}s.
./src/mapred/org/apache/hadoop/mapred/lib/TaggedInputSplit.java:56:   * @param inputSplit The InputSplit to be tagged
./src/mapred/org/apache/hadoop/mapred/lib/TaggedInputSplit.java:57:   * @param conf The configuration to use
./src/mapred/org/apache/hadoop/mapred/lib/TaggedInputSplit.java:58:   * @param inputFormatClass The InputFormat class to use for this job
./src/mapred/org/apache/hadoop/mapred/lib/TaggedInputSplit.java:59:   * @param mapperClass The Mapper class to use for this job
./src/mapred/org/apache/hadoop/mapred/lib/TaggedInputSplit.java:74:   * @return The InputSplit that was tagged
./src/mapred/org/apache/hadoop/mapred/lib/TaggedInputSplit.java:83:   * @return The InputFormat class to use
./src/mapred/org/apache/hadoop/mapred/lib/TaggedInputSplit.java:92:   * @return The Mapper class to use
./src/mapred/org/apache/hadoop/mapred/lib/TaggedInputSplit.java:106:  @SuppressWarnings("unchecked")
./src/mapred/org/apache/hadoop/mapred/lib/TokenCountMapper.java:32:/** A {@link Mapper} that maps text values into <token,freq> pairs.  Uses
./src/mapred/org/apache/hadoop/mapred/lib/TokenCountMapper.java:33: * {@link StringTokenizer} to break text into tokens. */
./src/mapred/org/apache/hadoop/mapred/lib/TotalOrderPartitioner.java:51:   * If the keytype is {@link org.apache.hadoop.io.BinaryComparable} and
./src/mapred/org/apache/hadoop/mapred/lib/TotalOrderPartitioner.java:55:   * the partition keyset using the {@link org.apache.hadoop.io.RawComparator}
./src/mapred/org/apache/hadoop/mapred/lib/TotalOrderPartitioner.java:57:   * comparator and contain {@link
./src/mapred/org/apache/hadoop/mapred/lib/TotalOrderPartitioner.java:60:  @SuppressWarnings("unchecked") // keytype from conf not static
./src/mapred/org/apache/hadoop/mapred/lib/TotalOrderPartitioner.java:96:  @SuppressWarnings("unchecked") // is memcmp-able and uses the trie
./src/mapred/org/apache/hadoop/mapred/lib/TotalOrderPartitioner.java:112:   * @see #setPartitionFile(JobConf,Path)
./src/mapred/org/apache/hadoop/mapred/lib/TotalOrderPartitioner.java:145:   * For types that are not {@link org.apache.hadoop.io.BinaryComparable} or
./src/mapred/org/apache/hadoop/mapred/lib/TotalOrderPartitioner.java:203:   * @param fs The file system
./src/mapred/org/apache/hadoop/mapred/lib/TotalOrderPartitioner.java:204:   * @param p The path to read
./src/mapred/org/apache/hadoop/mapred/lib/TotalOrderPartitioner.java:205:   * @param keyClass The map output key class
./src/mapred/org/apache/hadoop/mapred/lib/TotalOrderPartitioner.java:206:   * @param job The job config
./src/mapred/org/apache/hadoop/mapred/lib/TotalOrderPartitioner.java:207:   * @throws IOException
./src/mapred/org/apache/hadoop/mapred/lib/TotalOrderPartitioner.java:210:  @SuppressWarnings("unchecked") // map output key class
./src/mapred/org/apache/hadoop/mapred/lib/TotalOrderPartitioner.java:228:   * @param splits the list of cut points
./src/mapred/org/apache/hadoop/mapred/lib/TotalOrderPartitioner.java:229:   * @param lower the lower bound of partitions 0..numPartitions-1
./src/mapred/org/apache/hadoop/mapred/lib/TotalOrderPartitioner.java:230:   * @param upper the upper bound of partitions 0..numPartitions-1
./src/mapred/org/apache/hadoop/mapred/lib/TotalOrderPartitioner.java:231:   * @param prefix the prefix that we have already checked against
./src/mapred/org/apache/hadoop/mapred/lib/TotalOrderPartitioner.java:232:   * @param maxDepth the maximum depth we will build a trie for
./src/mapred/org/apache/hadoop/mapred/lib/TotalOrderPartitioner.java:233:   * @return the trie node that will divide the splits correctly
./src/mapred/org/apache/hadoop/mapred/LimitTasksPerJobTaskScheduler.java:30: * A {@link TaskScheduler} that limits the maximum number of tasks
./src/mapred/org/apache/hadoop/mapred/LimitTasksPerJobTaskScheduler.java:49:  @Override
./src/mapred/org/apache/hadoop/mapred/LimitTasksPerJobTaskScheduler.java:58:  @Override
./src/mapred/org/apache/hadoop/mapred/LimitTasksPerJobTaskScheduler.java:70:  @Override
./src/mapred/org/apache/hadoop/mapred/LimitTasksPerJobTaskScheduler.java:161:   * @param localMaxMapLoad The local maximum number of map tasks for a host
./src/mapred/org/apache/hadoop/mapred/LimitTasksPerJobTaskScheduler.java:162:   * @param localMaxReduceLoad The local maximum number of reduce tasks for a
./src/mapred/org/apache/hadoop/mapred/LimitTasksPerJobTaskScheduler.java:164:   * @return An array of the two maximums: map then reduce.
./src/mapred/org/apache/hadoop/mapred/LineRecordReader.java:52:   * @deprecated Use {@link org.apache.hadoop.util.LineReader} instead.
./src/mapred/org/apache/hadoop/mapred/LineRecordReader.java:54:  @Deprecated
./src/mapred/org/apache/hadoop/mapred/LocalJobRunner.java:101:    @Override
./src/mapred/org/apache/hadoop/mapred/LocalJobRunner.java:241:     * @param task A map or reduce task which has just been 
./src/mapred/org/apache/hadoop/mapred/LocalJobRunner.java:318:  /** Throws {@link UnsupportedOperationException} */
./src/mapred/org/apache/hadoop/mapred/LocalJobRunner.java:387:   * @see org.apache.hadoop.mapred.JobSubmissionProtocol#getSystemDir()
./src/mapred/org/apache/hadoop/mapred/LocalJobRunner.java:394:  @Override
./src/mapred/org/apache/hadoop/mapred/LocalJobRunner.java:399:  @Override
./src/mapred/org/apache/hadoop/mapred/LocalJobRunner.java:405:  @Override
./src/mapred/org/apache/hadoop/mapred/MapFileOutputFormat.java:38:/** An {@link OutputFormat} that writes {@link MapFile}s. */
./src/mapred/org/apache/hadoop/mapred/MapOutputFile.java:46:   * @param mapTaskId a map task id
./src/mapred/org/apache/hadoop/mapred/MapOutputFile.java:56:   * @param mapTaskId a map task id
./src/mapred/org/apache/hadoop/mapred/MapOutputFile.java:57:   * @param size the size of the file
./src/mapred/org/apache/hadoop/mapred/MapOutputFile.java:67:   * @param mapTaskId a map task id
./src/mapred/org/apache/hadoop/mapred/MapOutputFile.java:77:   * @param mapTaskId a map task id
./src/mapred/org/apache/hadoop/mapred/MapOutputFile.java:78:   * @param size the size of the file
./src/mapred/org/apache/hadoop/mapred/MapOutputFile.java:89:   * @param mapTaskId a map task id
./src/mapred/org/apache/hadoop/mapred/MapOutputFile.java:90:   * @param spillNumber the number
./src/mapred/org/apache/hadoop/mapred/MapOutputFile.java:101:   * @param mapTaskId a map task id
./src/mapred/org/apache/hadoop/mapred/MapOutputFile.java:102:   * @param spillNumber the number
./src/mapred/org/apache/hadoop/mapred/MapOutputFile.java:103:   * @param size the size of the file
./src/mapred/org/apache/hadoop/mapred/MapOutputFile.java:114:   * @param mapTaskId a map task id
./src/mapred/org/apache/hadoop/mapred/MapOutputFile.java:115:   * @param spillNumber the number
./src/mapred/org/apache/hadoop/mapred/MapOutputFile.java:126:   * @param mapTaskId a map task id
./src/mapred/org/apache/hadoop/mapred/MapOutputFile.java:127:   * @param spillNumber the number
./src/mapred/org/apache/hadoop/mapred/MapOutputFile.java:128:   * @param size the size of the file
./src/mapred/org/apache/hadoop/mapred/MapOutputFile.java:139:   * @param mapTaskId a map task id
./src/mapred/org/apache/hadoop/mapred/MapOutputFile.java:140:   * @param reduceTaskId a reduce task id
./src/mapred/org/apache/hadoop/mapred/MapOutputFile.java:152:   * @param mapTaskId a map task id
./src/mapred/org/apache/hadoop/mapred/MapOutputFile.java:153:   * @param reduceTaskId a reduce task id
./src/mapred/org/apache/hadoop/mapred/MapOutputFile.java:154:   * @param size the size of the file
./src/mapred/org/apache/hadoop/mapred/JobConfigurable.java:23:  /** Initializes a new instance from a {@link JobConf}.
./src/mapred/org/apache/hadoop/mapred/JobConfigurable.java:25:   * @param job the configuration
./src/mapred/org/apache/hadoop/mapred/JobTracker.java:134:   * @param conf configuration for the JobTracker.
./src/mapred/org/apache/hadoop/mapred/JobTracker.java:135:   * @throws IOException
./src/mapred/org/apache/hadoop/mapred/JobTracker.java:292:          // @see {@link JobTracker.processHeartbeat(TaskTrackerStatus, boolean)} 
./src/mapred/org/apache/hadoop/mapred/JobTracker.java:1421:   * @param taskTracker the tasktracker at which the 'task' was running
./src/mapred/org/apache/hadoop/mapred/JobTracker.java:1422:   * @param taskid completed (success/failure/killed) task
./src/mapred/org/apache/hadoop/mapred/JobTracker.java:1440:   * @param job the completed job
./src/mapred/org/apache/hadoop/mapred/JobTracker.java:1466:   * Remove all 'marked' tasks running on a given {@link TaskTracker}
./src/mapred/org/apache/hadoop/mapred/JobTracker.java:1467:   * from the {@link JobTracker}'s data-structures.
./src/mapred/org/apache/hadoop/mapred/JobTracker.java:1470:   * @param taskTracker tasktracker whose 'non-running' tasks are to be purged
./src/mapred/org/apache/hadoop/mapred/JobTracker.java:1488:   * Call {@link #removeTaskEntry(String)} for each of the
./src/mapred/org/apache/hadoop/mapred/JobTracker.java:1491:   * job, either because it has outlived {@link #RETIRE_JOB_INTERVAL}
./src/mapred/org/apache/hadoop/mapred/JobTracker.java:1492:   * or the limit of {@link #MAX_COMPLETE_USER_JOBS_IN_MEMORY} jobs 
./src/mapred/org/apache/hadoop/mapred/JobTracker.java:1496:   * @param job the job about to be 'retired'
./src/mapred/org/apache/hadoop/mapred/JobTracker.java:1518:   * @param job completed job.
./src/mapred/org/apache/hadoop/mapred/JobTracker.java:1624:   * @return a string with a unique identifier
./src/mapred/org/apache/hadoop/mapred/JobTracker.java:1696:   * @param status Task Tracker's status
./src/mapred/org/apache/hadoop/mapred/JobTracker.java:1697:   * @param resolveInline Should the resolution happen inline?
./src/mapred/org/apache/hadoop/mapred/JobTracker.java:1788:   * Return the {@link QueueManager} associated with the JobTracker.
./src/mapred/org/apache/hadoop/mapred/JobTracker.java:1803:   * The periodic heartbeat mechanism between the {@link TaskTracker} and
./src/mapred/org/apache/hadoop/mapred/JobTracker.java:1804:   * the {@link JobTracker}.
./src/mapred/org/apache/hadoop/mapred/JobTracker.java:1806:   * The {@link JobTracker} processes the status information sent by the 
./src/mapred/org/apache/hadoop/mapred/JobTracker.java:1807:   * {@link TaskTracker} and responds with instructions to start/stop 
./src/mapred/org/apache/hadoop/mapred/JobTracker.java:1852:        // {@link TaskTracker} since it resends the heartbeat when rpcs are 
./src/mapred/org/apache/hadoop/mapred/JobTracker.java:1853:        // lost see {@link TaskTracker.transmitHeartbeat()};
./src/mapred/org/apache/hadoop/mapred/JobTracker.java:1855:        // {@link TaskTracker} go forward. 
./src/mapred/org/apache/hadoop/mapred/JobTracker.java:1934:   * @return next heartbeat interval.
./src/mapred/org/apache/hadoop/mapred/JobTracker.java:1974:   * @param trackerName The name of the tracker
./src/mapred/org/apache/hadoop/mapred/JobTracker.java:1975:   * @param status The new status for the task tracker
./src/mapred/org/apache/hadoop/mapred/JobTracker.java:1976:   * @return Was an old status found?
./src/mapred/org/apache/hadoop/mapred/JobTracker.java:2237:   * @param jobId The id for the job submitted which needs to be added
./src/mapred/org/apache/hadoop/mapred/JobTracker.java:2311:   * @param jobid id of the job
./src/mapred/org/apache/hadoop/mapred/JobTracker.java:2312:   * @param priority new priority of the job
./src/mapred/org/apache/hadoop/mapred/JobTracker.java:2439:   * @see org.apache.hadoop.mapred.JobSubmissionProtocol#getTaskCompletionEvents(java.lang.String, int, int)
./src/mapred/org/apache/hadoop/mapred/JobTracker.java:2459:   * @param taskId the id of the task
./src/mapred/org/apache/hadoop/mapred/JobTracker.java:2460:   * @return an array of the diagnostic messages
./src/mapred/org/apache/hadoop/mapred/JobTracker.java:2504:   * @return the configured task scheduler
./src/mapred/org/apache/hadoop/mapred/JobTracker.java:2533:   * @param taskId the name of the task
./src/mapred/org/apache/hadoop/mapred/JobTracker.java:2534:   * @return The name of the task tracker
./src/mapred/org/apache/hadoop/mapred/JobTracker.java:2549:   * @see org.apache.hadoop.mapred.JobSubmissionProtocol#getSystemDir()
./src/mapred/org/apache/hadoop/mapred/JobTracker.java:2565:   * @param jobId job id
./src/mapred/org/apache/hadoop/mapred/JobTracker.java:2566:   * @param priority new {@link JobPriority} for the job
./src/mapred/org/apache/hadoop/mapred/JobTracker.java:2707:   * @param jobId id of the job
./src/mapred/org/apache/hadoop/mapred/JobTracker.java:2708:   * @return the path of the job conf file on the local file system
./src/mapred/org/apache/hadoop/mapred/JobTracker.java:2738:  @Override
./src/mapred/org/apache/hadoop/mapred/JobTracker.java:2744:  @Override
./src/mapred/org/apache/hadoop/mapred/JobTracker.java:2749:  @Override
./src/mapred/org/apache/hadoop/mapred/Mapper.java:37: * {@link InputSplit} generated by the {@link InputFormat} for the job.
./src/mapred/org/apache/hadoop/mapred/Mapper.java:38: * <code>Mapper</code> implementations can access the {@link JobConf} for the 
./src/mapred/org/apache/hadoop/mapred/Mapper.java:39: * job via the {@link JobConfigurable#configure(JobConf)} and initialize
./src/mapred/org/apache/hadoop/mapred/Mapper.java:40: * themselves. Similarly they can use the {@link Closeable#close()} method for
./src/mapred/org/apache/hadoop/mapred/Mapper.java:44: * {@link #map(Object, Object, OutputCollector, Reporter)} 
./src/mapred/org/apache/hadoop/mapred/Mapper.java:48: * subsequently grouped by the framework, and passed to a {@link Reducer} to  
./src/mapred/org/apache/hadoop/mapred/Mapper.java:51: * {@link JobConf#setOutputKeyComparatorClass(Class)}.</p>
./src/mapred/org/apache/hadoop/mapred/Mapper.java:55: * which <code>Reducer</code> by implementing a custom {@link Partitioner}.
./src/mapred/org/apache/hadoop/mapred/Mapper.java:58: * {@link JobConf#setCombinerClass(Class)}, to perform local aggregation of the 
./src/mapred/org/apache/hadoop/mapred/Mapper.java:63: * {@link SequenceFile}s. Applications can specify if and how the intermediate
./src/mapred/org/apache/hadoop/mapred/Mapper.java:64: * outputs are to be compressed and which {@link CompressionCodec}s are to be
./src/mapred/org/apache/hadoop/mapred/Mapper.java:68: * <a href="{@docRoot}/org/apache/hadoop/mapred/JobConf.html#ReducerNone">zero
./src/mapred/org/apache/hadoop/mapred/Mapper.java:70: * to the {@link FileSystem} without grouping by keys.</p>
./src/mapred/org/apache/hadoop/mapred/Mapper.java:120: * <p>Applications may write a custom {@link MapRunnable} to exert greater
./src/mapred/org/apache/hadoop/mapred/Mapper.java:123: * @see JobConf
./src/mapred/org/apache/hadoop/mapred/Mapper.java:124: * @see InputFormat
./src/mapred/org/apache/hadoop/mapred/Mapper.java:125: * @see Partitioner  
./src/mapred/org/apache/hadoop/mapred/Mapper.java:126: * @see Reducer
./src/mapred/org/apache/hadoop/mapred/Mapper.java:127: * @see MapReduceBase
./src/mapred/org/apache/hadoop/mapred/Mapper.java:128: * @see MapRunnable
./src/mapred/org/apache/hadoop/mapred/Mapper.java:129: * @see SequenceFile
./src/mapred/org/apache/hadoop/mapred/Mapper.java:139:   * {@link OutputCollector#collect(Object,Object)}.</p>
./src/mapred/org/apache/hadoop/mapred/Mapper.java:141:   * <p>Applications can use the {@link Reporter} provided to report progress 
./src/mapred/org/apache/hadoop/mapred/Mapper.java:146:   * <a href="{@docRoot}/../hadoop-default.html#mapred.task.timeout">
./src/mapred/org/apache/hadoop/mapred/Mapper.java:150:   * @param key the input key.
./src/mapred/org/apache/hadoop/mapred/Mapper.java:151:   * @param value the input value.
./src/mapred/org/apache/hadoop/mapred/Mapper.java:152:   * @param output collects mapped keys and values.
./src/mapred/org/apache/hadoop/mapred/Mapper.java:153:   * @param reporter facility to report progress.
./src/mapred/org/apache/hadoop/mapred/QueueManager.java:113:     * @param aclString String representation of the ACL
./src/mapred/org/apache/hadoop/mapred/QueueManager.java:159:   * in {@link org.apache.hadoop.conf.Configuration} object.
./src/mapred/org/apache/hadoop/mapred/QueueManager.java:161:   * @param conf Configuration object where queue configuration is specified.
./src/mapred/org/apache/hadoop/mapred/QueueManager.java:177:   * @return Set of queue names.
./src/mapred/org/apache/hadoop/mapred/QueueManager.java:184:   * Return true if the given {@link QueueManager.QueueOperation} can be 
./src/mapred/org/apache/hadoop/mapred/QueueManager.java:191:   * @param queueName Queue on which the operation needs to be performed. 
./src/mapred/org/apache/hadoop/mapred/QueueManager.java:192:   * @param oper The operation to perform
./src/mapred/org/apache/hadoop/mapred/QueueManager.java:193:   * @param ugi The user and groups who wish to perform the operation.
./src/mapred/org/apache/hadoop/mapred/QueueManager.java:195:   * @return true if the operation is allowed, false otherwise.
./src/mapred/org/apache/hadoop/mapred/QueueManager.java:203:   * Return true if the given {@link QueueManager.QueueOperation} can be 
./src/mapred/org/apache/hadoop/mapred/QueueManager.java:211:   * If the {@link QueueManager.QueueOperation} is not job specific then the 
./src/mapred/org/apache/hadoop/mapred/QueueManager.java:214:   * @param queueName Queue on which the operation needs to be performed.
./src/mapred/org/apache/hadoop/mapred/QueueManager.java:215:   * @param job The {@link JobInProgress} on which the operation is being
./src/mapred/org/apache/hadoop/mapred/QueueManager.java:217:   * @param oper The operation to perform
./src/mapred/org/apache/hadoop/mapred/QueueManager.java:218:   * @param ugi The user and groups who wish to perform the operation.
./src/mapred/org/apache/hadoop/mapred/QueueManager.java:220:   * @return true if the operation is allowed, false otherwise.
./src/mapred/org/apache/hadoop/mapred/QueueManager.java:257:   * @param queueName queue for which the scheduling information is to be set. 
./src/mapred/org/apache/hadoop/mapred/QueueManager.java:258:   * @param queueInfo scheduling information for this queue.
./src/mapred/org/apache/hadoop/mapred/QueueManager.java:268:   * @param queueName queue for which the scheduling information is required.
./src/mapred/org/apache/hadoop/mapred/QueueManager.java:269:   * @return The scheduling information for this queue.
./src/mapred/org/apache/hadoop/mapred/QueueManager.java:271:   * @see #setSchedulerInfo(String, Object)
./src/mapred/org/apache/hadoop/mapred/QueueManager.java:279:   * it from the passed in {@link org.apache.hadoop.conf.Configuration}.
./src/mapred/org/apache/hadoop/mapred/QueueManager.java:284:   * @param conf New configuration for the queues. 
./src/mapred/org/apache/hadoop/mapred/MapReduceBase.java:27: * Base class for {@link Mapper} and {@link Reducer} implementations.
./src/mapred/org/apache/hadoop/mapred/MapRunnable.java:24: * Expert: Generic interface for {@link Mapper}s.
./src/mapred/org/apache/hadoop/mapred/MapRunnable.java:29: * @see Mapper
./src/mapred/org/apache/hadoop/mapred/MapRunnable.java:40:   * @param input the {@link RecordReader} to read the input records.
./src/mapred/org/apache/hadoop/mapred/MapRunnable.java:41:   * @param output the {@link OutputCollector} to collect the outputrecords.
./src/mapred/org/apache/hadoop/mapred/MapRunnable.java:42:   * @param reporter {@link Reporter} to report progress, status-updates etc.
./src/mapred/org/apache/hadoop/mapred/MapRunnable.java:43:   * @throws IOException
./src/mapred/org/apache/hadoop/mapred/MapRunner.java:25:/** Default {@link MapRunnable} implementation.*/
./src/mapred/org/apache/hadoop/mapred/MapRunner.java:32:  @SuppressWarnings("unchecked")
./src/mapred/org/apache/hadoop/mapred/MapTask.java:96:  @Override
./src/mapred/org/apache/hadoop/mapred/MapTask.java:101:  @Override
./src/mapred/org/apache/hadoop/mapred/MapTask.java:113:  @Override
./src/mapred/org/apache/hadoop/mapred/MapTask.java:119:  @Override
./src/mapred/org/apache/hadoop/mapred/MapTask.java:127:  @Override
./src/mapred/org/apache/hadoop/mapred/MapTask.java:134:  @Override
./src/mapred/org/apache/hadoop/mapred/MapTask.java:142:   * @param <K>
./src/mapred/org/apache/hadoop/mapred/MapTask.java:143:   * @param <V>
./src/mapred/org/apache/hadoop/mapred/MapTask.java:254:    @SuppressWarnings("unchecked")
./src/mapred/org/apache/hadoop/mapred/MapTask.java:270:  @Override
./src/mapred/org/apache/hadoop/mapred/MapTask.java:271:  @SuppressWarnings("unchecked")
./src/mapred/org/apache/hadoop/mapred/MapTask.java:360:    @SuppressWarnings("unchecked")
./src/mapred/org/apache/hadoop/mapred/MapTask.java:455:    @SuppressWarnings("unchecked")
./src/mapred/org/apache/hadoop/mapred/MapTask.java:636:     * @see IndexedSortable#compare
./src/mapred/org/apache/hadoop/mapred/MapTask.java:656:     * @see IndexedSortable#swap
./src/mapred/org/apache/hadoop/mapred/MapTask.java:698:       * @see #markRecord()
./src/mapred/org/apache/hadoop/mapred/MapTask.java:725:      @Override
./src/mapred/org/apache/hadoop/mapred/MapTask.java:736:       * @throws MapBufferTooSmallException if record is too large to
./src/mapred/org/apache/hadoop/mapred/MapTask.java:739:      @Override
./src/mapred/org/apache/hadoop/mapred/MapTask.java:864:      @Override
./src/mapred/org/apache/hadoop/mapred/MapTask.java:1097:    @SuppressWarnings("unchecked")
./src/mapred/org/apache/hadoop/mapred/MapTask.java:1261:          @SuppressWarnings("unchecked")
./src/mapred/org/apache/hadoop/mapred/MapTask.java:1322:     * @param mapId
./src/mapred/org/apache/hadoop/mapred/MapTask.java:1323:     * @param spillNum
./src/mapred/org/apache/hadoop/mapred/MapTask.java:1324:     * @param reducer
./src/mapred/org/apache/hadoop/mapred/MapTask.java:1325:     * @return
./src/mapred/org/apache/hadoop/mapred/MapTask.java:1326:     * @throws IOException
./src/mapred/org/apache/hadoop/mapred/MapTask.java:1349:     * @param mapId
./src/mapred/org/apache/hadoop/mapred/MapTask.java:1350:     * @param finalName
./src/mapred/org/apache/hadoop/mapred/MapTask.java:1351:     * @throws IOException
./src/mapred/org/apache/hadoop/mapred/MapTask.java:1381:  @SuppressWarnings("serial")
./src/mapred/org/apache/hadoop/mapred/MapTaskStatus.java:33:  @Override
./src/mapred/org/apache/hadoop/mapred/MapTaskStatus.java:38:  @Override
./src/mapred/org/apache/hadoop/mapred/MapTaskStatus.java:43:  @Override
./src/mapred/org/apache/hadoop/mapred/MapTaskStatus.java:48:  @Override
./src/mapred/org/apache/hadoop/mapred/MapTaskStatus.java:53:  @Override
./src/mapred/org/apache/hadoop/mapred/Merger.java:299:    @SuppressWarnings("unchecked")
./src/mapred/org/apache/hadoop/mapred/MergeSorter.java:55:   * @param i
./src/mapred/org/apache/hadoop/mapred/MergeSorter.java:56:   * @param j
./src/mapred/org/apache/hadoop/mapred/MergeSorter.java:57:   * @return int as per the specification of Comparator.compare
./src/mapred/org/apache/hadoop/mapred/MultiFileInputFormat.java:30: * An abstract {@link InputFormat} that returns {@link MultiFileSplit}'s
./src/mapred/org/apache/hadoop/mapred/MultiFileInputFormat.java:31: * in {@link #getSplits(JobConf, int)} method. Splits are constructed from 
./src/mapred/org/apache/hadoop/mapred/MultiFileInputFormat.java:34: * Subclasses implement {@link #getRecordReader(InputSplit, JobConf, Reporter)}
./src/mapred/org/apache/hadoop/mapred/MultiFileInputFormat.java:36: * @see MultiFileSplit
./src/mapred/org/apache/hadoop/mapred/MultiFileInputFormat.java:41:  @Override
./src/mapred/org/apache/hadoop/mapred/MultiFileInputFormat.java:99:  @Override
./src/mapred/org/apache/hadoop/mapred/MultiFileSplit.java:34: * A sub-collection of input files. Unlike {@link FileSplit}, MultiFileSplit 
./src/mapred/org/apache/hadoop/mapred/MultiFileSplit.java:37: * MultiFileSplit can be used to implement {@link RecordReader}'s, with 
./src/mapred/org/apache/hadoop/mapred/MultiFileSplit.java:39: * @see FileSplit
./src/mapred/org/apache/hadoop/mapred/MultiFileSplit.java:40: * @see MultiFileInputFormat 
./src/mapred/org/apache/hadoop/mapred/MultiFileSplit.java:135:  @Override
./src/mapred/org/apache/hadoop/mapred/OutputCollector.java:24: * Collects the <code>&lt;key, value&gt;</code> pairs output by {@link Mapper}s
./src/mapred/org/apache/hadoop/mapred/OutputCollector.java:25: * and {@link Reducer}s.
./src/mapred/org/apache/hadoop/mapred/OutputCollector.java:36:   * @param key the key to collect.
./src/mapred/org/apache/hadoop/mapred/OutputCollector.java:37:   * @param value to value to collect.
./src/mapred/org/apache/hadoop/mapred/OutputCollector.java:38:   * @throws IOException
./src/mapred/org/apache/hadoop/mapred/OutputCommitter.java:53: * @see FileOutputCommitter 
./src/mapred/org/apache/hadoop/mapred/OutputCommitter.java:54: * @see JobContext
./src/mapred/org/apache/hadoop/mapred/OutputCommitter.java:55: * @see TaskAttemptContext 
./src/mapred/org/apache/hadoop/mapred/OutputCommitter.java:62:   * @param jobContext Context of the job whose output is being written.
./src/mapred/org/apache/hadoop/mapred/OutputCommitter.java:63:   * @throws IOException if temporary output could not be created
./src/mapred/org/apache/hadoop/mapred/OutputCommitter.java:70:   * @param jobContext Context of the job whose output is being written.
./src/mapred/org/apache/hadoop/mapred/OutputCommitter.java:71:   * @throws IOException
./src/mapred/org/apache/hadoop/mapred/OutputCommitter.java:78:   * @param taskContext Context of the task whose output is being written.
./src/mapred/org/apache/hadoop/mapred/OutputCommitter.java:79:   * @throws IOException
./src/mapred/org/apache/hadoop/mapred/OutputCommitter.java:87:   * @param taskContext
./src/mapred/org/apache/hadoop/mapred/OutputCommitter.java:88:   * @return true/false
./src/mapred/org/apache/hadoop/mapred/OutputCommitter.java:89:   * @throws IOException
./src/mapred/org/apache/hadoop/mapred/OutputCommitter.java:99:   * @param taskContext Context of the task whose output is being written.
./src/mapred/org/apache/hadoop/mapred/OutputCommitter.java:100:   * @throws IOException if commit is not 
./src/mapred/org/apache/hadoop/mapred/OutputCommitter.java:108:   * @param taskContext
./src/mapred/org/apache/hadoop/mapred/OutputCommitter.java:109:   * @throws IOException
./src/mapred/org/apache/hadoop/mapred/OutputFormat.java:37: *   Provide the {@link RecordWriter} implementation to be used to write out
./src/mapred/org/apache/hadoop/mapred/OutputFormat.java:39: *   {@link FileSystem}.
./src/mapred/org/apache/hadoop/mapred/OutputFormat.java:43: * @see RecordWriter
./src/mapred/org/apache/hadoop/mapred/OutputFormat.java:44: * @see JobConf
./src/mapred/org/apache/hadoop/mapred/OutputFormat.java:49:   * Get the {@link RecordWriter} for the given job.
./src/mapred/org/apache/hadoop/mapred/OutputFormat.java:51:   * @param ignored
./src/mapred/org/apache/hadoop/mapred/OutputFormat.java:52:   * @param job configuration for the job whose output is being written.
./src/mapred/org/apache/hadoop/mapred/OutputFormat.java:53:   * @param name the unique name for this part of the output.
./src/mapred/org/apache/hadoop/mapred/OutputFormat.java:54:   * @param progress mechanism for reporting progress while writing to file.
./src/mapred/org/apache/hadoop/mapred/OutputFormat.java:55:   * @return a {@link RecordWriter} to write the output for the job.
./src/mapred/org/apache/hadoop/mapred/OutputFormat.java:56:   * @throws IOException
./src/mapred/org/apache/hadoop/mapred/OutputFormat.java:70:   * @param ignored
./src/mapred/org/apache/hadoop/mapred/OutputFormat.java:71:   * @param job job configuration.
./src/mapred/org/apache/hadoop/mapred/OutputFormat.java:72:   * @throws IOException when output should not be attempted
./src/mapred/org/apache/hadoop/mapred/Partitioner.java:31: * @see Reducer
./src/mapred/org/apache/hadoop/mapred/Partitioner.java:41:   * @param key the key to be paritioned.
./src/mapred/org/apache/hadoop/mapred/Partitioner.java:42:   * @param value the entry value.
./src/mapred/org/apache/hadoop/mapred/Partitioner.java:43:   * @param numPartitions the total number of partitions.
./src/mapred/org/apache/hadoop/mapred/Partitioner.java:44:   * @return the partition number for the <code>key</code>.
./src/mapred/org/apache/hadoop/mapred/pipes/Application.java:64:   * @param conf the task's configuration
./src/mapred/org/apache/hadoop/mapred/pipes/Application.java:65:   * @param recordReader the fake record reader to update progress with
./src/mapred/org/apache/hadoop/mapred/pipes/Application.java:66:   * @param output the collector to send output to
./src/mapred/org/apache/hadoop/mapred/pipes/Application.java:67:   * @param reporter the reporter for the task
./src/mapred/org/apache/hadoop/mapred/pipes/Application.java:68:   * @param outputKeyClass the class of the output keys
./src/mapred/org/apache/hadoop/mapred/pipes/Application.java:69:   * @param outputValueClass the class of the output values
./src/mapred/org/apache/hadoop/mapred/pipes/Application.java:70:   * @throws IOException
./src/mapred/org/apache/hadoop/mapred/pipes/Application.java:71:   * @throws InterruptedException
./src/mapred/org/apache/hadoop/mapred/pipes/Application.java:112:   * @return the downlink proxy
./src/mapred/org/apache/hadoop/mapred/pipes/Application.java:120:   * @return did the application finish correctly?
./src/mapred/org/apache/hadoop/mapred/pipes/Application.java:121:   * @throws Throwable
./src/mapred/org/apache/hadoop/mapred/pipes/Application.java:130:   * @param t the exception that signalled the problem
./src/mapred/org/apache/hadoop/mapred/pipes/Application.java:131:   * @throws IOException A wrapper around the exception that was passed in
./src/mapred/org/apache/hadoop/mapred/pipes/Application.java:153:   * @throws IOException
./src/mapred/org/apache/hadoop/mapred/pipes/Application.java:167:   * @param command the command and its arguments
./src/mapred/org/apache/hadoop/mapred/pipes/Application.java:168:   * @param env the environment to run the process in
./src/mapred/org/apache/hadoop/mapred/pipes/Application.java:169:   * @return a handle on the process
./src/mapred/org/apache/hadoop/mapred/pipes/Application.java:170:   * @throws IOException
./src/mapred/org/apache/hadoop/mapred/pipes/BinaryProtocol.java:210:   * @param sock The socket to communicate on.
./src/mapred/org/apache/hadoop/mapred/pipes/BinaryProtocol.java:211:   * @param handler The handler for the received messages.
./src/mapred/org/apache/hadoop/mapred/pipes/BinaryProtocol.java:212:   * @param key The object to read keys into.
./src/mapred/org/apache/hadoop/mapred/pipes/BinaryProtocol.java:213:   * @param value The object to read values into.
./src/mapred/org/apache/hadoop/mapred/pipes/BinaryProtocol.java:214:   * @param config The job's configuration
./src/mapred/org/apache/hadoop/mapred/pipes/BinaryProtocol.java:215:   * @throws IOException
./src/mapred/org/apache/hadoop/mapred/pipes/BinaryProtocol.java:237:   * @throws IOException
./src/mapred/org/apache/hadoop/mapred/pipes/BinaryProtocol.java:238:   * @throws InterruptedException
./src/mapred/org/apache/hadoop/mapred/pipes/BinaryProtocol.java:323:   * @param obj the object to write
./src/mapred/org/apache/hadoop/mapred/pipes/BinaryProtocol.java:324:   * @throws IOException
./src/mapred/org/apache/hadoop/mapred/pipes/DownwardProtocol.java:36:   * @throws IOException
./src/mapred/org/apache/hadoop/mapred/pipes/DownwardProtocol.java:42:   * @param conf
./src/mapred/org/apache/hadoop/mapred/pipes/DownwardProtocol.java:43:   * @throws IOException
./src/mapred/org/apache/hadoop/mapred/pipes/DownwardProtocol.java:49:   * @param keyType the name of the key's type
./src/mapred/org/apache/hadoop/mapred/pipes/DownwardProtocol.java:50:   * @param valueType the name of the value's type
./src/mapred/org/apache/hadoop/mapred/pipes/DownwardProtocol.java:51:   * @throws IOException
./src/mapred/org/apache/hadoop/mapred/pipes/DownwardProtocol.java:57:   * @param split The input split for this map.
./src/mapred/org/apache/hadoop/mapred/pipes/DownwardProtocol.java:58:   * @param numReduces The number of reduces for this job.
./src/mapred/org/apache/hadoop/mapred/pipes/DownwardProtocol.java:59:   * @param pipedInput Is the input coming from Java?
./src/mapred/org/apache/hadoop/mapred/pipes/DownwardProtocol.java:60:   * @throws IOException
./src/mapred/org/apache/hadoop/mapred/pipes/DownwardProtocol.java:67:   * @param key The record's key
./src/mapred/org/apache/hadoop/mapred/pipes/DownwardProtocol.java:68:   * @param value The record's value
./src/mapred/org/apache/hadoop/mapred/pipes/DownwardProtocol.java:69:   * @throws IOException
./src/mapred/org/apache/hadoop/mapred/pipes/DownwardProtocol.java:75:   * @param reduce the index of the reduce (0 .. numReduces - 1)
./src/mapred/org/apache/hadoop/mapred/pipes/DownwardProtocol.java:76:   * @param pipedOutput is the output being sent to Java?
./src/mapred/org/apache/hadoop/mapred/pipes/DownwardProtocol.java:77:   * @throws IOException
./src/mapred/org/apache/hadoop/mapred/pipes/DownwardProtocol.java:83:   * @param key the new key
./src/mapred/org/apache/hadoop/mapred/pipes/DownwardProtocol.java:84:   * @throws IOException
./src/mapred/org/apache/hadoop/mapred/pipes/DownwardProtocol.java:90:   * @param value the new value
./src/mapred/org/apache/hadoop/mapred/pipes/DownwardProtocol.java:91:   * @throws IOException
./src/mapred/org/apache/hadoop/mapred/pipes/DownwardProtocol.java:98:   * @throws IOException
./src/mapred/org/apache/hadoop/mapred/pipes/DownwardProtocol.java:104:   * @throws IOException
./src/mapred/org/apache/hadoop/mapred/pipes/OutputHandler.java:54:   * @param collector the "real" collector that takes the output
./src/mapred/org/apache/hadoop/mapred/pipes/OutputHandler.java:55:   * @param reporter the reporter for reporting progress
./src/mapred/org/apache/hadoop/mapred/pipes/OutputHandler.java:114:   * @return a float between 0.0 and 1.0
./src/mapred/org/apache/hadoop/mapred/pipes/OutputHandler.java:132:   * @return did the task finish correctly?
./src/mapred/org/apache/hadoop/mapred/pipes/OutputHandler.java:133:   * @throws Throwable
./src/mapred/org/apache/hadoop/mapred/pipes/PipesMapRunner.java:44:   * @param job the job's configuration
./src/mapred/org/apache/hadoop/mapred/pipes/PipesMapRunner.java:55:   * @param input the set of inputs
./src/mapred/org/apache/hadoop/mapred/pipes/PipesMapRunner.java:56:   * @param output the object to collect the outputs of the map
./src/mapred/org/apache/hadoop/mapred/pipes/PipesMapRunner.java:57:   * @param reporter the object to update with status
./src/mapred/org/apache/hadoop/mapred/pipes/PipesMapRunner.java:59:  @SuppressWarnings("unchecked")
./src/mapred/org/apache/hadoop/mapred/pipes/PipesNonJavaInputFormat.java:34: * Dummy input format used when non-Java a {@link RecordReader} is used by
./src/mapred/org/apache/hadoop/mapred/pipes/PipesNonJavaInputFormat.java:38: * {@link PipesDummyRecordReader}, everything else left for the 'actual'
./src/mapred/org/apache/hadoop/mapred/pipes/PipesNonJavaInputFormat.java:60:   * A dummy {@link org.apache.hadoop.mapred.RecordReader} to help track the
./src/mapred/org/apache/hadoop/mapred/pipes/PipesNonJavaInputFormat.java:65:   * the task by the {@link OutputHandler#progress(float)} which calls the
./src/mapred/org/apache/hadoop/mapred/pipes/PipesNonJavaInputFormat.java:66:   * {@link #next(FloatWritable, NullWritable)} with the progress as the
./src/mapred/org/apache/hadoop/mapred/pipes/PipesPartitioner.java:38:  @SuppressWarnings("unchecked")
./src/mapred/org/apache/hadoop/mapred/pipes/PipesPartitioner.java:46:   * @param newValue the next partition value
./src/mapred/org/apache/hadoop/mapred/pipes/PipesPartitioner.java:55:   * @param key the key to partition
./src/mapred/org/apache/hadoop/mapred/pipes/PipesPartitioner.java:56:   * @param value the value to partition
./src/mapred/org/apache/hadoop/mapred/pipes/PipesPartitioner.java:57:   * @param numPartitions the number of reduces
./src/mapred/org/apache/hadoop/mapred/pipes/PipesReducer.java:76:  @SuppressWarnings("unchecked")
./src/mapred/org/apache/hadoop/mapred/pipes/Submitter.java:77:   * @param conf
./src/mapred/org/apache/hadoop/mapred/pipes/Submitter.java:78:   * @return the URI where the application's executable is located
./src/mapred/org/apache/hadoop/mapred/pipes/Submitter.java:87:   * @param conf
./src/mapred/org/apache/hadoop/mapred/pipes/Submitter.java:88:   * @param executable The URI of the application's executable.
./src/mapred/org/apache/hadoop/mapred/pipes/Submitter.java:96:   * @param conf the configuration to modify
./src/mapred/org/apache/hadoop/mapred/pipes/Submitter.java:97:   * @param value the new value
./src/mapred/org/apache/hadoop/mapred/pipes/Submitter.java:105:   * @param conf the configuration to check
./src/mapred/org/apache/hadoop/mapred/pipes/Submitter.java:106:   * @return is it a Java RecordReader?
./src/mapred/org/apache/hadoop/mapred/pipes/Submitter.java:114:   * @param conf the configuration to modify
./src/mapred/org/apache/hadoop/mapred/pipes/Submitter.java:115:   * @param value the new value
./src/mapred/org/apache/hadoop/mapred/pipes/Submitter.java:123:   * @param conf the configuration to check
./src/mapred/org/apache/hadoop/mapred/pipes/Submitter.java:124:   * @return is it a Java Mapper?
./src/mapred/org/apache/hadoop/mapred/pipes/Submitter.java:132:   * @param conf the configuration to modify
./src/mapred/org/apache/hadoop/mapred/pipes/Submitter.java:133:   * @param value the new value
./src/mapred/org/apache/hadoop/mapred/pipes/Submitter.java:141:   * @param conf the configuration to check
./src/mapred/org/apache/hadoop/mapred/pipes/Submitter.java:142:   * @return is it a Java Reducer?
./src/mapred/org/apache/hadoop/mapred/pipes/Submitter.java:150:   * @param conf the configuration to modify
./src/mapred/org/apache/hadoop/mapred/pipes/Submitter.java:151:   * @param value the new value to set
./src/mapred/org/apache/hadoop/mapred/pipes/Submitter.java:159:   * @param conf the configuration to check
./src/mapred/org/apache/hadoop/mapred/pipes/Submitter.java:160:   * @return true, if the output of the job will be written by Java
./src/mapred/org/apache/hadoop/mapred/pipes/Submitter.java:169:   * @param conf the configuration to modify
./src/mapred/org/apache/hadoop/mapred/pipes/Submitter.java:170:   * @param key the key to set
./src/mapred/org/apache/hadoop/mapred/pipes/Submitter.java:171:   * @param value the new "default" value to set
./src/mapred/org/apache/hadoop/mapred/pipes/Submitter.java:181:   * @param conf the configuration to modify
./src/mapred/org/apache/hadoop/mapred/pipes/Submitter.java:182:   * @param cls the user's partitioner class
./src/mapred/org/apache/hadoop/mapred/pipes/Submitter.java:190:   * @param conf the configuration to look in
./src/mapred/org/apache/hadoop/mapred/pipes/Submitter.java:191:   * @return the class that the user submitted
./src/mapred/org/apache/hadoop/mapred/pipes/Submitter.java:208:   * @param conf the configuration to check
./src/mapred/org/apache/hadoop/mapred/pipes/Submitter.java:209:   * @return will the framework save the command file?
./src/mapred/org/apache/hadoop/mapred/pipes/Submitter.java:217:   * @param conf the configuration to modify
./src/mapred/org/apache/hadoop/mapred/pipes/Submitter.java:218:   * @param keep the new value
./src/mapred/org/apache/hadoop/mapred/pipes/Submitter.java:227:   * @param conf the job to submit to the cluster (MODIFIED)
./src/mapred/org/apache/hadoop/mapred/pipes/Submitter.java:228:   * @throws IOException
./src/mapred/org/apache/hadoop/mapred/pipes/Submitter.java:229:   * @deprecated Use {@link Submitter#runJob(JobConf)}
./src/mapred/org/apache/hadoop/mapred/pipes/Submitter.java:231:  @Deprecated
./src/mapred/org/apache/hadoop/mapred/pipes/Submitter.java:239:   * @param conf the job to submit to the cluster (MODIFIED)
./src/mapred/org/apache/hadoop/mapred/pipes/Submitter.java:240:   * @throws IOException
./src/mapred/org/apache/hadoop/mapred/pipes/Submitter.java:249:   * This returns a handle to the {@link RunningJob} which can be used to track
./src/mapred/org/apache/hadoop/mapred/pipes/Submitter.java:252:   * @param conf the job configuration.
./src/mapred/org/apache/hadoop/mapred/pipes/Submitter.java:253:   * @return a handle to the {@link RunningJob} which can be used to track the
./src/mapred/org/apache/hadoop/mapred/pipes/Submitter.java:255:   * @throws IOException
./src/mapred/org/apache/hadoop/mapred/pipes/Submitter.java:377:  @Override
./src/mapred/org/apache/hadoop/mapred/pipes/Submitter.java:491:   * @param args
./src/mapred/org/apache/hadoop/mapred/pipes/UpwardProtocol.java:32:   * @param key the record's key
./src/mapred/org/apache/hadoop/mapred/pipes/UpwardProtocol.java:33:   * @param value the record's value
./src/mapred/org/apache/hadoop/mapred/pipes/UpwardProtocol.java:34:   * @throws IOException
./src/mapred/org/apache/hadoop/mapred/pipes/UpwardProtocol.java:41:   * @param reduce the reduce to send this record to
./src/mapred/org/apache/hadoop/mapred/pipes/UpwardProtocol.java:42:   * @param key the record's key
./src/mapred/org/apache/hadoop/mapred/pipes/UpwardProtocol.java:43:   * @param value the record's value
./src/mapred/org/apache/hadoop/mapred/pipes/UpwardProtocol.java:44:   * @throws IOException
./src/mapred/org/apache/hadoop/mapred/pipes/UpwardProtocol.java:51:   * @param msg the string to display to the user
./src/mapred/org/apache/hadoop/mapred/pipes/UpwardProtocol.java:52:   * @throws IOException
./src/mapred/org/apache/hadoop/mapred/pipes/UpwardProtocol.java:58:   * @param progress the current progress (0.0 to 1.0)
./src/mapred/org/apache/hadoop/mapred/pipes/UpwardProtocol.java:59:   * @throws IOException
./src/mapred/org/apache/hadoop/mapred/pipes/UpwardProtocol.java:66:   * @throws IOException
./src/mapred/org/apache/hadoop/mapred/pipes/UpwardProtocol.java:72:   * @param e
./src/mapred/org/apache/hadoop/mapred/pipes/UpwardProtocol.java:78:   * @param group counter group
./src/mapred/org/apache/hadoop/mapred/pipes/UpwardProtocol.java:79:   * @param name counter name
./src/mapred/org/apache/hadoop/mapred/pipes/UpwardProtocol.java:80:   * @throws IOException
./src/mapred/org/apache/hadoop/mapred/pipes/UpwardProtocol.java:86:   * @param id counter id of the registered counter
./src/mapred/org/apache/hadoop/mapred/pipes/UpwardProtocol.java:87:   * @param amount increment for the counter value
./src/mapred/org/apache/hadoop/mapred/pipes/UpwardProtocol.java:88:   * @throws IOException
./src/mapred/org/apache/hadoop/mapred/RamManager.java:29:   * @param requestedSize size of memory requested
./src/mapred/org/apache/hadoop/mapred/RamManager.java:30:   * @param in input stream
./src/mapred/org/apache/hadoop/mapred/RamManager.java:31:   * @throws InterruptedException
./src/mapred/org/apache/hadoop/mapred/RamManager.java:32:   * @return <code>true</code> if memory was allocated immediately, 
./src/mapred/org/apache/hadoop/mapred/RamManager.java:41:   * @param requestedSize size of memory returned to the pool
./src/mapred/org/apache/hadoop/mapred/RawKeyValueIterator.java:33:   * @return Gets the current raw key as a DataInputBuffer
./src/mapred/org/apache/hadoop/mapred/RawKeyValueIterator.java:34:   * @throws IOException
./src/mapred/org/apache/hadoop/mapred/RawKeyValueIterator.java:41:   * @return Gets the current raw value as a DataInputBuffer 
./src/mapred/org/apache/hadoop/mapred/RawKeyValueIterator.java:42:   * @throws IOException
./src/mapred/org/apache/hadoop/mapred/RawKeyValueIterator.java:49:   * @return <code>true</code> if there exists a key/value, 
./src/mapred/org/apache/hadoop/mapred/RawKeyValueIterator.java:51:   * @throws IOException
./src/mapred/org/apache/hadoop/mapred/RawKeyValueIterator.java:58:   * @throws IOException
./src/mapred/org/apache/hadoop/mapred/RecordReader.java:26: * {@link InputSplit}.
./src/mapred/org/apache/hadoop/mapred/RecordReader.java:30: * record-oriented view for the {@link Mapper} & {@link Reducer} tasks for 
./src/mapred/org/apache/hadoop/mapred/RecordReader.java:34: * @see InputSplit
./src/mapred/org/apache/hadoop/mapred/RecordReader.java:35: * @see InputFormat
./src/mapred/org/apache/hadoop/mapred/RecordReader.java:41:   * @param key the key to read data into
./src/mapred/org/apache/hadoop/mapred/RecordReader.java:42:   * @param value the value to read data into
./src/mapred/org/apache/hadoop/mapred/RecordReader.java:43:   * @return true iff a key/value was read, false if at EOF
./src/mapred/org/apache/hadoop/mapred/RecordReader.java:50:   * @return a new key object.
./src/mapred/org/apache/hadoop/mapred/RecordReader.java:57:   * @return a new value object.
./src/mapred/org/apache/hadoop/mapred/RecordReader.java:64:   * @return the current position in the input.
./src/mapred/org/apache/hadoop/mapred/RecordReader.java:65:   * @throws IOException
./src/mapred/org/apache/hadoop/mapred/RecordReader.java:70:   * Close this {@link InputSplit} to future operations.
./src/mapred/org/apache/hadoop/mapred/RecordReader.java:72:   * @throws IOException
./src/mapred/org/apache/hadoop/mapred/RecordReader.java:77:   * How much of the input has the {@link RecordReader} consumed i.e.
./src/mapred/org/apache/hadoop/mapred/RecordReader.java:80:   * @return progress from <code>0.0</code> to <code>1.0</code>.
./src/mapred/org/apache/hadoop/mapred/RecordReader.java:81:   * @throws IOException
./src/mapred/org/apache/hadoop/mapred/RecordWriter.java:30: * {@link FileSystem}.
./src/mapred/org/apache/hadoop/mapred/RecordWriter.java:32: * @see OutputFormat
./src/mapred/org/apache/hadoop/mapred/RecordWriter.java:38:   * @param key the key to write.
./src/mapred/org/apache/hadoop/mapred/RecordWriter.java:39:   * @param value the value to write.
./src/mapred/org/apache/hadoop/mapred/RecordWriter.java:40:   * @throws IOException
./src/mapred/org/apache/hadoop/mapred/RecordWriter.java:47:   * @param reporter facility to report progress.
./src/mapred/org/apache/hadoop/mapred/RecordWriter.java:48:   * @throws IOException
./src/mapred/org/apache/hadoop/mapred/Reducer.java:33: * {@link JobConf#setNumReduceTasks(int)}. <code>Reducer</code> implementations 
./src/mapred/org/apache/hadoop/mapred/Reducer.java:34: * can access the {@link JobConf} for the job via the 
./src/mapred/org/apache/hadoop/mapred/Reducer.java:35: * {@link JobConfigurable#configure(JobConf)} method and initialize themselves. 
./src/mapred/org/apache/hadoop/mapred/Reducer.java:36: * Similarly they can use the {@link Closeable#close()} method for
./src/mapred/org/apache/hadoop/mapred/Reducer.java:45: *   <p><code>Reducer</code> is input the grouped output of a {@link Mapper}.
./src/mapred/org/apache/hadoop/mapred/Reducer.java:66: *   {@link JobConf#setOutputValueGroupingComparator(Class)}.Since 
./src/mapred/org/apache/hadoop/mapred/Reducer.java:67: *   {@link JobConf#setOutputKeyComparatorClass(Class)} can be used to 
./src/mapred/org/apache/hadoop/mapred/Reducer.java:90: *   {@link #reduce(Object, Iterator, OutputCollector, Reporter)}
./src/mapred/org/apache/hadoop/mapred/Reducer.java:94: *   {@link FileSystem} via 
./src/mapred/org/apache/hadoop/mapred/Reducer.java:95: *   {@link OutputCollector#collect(Object, Object)}.</p>
./src/mapred/org/apache/hadoop/mapred/Reducer.java:159: * @see Mapper
./src/mapred/org/apache/hadoop/mapred/Reducer.java:160: * @see Partitioner
./src/mapred/org/apache/hadoop/mapred/Reducer.java:161: * @see Reporter
./src/mapred/org/apache/hadoop/mapred/Reducer.java:162: * @see MapReduceBase
./src/mapred/org/apache/hadoop/mapred/Reducer.java:179:   * {@link OutputCollector#collect(Object,Object)}.</p>
./src/mapred/org/apache/hadoop/mapred/Reducer.java:181:   * <p>Applications can use the {@link Reporter} provided to report progress 
./src/mapred/org/apache/hadoop/mapred/Reducer.java:186:   * <a href="{@docRoot}/../hadoop-default.html#mapred.task.timeout">
./src/mapred/org/apache/hadoop/mapred/Reducer.java:190:   * @param key the key.
./src/mapred/org/apache/hadoop/mapred/Reducer.java:191:   * @param values the list of values to reduce.
./src/mapred/org/apache/hadoop/mapred/Reducer.java:192:   * @param output to collect keys and combined values.
./src/mapred/org/apache/hadoop/mapred/Reducer.java:193:   * @param reporter facility to report progress.
./src/mapred/org/apache/hadoop/mapred/ReduceTask.java:165:  @Override
./src/mapred/org/apache/hadoop/mapred/ReduceTask.java:171:  @Override
./src/mapred/org/apache/hadoop/mapred/ReduceTask.java:181:  @Override
./src/mapred/org/apache/hadoop/mapred/ReduceTask.java:187:  @Override
./src/mapred/org/apache/hadoop/mapred/ReduceTask.java:194:  @Override
./src/mapred/org/apache/hadoop/mapred/ReduceTask.java:230:    @Override
./src/mapred/org/apache/hadoop/mapred/ReduceTask.java:318:     @SuppressWarnings("unchecked")
./src/mapred/org/apache/hadoop/mapred/ReduceTask.java:332:  @Override
./src/mapred/org/apache/hadoop/mapred/ReduceTask.java:333:  @SuppressWarnings("unchecked")
./src/mapred/org/apache/hadoop/mapred/ReduceTask.java:638:     * even after {@link #maxFetchRetriesPerMap} retries; after this the
./src/mapred/org/apache/hadoop/mapred/ReduceTask.java:645:     * even after {@link #maxFetchRetriesPerMap} retries.
./src/mapred/org/apache/hadoop/mapred/ReduceTask.java:1025:       * @return were we currently fetching?
./src/mapred/org/apache/hadoop/mapred/ReduceTask.java:1059:       * The thread exits when it is interrupted by {@link ReduceTaskRunner}
./src/mapred/org/apache/hadoop/mapred/ReduceTask.java:1061:      @Override
./src/mapred/org/apache/hadoop/mapred/ReduceTask.java:1111:       * @param currentLocation the map output location to be copied
./src/mapred/org/apache/hadoop/mapred/ReduceTask.java:1112:       * @return the path (fully qualified) of the copied file
./src/mapred/org/apache/hadoop/mapred/ReduceTask.java:1113:       * @throws IOException if there is an error copying the file
./src/mapred/org/apache/hadoop/mapred/ReduceTask.java:1114:       * @throws InterruptedException if the copier should give up
./src/mapred/org/apache/hadoop/mapred/ReduceTask.java:1201:       * @param taskId map taskid
./src/mapred/org/apache/hadoop/mapred/ReduceTask.java:1212:       * @param mapOutputLoc map-output to be fetched
./src/mapred/org/apache/hadoop/mapred/ReduceTask.java:1213:       * @param filename the filename to write the data into
./src/mapred/org/apache/hadoop/mapred/ReduceTask.java:1214:       * @param connectionTimeout number of milliseconds for connection timeout
./src/mapred/org/apache/hadoop/mapred/ReduceTask.java:1215:       * @param readTimeout number of milliseconds for read timeout
./src/mapred/org/apache/hadoop/mapred/ReduceTask.java:1216:       * @return the path of the file that got created
./src/mapred/org/apache/hadoop/mapred/ReduceTask.java:1217:       * @throws IOException when something goes wrong
./src/mapred/org/apache/hadoop/mapred/ReduceTask.java:2056:    @SuppressWarnings("unchecked")
./src/mapred/org/apache/hadoop/mapred/ReduceTask.java:2216:     * Queries the {@link TaskTracker} for a set of map-completion events from
./src/mapred/org/apache/hadoop/mapred/ReduceTask.java:2219:     * @param fromEventId the first event ID we want to start from, this is
./src/mapred/org/apache/hadoop/mapred/ReduceTask.java:2221:     * @param mapLocations the hash map of map locations by host
./src/mapred/org/apache/hadoop/mapred/ReduceTask.java:2222:     * @param hostsList    the list that contains unique hosts having 
./src/mapred/org/apache/hadoop/mapred/ReduceTask.java:2225:     * @return the number of new map-completion events from the given event ID 
./src/mapred/org/apache/hadoop/mapred/ReduceTask.java:2226:     * @throws IOException
./src/mapred/org/apache/hadoop/mapred/ReduceTask.java:2333:      @SuppressWarnings("unchecked")
./src/mapred/org/apache/hadoop/mapred/ReduceTask.java:2452:      @SuppressWarnings("unchecked")
./src/mapred/org/apache/hadoop/mapred/ReduceTask.java:2524:    @SuppressWarnings("unchecked")
./src/mapred/org/apache/hadoop/mapred/ReduceTaskStatus.java:44:  @Override
./src/mapred/org/apache/hadoop/mapred/ReduceTaskStatus.java:51:  @Override
./src/mapred/org/apache/hadoop/mapred/ReduceTaskStatus.java:56:  @Override
./src/mapred/org/apache/hadoop/mapred/ReduceTaskStatus.java:67:  @Override
./src/mapred/org/apache/hadoop/mapred/ReduceTaskStatus.java:72:  @Override
./src/mapred/org/apache/hadoop/mapred/ReduceTaskStatus.java:77:  @Override
./src/mapred/org/apache/hadoop/mapred/ReduceTaskStatus.java:82:  @Override
./src/mapred/org/apache/hadoop/mapred/ReduceTaskStatus.java:90:  @Override
./src/mapred/org/apache/hadoop/mapred/ReduceTaskStatus.java:95:  @Override
./src/mapred/org/apache/hadoop/mapred/ReduceTaskStatus.java:100:  @Override
./src/mapred/org/apache/hadoop/mapred/ReduceTaskStatus.java:120:  @Override
./src/mapred/org/apache/hadoop/mapred/ReduceTaskStatus.java:126:  @Override
./src/mapred/org/apache/hadoop/mapred/ReduceTaskStatus.java:138:  @Override
./src/mapred/org/apache/hadoop/mapred/ReinitTrackerAction.java:26: * Represents a directive from the {@link org.apache.hadoop.mapred.JobTracker} 
./src/mapred/org/apache/hadoop/mapred/ReinitTrackerAction.java:27: * to the {@link org.apache.hadoop.mapred.TaskTracker} to reinitialize itself.
./src/mapred/org/apache/hadoop/mapred/Reporter.java:28: * <p>{@link Mapper} and {@link Reducer} can use the <code>Reporter</code>
./src/mapred/org/apache/hadoop/mapred/Reporter.java:34: * <p>Applications can also update {@link Counters} via the provided 
./src/mapred/org/apache/hadoop/mapred/Reporter.java:37: * @see Progressable
./src/mapred/org/apache/hadoop/mapred/Reporter.java:38: * @see Counters
./src/mapred/org/apache/hadoop/mapred/Reporter.java:65:   * @param status brief description of the current status.
./src/mapred/org/apache/hadoop/mapred/Reporter.java:70:   * Get the {@link Counter} of the given group with the given name.
./src/mapred/org/apache/hadoop/mapred/Reporter.java:72:   * @param group counter group
./src/mapred/org/apache/hadoop/mapred/Reporter.java:73:   * @param name counter name
./src/mapred/org/apache/hadoop/mapred/Reporter.java:74:   * @return the <code>Counter</code> of the given group/name.
./src/mapred/org/apache/hadoop/mapred/Reporter.java:80:   * any {@link Enum} type, by the specified amount.
./src/mapred/org/apache/hadoop/mapred/Reporter.java:82:   * @param key key to identify the counter to be incremented. The key can be
./src/mapred/org/apache/hadoop/mapred/Reporter.java:84:   * @param amount A non-negative amount by which the counter is to 
./src/mapred/org/apache/hadoop/mapred/Reporter.java:93:   * @param group name to identify the group of the counter to be incremented.
./src/mapred/org/apache/hadoop/mapred/Reporter.java:94:   * @param counter name to identify the counter within the group.
./src/mapred/org/apache/hadoop/mapred/Reporter.java:95:   * @param amount A non-negative amount by which the counter is to 
./src/mapred/org/apache/hadoop/mapred/Reporter.java:101:   * Get the {@link InputSplit} object for a map.
./src/mapred/org/apache/hadoop/mapred/Reporter.java:103:   * @return the <code>InputSplit</code> that the map is reading from.
./src/mapred/org/apache/hadoop/mapred/Reporter.java:104:   * @throws UnsupportedOperationException if called outside a mapper
./src/mapred/org/apache/hadoop/mapred/ResourceEstimator.java:61:   * @return
./src/mapred/org/apache/hadoop/mapred/ResourceEstimator.java:95:   * @return estimated length of this job's total map output
./src/mapred/org/apache/hadoop/mapred/ResourceEstimator.java:118:   * @return estimated length of this job's average map output
./src/mapred/org/apache/hadoop/mapred/ResourceEstimator.java:127:   * @return estimated length of this job's average reduce input
./src/mapred/org/apache/hadoop/mapred/RunningJob.java:28: * <p>Clients can get hold of <code>RunningJob</code> via the {@link JobClient}
./src/mapred/org/apache/hadoop/mapred/RunningJob.java:32: * @see JobClient
./src/mapred/org/apache/hadoop/mapred/RunningJob.java:38:   * @return the job identifier.
./src/mapred/org/apache/hadoop/mapred/RunningJob.java:42:  /** @deprecated This method is deprecated and will be removed. Applications should 
./src/mapred/org/apache/hadoop/mapred/RunningJob.java:43:   * rather use {@link #getID()}.
./src/mapred/org/apache/hadoop/mapred/RunningJob.java:45:  @Deprecated
./src/mapred/org/apache/hadoop/mapred/RunningJob.java:51:   * @return the name of the job.
./src/mapred/org/apache/hadoop/mapred/RunningJob.java:58:   * @return the path of the submitted job configuration.
./src/mapred/org/apache/hadoop/mapred/RunningJob.java:65:   * @return the URL where some job progress information will be displayed.
./src/mapred/org/apache/hadoop/mapred/RunningJob.java:73:   * @return the progress of the job's map-tasks.
./src/mapred/org/apache/hadoop/mapred/RunningJob.java:74:   * @throws IOException
./src/mapred/org/apache/hadoop/mapred/RunningJob.java:82:   * @return the progress of the job's reduce-tasks.
./src/mapred/org/apache/hadoop/mapred/RunningJob.java:83:   * @throws IOException
./src/mapred/org/apache/hadoop/mapred/RunningJob.java:91:   * @return the progress of the job's cleanup-tasks.
./src/mapred/org/apache/hadoop/mapred/RunningJob.java:92:   * @throws IOException
./src/mapred/org/apache/hadoop/mapred/RunningJob.java:100:   * @return the progress of the job's setup-tasks.
./src/mapred/org/apache/hadoop/mapred/RunningJob.java:101:   * @throws IOException
./src/mapred/org/apache/hadoop/mapred/RunningJob.java:109:   * @return <code>true</code> if the job is complete, else <code>false</code>.
./src/mapred/org/apache/hadoop/mapred/RunningJob.java:110:   * @throws IOException
./src/mapred/org/apache/hadoop/mapred/RunningJob.java:117:   * @return <code>true</code> if the job succeeded, else <code>false</code>.
./src/mapred/org/apache/hadoop/mapred/RunningJob.java:118:   * @throws IOException
./src/mapred/org/apache/hadoop/mapred/RunningJob.java:125:   * @throws IOException
./src/mapred/org/apache/hadoop/mapred/RunningJob.java:131:   * {@link JobStatus}
./src/mapred/org/apache/hadoop/mapred/RunningJob.java:133:   * @throws IOException
./src/mapred/org/apache/hadoop/mapred/RunningJob.java:141:   * @throws IOException
./src/mapred/org/apache/hadoop/mapred/RunningJob.java:147:   * @param priority the new priority for the job.
./src/mapred/org/apache/hadoop/mapred/RunningJob.java:148:   * @throws IOException
./src/mapred/org/apache/hadoop/mapred/RunningJob.java:155:   * @param startFrom index to start fetching events from
./src/mapred/org/apache/hadoop/mapred/RunningJob.java:156:   * @return an array of {@link TaskCompletionEvent}s
./src/mapred/org/apache/hadoop/mapred/RunningJob.java:157:   * @throws IOException
./src/mapred/org/apache/hadoop/mapred/RunningJob.java:165:   * @param taskId the id of the task to be terminated.
./src/mapred/org/apache/hadoop/mapred/RunningJob.java:166:   * @param shouldFail if true the task is failed and added to failed tasks 
./src/mapred/org/apache/hadoop/mapred/RunningJob.java:169:   * @throws IOException
./src/mapred/org/apache/hadoop/mapred/RunningJob.java:173:  /** @deprecated Applications should rather use {@link #killTask(TaskAttemptID, boolean)}*/
./src/mapred/org/apache/hadoop/mapred/RunningJob.java:174:  @Deprecated
./src/mapred/org/apache/hadoop/mapred/RunningJob.java:180:   * @return the counters for this job.
./src/mapred/org/apache/hadoop/mapred/RunningJob.java:181:   * @throws IOException
./src/mapred/org/apache/hadoop/mapred/SequenceFileAsBinaryInputFormat.java:86:     * @see org.apache.hadoop.io.SequenceFile.Reader#getKeyClassName
./src/mapred/org/apache/hadoop/mapred/SequenceFileAsBinaryInputFormat.java:94:     * @see org.apache.hadoop.io.SequenceFile.Reader#getValueClassName
./src/mapred/org/apache/hadoop/mapred/SequenceFileAsBinaryInputFormat.java:129:     * @return 0.0 to 1.0 of the input byte range
./src/mapred/org/apache/hadoop/mapred/SequenceFileAsBinaryOutputFormat.java:39: * An {@link OutputFormat} that writes keys, values to 
./src/mapred/org/apache/hadoop/mapred/SequenceFileAsBinaryOutputFormat.java:40: * {@link SequenceFile}s in binary(raw) format
./src/mapred/org/apache/hadoop/mapred/SequenceFileAsBinaryOutputFormat.java:79:   * Set the key class for the {@link SequenceFile}
./src/mapred/org/apache/hadoop/mapred/SequenceFileAsBinaryOutputFormat.java:81:   * from the actual class ({@link BytesWritable}) used for writing </p>
./src/mapred/org/apache/hadoop/mapred/SequenceFileAsBinaryOutputFormat.java:83:   * @param conf the {@link JobConf} to modify
./src/mapred/org/apache/hadoop/mapred/SequenceFileAsBinaryOutputFormat.java:84:   * @param theClass the SequenceFile output key class.
./src/mapred/org/apache/hadoop/mapred/SequenceFileAsBinaryOutputFormat.java:92:   * Set the value class for the {@link SequenceFile}
./src/mapred/org/apache/hadoop/mapred/SequenceFileAsBinaryOutputFormat.java:94:   * from the actual class ({@link BytesWritable}) used for writing </p>
./src/mapred/org/apache/hadoop/mapred/SequenceFileAsBinaryOutputFormat.java:96:   * @param conf the {@link JobConf} to modify
./src/mapred/org/apache/hadoop/mapred/SequenceFileAsBinaryOutputFormat.java:97:   * @param theClass the SequenceFile output key class.
./src/mapred/org/apache/hadoop/mapred/SequenceFileAsBinaryOutputFormat.java:106:   * Get the key class for the {@link SequenceFile}
./src/mapred/org/apache/hadoop/mapred/SequenceFileAsBinaryOutputFormat.java:108:   * @return the key class of the {@link SequenceFile}
./src/mapred/org/apache/hadoop/mapred/SequenceFileAsBinaryOutputFormat.java:117:   * Get the value class for the {@link SequenceFile}
./src/mapred/org/apache/hadoop/mapred/SequenceFileAsBinaryOutputFormat.java:119:   * @return the value class of the {@link SequenceFile}
./src/mapred/org/apache/hadoop/mapred/SequenceFileAsBinaryOutputFormat.java:127:  @Override 
./src/mapred/org/apache/hadoop/mapred/SequenceFileAsBinaryOutputFormat.java:175:  @Override 
./src/mapred/org/apache/hadoop/mapred/SequenceFileInputFilter.java:53:   * @param split file split
./src/mapred/org/apache/hadoop/mapred/SequenceFileInputFilter.java:54:   * @param job job configuration
./src/mapred/org/apache/hadoop/mapred/SequenceFileInputFilter.java:55:   * @param reporter reporter who sends report to task tracker
./src/mapred/org/apache/hadoop/mapred/SequenceFileInputFilter.java:56:   * @return RecordReader
./src/mapred/org/apache/hadoop/mapred/SequenceFileInputFilter.java:70:   * @param conf application configuration
./src/mapred/org/apache/hadoop/mapred/SequenceFileInputFilter.java:71:   * @param filterClass filter class
./src/mapred/org/apache/hadoop/mapred/SequenceFileInputFilter.java:84:     * @param key record key
./src/mapred/org/apache/hadoop/mapred/SequenceFileInputFilter.java:85:     * @return true if a record is accepted; return false otherwise
./src/mapred/org/apache/hadoop/mapred/SequenceFileInputFilter.java:106:     * @param conf where the regex is set
./src/mapred/org/apache/hadoop/mapred/SequenceFileInputFilter.java:107:     * @param regex regex used as a filter
./src/mapred/org/apache/hadoop/mapred/SequenceFileInputFilter.java:134:     * @see org.apache.hadoop.mapred.SequenceFileInputFilter.Filter#accept(Object)
./src/mapred/org/apache/hadoop/mapred/SequenceFileInputFilter.java:151:     * @param conf configuration
./src/mapred/org/apache/hadoop/mapred/SequenceFileInputFilter.java:152:     * @param frequency filtering frequencey
./src/mapred/org/apache/hadoop/mapred/SequenceFileInputFilter.java:165:     * @param conf configuration
./src/mapred/org/apache/hadoop/mapred/SequenceFileInputFilter.java:178:     * @see org.apache.hadoop.mapred.SequenceFileInputFilter.Filter#accept(Object)
./src/mapred/org/apache/hadoop/mapred/SequenceFileInputFilter.java:212:     * @param conf configuration
./src/mapred/org/apache/hadoop/mapred/SequenceFileInputFilter.java:213:     * @param frequency filtering frequency
./src/mapred/org/apache/hadoop/mapred/SequenceFileInputFilter.java:226:     * @param conf configuration
./src/mapred/org/apache/hadoop/mapred/SequenceFileInputFilter.java:239:     * @see org.apache.hadoop.mapred.SequenceFileInputFilter.Filter#accept(Object)
./src/mapred/org/apache/hadoop/mapred/SequenceFileInputFormat.java:30:/** An {@link InputFormat} for {@link SequenceFile}s. */
./src/mapred/org/apache/hadoop/mapred/SequenceFileInputFormat.java:37:  @Override
./src/mapred/org/apache/hadoop/mapred/SequenceFileOutputFormat.java:35:/** An {@link OutputFormat} that writes {@link SequenceFile}s. */
./src/mapred/org/apache/hadoop/mapred/SequenceFileOutputFormat.java:94:   * Get the {@link CompressionType} for the output {@link SequenceFile}.
./src/mapred/org/apache/hadoop/mapred/SequenceFileOutputFormat.java:95:   * @param conf the {@link JobConf}
./src/mapred/org/apache/hadoop/mapred/SequenceFileOutputFormat.java:96:   * @return the {@link CompressionType} for the output {@link SequenceFile}, 
./src/mapred/org/apache/hadoop/mapred/SequenceFileOutputFormat.java:97:   *         defaulting to {@link CompressionType#RECORD}
./src/mapred/org/apache/hadoop/mapred/SequenceFileOutputFormat.java:106:   * Set the {@link CompressionType} for the output {@link SequenceFile}.
./src/mapred/org/apache/hadoop/mapred/SequenceFileOutputFormat.java:107:   * @param conf the {@link JobConf} to modify
./src/mapred/org/apache/hadoop/mapred/SequenceFileOutputFormat.java:108:   * @param style the {@link CompressionType} for the output
./src/mapred/org/apache/hadoop/mapred/SequenceFileOutputFormat.java:109:   *              {@link SequenceFile} 
./src/mapred/org/apache/hadoop/mapred/SequenceFileRecordReader.java:30:/** An {@link RecordReader} for {@link SequenceFile}s. */
./src/mapred/org/apache/hadoop/mapred/SequenceFileRecordReader.java:55:  /** The class of key that must be passed to {@link
./src/mapred/org/apache/hadoop/mapred/SequenceFileRecordReader.java:59:  /** The class of value that must be passed to {@link
./src/mapred/org/apache/hadoop/mapred/SequenceFileRecordReader.java:63:  @SuppressWarnings("unchecked")
./src/mapred/org/apache/hadoop/mapred/SequenceFileRecordReader.java:68:  @SuppressWarnings("unchecked")
./src/mapred/org/apache/hadoop/mapred/SequenceFileRecordReader.java:108:   * @return 0.0 to 1.0 of the input byte range
./src/mapred/org/apache/hadoop/mapred/SkipBadRecords.java:40: * see {@link SkipBadRecords#setMapperMaxSkipRecords(Configuration, long)}</p>
./src/mapred/org/apache/hadoop/mapred/SkipBadRecords.java:43: * see {@link SkipBadRecords#setAttemptsToStartSkipping(Configuration, int)}</p>
./src/mapred/org/apache/hadoop/mapred/SkipBadRecords.java:62:   * @see SkipBadRecords#getAutoIncrMapperProcCount(Configuration)
./src/mapred/org/apache/hadoop/mapred/SkipBadRecords.java:69:   * @see SkipBadRecords#getAutoIncrReducerProcCount(Configuration)
./src/mapred/org/apache/hadoop/mapred/SkipBadRecords.java:95:   * @param conf the configuration
./src/mapred/org/apache/hadoop/mapred/SkipBadRecords.java:96:   * @return attemptsToStartSkipping no of task attempts
./src/mapred/org/apache/hadoop/mapred/SkipBadRecords.java:111:   * @param conf the configuration
./src/mapred/org/apache/hadoop/mapred/SkipBadRecords.java:112:   * @param attemptsToStartSkipping no of task attempts
./src/mapred/org/apache/hadoop/mapred/SkipBadRecords.java:121:   * {@link SkipBadRecords#COUNTER_MAP_PROCESSED_RECORDS} is incremented 
./src/mapred/org/apache/hadoop/mapred/SkipBadRecords.java:128:   * @param conf the configuration
./src/mapred/org/apache/hadoop/mapred/SkipBadRecords.java:129:   * @return <code>true</code> if auto increment 
./src/mapred/org/apache/hadoop/mapred/SkipBadRecords.java:130:   *                       {@link SkipBadRecords#COUNTER_MAP_PROCESSED_RECORDS}.
./src/mapred/org/apache/hadoop/mapred/SkipBadRecords.java:139:   * {@link SkipBadRecords#COUNTER_MAP_PROCESSED_RECORDS} is incremented 
./src/mapred/org/apache/hadoop/mapred/SkipBadRecords.java:146:   * @param conf the configuration
./src/mapred/org/apache/hadoop/mapred/SkipBadRecords.java:147:   * @param autoIncr whether to auto increment 
./src/mapred/org/apache/hadoop/mapred/SkipBadRecords.java:148:   *        {@link SkipBadRecords#COUNTER_MAP_PROCESSED_RECORDS}.
./src/mapred/org/apache/hadoop/mapred/SkipBadRecords.java:157:   * {@link SkipBadRecords#COUNTER_REDUCE_PROCESSED_GROUPS} is incremented 
./src/mapred/org/apache/hadoop/mapred/SkipBadRecords.java:164:   * @param conf the configuration
./src/mapred/org/apache/hadoop/mapred/SkipBadRecords.java:165:   * @return <code>true</code> if auto increment 
./src/mapred/org/apache/hadoop/mapred/SkipBadRecords.java:166:   *                    {@link SkipBadRecords#COUNTER_REDUCE_PROCESSED_GROUPS}.
./src/mapred/org/apache/hadoop/mapred/SkipBadRecords.java:175:   * {@link SkipBadRecords#COUNTER_REDUCE_PROCESSED_GROUPS} is incremented 
./src/mapred/org/apache/hadoop/mapred/SkipBadRecords.java:182:   * @param conf the configuration
./src/mapred/org/apache/hadoop/mapred/SkipBadRecords.java:183:   * @param autoIncr whether to auto increment 
./src/mapred/org/apache/hadoop/mapred/SkipBadRecords.java:184:   *        {@link SkipBadRecords#COUNTER_REDUCE_PROCESSED_GROUPS}.
./src/mapred/org/apache/hadoop/mapred/SkipBadRecords.java:196:   * @param conf the configuration.
./src/mapred/org/apache/hadoop/mapred/SkipBadRecords.java:197:   * @return path skip output directory. Null is returned if this is not set 
./src/mapred/org/apache/hadoop/mapred/SkipBadRecords.java:218:   * @param conf the configuration.
./src/mapred/org/apache/hadoop/mapred/SkipBadRecords.java:219:   * @param path skip output directory path
./src/mapred/org/apache/hadoop/mapred/SkipBadRecords.java:243:   * @param conf the configuration
./src/mapred/org/apache/hadoop/mapred/SkipBadRecords.java:244:   * @return maxSkipRecs acceptable skip records.
./src/mapred/org/apache/hadoop/mapred/SkipBadRecords.java:262:   * @param conf the configuration
./src/mapred/org/apache/hadoop/mapred/SkipBadRecords.java:263:   * @param maxSkipRecs acceptable skip records.
./src/mapred/org/apache/hadoop/mapred/SkipBadRecords.java:282:   * @param conf the configuration
./src/mapred/org/apache/hadoop/mapred/SkipBadRecords.java:283:   * @return maxSkipGrps acceptable skip groups.
./src/mapred/org/apache/hadoop/mapred/SkipBadRecords.java:301:   * @param conf the configuration
./src/mapred/org/apache/hadoop/mapred/SkipBadRecords.java:302:   * @param maxSkipGrps acceptable skip groups.
./src/mapred/org/apache/hadoop/mapred/SortedRanges.java:49:   * @return SkipRangeIterator
./src/mapred/org/apache/hadoop/mapred/SortedRanges.java:57:   * @return indices count
./src/mapred/org/apache/hadoop/mapred/SortedRanges.java:65:   * @return ranges
./src/mapred/org/apache/hadoop/mapred/SortedRanges.java:78:   * @param range Range to be added.
./src/mapred/org/apache/hadoop/mapred/SortedRanges.java:132:   * @param range Range to be removed.
./src/mapred/org/apache/hadoop/mapred/SortedRanges.java:242:     * @return startIndex. 
./src/mapred/org/apache/hadoop/mapred/SortedRanges.java:250:     * @return endIndex.
./src/mapred/org/apache/hadoop/mapred/SortedRanges.java:258:    * @return length
./src/mapred/org/apache/hadoop/mapred/SortedRanges.java:266:     * @return <code>true</code> if empty
./src/mapred/org/apache/hadoop/mapred/SortedRanges.java:319:     * @param rangeIterator the iterator which gives the ranges.
./src/mapred/org/apache/hadoop/mapred/SortedRanges.java:328:     * @return <code>true</code> next index exists.
./src/mapred/org/apache/hadoop/mapred/SortedRanges.java:337:     * @return next index
./src/mapred/org/apache/hadoop/mapred/SortedRanges.java:367:     * @return <code>true</code> if all ranges have been skipped.
./src/mapred/org/apache/hadoop/mapred/StatusHttpServer.java:37:   * @param name The name of the server
./src/mapred/org/apache/hadoop/mapred/StatusHttpServer.java:38:   * @param port The port to use on the server
./src/mapred/org/apache/hadoop/mapred/StatusHttpServer.java:39:   * @param findPort whether the server should start at the given port and 
./src/mapred/org/apache/hadoop/mapred/StatusHttpServer.java:69:    @Override
./src/mapred/org/apache/hadoop/mapred/Task.java:166:   * @return the job name
./src/mapred/org/apache/hadoop/mapred/Task.java:174:   * @return the integer part of the task id
./src/mapred/org/apache/hadoop/mapred/Task.java:182:   * @return
./src/mapred/org/apache/hadoop/mapred/Task.java:189:   * @param p
./src/mapred/org/apache/hadoop/mapred/Task.java:232:   * @param skipping
./src/mapred/org/apache/hadoop/mapred/Task.java:278:  @Override
./src/mapred/org/apache/hadoop/mapred/Task.java:306:   * @param umbilical for progress reports
./src/mapred/org/apache/hadoop/mapred/Task.java:313:   * @param tip TODO*/
./src/mapred/org/apache/hadoop/mapred/Task.java:483:   * @param umbilical
./src/mapred/org/apache/hadoop/mapred/Task.java:484:   * @param nextRecIndex the record index which would be fed next.
./src/mapred/org/apache/hadoop/mapred/Task.java:485:   * @throws IOException
./src/mapred/org/apache/hadoop/mapred/Task.java:883:     * @throws IOException
./src/mapred/org/apache/hadoop/mapred/TaskAttemptContext.java:41:   * @return TaskAttemptID
./src/mapred/org/apache/hadoop/mapred/TaskAttemptContext.java:50:   * @return JobConf
./src/mapred/org/apache/hadoop/mapred/TaskAttemptID.java:31: * {@link TaskID}, that this TaskAttemptID belongs to.
./src/mapred/org/apache/hadoop/mapred/TaskAttemptID.java:39: * , but rather use appropriate constructors or {@link #forName(String)} 
./src/mapred/org/apache/hadoop/mapred/TaskAttemptID.java:42: * @see JobID
./src/mapred/org/apache/hadoop/mapred/TaskAttemptID.java:43: * @see TaskID
./src/mapred/org/apache/hadoop/mapred/TaskAttemptID.java:51:   * Constructs a TaskAttemptID object from given {@link TaskID}.  
./src/mapred/org/apache/hadoop/mapred/TaskAttemptID.java:52:   * @param taskId TaskID that this task belongs to  
./src/mapred/org/apache/hadoop/mapred/TaskAttemptID.java:53:   * @param id the task attempt number
./src/mapred/org/apache/hadoop/mapred/TaskAttemptID.java:65:   * @param jtIdentifier jobTracker identifier
./src/mapred/org/apache/hadoop/mapred/TaskAttemptID.java:66:   * @param jobId job number 
./src/mapred/org/apache/hadoop/mapred/TaskAttemptID.java:67:   * @param isMap whether the tip is a map 
./src/mapred/org/apache/hadoop/mapred/TaskAttemptID.java:68:   * @param taskId taskId number
./src/mapred/org/apache/hadoop/mapred/TaskAttemptID.java:69:   * @param id the task attempt number
./src/mapred/org/apache/hadoop/mapred/TaskAttemptID.java:78:  /** Returns the {@link JobID} object that this task attempt belongs to */
./src/mapred/org/apache/hadoop/mapred/TaskAttemptID.java:83:  /** Returns the {@link TaskID} object that this task attempt belongs to */
./src/mapred/org/apache/hadoop/mapred/TaskAttemptID.java:93:  @Override
./src/mapred/org/apache/hadoop/mapred/TaskAttemptID.java:106:  @Override
./src/mapred/org/apache/hadoop/mapred/TaskAttemptID.java:115:  @Override
./src/mapred/org/apache/hadoop/mapred/TaskAttemptID.java:128:  @Override
./src/mapred/org/apache/hadoop/mapred/TaskAttemptID.java:133:  @Override
./src/mapred/org/apache/hadoop/mapred/TaskAttemptID.java:139:  @Override
./src/mapred/org/apache/hadoop/mapred/TaskAttemptID.java:152:   * @return constructed TaskAttemptID object or null if the given String is null
./src/mapred/org/apache/hadoop/mapred/TaskAttemptID.java:153:   * @throws IllegalArgumentException if the given string is malformed
./src/mapred/org/apache/hadoop/mapred/TaskAttemptID.java:187:   * @param jtIdentifier jobTracker identifier, or null
./src/mapred/org/apache/hadoop/mapred/TaskAttemptID.java:188:   * @param jobId job number, or null
./src/mapred/org/apache/hadoop/mapred/TaskAttemptID.java:189:   * @param isMap whether the tip is a map, or null 
./src/mapred/org/apache/hadoop/mapred/TaskAttemptID.java:190:   * @param taskId taskId number, or null
./src/mapred/org/apache/hadoop/mapred/TaskAttemptID.java:191:   * @param attemptId the task attempt number, or null
./src/mapred/org/apache/hadoop/mapred/TaskAttemptID.java:192:   * @return a regex pattern matching TaskAttemptIDs
./src/mapred/org/apache/hadoop/mapred/TaskCompletionEvent.java:52:   * @param eventId event id, event id should be unique and assigned in
./src/mapred/org/apache/hadoop/mapred/TaskCompletionEvent.java:54:   * @param taskId task id
./src/mapred/org/apache/hadoop/mapred/TaskCompletionEvent.java:55:   * @param status task's status 
./src/mapred/org/apache/hadoop/mapred/TaskCompletionEvent.java:56:   * @param taskTrackerHttp task tracker's host:port for http. 
./src/mapred/org/apache/hadoop/mapred/TaskCompletionEvent.java:74:   * @return event id
./src/mapred/org/apache/hadoop/mapred/TaskCompletionEvent.java:81:   * @return task id
./src/mapred/org/apache/hadoop/mapred/TaskCompletionEvent.java:82:   * @deprecated use {@link #getTaskAttemptId()} instead.
./src/mapred/org/apache/hadoop/mapred/TaskCompletionEvent.java:84:  @Deprecated
./src/mapred/org/apache/hadoop/mapred/TaskCompletionEvent.java:91:   * @return task id
./src/mapred/org/apache/hadoop/mapred/TaskCompletionEvent.java:99:   * @return task tracker status
./src/mapred/org/apache/hadoop/mapred/TaskCompletionEvent.java:106:   * @return http location of tasktracker user logs
./src/mapred/org/apache/hadoop/mapred/TaskCompletionEvent.java:121:   * @param taskCompletionTime time (in millisec) the task took to complete
./src/mapred/org/apache/hadoop/mapred/TaskCompletionEvent.java:129:   * @param eventId
./src/mapred/org/apache/hadoop/mapred/TaskCompletionEvent.java:137:   * @param taskId
./src/mapred/org/apache/hadoop/mapred/TaskCompletionEvent.java:138:   * @deprecated use {@link #setTaskID(TaskAttemptID)} instead.
./src/mapred/org/apache/hadoop/mapred/TaskCompletionEvent.java:140:  @Deprecated
./src/mapred/org/apache/hadoop/mapred/TaskCompletionEvent.java:147:   * @param taskId
./src/mapred/org/apache/hadoop/mapred/TaskCompletionEvent.java:155:   * @param status
./src/mapred/org/apache/hadoop/mapred/TaskCompletionEvent.java:163:   * @param taskTrackerHttp
./src/mapred/org/apache/hadoop/mapred/TaskCompletionEvent.java:170:  @Override
./src/mapred/org/apache/hadoop/mapred/TaskCompletionEvent.java:180:  @Override
./src/mapred/org/apache/hadoop/mapred/TaskCompletionEvent.java:197:  @Override
./src/mapred/org/apache/hadoop/mapred/TaskID.java:32: * TaskID consists of 3 parts. First part is the {@link JobID}, that this 
./src/mapred/org/apache/hadoop/mapred/TaskID.java:42: * , but rather use appropriate constructors or {@link #forName(String)} 
./src/mapred/org/apache/hadoop/mapred/TaskID.java:45: * @see JobID
./src/mapred/org/apache/hadoop/mapred/TaskID.java:46: * @see TaskAttemptID
./src/mapred/org/apache/hadoop/mapred/TaskID.java:61:   * Constructs a TaskID object from given {@link JobID}.  
./src/mapred/org/apache/hadoop/mapred/TaskID.java:62:   * @param jobId JobID that this tip belongs to 
./src/mapred/org/apache/hadoop/mapred/TaskID.java:63:   * @param isMap whether the tip is a map 
./src/mapred/org/apache/hadoop/mapred/TaskID.java:64:   * @param id the tip number
./src/mapred/org/apache/hadoop/mapred/TaskID.java:77:   * @param jtIdentifier jobTracker identifier
./src/mapred/org/apache/hadoop/mapred/TaskID.java:78:   * @param jobId job number 
./src/mapred/org/apache/hadoop/mapred/TaskID.java:79:   * @param isMap whether the tip is a map 
./src/mapred/org/apache/hadoop/mapred/TaskID.java:80:   * @param id the tip number
./src/mapred/org/apache/hadoop/mapred/TaskID.java:88:  /** Returns the {@link JobID} object that this tip belongs to */
./src/mapred/org/apache/hadoop/mapred/TaskID.java:98:  @Override
./src/mapred/org/apache/hadoop/mapred/TaskID.java:113:  @Override
./src/mapred/org/apache/hadoop/mapred/TaskID.java:126:  @Override
./src/mapred/org/apache/hadoop/mapred/TaskID.java:140:  @Override
./src/mapred/org/apache/hadoop/mapred/TaskID.java:145:  @Override
./src/mapred/org/apache/hadoop/mapred/TaskID.java:152:  @Override
./src/mapred/org/apache/hadoop/mapred/TaskID.java:166:   * @return constructed TaskID object or null if the given String is null
./src/mapred/org/apache/hadoop/mapred/TaskID.java:167:   * @throws IllegalArgumentException if the given string is malformed
./src/mapred/org/apache/hadoop/mapred/TaskID.java:201:   * @param jtIdentifier jobTracker identifier, or null
./src/mapred/org/apache/hadoop/mapred/TaskID.java:202:   * @param jobId job number, or null
./src/mapred/org/apache/hadoop/mapred/TaskID.java:203:   * @param isMap whether the tip is a map, or null 
./src/mapred/org/apache/hadoop/mapred/TaskID.java:204:   * @param taskId taskId number, or null
./src/mapred/org/apache/hadoop/mapred/TaskID.java:205:   * @return a regex pattern matching TaskIDs
./src/mapred/org/apache/hadoop/mapred/TaskInProgress.java:171:   * @return int the tip index
./src/mapred/org/apache/hadoop/mapred/TaskInProgress.java:279:   * @param taskId
./src/mapred/org/apache/hadoop/mapred/TaskInProgress.java:280:   * @return
./src/mapred/org/apache/hadoop/mapred/TaskInProgress.java:288:   * @param taskId
./src/mapred/org/apache/hadoop/mapred/TaskInProgress.java:289:   * @return Returns true if the Task is the first attempt of the tip
./src/mapred/org/apache/hadoop/mapred/TaskInProgress.java:297:   * @return true if any tasks are running
./src/mapred/org/apache/hadoop/mapred/TaskInProgress.java:318:   * @return <code>true</code> if the tip is complete, else <code>false</code>
./src/mapred/org/apache/hadoop/mapred/TaskInProgress.java:327:   * @param taskid taskid of attempt to check for completion
./src/mapred/org/apache/hadoop/mapred/TaskInProgress.java:328:   * @return <code>true</code> if taskid is complete, else <code>false</code>
./src/mapred/org/apache/hadoop/mapred/TaskInProgress.java:338:   * @return <code>true</code> if tip has failed, else <code>false</code>
./src/mapred/org/apache/hadoop/mapred/TaskInProgress.java:411:   * @param taskid
./src/mapred/org/apache/hadoop/mapred/TaskInProgress.java:445:   * @param taskId the id of the required task
./src/mapred/org/apache/hadoop/mapred/TaskInProgress.java:446:   * @return the list of diagnostics for that task
./src/mapred/org/apache/hadoop/mapred/TaskInProgress.java:460:   * @param taskId id of the task 
./src/mapred/org/apache/hadoop/mapred/TaskInProgress.java:461:   * @param diagInfo diagnostic information for the task
./src/mapred/org/apache/hadoop/mapred/TaskInProgress.java:476:   * @return has the task changed its state noticably?
./src/mapred/org/apache/hadoop/mapred/TaskInProgress.java:499:      // @see {@link TaskTracker.transmitHeartbeat()}
./src/mapred/org/apache/hadoop/mapred/TaskInProgress.java:624:   * task-attempt of the {@link TaskInProgress} and hence might be declared 
./src/mapred/org/apache/hadoop/mapred/TaskInProgress.java:625:   * {@link TaskStatus.State.SUCCEEDED} or {@link TaskStatus.State.KILLED}
./src/mapred/org/apache/hadoop/mapred/TaskInProgress.java:627:   * @param taskId id of the completed task-attempt
./src/mapred/org/apache/hadoop/mapred/TaskInProgress.java:628:   * @param finalTaskState final {@link TaskStatus.State} of the task-attempt
./src/mapred/org/apache/hadoop/mapred/TaskInProgress.java:639:   * taskid as {@link TaskStatus.State.KILLED}. 
./src/mapred/org/apache/hadoop/mapred/TaskInProgress.java:693:   * @param taskid
./src/mapred/org/apache/hadoop/mapred/TaskInProgress.java:694:   * @return
./src/mapred/org/apache/hadoop/mapred/TaskInProgress.java:714:   * @return true if the task killed
./src/mapred/org/apache/hadoop/mapred/TaskInProgress.java:886:   * @param trackerHost The task tracker hostname
./src/mapred/org/apache/hadoop/mapred/TaskInProgress.java:887:   * @return Has it failed?
./src/mapred/org/apache/hadoop/mapred/TaskInProgress.java:895:   * @param trackerHost The task tracker hostname 
./src/mapred/org/apache/hadoop/mapred/TaskInProgress.java:896:   * @param trackerName The tracker name
./src/mapred/org/apache/hadoop/mapred/TaskInProgress.java:897:   * @return Was task scheduled on the tracker?
./src/mapred/org/apache/hadoop/mapred/TaskInProgress.java:905:   * @return the size of the failed machine set
./src/mapred/org/apache/hadoop/mapred/TaskInProgress.java:913:   * @return The index of this tip in the maps/reduces lists.
./src/mapred/org/apache/hadoop/mapred/TaskLog.java:162:  @SuppressWarnings("unchecked")
./src/mapred/org/apache/hadoop/mapred/TaskLog.java:211:    @Override
./src/mapred/org/apache/hadoop/mapred/TaskLog.java:232:   * @throws IOException
./src/mapred/org/apache/hadoop/mapred/TaskLog.java:257:     * @param taskid the id of the task to read the log file for
./src/mapred/org/apache/hadoop/mapred/TaskLog.java:258:     * @param kind the kind of log to read
./src/mapred/org/apache/hadoop/mapred/TaskLog.java:259:     * @param start the offset to read from (negative is relative to tail)
./src/mapred/org/apache/hadoop/mapred/TaskLog.java:260:     * @param end the offset to read upto (negative is relative to tail)
./src/mapred/org/apache/hadoop/mapred/TaskLog.java:261:     * @throws IOException
./src/mapred/org/apache/hadoop/mapred/TaskLog.java:294:    @Override
./src/mapred/org/apache/hadoop/mapred/TaskLog.java:304:    @Override
./src/mapred/org/apache/hadoop/mapred/TaskLog.java:314:    @Override
./src/mapred/org/apache/hadoop/mapred/TaskLog.java:319:    @Override
./src/mapred/org/apache/hadoop/mapred/TaskLog.java:330:   * @param conf the job to look in
./src/mapred/org/apache/hadoop/mapred/TaskLog.java:331:   * @return the number of bytes to cap the log files at
./src/mapred/org/apache/hadoop/mapred/TaskLog.java:340:   * @param cmd The command and the arguments that should be run
./src/mapred/org/apache/hadoop/mapred/TaskLog.java:341:   * @param stdoutFilename The filename that stdout should be saved to
./src/mapred/org/apache/hadoop/mapred/TaskLog.java:342:   * @param stderrFilename The filename that stderr should be saved to
./src/mapred/org/apache/hadoop/mapred/TaskLog.java:343:   * @param tailLength The length of the tail to be saved.
./src/mapred/org/apache/hadoop/mapred/TaskLog.java:344:   * @return the modified command that should be run
./src/mapred/org/apache/hadoop/mapred/TaskLog.java:360:   * @param setup The setup commands for the execed process.
./src/mapred/org/apache/hadoop/mapred/TaskLog.java:361:   * @param cmd The command and the arguments that should be run
./src/mapred/org/apache/hadoop/mapred/TaskLog.java:362:   * @param stdoutFilename The filename that stdout should be saved to
./src/mapred/org/apache/hadoop/mapred/TaskLog.java:363:   * @param stderrFilename The filename that stderr should be saved to
./src/mapred/org/apache/hadoop/mapred/TaskLog.java:364:   * @param tailLength The length of the tail to be saved.
./src/mapred/org/apache/hadoop/mapred/TaskLog.java:365:   * @return the modified command that should be run
./src/mapred/org/apache/hadoop/mapred/TaskLog.java:382:   * @param setup The setup commands for the execed process.
./src/mapred/org/apache/hadoop/mapred/TaskLog.java:383:   * @param cmd The command and the arguments that should be run
./src/mapred/org/apache/hadoop/mapred/TaskLog.java:384:   * @param stdoutFilename The filename that stdout should be saved to
./src/mapred/org/apache/hadoop/mapred/TaskLog.java:385:   * @param stderrFilename The filename that stderr should be saved to
./src/mapred/org/apache/hadoop/mapred/TaskLog.java:386:   * @param tailLength The length of the tail to be saved.
./src/mapred/org/apache/hadoop/mapred/TaskLog.java:387:   * @param pidFileName The name of the pid-file
./src/mapred/org/apache/hadoop/mapred/TaskLog.java:388:   * @return the modified command that should be run
./src/mapred/org/apache/hadoop/mapred/TaskLog.java:449:   * @param cmd The command to be quoted
./src/mapred/org/apache/hadoop/mapred/TaskLog.java:450:   * @param isExecutable makes shell path if the first 
./src/mapred/org/apache/hadoop/mapred/TaskLog.java:452:   * @return returns The quoted string. 
./src/mapred/org/apache/hadoop/mapred/TaskLog.java:453:   * @throws IOException
./src/mapred/org/apache/hadoop/mapred/TaskLog.java:477:   * @param cmd The command and the arguments that should be run
./src/mapred/org/apache/hadoop/mapred/TaskLog.java:478:   * @param debugoutFilename The filename that stdout and stderr
./src/mapred/org/apache/hadoop/mapred/TaskLog.java:480:   * @return the modified command that should be run
./src/mapred/org/apache/hadoop/mapred/TaskLog.java:481:   * @throws IOException
./src/mapred/org/apache/hadoop/mapred/TaskLogAppender.java:38:  @Override
./src/mapred/org/apache/hadoop/mapred/TaskLogAppender.java:51:  @Override
./src/mapred/org/apache/hadoop/mapred/TaskLogAppender.java:69:  @Override
./src/mapred/org/apache/hadoop/mapred/TaskLogServlet.java:45:   * @param taskTrackerHostName
./src/mapred/org/apache/hadoop/mapred/TaskLogServlet.java:46:   * @param httpPort
./src/mapred/org/apache/hadoop/mapred/TaskLogServlet.java:47:   * @param taskAttemptID
./src/mapred/org/apache/hadoop/mapred/TaskLogServlet.java:48:   * @return the taskLogUrl
./src/mapred/org/apache/hadoop/mapred/TaskLogServlet.java:58:   * @param data the bytes to look in
./src/mapred/org/apache/hadoop/mapred/TaskLogServlet.java:59:   * @param offset the first index to look in
./src/mapred/org/apache/hadoop/mapred/TaskLogServlet.java:60:   * @param end the index after the last one to look in
./src/mapred/org/apache/hadoop/mapred/TaskLogServlet.java:61:   * @return the index of the quotable character or end if none was found
./src/mapred/org/apache/hadoop/mapred/TaskLogServlet.java:154:  @Override
./src/mapred/org/apache/hadoop/mapred/TaskMemoryManagerThread.java:127:  @Override
./src/mapred/org/apache/hadoop/mapred/TaskMemoryManagerThread.java:235:   * @param tipID
./src/mapred/org/apache/hadoop/mapred/TaskMemoryManagerThread.java:236:   * @return the pid of the task process.
./src/mapred/org/apache/hadoop/mapred/TaskMemoryManagerThread.java:251:   * @param tipID
./src/mapred/org/apache/hadoop/mapred/TaskMemoryManagerThread.java:252:   * @return pidFile's Path
./src/mapred/org/apache/hadoop/mapred/TaskReport.java:53:  /** @deprecated use {@link #getTaskID()} instead */
./src/mapred/org/apache/hadoop/mapred/TaskReport.java:54:  @Deprecated
./src/mapred/org/apache/hadoop/mapred/TaskReport.java:60:  /** The most recent state, reported by a {@link Reporter}. */
./src/mapred/org/apache/hadoop/mapred/TaskReport.java:69:   * @return 0, if finish time was not set else returns finish time.
./src/mapred/org/apache/hadoop/mapred/TaskReport.java:77:   * @param finishTime finish time of task. 
./src/mapred/org/apache/hadoop/mapred/TaskReport.java:85:   * @return 0 if start time was not set, else start time. 
./src/mapred/org/apache/hadoop/mapred/TaskReport.java:98:  @Override
./src/mapred/org/apache/hadoop/mapred/TaskReport.java:116:  @Override
./src/mapred/org/apache/hadoop/mapred/TaskRunner.java:102:  @Override
./src/mapred/org/apache/hadoop/mapred/TaskRunner.java:283:      // + @taskid@ is interpolated with value of TaskID.
./src/mapred/org/apache/hadoop/mapred/TaskRunner.java:284:      // Other occurrences of @ will not be altered.
./src/mapred/org/apache/hadoop/mapred/TaskRunner.java:293:      //    <value>-verbose:gc -Xloggc:/tmp/@taskid@.gc \
./src/mapred/org/apache/hadoop/mapred/TaskRunner.java:300:      javaOpts = javaOpts.replace("@taskid@", taskid.toString());
./src/mapred/org/apache/hadoop/mapred/TaskScheduler.java:28: * Used by a {@link JobTracker} to schedule {@link Task}s on
./src/mapred/org/apache/hadoop/mapred/TaskScheduler.java:29: * {@link TaskTracker}s.
./src/mapred/org/apache/hadoop/mapred/TaskScheduler.java:31: * {@link TaskScheduler}s typically use one or more
./src/mapred/org/apache/hadoop/mapred/TaskScheduler.java:32: * {@link JobInProgressListener}s to receive notifications about jobs.
./src/mapred/org/apache/hadoop/mapred/TaskScheduler.java:34: * It is the responsibility of the {@link TaskScheduler}
./src/mapred/org/apache/hadoop/mapred/TaskScheduler.java:35: * to initialize tasks for a job, by calling {@link JobInProgress#initTasks()}
./src/mapred/org/apache/hadoop/mapred/TaskScheduler.java:37: * {@link JobInProgressListener#jobAdded(JobInProgress)} is called)
./src/mapred/org/apache/hadoop/mapred/TaskScheduler.java:39: * {@link #assignTasks(TaskTrackerStatus)}).
./src/mapred/org/apache/hadoop/mapred/TaskScheduler.java:40: * @see EagerTaskInitializationListener
./src/mapred/org/apache/hadoop/mapred/TaskScheduler.java:63:   * @throws IOException
./src/mapred/org/apache/hadoop/mapred/TaskScheduler.java:71:   * @throws IOException
./src/mapred/org/apache/hadoop/mapred/TaskScheduler.java:80:   * @param taskTracker The TaskTracker for which we're looking for tasks.
./src/mapred/org/apache/hadoop/mapred/TaskScheduler.java:81:   * @return A list of tasks to run on that TaskTracker, possibly empty.
./src/mapred/org/apache/hadoop/mapred/TaskScheduler.java:89:   * @param queueName
./src/mapred/org/apache/hadoop/mapred/TaskScheduler.java:90:   * @return
./src/mapred/org/apache/hadoop/mapred/TaskStatus.java:97:   * @return nextRecordRange
./src/mapred/org/apache/hadoop/mapred/TaskStatus.java:105:   * @param nextRecordRange
./src/mapred/org/apache/hadoop/mapred/TaskStatus.java:117:   * @return finish time of the task. 
./src/mapred/org/apache/hadoop/mapred/TaskStatus.java:125:   * @param finishTime finish time of task.
./src/mapred/org/apache/hadoop/mapred/TaskStatus.java:135:   * @return 0 if shuffleFinishTime, sortFinishTime and finish time are not set. else 
./src/mapred/org/apache/hadoop/mapred/TaskStatus.java:144:   * @param shuffleFinishTime 
./src/mapred/org/apache/hadoop/mapred/TaskStatus.java:152:   * @return 0 if sort finish time and finish time are not set, else returns sort
./src/mapred/org/apache/hadoop/mapred/TaskStatus.java:162:   * @param sortFinishTime
./src/mapred/org/apache/hadoop/mapred/TaskStatus.java:168:   * @return 0 is start time is not set, else returns start time. 
./src/mapred/org/apache/hadoop/mapred/TaskStatus.java:176:   * @param startTime start time
./src/mapred/org/apache/hadoop/mapred/TaskStatus.java:184:   * @return . 
./src/mapred/org/apache/hadoop/mapred/TaskStatus.java:191:   * @param phase phase of this task
./src/mapred/org/apache/hadoop/mapred/TaskStatus.java:222:   * @param counters
./src/mapred/org/apache/hadoop/mapred/TaskStatus.java:237:   * @param l the number of map output bytes
./src/mapred/org/apache/hadoop/mapred/TaskStatus.java:246:   * @return the list of maps from which output-fetches failed.
./src/mapred/org/apache/hadoop/mapred/TaskStatus.java:255:   * @param mapTaskId map from which fetch failed
./src/mapred/org/apache/hadoop/mapred/TaskStatus.java:262:   * @param progress
./src/mapred/org/apache/hadoop/mapred/TaskStatus.java:263:   * @param state
./src/mapred/org/apache/hadoop/mapred/TaskStatus.java:264:   * @param phase
./src/mapred/org/apache/hadoop/mapred/TaskStatus.java:265:   * @param counters
./src/mapred/org/apache/hadoop/mapred/TaskStatus.java:280:   * @param status updated status
./src/mapred/org/apache/hadoop/mapred/TaskStatus.java:304:   * from either the {@link Task} to the {@link TaskTracker} or from the
./src/mapred/org/apache/hadoop/mapred/TaskStatus.java:305:   * {@link TaskTracker} to the {@link JobTracker}. 
./src/mapred/org/apache/hadoop/mapred/TaskStatus.java:312:  @Override
./src/mapred/org/apache/hadoop/mapred/TaskTracker.java:545:    @Override
./src/mapred/org/apache/hadoop/mapred/TaskTracker.java:823:      @Override
./src/mapred/org/apache/hadoop/mapred/TaskTracker.java:921:   * @param fromEventId the first event ID we want to start from, this is
./src/mapred/org/apache/hadoop/mapred/TaskTracker.java:923:   * @param jobClient the job tracker
./src/mapred/org/apache/hadoop/mapred/TaskTracker.java:924:   * @return a set of locations to copy outputs from
./src/mapred/org/apache/hadoop/mapred/TaskTracker.java:925:   * @throws IOException
./src/mapred/org/apache/hadoop/mapred/TaskTracker.java:1104:   * @param now current time
./src/mapred/org/apache/hadoop/mapred/TaskTracker.java:1105:   * @return false if the tracker was unknown
./src/mapred/org/apache/hadoop/mapred/TaskTracker.java:1106:   * @throws IOException
./src/mapred/org/apache/hadoop/mapred/TaskTracker.java:1208:   * @return maximum amount of virtual memory
./src/mapred/org/apache/hadoop/mapred/TaskTracker.java:1224:   * @return amount of free virtual memory that can be assured for
./src/mapred/org/apache/hadoop/mapred/TaskTracker.java:1254:   * @param conf
./src/mapred/org/apache/hadoop/mapred/TaskTracker.java:1255:   * @return the memory allocated for the TIP.
./src/mapred/org/apache/hadoop/mapred/TaskTracker.java:1270:   * @param actions the directives of the jobtracker for the tasktracker.
./src/mapred/org/apache/hadoop/mapred/TaskTracker.java:1271:   * @return <code>true</code> if tasktracker is to be reset, 
./src/mapred/org/apache/hadoop/mapred/TaskTracker.java:1321:   * @param action The action with the job
./src/mapred/org/apache/hadoop/mapred/TaskTracker.java:1322:   * @throws IOException
./src/mapred/org/apache/hadoop/mapred/TaskTracker.java:1365:   * @param tip {@link TaskInProgress} to be removed.
./src/mapred/org/apache/hadoop/mapred/TaskTracker.java:1366:   * @param wasFailure did the task fail or was it killed?
./src/mapred/org/apache/hadoop/mapred/TaskTracker.java:1415:   * @return the task to kill or null, if one wasn't found
./src/mapred/org/apache/hadoop/mapred/TaskTracker.java:1454:   * @return
./src/mapred/org/apache/hadoop/mapred/TaskTracker.java:1455:   * @throws IOException 
./src/mapred/org/apache/hadoop/mapred/TaskTracker.java:1490:   * @return
./src/mapred/org/apache/hadoop/mapred/TaskTracker.java:1935:     * @return the task's configured timeout.
./src/mapred/org/apache/hadoop/mapred/TaskTracker.java:2103:     * @param args script name followed by its argumnets
./src/mapred/org/apache/hadoop/mapred/TaskTracker.java:2104:     * @param dir current working directory.
./src/mapred/org/apache/hadoop/mapred/TaskTracker.java:2105:     * @throws IOException
./src/mapred/org/apache/hadoop/mapred/TaskTracker.java:2121:     * @param file The file from which to collect diagnostics.
./src/mapred/org/apache/hadoop/mapred/TaskTracker.java:2122:     * @param num The number of lines to be sent to diagnostics.
./src/mapred/org/apache/hadoop/mapred/TaskTracker.java:2123:     * @param tag The tag is printed before the diagnostics are printed. 
./src/mapred/org/apache/hadoop/mapred/TaskTracker.java:2183:     * @param wasFailure did the task fail, as opposed to was it killed by
./src/mapred/org/apache/hadoop/mapred/TaskTracker.java:2202:     * @param wasFailure was it a failure (versus a kill request)?
./src/mapred/org/apache/hadoop/mapred/TaskTracker.java:2320:    @Override
./src/mapred/org/apache/hadoop/mapred/TaskTracker.java:2327:    @Override
./src/mapred/org/apache/hadoop/mapred/TaskTracker.java:2576:   * @return the string like "tracker_mymachine:50010"
./src/mapred/org/apache/hadoop/mapred/TaskTracker.java:2601:   * @return a copy of the list of TaskStatus objects
./src/mapred/org/apache/hadoop/mapred/TaskTracker.java:2613:   * @return
./src/mapred/org/apache/hadoop/mapred/TaskTracker.java:2628:   * @return a copy of the list of TaskStatus objects
./src/mapred/org/apache/hadoop/mapred/TaskTracker.java:2653:   * @param localDirs where the new TaskTracker should keep its local files.
./src/mapred/org/apache/hadoop/mapred/TaskTracker.java:2654:   * @throws DiskErrorException if all local directories are not writable
./src/mapred/org/apache/hadoop/mapred/TaskTracker.java:2678:   * @return has this task tracker finished and cleaned up all of its tasks?
./src/mapred/org/apache/hadoop/mapred/TaskTracker.java:2712:    @Override
./src/mapred/org/apache/hadoop/mapred/TaskTracker.java:2926:   * @return true if enabled, false otherwise.
./src/mapred/org/apache/hadoop/mapred/TaskTracker.java:2957:   * @param tid
./src/mapred/org/apache/hadoop/mapred/TaskTracker.java:2958:   * @param diagnosticMsg
./src/mapred/org/apache/hadoop/mapred/TaskTrackerAction.java:29: * A generic directive from the {@link org.apache.hadoop.mapred.JobTracker}
./src/mapred/org/apache/hadoop/mapred/TaskTrackerAction.java:30: * to the {@link org.apache.hadoop.mapred.TaskTracker} to take some 'action'. 
./src/mapred/org/apache/hadoop/mapred/TaskTrackerAction.java:36:   * Ennumeration of various 'actions' that the {@link JobTracker}
./src/mapred/org/apache/hadoop/mapred/TaskTrackerAction.java:37:   * directs the {@link TaskTracker} to perform periodically.
./src/mapred/org/apache/hadoop/mapred/TaskTrackerAction.java:58:   * A factory-method to create objects of given {@link ActionType}. 
./src/mapred/org/apache/hadoop/mapred/TaskTrackerAction.java:59:   * @param actionType the {@link ActionType} of object to create.
./src/mapred/org/apache/hadoop/mapred/TaskTrackerAction.java:60:   * @return an object of {@link ActionType}.
./src/mapred/org/apache/hadoop/mapred/TaskTrackerAction.java:103:   * Return the {@link ActionType}.
./src/mapred/org/apache/hadoop/mapred/TaskTrackerAction.java:104:   * @return the {@link ActionType}.
./src/mapred/org/apache/hadoop/mapred/TaskTrackerInstrumentation.java:44:   * @param t
./src/mapred/org/apache/hadoop/mapred/TaskTrackerInstrumentation.java:54:   * @param stdout the file containing standard out of the new task
./src/mapred/org/apache/hadoop/mapred/TaskTrackerInstrumentation.java:55:   * @param stderr the file containing standard error of the new task 
./src/mapred/org/apache/hadoop/mapred/TaskTrackerInstrumentation.java:61:   * @param t
./src/mapred/org/apache/hadoop/mapred/TaskTrackerManager.java:23: * Manages information about the {@link TaskTracker}s running on a cluster.
./src/mapred/org/apache/hadoop/mapred/TaskTrackerManager.java:24: * This interface exits primarily to test the {@link JobTracker}, and is not
./src/mapred/org/apache/hadoop/mapred/TaskTrackerManager.java:30:   * @return A collection of the {@link TaskTrackerStatus} for the tasktrackers
./src/mapred/org/apache/hadoop/mapred/TaskTrackerManager.java:36:   * @return The number of unique hosts running tasktrackers.
./src/mapred/org/apache/hadoop/mapred/TaskTrackerManager.java:41:   * @return a summary of the cluster's status.
./src/mapred/org/apache/hadoop/mapred/TaskTrackerManager.java:46:   * Registers a {@link JobInProgressListener} for updates from this
./src/mapred/org/apache/hadoop/mapred/TaskTrackerManager.java:47:   * {@link TaskTrackerManager}.
./src/mapred/org/apache/hadoop/mapred/TaskTrackerManager.java:48:   * @param jobInProgressListener the {@link JobInProgressListener} to add
./src/mapred/org/apache/hadoop/mapred/TaskTrackerManager.java:53:   * Unregisters a {@link JobInProgressListener} from this
./src/mapred/org/apache/hadoop/mapred/TaskTrackerManager.java:54:   * {@link TaskTrackerManager}.
./src/mapred/org/apache/hadoop/mapred/TaskTrackerManager.java:55:   * @param jobInProgressListener the {@link JobInProgressListener} to remove
./src/mapred/org/apache/hadoop/mapred/TaskTrackerManager.java:60:   * Return the {@link QueueManager} which manages the queues in this
./src/mapred/org/apache/hadoop/mapred/TaskTrackerManager.java:61:   * {@link TaskTrackerManager}.
./src/mapred/org/apache/hadoop/mapred/TaskTrackerManager.java:63:   * @return the {@link QueueManager}
./src/mapred/org/apache/hadoop/mapred/TaskTrackerManager.java:68:   * Return the current heartbeat interval that's used by {@link TaskTracker}s.
./src/mapred/org/apache/hadoop/mapred/TaskTrackerManager.java:70:   * @return the heartbeat interval used by {@link TaskTracker}s
./src/mapred/org/apache/hadoop/mapred/TaskTrackerStatus.java:70:     * @param freeVMem amount of free virtual memory in kilobytes
./src/mapred/org/apache/hadoop/mapred/TaskTrackerStatus.java:80:     * If this is {@link JobConf.DISABLED_VIRTUAL_MEMORY_LIMIT}, it should 
./src/mapred/org/apache/hadoop/mapred/TaskTrackerStatus.java:83:     *@return amount of free virtual memory in kilobytes.
./src/mapred/org/apache/hadoop/mapred/TaskTrackerStatus.java:91:     * @param vmem maximum amount of virtual memory on the tasktracker in kilobytes.
./src/mapred/org/apache/hadoop/mapred/TaskTrackerStatus.java:101:     * {@link JobConf.DISABLED_VIRTUAL_MEMORY_LIMIT}, it should be ignored 
./src/mapred/org/apache/hadoop/mapred/TaskTrackerStatus.java:104:     * @return maximum amount of virtual memory on the tasktracker in kilobytes. 
./src/mapred/org/apache/hadoop/mapred/TaskTrackerStatus.java:116:     * @return bytes of available local disk space on this tasktracker.
./src/mapred/org/apache/hadoop/mapred/TaskTrackerStatus.java:174:   * @return the http port
./src/mapred/org/apache/hadoop/mapred/TaskTrackerStatus.java:182:   * @return The number of failed tasks
./src/mapred/org/apache/hadoop/mapred/TaskTrackerStatus.java:190:   * Tasks are tracked by a {@link TaskStatus} object.
./src/mapred/org/apache/hadoop/mapred/TaskTrackerStatus.java:192:   * @return a list of {@link TaskStatus} representing 
./src/mapred/org/apache/hadoop/mapred/TaskTrackerStatus.java:248:   * @return maximum tasks this node supports
./src/mapred/org/apache/hadoop/mapred/TaskTrackerStatus.java:258:   * Return the {@link ResourceStatus} object configured with this
./src/mapred/org/apache/hadoop/mapred/TaskTrackerStatus.java:261:   * @return the resource status
./src/mapred/org/apache/hadoop/mapred/TaskUmbilicalProtocol.java:61:   * @param jvmId the ID of this JVM w.r.t the tasktracker that launched it
./src/mapred/org/apache/hadoop/mapred/TaskUmbilicalProtocol.java:62:   * @return Task object
./src/mapred/org/apache/hadoop/mapred/TaskUmbilicalProtocol.java:63:   * @throws IOException 
./src/mapred/org/apache/hadoop/mapred/TaskUmbilicalProtocol.java:70:   * @param taskId task-id of the child
./src/mapred/org/apache/hadoop/mapred/TaskUmbilicalProtocol.java:71:   * @param taskStatus status of the child
./src/mapred/org/apache/hadoop/mapred/TaskUmbilicalProtocol.java:72:   * @throws IOException
./src/mapred/org/apache/hadoop/mapred/TaskUmbilicalProtocol.java:73:   * @throws InterruptedException
./src/mapred/org/apache/hadoop/mapred/TaskUmbilicalProtocol.java:74:   * @return True if the task is known
./src/mapred/org/apache/hadoop/mapred/TaskUmbilicalProtocol.java:81:   *  @param taskid the id of the task involved
./src/mapred/org/apache/hadoop/mapred/TaskUmbilicalProtocol.java:82:   *  @param trace the text to report
./src/mapred/org/apache/hadoop/mapred/TaskUmbilicalProtocol.java:88:   * @param taskid the id of the task involved
./src/mapred/org/apache/hadoop/mapred/TaskUmbilicalProtocol.java:89:   * @param range the range of record sequence nos
./src/mapred/org/apache/hadoop/mapred/TaskUmbilicalProtocol.java:90:   * @throws IOException
./src/mapred/org/apache/hadoop/mapred/TaskUmbilicalProtocol.java:96:   * @return True if the task is known
./src/mapred/org/apache/hadoop/mapred/TaskUmbilicalProtocol.java:102:   * @param taskid task's id
./src/mapred/org/apache/hadoop/mapred/TaskUmbilicalProtocol.java:109:   * @param taskId task's id
./src/mapred/org/apache/hadoop/mapred/TaskUmbilicalProtocol.java:110:   * @param taskStatus status of the child
./src/mapred/org/apache/hadoop/mapred/TaskUmbilicalProtocol.java:111:   * @throws IOException
./src/mapred/org/apache/hadoop/mapred/TaskUmbilicalProtocol.java:118:   * @param taskid
./src/mapred/org/apache/hadoop/mapred/TaskUmbilicalProtocol.java:119:   * @return true/false 
./src/mapred/org/apache/hadoop/mapred/TaskUmbilicalProtocol.java:120:   * @throws IOException
./src/mapred/org/apache/hadoop/mapred/TaskUmbilicalProtocol.java:136:   * @param taskId the reduce task id
./src/mapred/org/apache/hadoop/mapred/TaskUmbilicalProtocol.java:137:   * @param fromIndex the index starting from which the locations should be 
./src/mapred/org/apache/hadoop/mapred/TaskUmbilicalProtocol.java:139:   * @param maxLocs the max number of locations to fetch
./src/mapred/org/apache/hadoop/mapred/TaskUmbilicalProtocol.java:140:   * @param id The attempt id of the task that is trying to communicate
./src/mapred/org/apache/hadoop/mapred/TaskUmbilicalProtocol.java:141:   * @return A {@link MapTaskCompletionEventsUpdate} 
./src/mapred/org/apache/hadoop/mapred/TextInputFormat.java:28:/** An {@link InputFormat} for plain text files.  Files are broken into lines.
./src/mapred/org/apache/hadoop/mapred/TextOutputFormat.java:35:/** An {@link OutputFormat} that writes plain text files. */
./src/mapred/org/apache/hadoop/mapred/TextOutputFormat.java:69:     * @param o the object to print
./src/mapred/org/apache/hadoop/mapred/TextOutputFormat.java:70:     * @throws IOException if the write throws, we pass it on
